============================= test session starts ==============================
platform darwin -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0 -- /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
cachedir: .pytest_cache
hypothesis profile 'default'
rootdir: /Users/odosmatthews/Documents/coding/sparkforge
configfile: pytest.ini
plugins: anyio-4.12.0, hypothesis-6.141.1, xdist-3.8.0, cov-7.0.0
created: 10/10 workers
10 workers [1803 items]

scheduling tests via LoadScheduling

tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_write_mode_regression_prevention 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_add_gold_transform 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_success 
scripts/test_python38_environment.py::test_python_version 
[gw0] [  0%] PASSED scripts/test_python38_environment.py::test_python_version 
scripts/test_python38_environment.py::test_java_environment 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_none_steps 
[gw5] [  0%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_none_steps 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_initial 
[gw5] [  0%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_initial 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_incremental 
[gw5] [  0%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_incremental 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_full_refresh 
[gw5] [  0%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_full_refresh 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_validation_only 
[gw5] [  0%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_validation_only 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_unknown 
[gw5] [  0%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_unknown 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_success 
tests/system/test_full_pipeline_with_logging_variations.py::TestDataQuality::test_pipeline_duplicate_data_with_logging 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_validation 
tests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_mixed_rules_conversion 
tests/builder_pyspark_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_validation_failures 
tests/builder_tests/test_iot_pipeline.py::TestIotPipeline::test_anomaly_detection_pipeline 
[gw0] [  0%] PASSED scripts/test_python38_environment.py::test_java_environment 
scripts/test_python38_environment.py::test_spark_session 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_add_gold_transform 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_add_silver_transform 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_add_silver_transform 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_builder_creation 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_builder_creation 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_builder_creation_with_custom_params 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_builder_creation_with_custom_params 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_to_pipeline_success 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_to_pipeline_success 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_to_pipeline_validation_error 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_to_pipeline_validation_error 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_error_concatenation 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_error_concatenation 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_errors 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_errors 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_return_type 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_return_type 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_success 
[gw4] [  0%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_success 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validator_return_types 
[gw4] [  1%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validator_return_types 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_with_bronze_rules 
[gw4] [  1%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_with_bronze_rules 
tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_with_silver_rules 
[gw4] [  1%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_with_silver_rules 
tests/integration/test_pipeline_builder.py::TestPipelineBuilderIntegration::test_complex_pipeline_construction 
[gw4] [  1%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineBuilderIntegration::test_complex_pipeline_construction 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_creation 
[gw4] [  1%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_creation 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_bronze_step_creation 
[gw4] [  1%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_bronze_step_creation 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_silver_step_creation 
[gw4] [  1%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_silver_step_creation 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_gold_step_creation 
[gw4] [  1%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_gold_step_creation 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_validation 
[gw4] [  1%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_validation 
[gw5] [  1%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_success 
[gw3] [  1%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_success 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_to_pipeline 
[gw8] [  1%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_validation 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_unknown_type 
[gw6] [  1%] PASSED tests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_mixed_rules_conversion 
[gw4] [  1%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_to_pipeline 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_with_bronze_sources 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_creation 
tests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_empty_rules 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_execution_with_mock_data 
[gw0] [  1%] PASSED scripts/test_python38_environment.py::test_spark_session 
scripts/test_python38_environment.py::test_sparkforge_imports 
[gw0] [  1%] PASSED scripts/test_python38_environment.py::test_sparkforge_imports 
[gw3] [  1%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_unknown_type 
scripts/test_python38_environment.py::test_type_annotations 
[gw0] [  1%] PASSED scripts/test_python38_environment.py::test_type_annotations 
scripts/test_python38_environment.py::test_sparkforge_functionality 
[gw0] [  2%] PASSED scripts/test_python38_environment.py::test_sparkforge_functionality 
scripts/test_python38_environment.py::test_dict_annotation_checker 
[gw5] [  2%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_with_bronze_sources 
[gw8] [  2%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_creation 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_bronze_without_source_path 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_without_bronze_sources 
[gw6] [  2%] PASSED tests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_empty_rules 
tests/integration/test_validation_integration.py::TestAndAllRules::test_empty_rules 
[gw4] [  2%] FAILED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_execution_with_mock_data 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_dataframe_operations 
[gw3] [  2%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_bronze_without_source_path 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_configuration_creation 
[gw5] [  2%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_without_bronze_sources 
[gw4] [  2%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_configuration_creation 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_bronze_step_not_in_context 
[gw6] [  2%] PASSED tests/integration/test_validation_integration.py::TestAndAllRules::test_empty_rules 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_execution_failure 
[gw2] [  2%] PASSED tests/builder_tests/test_iot_pipeline.py::TestIotPipeline::test_anomaly_detection_pipeline 
tests/builder_tests/test_iot_pipeline.py::TestIotPipeline::test_performance_monitoring 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_execution_engine_with_pipeline_config 
tests/integration/test_validation_integration.py::TestAndAllRules::test_single_column_single_rule 
[gw4] [  2%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_execution_engine_with_pipeline_config 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_step_execution_with_real_data 
[gw3] [  2%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_bronze_step_not_in_context 
[gw0] [  2%] PASSED scripts/test_python38_environment.py::test_dict_annotation_checker 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_full_pipeline_initial_load 
[gw5] [  2%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_execution_failure 
[gw0] [  2%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_full_pipeline_initial_load 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_with_validation_failure 
[gw0] [  2%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_with_validation_failure 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_with_multiple_silver_steps 
[gw0] [  2%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_with_multiple_silver_steps 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_creates_missing_tables 
[gw6] [  2%] PASSED tests/integration/test_validation_integration.py::TestAndAllRules::test_single_column_single_rule 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_initial_load_with_steps 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_creates_missing_tables 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_silver_table_recreated_on_initial 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_silver_table_recreated_on_initial 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_gold_table_recreated_every_run 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_bronze_step_with_data 
[gw4] [  3%] FAILED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_step_execution_with_real_data 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_gold_table_recreated_every_run 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_incremental_mode 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_incremental_mode 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_silver_append_mode_writes_new_rows 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_silver_append_mode_writes_new_rows 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_dependency_order 
tests/integration/test_validation_integration.py::TestAndAllRules::test_single_column_multiple_rules 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_dependency_order 
src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_error_handling 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_full_pipeline.py::test_pipeline_error_handling 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_sql_pipeline_builder_import 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_sql_pipeline_builder_import 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_sql_pipeline_builder_initialization 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_sql_pipeline_builder_initialization 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_bronze_step_creation 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_bronze_step_creation 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_silver_step_creation 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_silver_step_creation 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_gold_step_creation 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_gold_step_creation 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_add_silver_transform_requires_callable 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_add_silver_transform_requires_callable 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_add_silver_transform_duplicate_name 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_add_silver_transform_duplicate_name 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_add_gold_transform_duplicate_name 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_add_gold_transform_duplicate_name 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_async_engine_detection 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_step_validation_with_real_data 
[gw0] [  3%] SKIPPED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_async_engine_detection 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_sql_step_models 
[gw0] [  3%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_sql_step_models 
src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_to_pipeline_requires_primary_keys 
[gw0] [  4%] PASSED src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py::test_to_pipeline_requires_primary_keys 
src/sql_pipeline_builder/tests/test_validation_and_table_ops.py::test_apply_sql_validation_rules_filters_invalid_rows 
[gw0] [  4%] PASSED src/sql_pipeline_builder/tests/test_validation_and_table_ops.py::test_apply_sql_validation_rules_filters_invalid_rows 
src/sql_pipeline_builder/tests/test_validation_and_table_ops.py::test_write_table_handles_overwrite_and_append 
[gw0] [  4%] PASSED src/sql_pipeline_builder/tests/test_validation_and_table_ops.py::test_write_table_handles_overwrite_and_append 
test_environment.py::test_python_version 
[gw0] [  4%] PASSED test_environment.py::test_python_version 
test_environment.py::test_pyspark 
[gw0] [  4%] PASSED test_environment.py::test_pyspark 
test_environment.py::test_spark_session 
[gw4] [  4%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_step_validation_with_real_data 
[gw5] [  4%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_initial_load_with_steps 
[gw3] [  4%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_bronze_step_with_data 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_without_dependencies 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_initial_load_without_steps 
[gw5] [  4%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_initial_load_without_steps 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_incremental 
[gw6] [  4%] PASSED tests/integration/test_validation_integration.py::TestAndAllRules::test_single_column_multiple_rules 
tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_execution_flow_integration 
tests/integration/test_validation_integration.py::TestAndAllRules::test_multiple_columns 
[gw4] [  4%] PASSED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_execution_flow_integration 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_initialization_with_logger 
[gw4] [  4%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_initialization_with_logger 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_initialization_without_logger 
[gw4] [  4%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_initialization_without_logger 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution 
[gw4] [  4%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution_with_empty_steps 
[gw4] [  4%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution_with_empty_steps 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution_with_mocked_time 
[gw4] [  4%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution_with_mocked_time 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_success 
[gw4] [  4%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_success 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_failure 
[gw4] [  4%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_failure 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_without_active_report 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_without_active_report 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_failure_without_error_message 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_failure_without_error_message 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_success 
[gw1] [  5%] PASSED tests/builder_pyspark_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_validation_failures 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_success 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_failure 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_failure 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_without_active_report 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_without_active_report 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_with_zero_steps 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_with_zero_steps 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_with_mocked_time 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_with_mocked_time 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_mixed_results 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_mixed_results 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_pipeline_monitor_alias 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_pipeline_monitor_alias 
tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_logging_calls 
[gw4] [  5%] PASSED tests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_logging_calls 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_all_parameters 
[gw4] [  5%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_all_parameters 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_minimal_parameters 
[gw4] [  5%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_minimal_parameters 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_dataframe_operations 
tests/builder_pyspark_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_logging_and_monitoring 
[gw5] [  5%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_incremental 
[gw3] [  5%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_without_dependencies 
[gw6] [  5%] PASSED tests/integration/test_validation_integration.py::TestAndAllRules::test_multiple_columns 
[gw4] [  5%] FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_dataframe_operations 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_full_refresh 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_missing_dependency 
[gw0] [  5%] PASSED test_environment.py::test_spark_session 
test_environment.py::test_delta_lake 
[gw0] [  6%] PASSED test_environment.py::test_delta_lake 
test_environment.py::test_sparkforge 
[gw0] [  6%] PASSED test_environment.py::test_sparkforge 
test_environment.py::test_testing_tools 
[gw0] [  6%] PASSED test_environment.py::test_testing_tools 
test_environment.py::test_dev_tools 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_transformations 
[gw8] [  6%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_dataframe_operations 
tests/integration/test_validation_integration.py::TestAndAllRules::test_complex_rules 
[gw5] [  6%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_full_refresh 
[gw3] [  6%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_missing_dependency 
[gw4] [  6%] FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_transformations 
[gw2] [  6%] PASSED tests/builder_tests/test_iot_pipeline.py::TestIotPipeline::test_performance_monitoring 
tests/builder_tests/test_marketing_pipeline.py::TestMarketingPipeline::test_complete_marketing_pipeline_execution 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_without_transform 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_validation_only 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_execution_engine_initialization 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_validation_rules 
[gw6] [  6%] PASSED tests/integration/test_validation_integration.py::TestAndAllRules::test_complex_rules 
[gw4] [  6%] PASSED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_validation_rules 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_data_quality 
[gw5] [  6%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_validation_only 
tests/integration/test_validation_integration.py::TestAndAllRules::test_empty_rules_returns_true 
[gw8] [  6%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_execution_engine_initialization 
[gw3] [  6%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_without_transform 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_success 
[gw5] [  6%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_success 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_failure 
[gw5] [  6%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_failure 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_without_end_time 
[gw5] [  6%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_without_end_time 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_error_report 
[gw5] [  6%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_error_report 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_pipeline_runner_alias 
[gw5] [  6%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_pipeline_runner_alias 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_with_empty_steps 
[gw5] [  7%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_with_empty_steps 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_row_counts_accuracy 
[gw5] [  7%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_row_counts_accuracy 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_duration_by_layer_accuracy 
[gw5] [  7%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_duration_by_layer_accuracy 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_with_failed_steps 
[gw5] [  7%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_with_failed_steps 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_with_no_rows_processed 
[gw5] [  7%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_with_no_rows_processed 
tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_mixed_layers_comprehensive 
[gw5] [  7%] PASSED tests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_report_metrics_mixed_layers_comprehensive 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_bronze_step_execution_flow 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_type_detection 
[gw4] [  7%] FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_data_quality 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_without_dependencies 
[gw6] [  7%] PASSED tests/integration/test_validation_integration.py::TestAndAllRules::test_empty_rules_returns_true 
[gw5] [  7%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_bronze_step_execution_flow 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_metadata_operations 
tests/integration/test_validation_integration.py::TestAndAllRules::test_no_valid_expressions_returns_true 
[gw8] [  7%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_type_detection 
[gw4] [  7%] FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_metadata_operations 
[gw3] [  7%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_without_dependencies 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_configuration 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_performance 
[gw1] [  7%] PASSED tests/builder_pyspark_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_logging_and_monitoring 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_silver_step_execution_flow 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_missing_dependency 
tests/builder_pyspark_tests/test_financial_pipeline.py::TestFinancialPipeline::test_complete_financial_transaction_pipeline_execution 
[gw0] [  7%] PASSED test_environment.py::test_dev_tools 
tests/builder_pyspark_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_complete_customer_360_pipeline_execution 
[gw4] [  7%] FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_performance 
[gw8] [  7%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_configuration 
[gw6] [  7%] PASSED tests/integration/test_validation_integration.py::TestAndAllRules::test_no_valid_expressions_returns_true 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_error_handling 
[gw4] [  7%] FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_error_handling 
[gw5] [  8%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_silver_step_execution_flow 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_execution_mode_enum 
[gw8] [  8%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_execution_mode_enum 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_status_enum 
[gw8] [  8%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_status_enum 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_type_enum 
[gw8] [  8%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_type_enum 
tests/system/test_complete_pipeline.py::TestCompletePipeline::test_bronze_to_silver_to_gold_pipeline 
[gw8] [  8%] SKIPPED tests/system/test_complete_pipeline.py::TestCompletePipeline::test_bronze_to_silver_to_gold_pipeline 
tests/system/test_complete_pipeline.py::TestCompletePipeline::test_pipeline_with_data_validation 
[gw8] [  8%] SKIPPED tests/system/test_complete_pipeline.py::TestCompletePipeline::test_pipeline_with_data_validation 
tests/system/test_complete_pipeline.py::TestCompletePipeline::test_pipeline_with_logging_and_monitoring 
[gw8] [  8%] SKIPPED tests/system/test_complete_pipeline.py::TestCompletePipeline::test_pipeline_with_logging_and_monitoring 
tests/system/test_complete_pipeline.py::TestCompletePipeline::test_pipeline_error_recovery 
[gw8] [  8%] SKIPPED tests/system/test_complete_pipeline.py::TestCompletePipeline::test_pipeline_error_recovery 
tests/system/test_complete_pipeline.py::TestCompletePipeline::test_pipeline_with_different_data_sizes 
[gw8] [  8%] SKIPPED tests/system/test_complete_pipeline.py::TestCompletePipeline::test_pipeline_with_different_data_sizes 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_bronze_step_creation 
[gw3] [  8%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_missing_dependency 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_gold_step_execution_flow 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_schema_operations 
tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_valid_schema 
[gw4] [  8%] FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_schema_operations 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_without_transform 
tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_joins 
[gw8] [  8%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_bronze_step_creation 
[gw5] [  8%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_gold_step_execution_flow 
[gw4] [  8%] FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_joins 
[gw3] [  8%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_without_transform 
[gw6] [  8%] PASSED tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_valid_schema 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_silver_step_creation 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_validation_flow 
tests/system/test_system_exceptions.py::TestValidationError::test_validation_error_creation 
[gw4] [  8%] PASSED tests/system/test_system_exceptions.py::TestValidationError::test_validation_error_creation 
tests/system/test_system_exceptions.py::TestValidationError::test_validation_error_inheritance 
[gw4] [  8%] PASSED tests/system/test_system_exceptions.py::TestValidationError::test_validation_error_inheritance 
tests/system/test_system_exceptions.py::TestTableOperationError::test_table_operation_error_creation 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestTableOperationError::test_table_operation_error_creation 
tests/system/test_system_exceptions.py::TestTableOperationError::test_table_operation_error_inheritance 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestTableOperationError::test_table_operation_error_inheritance 
tests/system/test_system_exceptions.py::TestPerformanceError::test_performance_error_creation 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestPerformanceError::test_performance_error_creation 
tests/system/test_system_exceptions.py::TestPerformanceError::test_performance_error_inheritance 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestPerformanceError::test_performance_error_inheritance 
tests/system/test_system_exceptions.py::TestPipelineValidationError::test_pipeline_validation_error_creation 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestPipelineValidationError::test_pipeline_validation_error_creation 
tests/system/test_system_exceptions.py::TestPipelineValidationError::test_pipeline_validation_error_inheritance 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestPipelineValidationError::test_pipeline_validation_error_inheritance 
tests/system/test_system_exceptions.py::TestExecutionError::test_execution_error_creation 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestExecutionError::test_execution_error_creation 
tests/system/test_system_exceptions.py::TestExecutionError::test_execution_error_inheritance 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestExecutionError::test_execution_error_inheritance 
tests/system/test_system_exceptions.py::TestConfigurationError::test_configuration_error_creation 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestConfigurationError::test_configuration_error_creation 
tests/system/test_system_exceptions.py::TestConfigurationError::test_configuration_error_inheritance 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestConfigurationError::test_configuration_error_inheritance 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_validation_only_mode 
tests/system/test_system_exceptions.py::TestExceptionChaining::test_exception_with_cause 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestExceptionChaining::test_exception_with_cause 
tests/system/test_system_exceptions.py::TestExceptionChaining::test_exception_context 
[gw4] [  9%] PASSED tests/system/test_system_exceptions.py::TestExceptionChaining::test_exception_context 
tests/system/test_utils.py::TestDataValidation::test_and_all_rules 
tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_missing_columns 
[gw4] [  9%] PASSED tests/system/test_utils.py::TestDataValidation::test_and_all_rules 
tests/system/test_utils.py::TestDataValidation::test_and_all_rules_empty 
[gw8] [  9%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_silver_step_creation 
[gw4] [  9%] PASSED tests/system/test_utils.py::TestDataValidation::test_and_all_rules_empty 
[gw5] [  9%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_validation_flow 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_gold_step_creation 
[gw3] [  9%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_validation_only_mode 
tests/system/test_utils.py::TestDataValidation::test_apply_column_rules 
[gw6] [  9%] PASSED tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_missing_columns 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_type_detection_flow 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_exception_handling 
[gw4] [ 10%] FAILED tests/system/test_utils.py::TestDataValidation::test_apply_column_rules 
tests/system/test_utils.py::TestDataValidation::test_apply_column_rules_none_rules 
tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_extra_columns 
[gw8] [ 10%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_gold_step_creation 
[gw4] [ 10%] PASSED tests/system/test_utils.py::TestDataValidation::test_apply_column_rules_none_rules 
[gw5] [ 10%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_type_detection_flow 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_context_flow 
[gw3] [ 10%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_exception_handling 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_builder_validation 
[gw1] [ 10%] PASSED tests/builder_pyspark_tests/test_financial_pipeline.py::TestFinancialPipeline::test_complete_financial_transaction_pipeline_execution 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_success 
tests/system/test_utils.py::TestDataValidation::test_assess_data_quality 
tests/builder_pyspark_tests/test_financial_pipeline.py::TestFinancialPipeline::test_fraud_detection_scenarios 
[gw0] [ 10%] PASSED tests/builder_pyspark_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_complete_customer_360_pipeline_execution 
tests/builder_pyspark_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_churn_prediction 
[gw6] [ 10%] PASSED tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_extra_columns 
[gw4] [ 10%] FAILED tests/system/test_utils.py::TestDataValidation::test_assess_data_quality 
tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_empty_expected_columns 
[gw8] [ 10%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_builder_validation 
tests/system/test_utils.py::TestDataValidation::test_get_dataframe_info 
[gw3] [ 10%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_success 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_failure 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_creation 
[gw4] [ 10%] FAILED tests/system/test_utils.py::TestDataValidation::test_get_dataframe_info 
tests/system/test_utils.py::TestDataValidation::test_validate_dataframe_schema 
[gw6] [ 10%] PASSED tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_empty_expected_columns 
[gw4] [ 10%] PASSED tests/system/test_utils.py::TestDataValidation::test_validate_dataframe_schema 
[gw8] [ 10%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_creation 
tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_empty_dataframe 
tests/system/test_utils.py::TestDataTransformationUtilities::test_basic_dataframe_operations 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_dataframe_operations 
[gw4] [ 10%] FAILED tests/system/test_utils.py::TestDataTransformationUtilities::test_basic_dataframe_operations 
[gw7] [ 10%] PASSED tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_write_mode_regression_prevention 
tests/system/test_utils.py::TestDataTransformationUtilities::test_dataframe_filtering 
[gw5] [ 10%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_context_flow 
[gw0] [ 11%] PASSED tests/builder_pyspark_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_churn_prediction 
[gw6] [ 11%] PASSED tests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_empty_dataframe 
[gw4] [ 11%] FAILED tests/system/test_utils.py::TestDataTransformationUtilities::test_dataframe_filtering 
tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_log_writer_receives_correct_write_mode 
tests/builder_pyspark_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_lifetime_value_analysis 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_result_flow 
tests/system/test_utils.py::TestFactoryFunctions::test_create_validation_dict 
tests/integration/test_validation_integration.py::TestSafeDivide::test_normal_division 
[gw4] [ 11%] PASSED tests/system/test_utils.py::TestFactoryFunctions::test_create_validation_dict 
tests/system/test_utils.py::TestFactoryFunctions::test_create_write_dict 
[gw4] [ 11%] PASSED tests/system/test_utils.py::TestFactoryFunctions::test_create_write_dict 
[gw1] [ 11%] PASSED tests/builder_pyspark_tests/test_financial_pipeline.py::TestFinancialPipeline::test_fraud_detection_scenarios 
[gw5] [ 11%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_result_flow 
tests/builder_pyspark_tests/test_financial_pipeline.py::TestFinancialPipeline::test_compliance_monitoring 
[gw6] [ 11%] PASSED tests/integration/test_validation_integration.py::TestSafeDivide::test_normal_division 
tests/system/test_utils.py::TestPerformanceWithRealData::test_large_dataset_validation 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_mode_flow 
tests/integration/test_validation_integration.py::TestSafeDivide::test_division_by_zero 
[gw4] [ 11%] FAILED tests/system/test_utils.py::TestPerformanceWithRealData::test_large_dataset_validation 
[gw8] [ 11%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_dataframe_operations 
tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_inheritance 
[gw4] [ 11%] PASSED tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_inheritance 
tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_list_immutability 
[gw4] [ 11%] PASSED tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_list_immutability 
tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_basic 
[gw4] [ 11%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_basic 
tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_with_step_name 
[gw4] [ 11%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_with_step_name 
tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_empty_list 
[gw4] [ 11%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_empty_list 
tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_single_step 
[gw4] [ 11%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_single_step 
tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_inheritance 
[gw4] [ 11%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_inheritance 
tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_list_immutability 
[gw4] [ 11%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_list_immutability 
tests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_dependency_error_chaining 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_dependency_error_chaining 
tests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_circular_dependency_error_chaining 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_circular_dependency_error_chaining 
tests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_exception_attributes_preserved 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_exception_attributes_preserved 
tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_error_str 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_error_str 
tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_analysis_error_str 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_analysis_error_str 
tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_circular_dependency_error_str 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_circular_dependency_error_str 
tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_invalid_dependency_error_str 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_invalid_dependency_error_str 
tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_conflict_error_str 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_conflict_error_str 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_add_dependency_missing_nodes 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_add_dependency_missing_nodes 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_dependencies_missing_node 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_dependencies_missing_node 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_dependents_missing_node 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_dependents_missing_node 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_detect_cycles 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_detect_cycles 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_execution_groups_missing_dependency 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_execution_groups_missing_dependency 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_validate_cycles 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_validate_cycles 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_validate_missing_dependencies 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_validate_missing_dependencies 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_execution_groups 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_execution_groups 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_stats 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_stats 
tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_parallel_candidates 
[gw4] [ 12%] PASSED tests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_parallel_candidates 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_base_builder_initialization 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_base_builder_initialization 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_base_builder_default_logger 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_base_builder_default_logger 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_check_duplicate_step_name_bronze 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_check_duplicate_step_name_bronze 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_check_duplicate_step_name_silver 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_check_duplicate_step_name_silver 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_check_duplicate_step_name_gold 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_check_duplicate_step_name_gold 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_check_duplicate_step_name_no_duplicate 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_check_duplicate_step_name_no_duplicate 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_silver_valid 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_silver_valid 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_silver_missing 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_silver_missing 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_gold_valid 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_gold_valid 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_gold_missing 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_gold_missing 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_gold_invalid_list 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_gold_invalid_list 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_auto_inference 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_step_dependencies_auto_inference 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_schema_valid 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_schema_valid 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_schema_invalid 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_schema_invalid 
tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_pipeline 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestBasePipelineBuilder::test_validate_pipeline 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_bronze 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_bronze 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_silver 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_silver 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_gold 
[gw4] [ 13%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_gold 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_from_attribute 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_from_attribute 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_unknown 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_classify_step_type_unknown 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_extract_step_dependencies_silver 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_extract_step_dependencies_silver 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_extract_step_dependencies_gold 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_extract_step_dependencies_gold 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_extract_step_dependencies_none 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_extract_step_dependencies_none 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_group_steps_by_type 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_group_steps_by_type 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_get_all_step_names 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_get_all_step_names 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_build_dependency_graph 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_build_dependency_graph 
tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_get_execution_order 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestStepClassifier::test_get_execution_order 
tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_bronze_step_dict 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_bronze_step_dict 
tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_bronze_step_dict_with_metadata 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_bronze_step_dict_with_metadata 
tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_silver_step_dict 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_silver_step_dict 
tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_silver_step_dict_with_watermark 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_silver_step_dict_with_watermark 
tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_gold_step_dict 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_gold_step_dict 
tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_gold_step_dict_no_sources 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_builder.py::TestHelperFunctions::test_create_gold_step_dict_no_sources 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_development_config 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_development_config 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_development_config_overrides 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_development_config_overrides 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_production_config 
[gw4] [ 14%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_production_config 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_production_config_overrides 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_production_config_overrides 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_test_config 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_test_config 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_test_config_overrides 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationFactories::test_create_test_config_overrides 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_pipeline_config_valid 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_pipeline_config_valid 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_pipeline_config_invalid 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_pipeline_config_invalid 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_parallel_config_valid 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_parallel_config_valid 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_parallel_config_invalid 
[gw5] [ 15%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_mode_flow 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_parallel_config_invalid 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_parallel_config_too_large 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_parallel_config_too_large 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_thresholds_valid 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_thresholds_valid 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_thresholds_invalid 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_thresholds_invalid 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_thresholds_range 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_thresholds_range 
tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_thresholds_order 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_config.py::TestConfigurationValidators::test_validate_thresholds_order 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_dependency_graph_initialization 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_dependency_graph_initialization 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_add_node 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_add_node 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_add_dependency 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_add_dependency 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_add_dependency_missing_node 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_add_dependency_missing_node 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_node 
[gw4] [ 15%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_node 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_node_missing 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_node_missing 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_dependencies 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_dependencies 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_dependents 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_dependents 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_has_cycle 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_has_cycle 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_has_cycle_no_cycle 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_has_cycle_no_cycle 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_topological_sort 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_topological_sort 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_topological_sort_cycle 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_topological_sort_cycle 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_execution_groups 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_execution_groups 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_execution_groups_parallel 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyGraph::test_get_execution_groups_parallel 
tests/unit/pipeline_builder_base/test_dependencies.py::TestStepNode::test_step_node_creation 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestStepNode::test_step_node_creation 
tests/unit/pipeline_builder_base/test_dependencies.py::TestStepNode::test_step_node_with_dependencies 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestStepNode::test_step_node_with_dependencies 
tests/unit/pipeline_builder_base/test_dependencies.py::TestStepNode::test_step_node_with_metadata 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestStepNode::test_step_node_with_metadata 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_dependency_error_creation 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_dependency_error_creation 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_cycle_error_creation 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_cycle_error_creation 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_missing_dependency_error 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_missing_dependency_error 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_dependency_analysis_error 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_dependency_analysis_error 
tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_dependency_conflict_error 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_dependencies.py::TestDependencyExceptions::test_dependency_conflict_error 
tests/unit/pipeline_builder_base/test_errors.py::TestErrorContext::test_error_context_creation 
[gw4] [ 16%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestErrorContext::test_error_context_creation 
tests/unit/pipeline_builder_base/test_errors.py::TestErrorContext::test_error_context_to_dict 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestErrorContext::test_error_context_to_dict 
tests/unit/pipeline_builder_base/test_errors.py::TestErrorContext::test_error_context_add 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestErrorContext::test_error_context_add 
tests/unit/pipeline_builder_base/test_errors.py::TestErrorContext::test_error_context_merge 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestErrorContext::test_error_context_merge 
tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_suggestion_generator_initialization 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_suggestion_generator_initialization 
tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_generate_suggestions_validation_error 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_generate_suggestions_validation_error 
tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_generate_suggestions_dependency_error 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_generate_suggestions_dependency_error 
tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_generate_suggestions_config_error 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_generate_suggestions_config_error 
tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_generate_suggestions_empty 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestSuggestionGenerator::test_generate_suggestions_empty 
tests/unit/pipeline_builder_base/test_errors.py::TestErrorContextBuilders::test_build_execution_context 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestErrorContextBuilders::test_build_execution_context 
tests/unit/pipeline_builder_base/test_errors.py::TestErrorContextBuilders::test_build_execution_context_with_metadata 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestErrorContextBuilders::test_build_execution_context_with_metadata 
tests/unit/pipeline_builder_base/test_errors.py::TestErrorContextBuilders::test_build_validation_context 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestErrorContextBuilders::test_build_validation_context 
tests/unit/pipeline_builder_base/test_errors.py::TestErrorContextBuilders::test_build_validation_context_gold 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_errors.py::TestErrorContextBuilders::test_build_validation_context_gold 
tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_builder_runner_integration 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_builder_runner_integration 
tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_validator_builder_integration 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_validator_builder_integration 
tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_config_factory_validation 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_config_factory_validation 
tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_dependency_graph_builder 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_dependency_graph_builder 
tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_error_context_full_flow 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_integration.py::TestBuilderRunnerIntegration::test_error_context_full_flow 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_validation_report_creation 
[gw4] [ 17%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_validation_report_creation 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_validation_report_to_dict 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_validation_report_to_dict 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_transform_report_creation 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_transform_report_creation 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_write_report_creation 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_write_report_creation 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_execution_summary_creation 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_execution_summary_creation 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_performance_metrics_creation 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_performance_metrics_creation 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_data_metrics_creation 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_data_metrics_creation 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_summary_report_creation 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportClasses::test_summary_report_creation 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportUtilities::test_create_validation_dict 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportUtilities::test_create_validation_dict 
tests/unit/pipeline_builder_base/test_reporting.py::TestReportUtilities::test_create_validation_dict_none_stats 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_reporting.py::TestReportUtilities::test_create_validation_dict_none_stats 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_base_runner_initialization 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_base_runner_initialization 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_base_runner_default_logger 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_base_runner_default_logger 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_handle_step_error 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_handle_step_error 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_handle_step_error_with_context 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_handle_step_error_with_context 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_collect_step_results 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_collect_step_results 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_collect_step_results_empty 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_collect_step_results_empty 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_collect_step_results_with_errors 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_collect_step_results_with_errors 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_create_pipeline_report 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_create_pipeline_report 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_aggregate_step_reports 
[gw4] [ 18%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_aggregate_step_reports 
tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_aggregate_step_reports_empty 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestBaseRunner::test_aggregate_step_reports_empty 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_determine_execution_mode_initial 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_determine_execution_mode_initial 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_determine_execution_mode_initial_empty_sources 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_determine_execution_mode_initial_empty_sources 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_determine_execution_mode_incremental 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_determine_execution_mode_incremental 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_should_run_incremental_with_last_run 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_should_run_incremental_with_last_run 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_should_run_incremental_no_last_run 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_should_run_incremental_no_last_run 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_prepare_sources_for_execution_bronze 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_prepare_sources_for_execution_bronze 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_prepare_sources_for_execution_silver 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_prepare_sources_for_execution_silver 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_prepare_sources_for_execution_gold 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_prepare_sources_for_execution_gold 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_valid 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_valid 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_missing 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_missing 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_unexpected 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_unexpected 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_with_validator 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_with_validator 
tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_with_invalid_validator 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_runner.py::TestExecutionHelpers::test_validate_bronze_sources_with_invalid_validator 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_step_manager_initialization 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_step_manager_initialization 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_add_step 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_add_step 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_add_step_duplicate 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_add_step_duplicate 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_step 
[gw4] [ 19%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_step 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_step_missing 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_step_missing 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_step_all_types 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_step_all_types 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_steps_by_type 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_steps_by_type 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_remove_step 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_remove_step 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_clear_steps 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_clear_steps 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_all_steps 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_get_all_steps 
tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_validate_all_steps 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepManager::test_validate_all_steps 
tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_classify_step_type 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_classify_step_type 
tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_classify_step_type_from_attribute 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_classify_step_type_from_attribute 
tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_extract_step_dependencies 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_extract_step_dependencies 
tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_extract_step_dependencies_gold 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_extract_step_dependencies_gold 
tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_get_step_target 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_get_step_target 
tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_get_step_target_missing 
[gw6] [ 20%] PASSED tests/integration/test_validation_integration.py::TestSafeDivide::test_division_by_zero 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_get_step_target_missing 
tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_normalize_step_name 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_normalize_step_name 
tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_normalize_step_name_edge_cases 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_steps.py::TestStepUtils::test_normalize_step_name_edge_cases 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_pipeline_validator_initialization 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_pipeline_validator_initialization 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_pipeline_validator_default_logger 
[gw4] [ 20%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_pipeline_validator_default_logger 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_step_names_unique 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_step_names_unique 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_step_names_duplicate 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_step_names_duplicate 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_step_names_invalid 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_step_names_invalid 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_dependency_chain_valid 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_dependency_chain_valid 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_dependency_chain_cycle 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_dependency_chain_cycle 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_schema_valid 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_schema_valid 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_schema_invalid 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_schema_invalid 
tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_pipeline_structure 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestPipelineValidator::test_validate_pipeline_structure 
tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_step_validator_initialization 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_step_validator_initialization 
tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_step_name_valid 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_step_name_valid 
tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_step_name_invalid 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_step_name_invalid 
tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_step_name_reserved 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_step_name_reserved 
tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_schema_name_valid 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_schema_name_valid 
tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_schema_name_invalid 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestStepValidator::test_validate_schema_name_invalid 
tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_check_duplicate_names 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_check_duplicate_names 
tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_check_duplicate_names_no_duplicates 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_check_duplicate_names_no_duplicates 
tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_validate_dependency_chain 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_validate_dependency_chain 
tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_validate_schema_name 
[gw4] [ 21%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_validate_schema_name 
tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_validate_step_name 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_validate_step_name 
tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_safe_divide 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_safe_divide 
tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_safe_divide_by_zero 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_validation.py::TestValidationUtils::test_safe_divide_by_zero 
tests/unit/pipeline_builder_base/test_writer.py::TestBaseWriter::test_base_writer_initialization 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestBaseWriter::test_base_writer_initialization 
tests/unit/pipeline_builder_base/test_writer.py::TestBaseWriter::test_base_writer_abstract_methods 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestBaseWriter::test_base_writer_abstract_methods 
tests/unit/pipeline_builder_base/test_writer.py::TestBaseWriter::test_base_writer_table_fqn 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestBaseWriter::test_base_writer_table_fqn 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterModels::test_write_result_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterModels::test_write_result_creation 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterModels::test_writer_metrics_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterModels::test_writer_metrics_creation 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterModels::test_log_row_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterModels::test_log_row_creation 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_write_error_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_write_error_creation 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_table_error_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_table_error_creation 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_validation_error_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_validation_error_creation 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_configuration_error_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_configuration_error_creation 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_data_quality_error_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_data_quality_error_creation 
tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_performance_error_creation 
[gw4] [ 22%] PASSED tests/unit/pipeline_builder_base/test_writer.py::TestWriterExceptions::test_performance_error_creation 
tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_missing_columns_validation_error 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_status_flow 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_execution_engine_initialization 
tests/integration/test_validation_integration.py::TestSafeDivide::test_division_by_zero_custom_default 
[gw4] [ 22%] PASSED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_missing_columns_validation_error 
tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_existing_columns_validation_success 
[gw8] [ 22%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_execution_engine_initialization 
[gw5] [ 22%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_status_flow 
[gw6] [ 23%] PASSED tests/integration/test_validation_integration.py::TestSafeDivide::test_division_by_zero_custom_default 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_pipeline_configuration_flow 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_type_detection 
tests/integration/test_validation_integration.py::TestSafeDivide::test_float_division 
[gw4] [ 23%] FAILED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_existing_columns_validation_success 
[gw3] [ 23%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_failure 
[gw0] [ 23%] PASSED tests/builder_pyspark_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_lifetime_value_analysis 
tests/builder_pyspark_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_analytics_logging 
tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_empty_rules_validation_success 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_with_different_step_types 
[gw6] [ 23%] PASSED tests/integration/test_validation_integration.py::TestSafeDivide::test_float_division 
[gw5] [ 23%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_pipeline_configuration_flow 
[gw8] [ 23%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_type_detection 
[gw4] [ 23%] FAILED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_empty_rules_validation_success 
tests/integration/test_validation_integration.py::TestSafeDivide::test_negative_numbers 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_engine_initialization_flow 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_configuration 
[gw1] [ 23%] PASSED tests/builder_pyspark_tests/test_financial_pipeline.py::TestFinancialPipeline::test_compliance_monitoring 
tests/builder_pyspark_tests/test_financial_pipeline.py::TestFinancialPipeline::test_financial_audit_logging 
tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_bronze_step_with_provided_data 
[gw3] [ 23%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_with_different_step_types 
[gw8] [ 23%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_configuration 
[gw6] [ 23%] PASSED tests/integration/test_validation_integration.py::TestSafeDivide::test_negative_numbers 
[gw4] [ 23%] FAILED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_bronze_step_with_provided_data 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_with_max_workers 
[gw5] [ 23%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_engine_initialization_flow 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_execution_mode_enum 
[gw8] [ 23%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_execution_mode_enum 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_status_enum 
[gw8] [ 23%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_status_enum 
tests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_type_enum 
[gw8] [ 23%] PASSED tests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_type_enum 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_acid_transactions 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_error_handling_flow 
tests/integration/test_validation_integration.py::TestSafeDivide::test_zero_numerator 
tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_pipeline_builder_with_missing_columns 
[gw3] [ 23%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_with_max_workers 
[gw4] [ 24%] PASSED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_pipeline_builder_with_missing_columns 
[gw5] [ 24%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_error_handling_flow 
[gw6] [ 24%] PASSED tests/integration/test_validation_integration.py::TestSafeDivide::test_zero_numerator 
tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_bronze_step_missing_data_error 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_empty_steps 
tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_with_mock_data 
[gw4] [ 24%] PASSED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_bronze_step_missing_data_error 
tests/integration/test_validation_integration.py::TestGetDataframeInfo::test_basic_info 
tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_dict_data 
[gw4] [ 24%] FAILED tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_dict_data 
[gw3] [ 24%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_empty_steps 
tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_tuple_data 
[gw1] [ 24%] PASSED tests/builder_pyspark_tests/test_financial_pipeline.py::TestFinancialPipeline::test_financial_audit_logging 
tests/builder_pyspark_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_complete_healthcare_pipeline_execution 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_step_failure_continues 
[gw4] [ 24%] FAILED tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_tuple_data 
tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_structtype_schema 
[gw0] [ 24%] PASSED tests/builder_pyspark_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_analytics_logging 
tests/builder_pyspark_tests/test_data_quality_pipeline.py::TestDataQualityPipeline::test_complete_data_quality_pipeline_execution 
[gw4] [ 24%] FAILED tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_structtype_schema 
[gw3] [ 24%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_step_failure_continues 
tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_tuple_and_structtype 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_backward_compatibility_aliases 
[gw4] [ 24%] FAILED tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_tuple_and_structtype 
[gw5] [ 24%] PASSED tests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_with_mock_data 
tests/unit/test_compat_helpers.py::TestIsDataframeLike::test_pyspark_dataframe 
[gw4] [ 24%] PASSED tests/unit/test_compat_helpers.py::TestIsDataframeLike::test_pyspark_dataframe 
tests/unit/test_compat_helpers.py::TestIsDataframeLike::test_mock_spark_dataframe 
tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_not_null_rule 
[gw3] [ 24%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_backward_compatibility_aliases 
[gw4] [ 24%] PASSED tests/unit/test_compat_helpers.py::TestIsDataframeLike::test_mock_spark_dataframe 
tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_engine_initialization 
tests/unit/test_compat_helpers.py::TestIsDataframeLike::test_non_dataframe_object 
[gw4] [ 24%] PASSED tests/unit/test_compat_helpers.py::TestIsDataframeLike::test_non_dataframe_object 
tests/unit/test_compat_helpers.py::TestIsDataframeLike::test_object_with_some_methods 
[gw4] [ 24%] PASSED tests/unit/test_compat_helpers.py::TestIsDataframeLike::test_object_with_some_methods 
tests/unit/test_compat_helpers.py::TestDetectSparkType::test_detect_mock_spark 
[gw4] [ 25%] PASSED tests/unit/test_compat_helpers.py::TestDetectSparkType::test_detect_mock_spark 
tests/unit/test_compat_helpers.py::TestDetectSparkType::test_detect_pyspark 
[gw5] [ 25%] PASSED tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_not_null_rule 
[gw6] [ 25%] PASSED tests/integration/test_validation_integration.py::TestGetDataframeInfo::test_basic_info 
[gw4] [ 25%] PASSED tests/unit/test_compat_helpers.py::TestDetectSparkType::test_detect_pyspark 
[gw3] [ 25%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_engine_initialization 
tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_dict 
tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_engine_without_logger 
tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_positive_rule 
[gw4] [ 25%] FAILED tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_dict 
tests/integration/test_validation_integration.py::TestGetDataframeInfo::test_empty_dataframe 
tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_schema 
[gw5] [ 25%] PASSED tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_positive_rule 
[gw3] [ 25%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_engine_without_logger 
tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_non_negative_rule 
[gw4] [ 25%] FAILED tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_schema 
tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_type_detection 
tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_tuples 
[gw2] [ 25%] PASSED tests/builder_tests/test_marketing_pipeline.py::TestMarketingPipeline::test_complete_marketing_pipeline_execution 
tests/builder_tests/test_marketing_pipeline.py::TestMarketingPipeline::test_incremental_marketing_processing 
[gw4] [ 25%] FAILED tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_tuples 
[gw5] [ 25%] PASSED tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_non_negative_rule 
tests/unit/test_constants.py::TestMemoryConstants::test_bytes_per_kb 
[gw4] [ 25%] PASSED tests/unit/test_constants.py::TestMemoryConstants::test_bytes_per_kb 
tests/unit/test_constants.py::TestMemoryConstants::test_bytes_per_mb 
[gw4] [ 25%] PASSED tests/unit/test_constants.py::TestMemoryConstants::test_bytes_per_mb 
tests/unit/test_constants.py::TestMemoryConstants::test_bytes_per_gb 
[gw4] [ 25%] PASSED tests/unit/test_constants.py::TestMemoryConstants::test_bytes_per_gb 
tests/unit/test_constants.py::TestDefaultMemoryLimits::test_default_max_memory_mb 
[gw4] [ 25%] PASSED tests/unit/test_constants.py::TestDefaultMemoryLimits::test_default_max_memory_mb 
tests/unit/test_constants.py::TestDefaultMemoryLimits::test_default_cache_memory_mb 
[gw4] [ 25%] PASSED tests/unit/test_constants.py::TestDefaultMemoryLimits::test_default_cache_memory_mb 
tests/unit/test_constants.py::TestFileSizeConstants::test_default_max_file_size_mb 
[gw4] [ 25%] PASSED tests/unit/test_constants.py::TestFileSizeConstants::test_default_max_file_size_mb 
tests/unit/test_constants.py::TestFileSizeConstants::test_default_backup_count 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestFileSizeConstants::test_default_backup_count 
tests/unit/test_constants.py::TestPerformanceConstants::test_default_cache_partitions 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestPerformanceConstants::test_default_cache_partitions 
tests/unit/test_constants.py::TestPerformanceConstants::test_default_shuffle_partitions 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestPerformanceConstants::test_default_shuffle_partitions 
tests/unit/test_constants.py::TestValidationConstants::test_default_bronze_threshold 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestValidationConstants::test_default_bronze_threshold 
tests/unit/test_constants.py::TestValidationConstants::test_default_silver_threshold 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestValidationConstants::test_default_silver_threshold 
tests/unit/test_constants.py::TestValidationConstants::test_default_gold_threshold 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestValidationConstants::test_default_gold_threshold 
tests/unit/test_constants.py::TestValidationConstants::test_threshold_ordering 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestValidationConstants::test_threshold_ordering 
tests/unit/test_constants.py::TestTimeoutConstants::test_default_timeout_seconds 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestTimeoutConstants::test_default_timeout_seconds 
tests/unit/test_constants.py::TestTimeoutConstants::test_default_retry_timeout_seconds 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestTimeoutConstants::test_default_retry_timeout_seconds 
tests/unit/test_constants.py::TestTimeoutConstants::test_timeout_ordering 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestTimeoutConstants::test_timeout_ordering 
tests/unit/test_constants.py::TestLoggingConstants::test_default_log_level 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestLoggingConstants::test_default_log_level 
tests/unit/test_constants.py::TestLoggingConstants::test_default_verbose 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestLoggingConstants::test_default_verbose 
tests/unit/test_constants.py::TestSchemaConstants::test_default_schema 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestSchemaConstants::test_default_schema 
tests/unit/test_constants.py::TestSchemaConstants::test_test_schema 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestSchemaConstants::test_test_schema 
tests/unit/test_constants.py::TestErrorConstants::test_max_error_message_length 
[gw4] [ 26%] PASSED tests/unit/test_constants.py::TestErrorConstants::test_max_error_message_length 
tests/unit/test_dict_annotations.py::test_no_dict_annotations 
[gw3] [ 26%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_type_detection 
[gw4] [ 26%] PASSED tests/unit/test_dict_annotations.py::test_no_dict_annotations 
tests/unit/test_dict_annotations.py::test_python_version 
[gw4] [ 26%] PASSED tests/unit/test_dict_annotations.py::test_python_version 
tests/unit/test_dict_annotations.py::test_dict_vs_Dict_equivalence 
[gw4] [ 27%] PASSED tests/unit/test_dict_annotations.py::test_dict_vs_Dict_equivalence 
tests/unit/test_dict_annotations.py::test_typeddict_compatibility 
[gw4] [ 27%] PASSED tests/unit/test_dict_annotations.py::test_typeddict_compatibility 
tests/unit/test_dict_annotations.py::test_import_compatibility 
[gw4] [ 27%] PASSED tests/unit/test_dict_annotations.py::test_import_compatibility 
tests/unit/test_dict_annotations.py::test_writer_imports 
[gw4] [ 27%] PASSED tests/unit/test_dict_annotations.py::test_writer_imports 
tests/unit/test_dict_annotations.py::test_models_imports 
[gw4] [ 27%] PASSED tests/unit/test_dict_annotations.py::test_models_imports 
tests/unit/test_edge_cases.py::TestEdgeCases::test_empty_dataframe_operations 
tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_non_zero_rule 
tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_context_creation 
[gw4] [ 27%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_empty_dataframe_operations 
tests/unit/test_edge_cases.py::TestEdgeCases::test_null_value_handling 
[gw6] [ 27%] PASSED tests/integration/test_validation_integration.py::TestGetDataframeInfo::test_empty_dataframe 
[gw4] [ 27%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_null_value_handling 
[gw5] [ 27%] PASSED tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_non_zero_rule 
tests/unit/test_edge_cases.py::TestEdgeCases::test_large_dataset_operations 
tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_unknown_rule 
tests/integration/test_validation_integration.py::TestGetDataframeInfo::test_error_handling 
[gw4] [ 27%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_large_dataset_operations 
tests/unit/test_edge_cases.py::TestEdgeCases::test_complex_schema_operations 
[gw3] [ 27%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_context_creation 
[gw5] [ 27%] PASSED tests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_unknown_rule 
[gw6] [ 27%] PASSED tests/integration/test_validation_integration.py::TestGetDataframeInfo::test_error_handling 
[gw4] [ 27%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_complex_schema_operations 
tests/unit/test_edge_cases.py::TestEdgeCases::test_error_conditions 
tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_execution_result_creation 
tests/integration/test_validation_integration.py::TestAssessDataQuality::test_empty_dataframe 
tests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_string_rules_conversion 
[gw4] [ 27%] PASSED tests/unit/test_edge_cases.py::TestEdgeCases::test_error_conditions 
[gw3] [ 27%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_execution_result_creation 
[gw5] [ 27%] PASSED tests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_string_rules_conversion 
tests/unit/test_edge_cases.py::TestEdgeCases::test_boundary_values 
tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_mode_enum 
[gw3] [ 27%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_mode_enum 
tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_status_enum 
[gw3] [ 28%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_status_enum 
tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_type_enum 
[gw3] [ 28%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_type_enum 
tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_bronze_step_validation 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestExecutionMode::test_execution_mode_values 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestExecutionMode::test_execution_mode_values 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_analysis_result_creation 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_analysis_result_creation 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_get_parallelization_ratio 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_get_parallelization_ratio 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_get_total_execution_time 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_get_total_execution_time 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_basic 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_basic 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_caching 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_caching 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_force_refresh 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_force_refresh 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_with_bronze_steps 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_with_bronze_steps 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_with_gold_steps 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_with_gold_steps 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyzer_creation 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyzer_creation 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyzer_creation_with_custom_params 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyzer_creation_with_custom_params 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_complex_pipeline_analysis 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_complex_pipeline_analysis 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_different_strategies_comparison 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_different_strategies_comparison 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_error_handling 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_error_handling 
tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_basic 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_basic 
tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_step_name 
[gw5] [ 28%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_step_name 
tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_inheritance 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_inheritance 
tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_empty_message 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_empty_message 
tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_none_step_name 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_none_step_name 
tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_basic 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_basic 
tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_step_name 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_step_name 
tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_analysis_step 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_analysis_step 
tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_both_steps 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_both_steps 
tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_inheritance 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_inheritance 
tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_constructor_parameters 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_constructor_parameters 
tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_basic 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_basic 
tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_with_step_name 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_with_step_name 
tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_empty_cycle 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_empty_cycle 
tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_single_step_cycle 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_single_step_cycle 
tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_inheritance 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_inheritance 
tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_cycle_immutability 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_cycle_immutability 
tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_basic 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_basic 
tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_with_step_name 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_with_step_name 
tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_empty_list 
[gw5] [ 29%] PASSED tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_empty_list 
tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_single_dependency 
[gw5] [ 30%] PASSED tests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_single_dependency 
tests/unit/test_execution_comprehensive.py::TestExecuteStep::test_execute_bronze_step_success 
[gw4] [ 30%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_boundary_values 
[gw3] [ 30%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_bronze_step_validation 
tests/unit/test_edge_cases.py::TestEdgeCases::test_concurrent_operations 
tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_silver_step_validation 
[gw6] [ 30%] PASSED tests/integration/test_validation_integration.py::TestAssessDataQuality::test_empty_dataframe 
[gw1] [ 30%] PASSED tests/builder_pyspark_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_complete_healthcare_pipeline_execution 
tests/builder_pyspark_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_incremental_healthcare_processing 
tests/integration/test_validation_integration.py::TestAssessDataQuality::test_dataframe_without_rules 
[gw4] [ 30%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_concurrent_operations 
tests/unit/test_edge_cases.py::TestEdgeCases::test_memory_management 
[gw3] [ 30%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_silver_step_validation 
tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_gold_step_validation 
[gw4] [ 30%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_memory_management 
tests/unit/test_edge_cases.py::TestEdgeCases::test_schema_evolution 
[gw3] [ 30%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_gold_step_validation 
tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_pipeline_config_validation 
[gw4] [ 30%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_schema_evolution 
[gw6] [ 30%] PASSED tests/integration/test_validation_integration.py::TestAssessDataQuality::test_dataframe_without_rules 
tests/unit/test_edge_cases.py::TestEdgeCases::test_pipeline_builder_edge_cases 
tests/integration/test_validation_integration.py::TestAssessDataQuality::test_dataframe_with_rules 
[gw4] [ 30%] PASSED tests/unit/test_edge_cases.py::TestEdgeCases::test_pipeline_builder_edge_cases 
[gw3] [ 30%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_pipeline_config_validation 
[gw5] [ 30%] PASSED tests/unit/test_execution_comprehensive.py::TestExecuteStep::test_execute_bronze_step_success 
tests/unit/test_edge_cases.py::TestEdgeCases::test_execution_engine_edge_cases 
tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_execution_engine_with_mock_steps 
[gw4] [ 30%] PASSED tests/unit/test_edge_cases.py::TestEdgeCases::test_execution_engine_edge_cases 
tests/unit/test_execution_comprehensive.py::TestExecuteStep::test_execute_step_unknown_step_type 
tests/unit/test_edge_cases.py::TestEdgeCases::test_validation_edge_cases 
[gw4] [ 30%] PASSED tests/unit/test_edge_cases.py::TestEdgeCases::test_validation_edge_cases 
[gw3] [ 30%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_execution_engine_with_mock_steps 
tests/unit/test_edge_cases.py::TestEdgeCases::test_writer_edge_cases 
[gw4] [ 30%] PASSED tests/unit/test_edge_cases.py::TestEdgeCases::test_writer_edge_cases 
tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_execution_engine_error_handling 
[gw5] [ 31%] PASSED tests/unit/test_execution_comprehensive.py::TestExecuteStep::test_execute_step_unknown_step_type 
tests/unit/test_edge_cases.py::TestEdgeCases::test_storage_edge_cases 
[gw2] [ 31%] PASSED tests/builder_tests/test_marketing_pipeline.py::TestMarketingPipeline::test_incremental_marketing_processing 
tests/builder_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_complete_multi_source_integration_pipeline_execution 
[gw4] [ 31%] SKIPPED tests/unit/test_edge_cases.py::TestEdgeCases::test_storage_edge_cases 
tests/unit/test_execution_comprehensive.py::TestExecuteStep::test_execute_bronze_step_missing_context 
tests/unit/test_edge_cases.py::TestEdgeCases::test_function_edge_cases 
[gw4] [ 31%] PASSED tests/unit/test_edge_cases.py::TestEdgeCases::test_function_edge_cases 
[gw3] [ 31%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_execution_engine_error_handling 
[gw6] [ 31%] PASSED tests/integration/test_validation_integration.py::TestAssessDataQuality::test_dataframe_with_rules 
[gw2] [ 31%] PASSED tests/builder_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_complete_multi_source_integration_pipeline_execution 
tests/builder_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_schema_evolution_handling 
[gw5] [ 31%] PASSED tests/unit/test_execution_comprehensive.py::TestExecuteStep::test_execute_bronze_step_missing_context 
tests/unit/test_edge_cases.py::TestEdgeCases::test_dataframe_edge_cases 
tests/integration/test_execution_engine_new.py::TestExecutionEnginePerformance::test_execution_engine_memory_usage 
tests/integration/test_validation_integration.py::TestAssessDataQuality::test_error_handling 
[gw8] [ 31%] PASSED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_acid_transactions 
[gw4] [ 31%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_dataframe_edge_cases 
tests/unit/test_execution_comprehensive.py::TestExecutePipeline::test_execute_pipeline_with_none_context 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_schema_evolution 
[gw8] [ 31%] SKIPPED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_schema_evolution 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_time_travel 
tests/unit/test_edge_cases.py::TestEdgeCases::test_session_edge_cases 
[gw2] [ 31%] PASSED tests/builder_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_schema_evolution_handling 
tests/builder_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_complex_dependency_handling 
[gw1] [ 31%] PASSED tests/builder_pyspark_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_incremental_healthcare_processing 
tests/builder_pyspark_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_healthcare_logging 
[gw4] [ 31%] FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_session_edge_cases 
[gw3] [ 31%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEnginePerformance::test_execution_engine_memory_usage 
tests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_value_type_validation 
[gw4] [ 31%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_value_type_validation 
tests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_type_validation 
[gw4] [ 31%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_type_validation 
tests/unit/test_errors.py::TestErrorTypeSafety::test_error_suggestions_type_validation 
[gw4] [ 31%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_error_suggestions_type_validation 
tests/unit/test_errors.py::TestErrorTypeSafety::test_base_error_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_base_error_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_configuration_error_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_configuration_error_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_data_error_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_data_error_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_pipeline_error_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_pipeline_error_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_step_error_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_step_error_explicit_types 
[gw5] [ 32%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutePipeline::test_execute_pipeline_with_none_context 
tests/unit/test_errors.py::TestErrorTypeSafety::test_execution_error_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_execution_error_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_system_error_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_system_error_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_performance_error_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_performance_error_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_error_serialization_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_error_serialization_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_manipulation_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_manipulation_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_error_suggestion_manipulation_explicit_types 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_error_suggestion_manipulation_explicit_types 
tests/unit/test_errors.py::TestErrorTypeSafety::test_no_any_types_in_error_classes 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_no_any_types_in_error_classes 
tests/unit/test_errors.py::TestErrorTypeSafety::test_no_args_kwargs_in_error_constructors 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorTypeSafety::test_no_args_kwargs_in_error_constructors 
tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_existing_error_usage_still_works 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_existing_error_usage_still_works 
tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_inheritance_still_works 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_inheritance_still_works 
tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_context_manipulation_still_works 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_context_manipulation_still_works 
tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_serialization_still_works 
[gw4] [ 32%] PASSED tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_serialization_still_works 
tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_resource_error_creation 
[gw4] [ 33%] PASSED tests/unit/test_errors.py::TestErrorBackwardCompatibility::test_resource_error_creation 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_silver_step_success 
[gw6] [ 33%] PASSED tests/integration/test_validation_integration.py::TestAssessDataQuality::test_error_handling 
tests/integration/test_execution_engine_new.py::TestExecutionEnginePerformance::test_execution_engine_concurrent_creation 
tests/unit/test_execution_comprehensive.py::TestExecutePipeline::test_execute_pipeline_invalid_context_type 
tests/integration/test_validation_integration.py::TestValidationResult::test_validation_result_creation 
[gw4] [ 33%] FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_silver_step_success 
[gw3] [ 33%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEnginePerformance::test_execution_engine_concurrent_creation 
[gw5] [ 33%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutePipeline::test_execute_pipeline_invalid_context_type 
[gw6] [ 33%] PASSED tests/integration/test_validation_integration.py::TestValidationResult::test_validation_result_creation 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_gold_step_success 
tests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_logging_initialization 
tests/unit/test_execution_comprehensive.py::TestExecutePipeline::test_execute_pipeline_empty_steps 
tests/integration/test_validation_integration.py::TestValidationResult::test_validation_result_defaults 
[gw2] [ 33%] PASSED tests/builder_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_complex_dependency_handling 
tests/builder_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_multi_source_logging 
[gw4] [ 33%] FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_gold_step_success 
[gw7] [ 33%] PASSED tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_log_writer_receives_correct_write_mode 
[gw5] [ 33%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutePipeline::test_execute_pipeline_empty_steps 
[gw3] [ 33%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_logging_initialization 
[gw6] [ 33%] PASSED tests/integration/test_validation_integration.py::TestValidationResult::test_validation_result_defaults 
tests/unit/test_execution_comprehensive.py::TestPrivateMethods::test_execute_bronze_step_empty_dataframe 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_with_rules_validation 
tests/performance/test_performance.py::TestValidationPerformance::test_safe_divide_performance 
[gw7] [ 33%] PASSED tests/performance/test_performance.py::TestValidationPerformance::test_safe_divide_performance 
tests/performance/test_performance.py::TestValidationPerformance::test_safe_divide_zero_denominator_performance 
[gw0] [ 33%] PASSED tests/builder_pyspark_tests/test_data_quality_pipeline.py::TestDataQualityPipeline::test_complete_data_quality_pipeline_execution 
tests/builder_pyspark_tests/test_data_quality_pipeline.py::TestDataQualityPipeline::test_incremental_data_quality_processing 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_unified_validator_initialization 
[gw7] [ 33%] PASSED tests/performance/test_performance.py::TestValidationPerformance::test_safe_divide_zero_denominator_performance 
tests/performance/test_performance.py::TestValidationPerformance::test_validate_dataframe_schema_performance 
tests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_default_logging 
[gw7] [ 33%] PASSED tests/performance/test_performance.py::TestValidationPerformance::test_validate_dataframe_schema_performance 
tests/performance/test_performance.py::TestValidationPerformance::test_assess_data_quality_performance 
[gw7] [ 33%] SKIPPED tests/performance/test_performance.py::TestValidationPerformance::test_assess_data_quality_performance 
tests/performance/test_performance.py::TestValidationPerformance::test_get_dataframe_info_performance 
[gw7] [ 33%] PASSED tests/performance/test_performance.py::TestValidationPerformance::test_get_dataframe_info_performance 
tests/performance/test_performance.py::TestModelCreationPerformance::test_validation_thresholds_creation_performance 
[gw7] [ 33%] PASSED tests/performance/test_performance.py::TestModelCreationPerformance::test_validation_thresholds_creation_performance 
tests/performance/test_performance.py::TestModelCreationPerformance::test_parallel_config_creation_performance 
[gw4] [ 34%] FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_with_rules_validation 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestModelCreationPerformance::test_parallel_config_creation_performance 
tests/performance/test_performance.py::TestModelCreationPerformance::test_pipeline_config_creation_performance 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestModelCreationPerformance::test_pipeline_config_creation_performance 
tests/performance/test_performance.py::TestModelCreationPerformance::test_bronze_step_creation_performance 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestModelCreationPerformance::test_bronze_step_creation_performance 
tests/performance/test_performance.py::TestModelCreationPerformance::test_silver_step_creation_performance 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestModelCreationPerformance::test_silver_step_creation_performance 
tests/performance/test_performance.py::TestModelCreationPerformance::test_gold_step_creation_performance 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestModelCreationPerformance::test_gold_step_creation_performance 
tests/performance/test_performance.py::TestSerializationPerformance::test_model_to_dict_performance 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_validation_only_mode 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestSerializationPerformance::test_model_to_dict_performance 
tests/performance/test_performance.py::TestSerializationPerformance::test_model_to_json_performance 
[gw3] [ 34%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_default_logging 
[gw6] [ 34%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_unified_validator_initialization 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestSerializationPerformance::test_model_to_json_performance 
tests/performance/test_performance.py::TestSerializationPerformance::test_model_validation_performance 
tests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_logging_methods 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestSerializationPerformance::test_model_validation_performance 
tests/performance/test_performance.py::TestMemoryUsagePerformance::test_model_creation_memory_usage 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestMemoryUsagePerformance::test_model_creation_memory_usage 
tests/performance/test_performance.py::TestMemoryUsagePerformance::test_serialization_memory_usage 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_unified_validator_with_custom_logger 
[gw2] [ 34%] PASSED tests/builder_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_multi_source_logging 
tests/builder_tests/test_simple.py::test_simple_pipeline_creation 
[gw2] [ 34%] PASSED tests/builder_tests/test_simple.py::test_simple_pipeline_creation 
tests/builder_tests/test_simple_pipeline.py::TestSimplePipeline::test_simple_pipeline_execution 
[gw4] [ 34%] FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_validation_only_mode 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::TestMemoryUsagePerformance::test_serialization_memory_usage 
tests/performance/test_performance.py::test_performance_summary 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::test_performance_summary 
tests/performance/test_performance.py::test_update_baselines 
[gw7] [ 34%] PASSED tests/performance/test_performance.py::test_update_baselines 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_test_suite_integration 
[gw5] [ 35%] PASSED tests/unit/test_execution_comprehensive.py::TestPrivateMethods::test_execute_bronze_step_empty_dataframe 
[gw3] [ 35%] PASSED tests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_logging_methods 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_silver_missing_schema 
tests/integration/test_parallel_execution.py::test_parallel_execution 
[gw9] [ 35%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestDataQuality::test_pipeline_duplicate_data_with_logging 
tests/unit/test_execution_comprehensive.py::TestBackwardCompatibility::test_unified_execution_engine_alias 
[gw5] [ 35%] PASSED tests/unit/test_execution_comprehensive.py::TestBackwardCompatibility::test_unified_execution_engine_alias 
tests/unit/test_execution_comprehensive.py::TestBackwardCompatibility::test_unified_step_execution_result_alias 
[gw5] [ 35%] PASSED tests/unit/test_execution_comprehensive.py::TestBackwardCompatibility::test_unified_step_execution_result_alias 
tests/unit/test_execution_comprehensive.py::TestExecutionIntegration::test_different_execution_modes 
[gw1] [ 35%] PASSED tests/builder_pyspark_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_healthcare_logging 
[gw2] [ 35%] PASSED tests/builder_tests/test_simple_pipeline.py::TestSimplePipeline::test_simple_pipeline_execution 
tests/builder_tests/test_streaming_hybrid_pipeline.py::TestStreamingHybridPipeline::test_complete_streaming_hybrid_pipeline_execution 
tests/builder_pyspark_tests/test_iot_pipeline.py::TestIotPipeline::test_complete_iot_sensor_pipeline_execution 
[gw6] [ 35%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_unified_validator_with_custom_logger 
[gw4] [ 35%] FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_silver_missing_schema 
tests/system/test_full_pipeline_with_logging_variations.py::TestHighVolume::test_pipeline_high_volume_with_logging 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_add_validator 
[gw3] [ 35%] SKIPPED tests/integration/test_parallel_execution.py::test_parallel_execution 
tests/integration/test_pipeline_builder.py::TestPipelineMode::test_pipeline_mode_values 
[gw3] [ 35%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineMode::test_pipeline_mode_values 
tests/integration/test_pipeline_builder.py::TestPipelineStatus::test_pipeline_status_values 
[gw3] [ 35%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineStatus::test_pipeline_status_values 
tests/integration/test_pipeline_builder.py::TestPipelineMetrics::test_pipeline_metrics_creation 
[gw3] [ 35%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineMetrics::test_pipeline_metrics_creation 
tests/integration/test_pipeline_builder.py::TestPipelineReport::test_pipeline_report_creation 
[gw3] [ 35%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineReport::test_pipeline_report_creation 
tests/integration/test_pipeline_builder.py::TestPipelineReport::test_pipeline_report_to_dict 
[gw3] [ 35%] PASSED tests/integration/test_pipeline_builder.py::TestPipelineReport::test_pipeline_report_to_dict 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_transform_error 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_gold_missing_schema 
[gw3] [ 35%] PASSED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_transform_error 
tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_success_with_silver_steps 
[gw4] [ 35%] FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_gold_missing_schema 
[gw6] [ 35%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_add_validator 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_step_with_custom_validators 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_silver_missing_source 
[gw4] [ 36%] PASSED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_silver_missing_source 
[gw6] [ 36%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_step_with_custom_validators 
tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_gold_missing_source 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_step_validator_exception 
[gw4] [ 36%] PASSED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_gold_missing_source 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_with_sample_data 
[gw0] [ 36%] PASSED tests/builder_pyspark_tests/test_data_quality_pipeline.py::TestDataQualityPipeline::test_incremental_data_quality_processing 
tests/builder_pyspark_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_complete_ecommerce_pipeline_execution 
[gw6] [ 36%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_step_validator_exception 
[gw4] [ 36%] FAILED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_with_sample_data 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_pipeline_config_validation 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_error_handling 
[gw4] [ 36%] FAILED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_error_handling 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_metrics_collection 
[gw8] [ 36%] PASSED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_time_travel 
[gw6] [ 36%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_pipeline_config_validation 
[gw4] [ 36%] FAILED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_metrics_collection 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_merge_operations 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_pipeline_success 
tests/unit/test_execution_final_coverage.py::TestExecutionFinalCoverage::test_execute_pipeline_with_none_steps_result 
[gw4] [ 36%] PASSED tests/unit/test_execution_final_coverage.py::TestExecutionFinalCoverage::test_execute_pipeline_with_none_steps_result 
tests/unit/test_execution_final_coverage.py::TestExecutionFinalCoverage::test_execute_pipeline_silver_step_no_schema_logging 
[gw5] [ 36%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionIntegration::test_different_execution_modes 
[gw6] [ 36%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_pipeline_success 
[gw4] [ 36%] PASSED tests/unit/test_execution_final_coverage.py::TestExecutionFinalCoverage::test_execute_pipeline_silver_step_no_schema_logging 
[gw0] [ 36%] PASSED tests/builder_pyspark_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_complete_ecommerce_pipeline_execution 
tests/unit/test_execution_comprehensive.py::TestExecutionIntegration::test_logging_integration 
tests/builder_pyspark_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_incremental_order_processing 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_bronze_steps 
tests/unit/test_execution_final_coverage.py::TestExecutionFinalCoverage::test_execute_pipeline_gold_step_no_schema_logging 
[gw4] [ 36%] PASSED tests/unit/test_execution_final_coverage.py::TestExecutionFinalCoverage::test_execute_pipeline_gold_step_no_schema_logging 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_mode_uses_append_for_silver_step 
[gw6] [ 36%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_bronze_steps 
[gw4] [ 36%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_mode_uses_append_for_silver_step 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_silver_steps 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_mode_uses_overwrite_for_gold_step 
[gw6] [ 37%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_silver_steps 
[gw4] [ 37%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_mode_uses_overwrite_for_gold_step 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_gold_steps 
[gw5] [ 37%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionIntegration::test_logging_integration 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_initial_mode_uses_overwrite_for_silver_step 
[gw1] [ 37%] PASSED tests/builder_pyspark_tests/test_iot_pipeline.py::TestIotPipeline::test_complete_iot_sensor_pipeline_execution 
tests/builder_pyspark_tests/test_iot_pipeline.py::TestIotPipeline::test_incremental_sensor_processing 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_initialization 
[gw6] [ 37%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_gold_steps 
[gw4] [ 37%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_initial_mode_uses_overwrite_for_silver_step 
[gw7] [ 37%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_test_suite_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_vulnerability_scanner_integration 
tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_dependencies 
[gw5] [ 37%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_initialization 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_initial_mode_uses_overwrite_for_gold_step 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_initialization_with_mode 
[gw6] [ 37%] PASSED tests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_dependencies 
[gw4] [ 37%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_initial_mode_uses_overwrite_for_gold_step 
tests/integration/test_validation_integration.py::TestApplyValidationRules::test_apply_column_rules_deprecated 
[gw5] [ 37%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_initialization_with_mode 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_full_refresh_mode_uses_overwrite_for_silver_step 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_invalid_spark_session 
[gw5] [ 37%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_invalid_spark_session 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_get_spark 
[gw3] [ 37%] PASSED tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_success_with_silver_steps 
[gw6] [ 37%] PASSED tests/integration/test_validation_integration.py::TestApplyValidationRules::test_apply_column_rules_deprecated 
[gw7] [ 37%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_vulnerability_scanner_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_compliance_checker_integration 
[gw4] [ 37%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_full_refresh_mode_uses_overwrite_for_silver_step 
[gw5] [ 37%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_get_spark 
tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_incremental_pipeline_preserves_data 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_full_refresh_mode_uses_overwrite_for_gold_step 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_get_mode 
tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_success_with_gold_steps 
[gw4] [ 37%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_full_refresh_mode_uses_overwrite_for_gold_step 
[gw5] [ 38%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_get_mode 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_validation_only_mode_has_no_write_mode_for_silver_step 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_default_mode 
[gw4] [ 38%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_validation_only_mode_has_no_write_mode_for_silver_step 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_filter_excludes_existing_rows 
[gw4] [ 38%] PASSED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_filter_excludes_existing_rows 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_filter_uses_mock_fallback_when_needed 
[gw4] [ 38%] SKIPPED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_filter_uses_mock_fallback_when_needed 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_validation_only_mode_has_no_write_mode_for_gold_step 
[gw5] [ 38%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_default_mode 
[gw8] [ 38%] PASSED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_merge_operations 
[gw4] [ 38%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_validation_only_mode_has_no_write_mode_for_gold_step 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_step_status_enum 
[gw5] [ 38%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_step_status_enum 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_step_type_enum 
[gw5] [ 38%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_step_type_enum 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_execution_mode_enum 
[gw5] [ 38%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_execution_mode_enum 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_step_execution_result_creation 
[gw0] [ 38%] PASSED tests/builder_pyspark_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_incremental_order_processing 
tests/system/test_utils.py::TestPerformanceWithRealData::test_complex_transformations 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_spark_write_mode_matches_result_write_mode 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_optimization 
[gw4] [ 38%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_spark_write_mode_matches_result_write_mode 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_all_execution_modes_covered 
[gw4] [ 38%] PASSED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_all_execution_modes_covered 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_write_mode_consistency_across_step_types 
[gw5] [ 38%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_step_execution_result_creation 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_result_creation 
[gw4] [ 38%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_write_mode_consistency_across_step_types 
[gw0] [ 38%] FAILED tests/system/test_utils.py::TestPerformanceWithRealData::test_complex_transformations 
tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_bronze_step_has_no_write_mode 
tests/unit/dependencies/test_analyzer.py::TestAnalysisStrategy::test_analysis_strategy_values 
[gw0] [ 38%] PASSED tests/unit/dependencies/test_analyzer.py::TestAnalysisStrategy::test_analysis_strategy_values 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalysisResult::test_dependency_analysis_result_creation 
[gw0] [ 38%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalysisResult::test_dependency_analysis_result_creation 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_dependency_analyzer_creation_default 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_dependency_analyzer_creation_default 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_dependency_analyzer_creation_custom 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_dependency_analyzer_creation_custom 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_empty 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_empty 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_bronze_only 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_bronze_only 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_with_bronze 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_with_bronze 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_gold_with_silver 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_gold_with_silver 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_missing_bronze_dependency 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_missing_bronze_dependency 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_warning_scenarios 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_warning_scenarios 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_depends_on_warning 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_depends_on_warning 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_missing_silver_dependency 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_missing_silver_dependency 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_force_refresh 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_force_refresh 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_cached 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_cached 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_exception 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_exception 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_empty 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_empty 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_bronze_steps 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_bronze_steps 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_silver_steps 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_silver_steps 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_gold_steps 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_gold_steps 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_resolve_cycles 
[gw0] [ 39%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_resolve_cycles 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_detect_conflicts 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_detect_conflicts 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_cycle_warning 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_cycle_warning 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_conflict_warning 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_conflict_warning 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_valid_dependency 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_valid_dependency 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_detect_conflicts_duplicate_names 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_detect_conflicts_duplicate_names 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_no_issues 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_no_issues 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_with_cycles 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_with_cycles 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_with_conflicts 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_with_conflicts 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_high_dependencies 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_high_dependencies 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_large_pipeline 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_large_pipeline 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_create_cache_key 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_create_cache_key 
tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_clear_cache 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_clear_cache 
tests/unit/dependencies/test_analyzer_comprehensive.py::TestAnalysisStrategy::test_strategy_values 
[gw0] [ 40%] PASSED tests/unit/dependencies/test_analyzer_comprehensive.py::TestAnalysisStrategy::test_strategy_values 
tests/unit/test_logwriter_new_api.py::TestCreateTableMethod::test_create_table_returns_success_result 
[gw0] [ 40%] PASSED tests/unit/test_logwriter_new_api.py::TestCreateTableMethod::test_create_table_returns_success_result 
tests/unit/test_logwriter_new_api.py::TestCreateTableMethod::test_create_table_generates_run_id_if_not_provided 
[gw0] [ 40%] PASSED tests/unit/test_logwriter_new_api.py::TestCreateTableMethod::test_create_table_generates_run_id_if_not_provided 
tests/unit/test_logwriter_new_api.py::TestAppendMethod::test_append_calls_storage_with_append_mode 
[gw5] [ 40%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_result_creation 
[gw0] [ 40%] PASSED tests/unit/test_logwriter_new_api.py::TestAppendMethod::test_append_calls_storage_with_append_mode 
tests/unit/test_logwriter_new_api.py::TestAppendMethod::test_append_returns_success_result 
[gw0] [ 40%] PASSED tests/unit/test_logwriter_new_api.py::TestAppendMethod::test_append_returns_success_result 
tests/unit/test_logwriter_new_api.py::TestAppendMethod::test_append_multiple_times 
[gw0] [ 41%] PASSED tests/unit/test_logwriter_new_api.py::TestAppendMethod::test_append_multiple_times 
tests/unit/test_logwriter_new_api.py::TestAppendMethod::test_append_refreshes_table_totals_between_runs 
[gw0] [ 41%] PASSED tests/unit/test_logwriter_new_api.py::TestAppendMethod::test_append_refreshes_table_totals_between_runs 
tests/unit/test_logwriter_new_api.py::TestLogWriterNewAPIIntegration::test_complete_workflow_create_and_append 
[gw0] [ 41%] PASSED tests/unit/test_logwriter_new_api.py::TestLogWriterNewAPIIntegration::test_complete_workflow_create_and_append 
tests/unit/test_models.py::TestExceptions::test_pipeline_configuration_error 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestExceptions::test_pipeline_configuration_error 
tests/unit/test_models.py::TestExceptions::test_pipeline_execution_error 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestExceptions::test_pipeline_execution_error 
tests/unit/test_models.py::TestEnums::test_pipeline_phase_enum 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestEnums::test_pipeline_phase_enum 
tests/unit/test_models.py::TestEnums::test_execution_mode_enum 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestEnums::test_execution_mode_enum 
tests/unit/test_models.py::TestEnums::test_write_mode_enum 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestEnums::test_write_mode_enum 
tests/unit/test_models.py::TestEnums::test_validation_result_enum 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestEnums::test_validation_result_enum 
tests/unit/test_models.py::TestTypeDefinitions::test_model_value_types 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestTypeDefinitions::test_model_value_types 
tests/unit/test_models.py::TestTypeDefinitions::test_column_rule_types 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestTypeDefinitions::test_column_rule_types 
tests/unit/test_models.py::TestTypeDefinitions::test_resource_value_types 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestTypeDefinitions::test_resource_value_types 
tests/unit/test_models.py::TestBaseModel::test_base_model_abstract 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestBaseModel::test_base_model_abstract 
tests/unit/test_models.py::TestBaseModel::test_base_model_validation_abstract 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestBaseModel::test_base_model_validation_abstract 
tests/unit/test_models.py::TestBronzeStep::test_bronze_step_creation 
[gw4] [ 41%] FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_bronze_step_has_no_write_mode 
[gw1] [ 41%] PASSED tests/builder_pyspark_tests/test_iot_pipeline.py::TestIotPipeline::test_incremental_sensor_processing 
tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_with_empty_data 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_creation 
tests/builder_pyspark_tests/test_iot_pipeline.py::TestIotPipeline::test_anomaly_detection_pipeline 
tests/unit/test_execution_write_mode.py::TestWriteModeRegression::test_incremental_mode_uses_append_for_silver_steps 
tests/unit/test_models.py::TestBronzeStep::test_bronze_step_creation_minimal 
[gw0] [ 41%] PASSED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_creation_minimal 
tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_success 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_success 
tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_empty_name 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_empty_name 
tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_none_name 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_none_name 
tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_invalid_rules_type 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_invalid_rules_type 
tests/unit/test_models.py::TestBronzeStep::test_bronze_step_has_incremental_capability 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_has_incremental_capability 
tests/unit/test_models.py::TestSilverStep::test_silver_step_creation 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestSilverStep::test_silver_step_creation 
tests/unit/test_models.py::TestSilverStep::test_silver_step_creation_minimal 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestSilverStep::test_silver_step_creation_minimal 
tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_success 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_success 
tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_name 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_name 
tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_source_bronze 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_source_bronze 
tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_none_transform 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_none_transform 
tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_invalid_rules_type 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_invalid_rules_type 
tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_table_name 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_table_name 
tests/unit/test_models.py::TestGoldStep::test_gold_step_creation 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestGoldStep::test_gold_step_creation 
tests/unit/test_models.py::TestGoldStep::test_gold_step_creation_minimal 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestGoldStep::test_gold_step_creation_minimal 
tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_success 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_success 
tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_empty_name 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_empty_name 
tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_none_transform 
[gw0] [ 42%] PASSED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_none_transform 
tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_invalid_rules_type 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_invalid_rules_type 
tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_empty_table_name 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_empty_table_name 
tests/unit/test_models.py::TestStepResult::test_step_result_creation 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestStepResult::test_step_result_creation 
tests/unit/test_models.py::TestStepResult::test_step_result_is_high_quality 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestStepResult::test_step_result_is_high_quality 
tests/unit/test_models.py::TestStepResult::test_step_result_create_success 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestStepResult::test_step_result_create_success 
tests/unit/test_models.py::TestStepResult::test_step_result_create_failure 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestStepResult::test_step_result_create_failure 
tests/unit/test_models.py::TestPipelineMetrics::test_pipeline_metrics_creation_default 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestPipelineMetrics::test_pipeline_metrics_creation_default 
tests/unit/test_models.py::TestPipelineMetrics::test_pipeline_metrics_creation_custom 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestPipelineMetrics::test_pipeline_metrics_creation_custom 
tests/unit/test_models.py::TestPipelineConfig::test_pipeline_config_creation_default 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestPipelineConfig::test_pipeline_config_creation_default 
tests/unit/test_models.py::TestPipelineConfig::test_pipeline_config_creation_custom 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestPipelineConfig::test_pipeline_config_creation_custom 
tests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_creation 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_creation 
tests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_success 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_success 
[gw4] [ 43%] FAILED tests/unit/test_execution_write_mode.py::TestWriteModeRegression::test_incremental_mode_uses_append_for_silver_steps 
tests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_empty_step_name 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_empty_step_name 
tests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_empty_source_bronze 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_empty_source_bronze 
tests/unit/test_models.py::TestParallelConfig::test_parallel_config_creation 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestParallelConfig::test_parallel_config_creation 
tests/unit/test_models.py::TestParallelConfig::test_parallel_config_creation_minimal 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestParallelConfig::test_parallel_config_creation_minimal 
tests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_success 
[gw0] [ 43%] PASSED tests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_success 
tests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_negative_max_workers 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_negative_max_workers 
tests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_negative_worker_timeout 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_negative_worker_timeout 
tests/unit/test_models.py::TestParallelConfig::test_base_model_to_dict_with_nested_objects 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_base_model_to_dict_with_nested_objects 
tests/unit/test_models.py::TestParallelConfig::test_base_model_to_dict_without_nested_objects 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_base_model_to_dict_without_nested_objects 
tests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_validation_edge_cases 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_validation_edge_cases 
tests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_get_threshold 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_get_threshold 
tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_validation_invalid_schema 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_validation_invalid_schema 
tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_default 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_default 
tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_high_performance 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_high_performance 
tests/unit/test_models.py::TestParallelConfig::test_base_model_to_json 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_base_model_to_json 
tests/unit/test_models.py::TestParallelConfig::test_base_model_str_representation 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_base_model_str_representation 
tests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_create_loose 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_create_loose 
tests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_create_strict 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_create_strict 
tests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_max_workers_exceeded 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_max_workers_exceeded 
tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_properties 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_properties 
tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_conservative 
[gw0] [ 44%] PASSED tests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_conservative 
tests/unit/test_models_new.py::TestBaseModel::test_validate_default 
[gw0] [ 44%] PASSED tests/unit/test_models_new.py::TestBaseModel::test_validate_default 
tests/unit/test_models_new.py::TestBaseModel::test_to_dict_simple 
[gw0] [ 44%] PASSED tests/unit/test_models_new.py::TestBaseModel::test_to_dict_simple 
tests/unit/test_models_new.py::TestBaseModel::test_to_dict_nested_objects 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestBaseModel::test_to_dict_nested_objects 
tests/unit/test_models_new.py::TestBaseModel::test_to_json 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestBaseModel::test_to_json 
tests/unit/test_models_new.py::TestBaseModel::test_str_representation 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestBaseModel::test_str_representation 
tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_creation 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_creation 
tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_defaults 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_defaults 
tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_validation 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_validation 
tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_bronze 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_bronze 
tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_silver 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_silver 
tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_gold 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_gold 
tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_hierarchy 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_hierarchy 
tests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_creation 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_creation 
tests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_defaults 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_defaults 
tests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_validation 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_validation 
tests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_invalid_max_workers 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_invalid_max_workers 
tests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_creation 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_creation 
tests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_validation 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_validation 
tests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_invalid_schema 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_invalid_schema 
tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_creation 
tests/unit/test_execution_write_mode.py::TestWriteModeRegression::test_gold_incremental_mode_uses_overwrite 
[gw0] [ 45%] PASSED tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_creation 
tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_validation 
[gw0] [ 46%] PASSED tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_validation 
[gw5] [ 46%] PASSED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_with_empty_data 
[gw4] [ 46%] FAILED tests/unit/test_execution_write_mode.py::TestWriteModeRegression::test_gold_incremental_mode_uses_overwrite 
[gw3] [ 46%] PASSED tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_success_with_gold_steps 
tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_invalid_name 
[gw0] [ 46%] PASSED tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_invalid_name 
tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_has_write_mode_field 
[gw4] [ 46%] PASSED tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_has_write_mode_field 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_complete 
[gw4] [ 46%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_complete 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_complete_no_rows 
[gw4] [ 46%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_complete_no_rows 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_failed 
[gw4] [ 46%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_failed 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_failed_no_duration 
[gw4] [ 46%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_failed_no_duration 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_validation_passed 
[gw4] [ 46%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_validation_passed 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_validation_failed 
[gw4] [ 46%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_validation_failed 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_start 
tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_has_validation_rate_field 
[gw4] [ 46%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_start 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_end 
[gw5] [ 46%] PASSED tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_has_validation_rate_field 
tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_has_rows_written_field 
[gw4] [ 46%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_end 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_end_nonexistent 
[gw5] [ 46%] PASSED tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_has_rows_written_field 
tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_has_input_rows_field 
[gw4] [ 46%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_end_nonexistent 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_context_manager 
[gw4] [ 46%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_context_manager 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_context_manager_exception 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_has_input_rows_field 
tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_default_values 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestStepExecutionResultFields::test_step_execution_result_default_values 
[gw4] [ 47%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_context_manager_exception 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_console_only 
tests/unit/test_log_row_fields.py::TestPipelineReportFieldPropagation::test_pipeline_report_includes_write_mode 
[gw4] [ 47%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_console_only 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestPipelineReportFieldPropagation::test_pipeline_report_includes_write_mode 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_write_mode_populated_from_step_info 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_with_file 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_write_mode_populated_from_step_info 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_validation_fields_calculated_correctly 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_validation_fields_calculated_correctly 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_validation_perfect_rate 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_validation_perfect_rate 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_validation_low_rate 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_validation_low_rate 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_rows_written_vs_rows_processed 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_rows_written_vs_rows_processed 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_all_phases_have_correct_fields 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_all_phases_have_correct_fields 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_failed_step_has_zero_validation 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_failed_step_has_zero_validation 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_validation_calculation_edge_cases 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_validation_calculation_edge_cases 
tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_different_write_modes 
[gw4] [ 47%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_with_file 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestLogRowFieldCalculations::test_log_row_different_write_modes 
tests/unit/test_log_row_fields.py::TestMultiStepLogRowFields::test_multiple_steps_each_have_correct_fields 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_verbose_false 
[gw5] [ 47%] PASSED tests/unit/test_log_row_fields.py::TestMultiStepLogRowFields::test_multiple_steps_each_have_correct_fields 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_start 
[gw5] [ 47%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_start 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_start_custom_mode 
[gw5] [ 47%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_start_custom_mode 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_end_success 
[gw5] [ 48%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_end_success 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_end_failure 
[gw5] [ 48%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_end_failure 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_performance_metric 
[gw5] [ 48%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_performance_metric 
[gw4] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_verbose_false 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_custom_name 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_performance_metric_custom_unit 
[gw4] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_custom_name 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_custom_level 
[gw4] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_custom_level 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_file 
[gw5] [ 48%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_performance_metric_custom_unit 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_format_message_with_kwargs 
[gw5] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_format_message_with_kwargs 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_format_message_without_kwargs 
[gw5] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_format_message_without_kwargs 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_context_manager 
[gw5] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_context_manager 
[gw4] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_file 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_basic_logging_methods 
[gw4] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_basic_logging_methods 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_set_level 
[gw4] [ 48%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_set_level 
tests/unit/test_logging.py::TestTimerContextManager::test_timer_context_manager_success 
[gw4] [ 48%] PASSED tests/unit/test_logging.py::TestTimerContextManager::test_timer_context_manager_success 
tests/unit/test_logging.py::TestTimerContextManager::test_timer_context_manager_exception 
[gw4] [ 48%] PASSED tests/unit/test_logging.py::TestTimerContextManager::test_timer_context_manager_exception 
tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_with_schema_and_table_name 
[gw4] [ 48%] PASSED tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_with_schema_and_table_name 
tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_with_config_shows_deprecation_warning 
[gw4] [ 48%] PASSED tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_with_config_shows_deprecation_warning 
tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_without_required_params_raises_error 
[gw4] [ 48%] PASSED tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_without_required_params_raises_error 
tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_with_only_schema_raises_error 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_context_manager_with_existing_extra 
[gw4] [ 49%] PASSED tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_with_only_schema_raises_error 
[gw5] [ 49%] SKIPPED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_context_manager_with_existing_extra 
tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_with_only_table_name_raises_error 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_start 
[gw5] [ 49%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_start 
tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_start_different_stage 
[gw5] [ 49%] PASSED tests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_start_different_stage 
[gw4] [ 49%] PASSED tests/unit/test_logwriter_new_api.py::TestLogWriterSimplifiedInit::test_init_with_only_table_name_raises_error 
tests/unit/test_models_new.py::TestEnums::test_pipeline_status_enum 
[gw5] [ 49%] PASSED tests/unit/test_models_new.py::TestEnums::test_pipeline_status_enum 
tests/unit/test_models_new.py::TestEnums::test_pipeline_mode_enum 
tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_creates_log_row 
[gw5] [ 49%] PASSED tests/unit/test_models_new.py::TestEnums::test_pipeline_mode_enum 
tests/unit/test_models_new.py::TestEnums::test_step_status_enum 
[gw5] [ 49%] PASSED tests/unit/test_models_new.py::TestEnums::test_step_status_enum 
tests/unit/test_models_new.py::TestEnums::test_step_type_enum 
[gw5] [ 49%] PASSED tests/unit/test_models_new.py::TestEnums::test_step_type_enum 
tests/unit/test_models_new.py::TestStepResult::test_step_result_creation 
[gw4] [ 49%] PASSED tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_creates_log_row 
tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_populates_table_total_rows 
[gw5] [ 49%] PASSED tests/unit/test_models_new.py::TestStepResult::test_step_result_creation 
tests/unit/test_models_new.py::TestStepResult::test_step_result_validation 
[gw5] [ 49%] PASSED tests/unit/test_models_new.py::TestStepResult::test_step_result_validation 
tests/unit/test_models_new.py::TestStepResult::test_step_result_invalid_step_name 
[gw5] [ 49%] PASSED tests/unit/test_models_new.py::TestStepResult::test_step_result_invalid_step_name 
tests/unit/test_models_simple.py::TestBaseModel::test_validate_default 
[gw5] [ 49%] PASSED tests/unit/test_models_simple.py::TestBaseModel::test_validate_default 
tests/unit/test_models_simple.py::TestBaseModel::test_to_dict_simple 
[gw5] [ 49%] PASSED tests/unit/test_models_simple.py::TestBaseModel::test_to_dict_simple 
tests/unit/test_models_simple.py::TestBaseModel::test_to_dict_nested_objects 
[gw5] [ 49%] PASSED tests/unit/test_models_simple.py::TestBaseModel::test_to_dict_nested_objects 
tests/unit/test_models_simple.py::TestBaseModel::test_to_json 
[gw4] [ 49%] PASSED tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_populates_table_total_rows 
[gw5] [ 49%] PASSED tests/unit/test_models_simple.py::TestBaseModel::test_to_json 
tests/unit/test_models_simple.py::TestBaseModel::test_str_representation 
[gw5] [ 50%] PASSED tests/unit/test_models_simple.py::TestBaseModel::test_str_representation 
tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_creation 
[gw5] [ 50%] PASSED tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_creation 
tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_validation 
[gw5] [ 50%] PASSED tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_validation 
tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_bronze 
tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_with_errors 
[gw5] [ 50%] PASSED tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_bronze 
tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_silver 
[gw5] [ 50%] PASSED tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_silver 
tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_gold 
[gw5] [ 50%] PASSED tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_gold 
tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_hierarchy 
[gw5] [ 50%] PASSED tests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_hierarchy 
tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_source_silvers 
[gw4] [ 50%] PASSED tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_with_errors 
tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_generates_run_id_if_not_provided 
[gw4] [ 50%] PASSED tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_generates_run_id_if_not_provided 
tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_includes_metadata 
[gw4] [ 50%] PASSED tests/unit/test_logwriter_new_api.py::TestConvertReportToLogRows::test_convert_report_includes_metadata 
tests/unit/test_logwriter_new_api.py::TestCreateTableMethod::test_create_table_calls_storage_with_overwrite 
[gw4] [ 50%] PASSED tests/unit/test_logwriter_new_api.py::TestCreateTableMethod::test_create_table_calls_storage_with_overwrite 
tests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_creation 
[gw4] [ 50%] PASSED tests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_creation 
tests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_validation 
[gw4] [ 50%] PASSED tests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_validation 
tests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_invalid_max_workers 
[gw4] [ 50%] PASSED tests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_invalid_max_workers 
tests/unit/test_models_simple.py::TestPipelineConfig::test_pipeline_config_creation 
[gw4] [ 50%] PASSED tests/unit/test_models_simple.py::TestPipelineConfig::test_pipeline_config_creation 
tests/unit/test_models_simple.py::TestPipelineConfig::test_pipeline_config_validation 
[gw4] [ 50%] PASSED tests/unit/test_models_simple.py::TestPipelineConfig::test_pipeline_config_validation 
tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_creation 
tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_invalid_rules 
[gw0] [ 50%] PASSED tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_invalid_rules 
tests/unit/test_models_new.py::TestSilverStep::test_silver_step_creation 
[gw4] [ 50%] PASSED tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_creation 
[gw0] [ 51%] PASSED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_creation 
tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_with_failed_silver_step 
tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_validation 
tests/unit/test_models_new.py::TestSilverStep::test_silver_step_validation 
[gw4] [ 51%] PASSED tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_validation 
[gw0] [ 51%] PASSED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_validation 
tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_source_bronze 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_source_silvers 
[gw0] [ 51%] PASSED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_source_bronze 
tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_invalid_name 
[gw4] [ 51%] PASSED tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_invalid_name 
tests/unit/test_models_simple.py::TestStageStats::test_stage_stats_creation 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestStageStats::test_stage_stats_creation 
tests/unit/test_models_simple.py::TestStageStats::test_stage_stats_validation 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestStageStats::test_stage_stats_validation 
tests/unit/test_models_simple.py::TestStageStats::test_stage_stats_invalid_validation_rate 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestStageStats::test_stage_stats_invalid_validation_rate 
tests/unit/test_models_simple.py::TestStageStats::test_stage_stats_negative_values 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestStageStats::test_stage_stats_negative_values 
tests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_creation 
tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_transform 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_creation 
tests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_validation 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_validation 
tests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_invalid_execution_id 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_invalid_execution_id 
tests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_creation 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_creation 
tests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_validation 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_validation 
tests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_invalid_values 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_invalid_values 
tests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_creation 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_creation 
tests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_validation 
[gw5] [ 51%] PASSED tests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_validation 
tests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_invalid_pipeline_id 
[gw5] [ 52%] PASSED tests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_invalid_pipeline_id 
tests/unit/test_models_simple.py::TestStepResult::test_step_result_creation 
[gw5] [ 52%] PASSED tests/unit/test_models_simple.py::TestStepResult::test_step_result_creation 
tests/unit/test_models_simple.py::TestStepResult::test_step_result_validation 
[gw5] [ 52%] PASSED tests/unit/test_models_simple.py::TestStepResult::test_step_result_validation 
tests/unit/test_models_simple.py::TestStepResult::test_step_result_invalid_step_name 
[gw5] [ 52%] PASSED tests/unit/test_models_simple.py::TestStepResult::test_step_result_invalid_step_name 
tests/unit/test_performance.py::TestTimeOperation::test_time_operation_failure 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestTimeOperation::test_time_operation_failure 
tests/unit/test_performance.py::TestTimeOperation::test_time_operation_preserves_function_metadata 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestTimeOperation::test_time_operation_preserves_function_metadata 
tests/unit/test_performance.py::TestTimeOperation::test_time_operation_with_args_and_kwargs 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestTimeOperation::test_time_operation_with_args_and_kwargs 
tests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_success 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_success 
tests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_failure 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_failure 
tests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_with_max_duration_warning 
[gw3] [ 52%] PASSED tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_with_failed_silver_step 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_with_max_duration_warning 
tests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_with_max_duration_no_warning 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_with_max_duration_no_warning 
tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_invalid_mode 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_invalid_mode 
tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_imports_table_operations 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_imports_table_operations 
tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_append_mode 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_append_mode 
tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_with_options 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_with_options 
tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_failure 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_failure 
tests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_success 
[gw5] [ 52%] PASSED tests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_success 
tests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_failure 
[gw5] [ 53%] PASSED tests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_failure 
tests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_with_max_duration 
[gw5] [ 53%] PASSED tests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_with_max_duration 
tests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_preserves_function_metadata 
[gw5] [ 53%] PASSED tests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_preserves_function_metadata 
tests/unit/test_performance.py::TestPerformanceIntegration::test_all_functions_work_together 
[gw5] [ 53%] PASSED tests/unit/test_performance.py::TestPerformanceIntegration::test_all_functions_work_together 
tests/unit/test_performance.py::TestPerformanceIntegration::test_duration_formatting_integration 
[gw5] [ 53%] PASSED tests/unit/test_performance.py::TestPerformanceIntegration::test_duration_formatting_integration 
tests/unit/test_performance.py::TestPerformanceIntegration::test_now_dt_consistency 
[gw5] [ 53%] PASSED tests/unit/test_performance.py::TestPerformanceIntegration::test_now_dt_consistency 
tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_basic 
tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_invalid_rules 
[gw4] [ 53%] PASSED tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_invalid_rules 
tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_creation 
[gw0] [ 53%] PASSED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_transform 
[gw4] [ 53%] PASSED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_creation 
tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_table_name 
tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_with_failed_gold_step 
tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_validation 
[gw0] [ 53%] PASSED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_table_name 
[gw4] [ 53%] PASSED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_validation 
tests/unit/test_models_new.py::TestGoldStep::test_gold_step_creation 
[gw5] [ 53%] PASSED tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_basic 
[gw0] [ 53%] PASSED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_creation 
tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_source_bronze 
tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_with_quality_rates 
[gw4] [ 53%] PASSED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_source_bronze 
tests/unit/test_models_new.py::TestGoldStep::test_gold_step_validation 
[gw3] [ 53%] PASSED tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_with_failed_gold_step 
tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_transform 
[gw0] [ 53%] PASSED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_validation 
[gw4] [ 53%] PASSED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_transform 
[gw8] [ 53%] PASSED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_optimization 
[gw2] [ 54%] PASSED tests/builder_tests/test_streaming_hybrid_pipeline.py::TestStreamingHybridPipeline::test_complete_streaming_hybrid_pipeline_execution 
tests/builder_tests/test_streaming_hybrid_pipeline.py::TestStreamingHybridPipeline::test_incremental_streaming_processing 
tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_silver_step_without_schema 
[gw5] [ 54%] PASSED tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_with_quality_rates 
tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_table_name 
tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_transform 
[gw1] [ 54%] PASSED tests/builder_pyspark_tests/test_iot_pipeline.py::TestIotPipeline::test_anomaly_detection_pipeline 
[gw4] [ 54%] PASSED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_table_name 
tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_invalid_spark 
tests/builder_pyspark_tests/test_iot_pipeline.py::TestIotPipeline::test_performance_monitoring 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_transform 
tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_creation 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_history_and_metadata 
[gw4] [ 54%] PASSED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_creation 
tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_table_name 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_table_name 
[gw5] [ 54%] PASSED tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_invalid_spark 
tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_validation 
[gw4] [ 54%] PASSED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_validation 
tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_source_silvers 
tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_empty_schema 
tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_transform 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_source_silvers 
[gw4] [ 54%] PASSED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_transform 
tests/unit/test_models_new.py::TestStageStats::test_stage_stats_creation 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestStageStats::test_stage_stats_creation 
tests/unit/test_models_new.py::TestStageStats::test_stage_stats_validation 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestStageStats::test_stage_stats_validation 
tests/unit/test_models_new.py::TestStageStats::test_stage_stats_invalid_validation_rate 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestStageStats::test_stage_stats_invalid_validation_rate 
tests/unit/test_models_new.py::TestStageStats::test_stage_stats_negative_values 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestStageStats::test_stage_stats_negative_values 
tests/unit/test_models_new.py::TestExecutionContext::test_execution_context_creation 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestExecutionContext::test_execution_context_creation 
tests/unit/test_models_new.py::TestExecutionContext::test_execution_context_validation 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestExecutionContext::test_execution_context_validation 
tests/unit/test_models_new.py::TestExecutionContext::test_execution_context_invalid_pipeline_id 
[gw0] [ 54%] PASSED tests/unit/test_models_new.py::TestExecutionContext::test_execution_context_invalid_pipeline_id 
tests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_creation 
[gw0] [ 55%] PASSED tests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_creation 
tests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_validation 
[gw0] [ 55%] PASSED tests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_validation 
tests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_invalid_values 
[gw0] [ 55%] PASSED tests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_invalid_values 
tests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_creation 
[gw0] [ 55%] PASSED tests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_creation 
tests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_validation 
[gw0] [ 55%] PASSED tests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_validation 
tests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_invalid_pipeline_id 
[gw0] [ 55%] PASSED tests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_invalid_pipeline_id 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_basic 
tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_table_name 
[gw0] [ 55%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_basic 
[gw5] [ 55%] PASSED tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_empty_schema 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_table_name 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_with_quality_rates 
tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_pipeline_id 
tests/unit/test_models_simple.py::TestEnums::test_pipeline_phase_enum 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestEnums::test_pipeline_phase_enum 
tests/unit/test_models_simple.py::TestEnums::test_execution_mode_enum 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestEnums::test_execution_mode_enum 
tests/unit/test_models_simple.py::TestEnums::test_write_mode_enum 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestEnums::test_write_mode_enum 
tests/unit/test_models_simple.py::TestEnums::test_validation_result_enum 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestEnums::test_validation_result_enum 
tests/unit/test_models_simple.py::TestEnums::test_step_status_enum 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestEnums::test_step_status_enum 
tests/unit/test_models_simple.py::TestEnums::test_step_type_enum 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestEnums::test_step_type_enum 
tests/unit/test_models_simple.py::TestEnums::test_pipeline_status_enum 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestEnums::test_pipeline_status_enum 
tests/unit/test_models_simple.py::TestEnums::test_pipeline_mode_enum 
[gw4] [ 55%] PASSED tests/unit/test_models_simple.py::TestEnums::test_pipeline_mode_enum 
tests/unit/test_performance.py::TestNowDt::test_now_dt_returns_datetime 
[gw4] [ 55%] PASSED tests/unit/test_performance.py::TestNowDt::test_now_dt_returns_datetime 
tests/unit/test_performance.py::TestNowDt::test_now_dt_returns_utc 
[gw4] [ 56%] PASSED tests/unit/test_performance.py::TestNowDt::test_now_dt_returns_utc 
tests/unit/test_performance.py::TestNowDt::test_now_dt_recent_time 
[gw4] [ 56%] PASSED tests/unit/test_performance.py::TestNowDt::test_now_dt_recent_time 
tests/unit/test_performance.py::TestFormatDuration::test_format_duration_seconds 
[gw4] [ 56%] PASSED tests/unit/test_performance.py::TestFormatDuration::test_format_duration_seconds 
tests/unit/test_performance.py::TestFormatDuration::test_format_duration_minutes 
[gw4] [ 56%] PASSED tests/unit/test_performance.py::TestFormatDuration::test_format_duration_minutes 
tests/unit/test_performance.py::TestFormatDuration::test_format_duration_hours 
[gw4] [ 56%] PASSED tests/unit/test_performance.py::TestFormatDuration::test_format_duration_hours 
tests/unit/test_performance.py::TestFormatDuration::test_format_duration_zero 
[gw4] [ 56%] PASSED tests/unit/test_performance.py::TestFormatDuration::test_format_duration_zero 
tests/unit/test_performance.py::TestFormatDuration::test_format_duration_negative 
[gw4] [ 56%] PASSED tests/unit/test_performance.py::TestFormatDuration::test_format_duration_negative 
tests/unit/test_performance.py::TestTimeOperation::test_time_operation_success 
[gw4] [ 56%] PASSED tests/unit/test_performance.py::TestTimeOperation::test_time_operation_success 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_basic 
[gw0] [ 56%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_with_quality_rates 
[gw4] [ 56%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_basic 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_invalid_spark 
[gw3] [ 56%] PASSED tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_silver_step_without_schema 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_auto_inference 
[gw0] [ 56%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_invalid_spark 
[gw5] [ 56%] PASSED tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_pipeline_id 
[gw4] [ 56%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_auto_inference 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_empty_schema 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_no_bronze_steps 
tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_validators 
[gw0] [ 56%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_empty_schema 
[gw4] [ 56%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_no_bronze_steps 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_pipeline_id 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_invalid_source 
[gw0] [ 56%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_pipeline_id 
[gw4] [ 56%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_invalid_source 
tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_gold_step_without_schema 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_duplicate_name 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_validators 
[gw4] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverTransform::test_add_silver_transform_duplicate_name 
[gw5] [ 57%] PASSED tests/unit/test_pipeline_builder_basic.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_validators 
[gw0] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_validators 
[gw1] [ 57%] PASSED tests/builder_pyspark_tests/test_iot_pipeline.py::TestIotPipeline::test_performance_monitoring 
tests/builder_pyspark_tests/test_marketing_pipeline.py::TestMarketingPipeline::test_complete_marketing_pipeline_execution 
tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_basic 
tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_basic 
tests/unit/test_pipeline_builder_basic.py::TestValidatorManagement::test_add_validator 
[gw4] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_basic 
[gw0] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_basic 
tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_with_incremental_col 
tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_auto_inference 
[gw0] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_with_incremental_col 
[gw4] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_auto_inference 
[gw5] [ 57%] PASSED tests/unit/test_pipeline_builder_basic.py::TestValidatorManagement::test_add_validator 
tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_with_schema 
tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_no_silver_steps 
tests/unit/test_pipeline_builder_basic.py::TestValidatorManagement::test_add_multiple_validators 
[gw0] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_with_schema 
[gw4] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_no_silver_steps 
tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_duplicate_name 
[gw7] [ 57%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_compliance_checker_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_monitor_integration 
[gw0] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_duplicate_name 
tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_invalid_sources 
tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_empty_name 
[gw4] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_invalid_sources 
[gw5] [ 57%] PASSED tests/unit/test_pipeline_builder_basic.py::TestValidatorManagement::test_add_multiple_validators 
[gw0] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_empty_name 
tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_pyspark_rules 
tests/unit/test_pipeline_builder_basic.py::TestPipelineValidation::test_validate_pipeline_empty 
tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_duplicate_name 
[gw0] [ 57%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestBronzeRules::test_with_bronze_rules_pyspark_rules 
[gw3] [ 57%] PASSED tests/unit/test_execution_100_coverage.py::TestExecutePipelineComplete::test_execute_pipeline_gold_step_without_schema 
[gw4] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestGoldTransform::test_add_gold_transform_duplicate_name 
[gw9] [ 58%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestHighVolume::test_pipeline_high_volume_with_logging 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverRules::test_with_silver_rules_basic 
tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_silver_step_success 
tests/system/test_full_pipeline_with_logging_variations.py::TestComplexDependencies::test_pipeline_complex_dependencies_with_logging 
[gw6] [ 58%] PASSED tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_incremental_pipeline_preserves_data 
[gw0] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverRules::test_with_silver_rules_basic 
[gw5] [ 58%] PASSED tests/unit/test_pipeline_builder_basic.py::TestPipelineValidation::test_validate_pipeline_empty 
tests/unit/test_pipeline_builder_comprehensive.py::TestValidatorManagement::test_add_validator 
[gw4] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestValidatorManagement::test_add_validator 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverRules::test_with_silver_rules_with_watermark 
tests/unit/test_pipeline_builder_basic.py::TestPipelineValidation::test_validate_pipeline_invalid_config 
[gw0] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverRules::test_with_silver_rules_with_watermark 
tests/unit/test_pipeline_builder_comprehensive.py::TestValidatorManagement::test_add_multiple_validators 
[gw4] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestValidatorManagement::test_add_multiple_validators 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverRules::test_with_silver_rules_duplicate_name 
[gw2] [ 58%] PASSED tests/builder_tests/test_streaming_hybrid_pipeline.py::TestStreamingHybridPipeline::test_incremental_streaming_processing 
tests/builder_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_complete_supply_chain_pipeline_execution 
[gw0] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverRules::test_with_silver_rules_duplicate_name 
tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_initial_pipeline_overwrites_data 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineValidation::test_validate_pipeline_empty 
[gw5] [ 58%] PASSED tests/unit/test_pipeline_builder_basic.py::TestPipelineValidation::test_validate_pipeline_invalid_config 
[gw7] [ 58%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_monitor_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_components_workflow 
tests/unit/test_pipeline_builder_comprehensive.py::TestSilverRules::test_with_silver_rules_empty_name 
[gw4] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineValidation::test_validate_pipeline_empty 
[gw0] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestSilverRules::test_with_silver_rules_empty_name 
tests/unit/test_pipeline_builder_basic.py::TestToPipeline::test_to_pipeline_empty 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineValidation::test_validate_pipeline_with_steps 
tests/unit/test_pipeline_builder_comprehensive.py::TestToPipeline::test_to_pipeline_basic 
[gw8] [ 58%] PASSED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_history_and_metadata 
[gw4] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineValidation::test_validate_pipeline_with_steps 
[gw0] [ 58%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestToPipeline::test_to_pipeline_basic 
[gw3] [ 58%] PASSED tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_silver_step_success 
[gw5] [ 59%] PASSED tests/unit/test_pipeline_builder_basic.py::TestToPipeline::test_to_pipeline_empty 
tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineValidation::test_validate_pipeline_invalid_config 
tests/unit/test_pipeline_builder_comprehensive.py::TestToPipeline::test_to_pipeline_validation_failure 
tests/unit/test_pipeline_builder_basic.py::TestHelperMethods::test_get_effective_schema 
[gw4] [ 59%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestPipelineValidation::test_validate_pipeline_invalid_config 
[gw0] [ 59%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestToPipeline::test_to_pipeline_validation_failure 
tests/unit/test_pipeline_builder_simple.py::TestSilverRules::test_with_silver_rules_with_watermark 
tests/unit/test_pipeline_builder_comprehensive.py::TestToPipeline::test_to_pipeline_empty 
[gw4] [ 59%] PASSED tests/unit/test_pipeline_builder_simple.py::TestSilverRules::test_with_silver_rules_with_watermark 
tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_silver_step_missing_source 
[gw0] [ 59%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestToPipeline::test_to_pipeline_empty 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_concurrent_writes 
tests/unit/test_pipeline_builder_simple.py::TestSilverRules::test_with_silver_rules_duplicate_name 
tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_get_effective_schema 
[gw5] [ 59%] PASSED tests/unit/test_pipeline_builder_basic.py::TestHelperMethods::test_get_effective_schema 
[gw4] [ 59%] PASSED tests/unit/test_pipeline_builder_simple.py::TestSilverRules::test_with_silver_rules_duplicate_name 
[gw0] [ 59%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_get_effective_schema 
tests/unit/test_pipeline_builder_basic.py::TestHelperMethods::test_create_schema_if_not_exists 
tests/unit/test_pipeline_builder_simple.py::TestSilverRules::test_with_silver_rules_empty_name 
tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_validate_schema_existing 
[gw3] [ 59%] PASSED tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_silver_step_missing_source 
[gw4] [ 59%] PASSED tests/unit/test_pipeline_builder_simple.py::TestSilverRules::test_with_silver_rules_empty_name 
tests/unit/test_pipeline_builder_simple.py::TestSilverTransform::test_add_silver_transform_basic 
tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_gold_step_success 
[gw0] [ 59%] FAILED tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_validate_schema_existing 
[gw4] [ 59%] PASSED tests/unit/test_pipeline_builder_simple.py::TestSilverTransform::test_add_silver_transform_basic 
tests/unit/test_pipeline_builder_simple.py::TestSilverTransform::test_add_silver_transform_auto_inference 
tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_validate_schema_nonexistent 
[gw4] [ 59%] PASSED tests/unit/test_pipeline_builder_simple.py::TestSilverTransform::test_add_silver_transform_auto_inference 
[gw0] [ 59%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_validate_schema_nonexistent 
tests/unit/test_pipeline_builder_simple.py::TestSilverTransform::test_add_silver_transform_no_bronze_steps 
[gw4] [ 59%] PASSED tests/unit/test_pipeline_builder_simple.py::TestSilverTransform::test_add_silver_transform_no_bronze_steps 
tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_create_schema_if_not_exists 
tests/unit/test_pipeline_builder_simple.py::TestSilverTransform::test_add_silver_transform_invalid_source 
[gw4] [ 59%] PASSED tests/unit/test_pipeline_builder_simple.py::TestSilverTransform::test_add_silver_transform_invalid_source 
[gw0] [ 59%] FAILED tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_create_schema_if_not_exists 
[gw5] [ 59%] PASSED tests/unit/test_pipeline_builder_basic.py::TestHelperMethods::test_create_schema_if_not_exists 
tests/unit/test_pipeline_builder_simple.py::TestGoldTransform::test_add_gold_transform_basic 
tests/unit/test_pipeline_builder_basic.py::TestHelperMethods::test_create_schema_if_not_exists_failure 
[gw4] [ 60%] PASSED tests/unit/test_pipeline_builder_simple.py::TestGoldTransform::test_add_gold_transform_basic 
tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_create_schema_if_not_exists_failure 
[gw0] [ 60%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_create_schema_if_not_exists_failure 
[gw3] [ 60%] PASSED tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_gold_step_success 
tests/unit/test_pipeline_builder_simple.py::TestGoldTransform::test_add_gold_transform_auto_inference 
[gw4] [ 60%] PASSED tests/unit/test_pipeline_builder_simple.py::TestGoldTransform::test_add_gold_transform_auto_inference 
tests/unit/test_pipeline_builder_comprehensive.py::TestClassMethods::test_for_development 
[gw5] [ 60%] PASSED tests/unit/test_pipeline_builder_basic.py::TestHelperMethods::test_create_schema_if_not_exists_failure 
tests/unit/test_pipeline_builder_simple.py::TestGoldTransform::test_add_gold_transform_no_silver_steps 
tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_gold_step_missing_source 
[gw0] [ 60%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestClassMethods::test_for_development 
tests/unit/test_pipeline_builder_basic.py::TestClassMethods::test_for_development 
[gw4] [ 60%] PASSED tests/unit/test_pipeline_builder_simple.py::TestGoldTransform::test_add_gold_transform_no_silver_steps 
tests/unit/test_pipeline_builder_simple.py::TestGoldTransform::test_add_gold_transform_invalid_sources 
[gw4] [ 60%] PASSED tests/unit/test_pipeline_builder_simple.py::TestGoldTransform::test_add_gold_transform_invalid_sources 
tests/unit/test_pipeline_builder_comprehensive.py::TestClassMethods::test_for_production 
[gw0] [ 60%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestClassMethods::test_for_production 
tests/unit/test_pipeline_builder_simple.py::TestValidatorManagement::test_add_validator 
[gw5] [ 60%] PASSED tests/unit/test_pipeline_builder_basic.py::TestClassMethods::test_for_development 
[gw3] [ 60%] PASSED tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_gold_step_missing_source 
[gw4] [ 60%] PASSED tests/unit/test_pipeline_builder_simple.py::TestValidatorManagement::test_add_validator 
tests/unit/test_pipeline_builder_comprehensive.py::TestIntegration::test_complete_pipeline_workflow 
[gw0] [ 60%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestIntegration::test_complete_pipeline_workflow 
tests/unit/test_pipeline_builder_basic.py::TestClassMethods::test_for_production 
tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_gold_step_with_none_source_silvers 
tests/unit/test_pipeline_builder_simple.py::TestValidatorManagement::test_add_multiple_validators 
tests/unit/test_pipeline_builder_comprehensive.py::TestIntegration::test_pipeline_with_custom_validator 
[gw4] [ 60%] PASSED tests/unit/test_pipeline_builder_simple.py::TestValidatorManagement::test_add_multiple_validators 
[gw0] [ 60%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestIntegration::test_pipeline_with_custom_validator 
tests/unit/test_pipeline_builder_simple.py::TestPipelineValidation::test_validate_pipeline_empty 
tests/unit/test_pipeline_builder_comprehensive.py::TestIntegration::test_pipeline_with_multiple_schemas 
[gw4] [ 60%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineValidation::test_validate_pipeline_empty 
[gw0] [ 60%] PASSED tests/unit/test_pipeline_builder_comprehensive.py::TestIntegration::test_pipeline_with_multiple_schemas 
tests/unit/test_pipeline_builder_simple.py::TestPipelineValidation::test_validate_pipeline_with_steps 
[gw5] [ 60%] PASSED tests/unit/test_pipeline_builder_basic.py::TestClassMethods::test_for_production 
tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_basic 
[gw4] [ 61%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineValidation::test_validate_pipeline_with_steps 
[gw3] [ 61%] PASSED tests/unit/test_execution_100_coverage.py::TestPrivateMethodsComplete::test_execute_gold_step_with_none_source_silvers 
tests/unit/test_pipeline_builder_basic.py::TestErrorHandling::test_with_bronze_rules_empty_name 
[gw0] [ 61%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_basic 
tests/unit/test_pipeline_builder_simple.py::TestPipelineValidation::test_validate_pipeline_invalid_config 
tests/unit/test_execution_comprehensive.py::TestExecutionMode::test_execution_mode_values 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionMode::test_execution_mode_values 
tests/unit/test_execution_comprehensive.py::TestExecutionMode::test_execution_mode_enumeration 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionMode::test_execution_mode_enumeration 
tests/unit/test_execution_comprehensive.py::TestStepStatus::test_step_status_values 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestStepStatus::test_step_status_values 
tests/unit/test_execution_comprehensive.py::TestStepStatus::test_step_status_enumeration 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestStepStatus::test_step_status_enumeration 
tests/unit/test_execution_comprehensive.py::TestStepType::test_step_type_values 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestStepType::test_step_type_values 
tests/unit/test_execution_comprehensive.py::TestStepType::test_step_type_enumeration 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestStepType::test_step_type_enumeration 
tests/unit/test_execution_comprehensive.py::TestStepExecutionResult::test_step_execution_result_creation 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestStepExecutionResult::test_step_execution_result_creation 
tests/unit/test_execution_comprehensive.py::TestStepExecutionResult::test_step_execution_result_with_end_time 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestStepExecutionResult::test_step_execution_result_with_end_time 
tests/unit/test_execution_comprehensive.py::TestStepExecutionResult::test_step_execution_result_with_all_fields 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestStepExecutionResult::test_step_execution_result_with_all_fields 
tests/unit/test_execution_comprehensive.py::TestExecutionResult::test_execution_result_creation 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionResult::test_execution_result_creation 
tests/unit/test_execution_comprehensive.py::TestExecutionResult::test_execution_result_with_end_time 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionResult::test_execution_result_with_end_time 
tests/unit/test_execution_comprehensive.py::TestExecutionResult::test_execution_result_with_steps 
[gw3] [ 61%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionResult::test_execution_result_with_steps 
tests/unit/test_execution_comprehensive.py::TestExecutionEngineInitialization::test_execution_engine_initialization 
[gw4] [ 61%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineValidation::test_validate_pipeline_invalid_config 
tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_with_quality_rates 
[gw0] [ 61%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_with_quality_rates 
tests/unit/test_pipeline_builder_simple.py::TestToPipeline::test_to_pipeline_basic 
tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_invalid_spark 
[gw4] [ 61%] PASSED tests/unit/test_pipeline_builder_simple.py::TestToPipeline::test_to_pipeline_basic 
[gw5] [ 62%] PASSED tests/unit/test_pipeline_builder_basic.py::TestErrorHandling::test_with_bronze_rules_empty_name 
[gw3] [ 62%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionEngineInitialization::test_execution_engine_initialization 
[gw0] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_invalid_spark 
tests/unit/test_pipeline_builder_simple.py::TestToPipeline::test_to_pipeline_validation_failure 
tests/unit/test_pipeline_builder_basic.py::TestErrorHandling::test_with_silver_rules_empty_name 
tests/unit/test_execution_comprehensive.py::TestExecutionEngineInitialization::test_execution_engine_with_custom_logger 
tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_empty_schema 
[gw4] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestToPipeline::test_to_pipeline_validation_failure 
[gw0] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_empty_schema 
tests/unit/test_pipeline_builder_simple.py::TestToPipeline::test_to_pipeline_empty 
tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_pipeline_id 
[gw4] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestToPipeline::test_to_pipeline_empty 
[gw0] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_pipeline_id 
[gw5] [ 62%] PASSED tests/unit/test_pipeline_builder_basic.py::TestErrorHandling::test_with_silver_rules_empty_name 
[gw3] [ 62%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionEngineInitialization::test_execution_engine_with_custom_logger 
tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_get_effective_schema 
tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_validators 
tests/unit/test_execution_comprehensive.py::TestExecutionEngineInitialization::test_execution_engine_with_none_logger 
tests/unit/test_pipeline_builder_basic.py::TestErrorHandling::test_add_silver_transform_no_bronze_steps 
[gw4] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_get_effective_schema 
[gw0] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestPipelineBuilderInitialization::test_pipeline_builder_initialization_validators 
tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_basic 
tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_validate_schema_existing 
[gw0] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_basic 
[gw5] [ 62%] PASSED tests/unit/test_pipeline_builder_basic.py::TestErrorHandling::test_add_silver_transform_no_bronze_steps 
[gw3] [ 62%] PASSED tests/unit/test_execution_comprehensive.py::TestExecutionEngineInitialization::test_execution_engine_with_none_logger 
[gw4] [ 62%] FAILED tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_validate_schema_existing 
tests/unit/test_pipeline_builder_simple.py::TestClassMethods::test_for_development 
tests/unit/test_pipeline_builder_basic.py::TestErrorHandling::test_add_gold_transform_no_silver_steps 
tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_with_incremental_col 
[gw0] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_with_incremental_col 
tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_validate_schema_nonexistent 
tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_with_schema 
[gw4] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_validate_schema_nonexistent 
[gw0] [ 62%] PASSED tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_with_schema 
[gw6] [ 63%] PASSED tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_initial_pipeline_overwrites_data 
[gw3] [ 63%] PASSED tests/unit/test_pipeline_builder_simple.py::TestClassMethods::test_for_development 
[gw5] [ 63%] PASSED tests/unit/test_pipeline_builder_basic.py::TestErrorHandling::test_add_gold_transform_no_silver_steps 
tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_duplicate_name 
tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_create_schema_if_not_exists 
tests/unit/test_pipeline_builder_simple.py::TestClassMethods::test_for_production 
[gw0] [ 63%] PASSED tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_duplicate_name 
tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_bronze_steps_storage 
tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_write_mode_consistency_across_pipeline_runs 
[gw4] [ 63%] FAILED tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_create_schema_if_not_exists 
tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_empty_name 
tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_create_schema_if_not_exists_failure 
[gw0] [ 63%] PASSED tests/unit/test_pipeline_builder_simple.py::TestBronzeRules::test_with_bronze_rules_empty_name 
[gw4] [ 63%] PASSED tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_create_schema_if_not_exists_failure 
tests/unit/test_pipeline_builder_simple.py::TestSilverRules::test_with_silver_rules_basic 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_execution_engine_receives_correct_mode 
[gw3] [ 63%] PASSED tests/unit/test_pipeline_builder_simple.py::TestClassMethods::test_for_production 
[gw0] [ 63%] PASSED tests/unit/test_pipeline_builder_simple.py::TestSilverRules::test_with_silver_rules_basic 
[gw5] [ 63%] PASSED tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_bronze_steps_storage 
tests/unit/test_reporting.py::TestCreateWriteDict::test_create_write_dict_type_conversion 
[gw0] [ 63%] PASSED tests/unit/test_reporting.py::TestCreateWriteDict::test_create_write_dict_type_conversion 
tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_basic 
[gw0] [ 63%] PASSED tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_basic 
tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_zero_steps 
[gw0] [ 63%] PASSED tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_zero_steps 
tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_perfect_success 
[gw0] [ 63%] PASSED tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_perfect_success 
tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_rounding 
[gw0] [ 63%] PASSED tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_rounding 
tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_zero_division_handling 
[gw0] [ 63%] PASSED tests/unit/test_reporting.py::TestCreateSummaryReport::test_create_summary_report_zero_division_handling 
tests/unit/test_reporting.py::TestReportingIntegration::test_all_functions_return_dicts 
[gw0] [ 63%] PASSED tests/unit/test_reporting.py::TestReportingIntegration::test_all_functions_return_dicts 
tests/unit/test_reporting.py::TestReportingIntegration::test_consistent_datetime_handling 
[gw0] [ 63%] PASSED tests/unit/test_reporting.py::TestReportingIntegration::test_consistent_datetime_handling 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_import 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_import 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_functions 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_functions 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_generate_execution_report 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_generate_execution_report 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_generate_quality_report 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_generate_quality_report 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_generate_performance_report 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_generate_performance_report 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_generate_error_report 
tests/unit/test_pipeline_builder_simple.py::TestIntegration::test_complete_pipeline_workflow 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_generate_error_report 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_format_report 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_format_report 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_save_report 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_save_report 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_print_report 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_print_report 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_module_attributes 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_module_attributes 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_error_handling 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_error_handling 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_module_reload 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_module_reload 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_imports 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_imports 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_function_signatures 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_function_signatures 
tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_module_documentation 
[gw0] [ 64%] PASSED tests/unit/test_reporting_coverage.py::TestReportingCoverage::test_reporting_module_documentation 
tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_dict_instantiation 
[gw0] [ 64%] PASSED tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_dict_instantiation 
tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_dict_type_annotation_works 
[gw0] [ 64%] PASSED tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_dict_type_annotation_works 
tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_dict_vs_Dict_equivalence 
[gw0] [ 64%] PASSED tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_dict_vs_Dict_equivalence 
tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_python_version 
[gw0] [ 65%] PASSED tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_python_version 
tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_typeddict_compatibility 
[gw0] [ 65%] PASSED tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_typeddict_compatibility 
tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_union_with_dict_works 
tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_silver_steps_storage 
[gw0] [ 65%] PASSED tests/unit/test_simple_dict_check.py::SimpleDictCheckTest::test_union_with_dict_works 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_pipeline_builder_working 
[gw4] [ 65%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_execution_engine_receives_correct_mode 
[gw0] [ 65%] FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_pipeline_builder_working 
[gw3] [ 65%] PASSED tests/unit/test_pipeline_builder_simple.py::TestIntegration::test_complete_pipeline_workflow 
[gw5] [ 65%] PASSED tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_silver_steps_storage 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_execution_engine_working 
tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_all_files_parseable 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_all_files_parseable 
tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_import_compatibility 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_import_compatibility 
tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_no_dict_type_annotations 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_no_dict_type_annotations 
tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_no_legacy_typing_imports 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_no_legacy_typing_imports 
tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_python_version 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::Python38CompatibilityTest::test_python_version 
tests/unit/test_python38_compatibility.py::DictTypeAnnotationTest::test_dict_vs_Dict_compatibility 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::DictTypeAnnotationTest::test_dict_vs_Dict_compatibility 
tests/unit/test_python38_compatibility.py::DictTypeAnnotationTest::test_typeddict_compatibility 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::DictTypeAnnotationTest::test_typeddict_compatibility 
tests/unit/test_python38_compatibility.py::DictTypeAnnotationTest::test_union_type_compatibility 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::DictTypeAnnotationTest::test_union_type_compatibility 
tests/unit/test_python38_compatibility.py::ImportCompatibilityTest::test_core_imports 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::ImportCompatibilityTest::test_core_imports 
tests/unit/test_python38_compatibility.py::ImportCompatibilityTest::test_models_imports 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::ImportCompatibilityTest::test_models_imports 
tests/unit/test_python38_compatibility.py::ImportCompatibilityTest::test_writer_imports 
[gw4] [ 65%] PASSED tests/unit/test_python38_compatibility.py::ImportCompatibilityTest::test_writer_imports 
tests/unit/test_reporting.py::TestCreateValidationDict::test_create_validation_dict_with_stats 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateValidationDict::test_create_validation_dict_with_stats 
tests/unit/test_reporting.py::TestCreateValidationDict::test_create_validation_dict_without_stats 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateValidationDict::test_create_validation_dict_without_stats 
tests/unit/test_reporting.py::TestCreateValidationDict::test_create_validation_dict_rounding 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateValidationDict::test_create_validation_dict_rounding 
tests/unit/test_reporting.py::TestCreateTransformDict::test_create_transform_dict_basic 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateTransformDict::test_create_transform_dict_basic 
tests/unit/test_reporting.py::TestCreateTransformDict::test_create_transform_dict_skipped 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateTransformDict::test_create_transform_dict_skipped 
tests/unit/test_reporting.py::TestCreateTransformDict::test_create_transform_dict_rounding 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateTransformDict::test_create_transform_dict_rounding 
tests/unit/test_reporting.py::TestCreateTransformDict::test_create_transform_dict_type_conversion 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateTransformDict::test_create_transform_dict_type_conversion 
tests/unit/test_reporting.py::TestCreateWriteDict::test_create_write_dict_basic 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateWriteDict::test_create_write_dict_basic 
tests/unit/test_reporting.py::TestCreateWriteDict::test_create_write_dict_skipped 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateWriteDict::test_create_write_dict_skipped 
tests/unit/test_reporting.py::TestCreateWriteDict::test_create_write_dict_rounding 
[gw4] [ 66%] PASSED tests/unit/test_reporting.py::TestCreateWriteDict::test_create_write_dict_rounding 
tests/unit/test_table_operations.py::TestFqn::test_fqn_basic 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestFqn::test_fqn_basic 
tests/unit/test_table_operations.py::TestFqn::test_fqn_different_names 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestFqn::test_fqn_different_names 
tests/unit/test_table_operations.py::TestFqn::test_fqn_empty_schema 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestFqn::test_fqn_empty_schema 
tests/unit/test_table_operations.py::TestFqn::test_fqn_empty_table 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestFqn::test_fqn_empty_table 
tests/unit/test_table_operations.py::TestFqn::test_fqn_both_empty 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestFqn::test_fqn_both_empty 
tests/unit/test_table_operations.py::TestFqn::test_fqn_none_schema 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestFqn::test_fqn_none_schema 
tests/unit/test_table_operations.py::TestFqn::test_fqn_none_table 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestFqn::test_fqn_none_table 
tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_table_success 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_table_success 
tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_table_with_options 
[gw4] [ 66%] PASSED tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_table_with_options 
tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_table_failure 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_table_failure 
tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_table_zero_rows 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_table_zero_rows 
tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_table_success 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_table_success 
tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_table_with_options 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_table_with_options 
tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_table_failure 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_table_failure 
tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_table_zero_rows 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_table_zero_rows 
tests/unit/test_table_operations.py::TestReadTable::test_read_table_success 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestReadTable::test_read_table_success 
tests/unit/test_table_operations.py::TestReadTable::test_read_table_analysis_exception 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestReadTable::test_read_table_analysis_exception 
tests/unit/test_table_operations.py::TestReadTable::test_read_table_other_exception 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestReadTable::test_read_table_other_exception 
tests/unit/test_table_operations.py::TestTableExists::test_table_exists_true 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestTableExists::test_table_exists_true 
tests/unit/test_table_operations.py::TestTableExists::test_table_exists_false_analysis_exception 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestTableExists::test_table_exists_false_analysis_exception 
tests/unit/test_table_operations.py::TestTableExists::test_table_exists_false_other_exception 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestTableExists::test_table_exists_false_other_exception 
tests/unit/test_pipeline_builder_simple.py::TestIntegration::test_pipeline_with_custom_validator 
tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_gold_steps_storage 
[gw0] [ 67%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_execution_engine_working 
tests/unit/test_table_operations.py::TestDropTable::test_drop_table_success 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestDropTable::test_drop_table_success 
tests/unit/test_table_operations.py::TestDropTable::test_drop_table_with_default_schema 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestDropTable::test_drop_table_with_default_schema 
tests/unit/test_table_operations.py::TestDropTable::test_drop_table_not_exists 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestDropTable::test_drop_table_not_exists 
tests/unit/test_table_operations.py::TestDropTable::test_drop_table_failure 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestDropTable::test_drop_table_failure 
tests/unit/test_table_operations.py::TestDropTable::test_drop_table_exception_during_check 
[gw4] [ 67%] PASSED tests/unit/test_table_operations.py::TestDropTable::test_drop_table_exception_during_check 
tests/unit/test_table_operations.py::TestTableOperationsIntegration::test_fqn_with_table_operations 
[gw4] [ 68%] PASSED tests/unit/test_table_operations.py::TestTableOperationsIntegration::test_fqn_with_table_operations 
tests/unit/test_table_operations.py::TestTableOperationsIntegration::test_write_and_read_workflow 
[gw4] [ 68%] PASSED tests/unit/test_table_operations.py::TestTableOperationsIntegration::test_write_and_read_workflow 
tests/unit/test_table_operations.py::TestTableOperationsIntegration::test_table_lifecycle 
[gw4] [ 68%] PASSED tests/unit/test_table_operations.py::TestTableOperationsIntegration::test_table_lifecycle 
tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_all_files_parseable 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_all_files_parseable 
tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_import_compatibility 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_import_compatibility 
tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_no_dict_type_annotations 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_no_dict_type_annotations 
tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_no_legacy_typing_imports 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_no_legacy_typing_imports 
tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_python_version 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::Python38CompatibilityTest::test_python_version 
tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_parsing_errors_are_logged_and_tracked 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_parsing_errors_are_logged_and_tracked 
tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_parsing_errors_in_dict_syntax_are_logged_and_tracked 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_parsing_errors_in_dict_syntax_are_logged_and_tracked 
tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_valid_files_are_processed_normally 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_valid_files_are_processed_normally 
tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_multiple_parsing_errors_are_all_tracked 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_multiple_parsing_errors_are_all_tracked 
tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_logging_uses_correct_module_name 
[gw4] [ 68%] PASSED tests/unit/test_trap_10_silent_test_skip.py::TestTrap10SilentTestSkip::test_logging_uses_correct_module_name 
tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_validation_error_is_re_raised 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_validation_system_working 
[gw0] [ 68%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_validation_system_working 
[gw4] [ 68%] FAILED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_validation_error_is_re_raised 
tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_unexpected_error_is_logged_and_re_raised 
[gw3] [ 68%] PASSED tests/unit/test_pipeline_builder_simple.py::TestIntegration::test_pipeline_with_custom_validator 
[gw5] [ 68%] PASSED tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_gold_steps_storage 
[gw4] [ 68%] PASSED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_unexpected_error_is_logged_and_re_raised 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_writer_system_working 
tests/unit/test_pipeline_builder_simple.py::TestIntegration::test_pipeline_with_multiple_schemas 
tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_pipeline_configuration 
tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_successful_assessment_returns_correct_metrics 
[gw0] [ 69%] FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_writer_system_working 
[gw4] [ 69%] FAILED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_successful_assessment_returns_correct_metrics 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_models_working 
[gw8] [ 69%] PASSED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_concurrent_writes 
[gw0] [ 69%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_models_working 
[gw3] [ 69%] PASSED tests/unit/test_pipeline_builder_simple.py::TestIntegration::test_pipeline_with_multiple_schemas 
tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_successful_assessment_with_rules_returns_correct_metrics 
[gw5] [ 69%] PASSED tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_pipeline_configuration 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_incremental_sets_expected_write_modes 
tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_pipeline_logger 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_logging_system_working 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_performance_characteristics 
[gw0] [ 69%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_logging_system_working 
[gw4] [ 69%] FAILED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_successful_assessment_with_rules_returns_correct_metrics 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_performance_system_working 
[gw0] [ 69%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_performance_system_working 
tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_empty_dataframe_returns_correct_metrics 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_table_operations_working 
[gw4] [ 69%] FAILED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_empty_dataframe_returns_correct_metrics 
[gw0] [ 69%] FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_table_operations_working 
[gw5] [ 69%] PASSED tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_pipeline_logger 
tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_no_fallback_response_for_errors 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_error_handling_working 
tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_pipeline_id_generation 
[gw4] [ 69%] PASSED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_no_fallback_response_for_errors 
[gw0] [ 69%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_error_handling_working 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_validation_utils_working 
tests/unit/test_trap_2_missing_object_creation.py::TestTrap2MissingObjectCreation::test_execution_engine_creation_in_to_pipeline 
[gw0] [ 69%] FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_validation_utils_working 
[gw4] [ 69%] PASSED tests/unit/test_trap_2_missing_object_creation.py::TestTrap2MissingObjectCreation::test_execution_engine_creation_in_to_pipeline 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_pipeline_validation_working 
tests/unit/test_trap_2_missing_object_creation.py::TestTrap2MissingObjectCreation::test_objects_are_not_garbage_collected 
[gw0] [ 69%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_pipeline_validation_working 
[gw5] [ 69%] PASSED tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_pipeline_id_generation 
[gw4] [ 70%] PASSED tests/unit/test_trap_2_missing_object_creation.py::TestTrap2MissingObjectCreation::test_objects_are_not_garbage_collected 
tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_schema_property 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_edge_cases_working 
tests/unit/test_trap_2_missing_object_creation.py::TestTrap2MissingObjectCreation::test_pipeline_validation_before_object_creation 
[gw4] [ 70%] PASSED tests/unit/test_trap_2_missing_object_creation.py::TestTrap2MissingObjectCreation::test_pipeline_validation_before_object_creation 
[gw0] [ 70%] FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_edge_cases_working 
tests/unit/test_trap_2_missing_object_creation.py::TestTrap2MissingObjectCreation::test_objects_are_accessible_after_creation 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_step_result_validation_working 
[gw5] [ 70%] PASSED tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_schema_property 
[gw4] [ 70%] PASSED tests/unit/test_trap_2_missing_object_creation.py::TestTrap2MissingObjectCreation::test_objects_are_accessible_after_creation 
[gw0] [ 70%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_step_result_validation_working 
tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_validators_property 
tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_uses_actual_step_type 
[gw4] [ 70%] PASSED tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_uses_actual_step_type 
tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_uses_actual_table_info 
[gw4] [ 70%] PASSED tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_uses_actual_table_info 
tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_uses_actual_input_rows 
[gw4] [ 70%] PASSED tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_uses_actual_input_rows 
tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_no_silent_fallback_to_default_schema 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_stage_stats_validation_working 
[gw0] [ 70%] PASSED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_stage_stats_validation_working 
[gw4] [ 70%] PASSED tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_no_silent_fallback_to_default_schema 
tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_comprehensive_coverage_working 
[gw5] [ 70%] PASSED tests/unit/test_pipeline_builder_basic.py::TestStepManagement::test_validators_property 
[gw0] [ 70%] FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_comprehensive_coverage_working 
tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_fallback_for_missing_data 
[gw5] [ 70%] PASSED tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_fallback_for_missing_data 
tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_validation_metrics_are_calculated_correctly 
[gw5] [ 70%] PASSED tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_validation_metrics_are_calculated_correctly 
tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_uses_actual_execution_context_data 
[gw5] [ 70%] PASSED tests/unit/test_trap_3_hardcoded_fallback_values.py::TestTrap3HardcodedFallbackValues::test_log_row_uses_actual_execution_context_data 
tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_core_writer_raises_specific_exceptions 
tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_validation_mode_skips_schema_validation 
tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_helpful_error_message_format 
[gw0] [ 70%] PASSED tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_helpful_error_message_format 
tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_isolated_session_helpful_error_message_format 
[gw0] [ 70%] PASSED tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_isolated_session_helpful_error_message_format 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_start_operation_raises_specific_exception_on_failure 
[gw0] [ 71%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_start_operation_raises_specific_exception_on_failure 
[gw4] [ 71%] FAILED tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_validation_mode_skips_schema_validation 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_end_operation_raises_specific_exception_on_failure 
tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_execution_engine_rules_check_without_hasattr 
[gw0] [ 71%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_end_operation_raises_specific_exception_on_failure 
[gw5] [ 71%] PASSED tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_core_writer_raises_specific_exceptions 
[gw4] [ 71%] PASSED tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_execution_engine_rules_check_without_hasattr 
tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_dependency_analyzer_source_bronze_without_hasattr 
[gw4] [ 71%] PASSED tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_dependency_analyzer_source_bronze_without_hasattr 
tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_dependency_analyzer_source_silvers_without_hasattr 
[gw4] [ 71%] PASSED tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_dependency_analyzer_source_silvers_without_hasattr 
tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_pipeline_validator_dependencies_hasattr_improved 
[gw4] [ 71%] PASSED tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_pipeline_validator_dependencies_hasattr_improved 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_check_performance_thresholds_raises_specific_exception_on_failure 
tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_logging_context_removes_redundant_hasattr 
[gw4] [ 71%] PASSED tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_logging_context_removes_redundant_hasattr 
tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_base_model_to_dict_keeps_appropriate_hasattr 
[gw4] [ 71%] PASSED tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_base_model_to_dict_keeps_appropriate_hasattr 
tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_execution_context_mode_handling_keeps_appropriate_hasattr 
[gw4] [ 71%] PASSED tests/unit/test_trap_6_hasattr_checks.py::TestTrap6HasattrChecks::test_execution_context_mode_handling_keeps_appropriate_hasattr 
tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_environment_variable_parsing 
[gw4] [ 71%] PASSED tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_environment_variable_parsing 
tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_environment_variable_parsing_basic_spark 
[gw4] [ 71%] PASSED tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_environment_variable_parsing_basic_spark 
tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_error_message_contains_helpful_guidance 
[gw4] [ 71%] PASSED tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_error_message_contains_helpful_guidance 
tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_isolated_session_error_message_contains_helpful_guidance 
[gw4] [ 71%] PASSED tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_isolated_session_error_message_contains_helpful_guidance 
tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_environment_variable_combination_logic 
[gw4] [ 71%] PASSED tests/unit/test_trap_7_silent_fallback_test_config.py::TestTrap7SilentFallbackTestConfig::test_environment_variable_combination_logic 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_execution_engine_context_validation 
tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_core_writer_analytics_raises_specific_exceptions 
[gw0] [ 71%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_check_performance_thresholds_raises_specific_exception_on_failure 
[gw4] [ 71%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_execution_engine_context_validation 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_get_memory_usage_raises_specific_exception_on_failure 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_execution_engine_context_type_validation 
[gw0] [ 72%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_get_memory_usage_raises_specific_exception_on_failure 
[gw4] [ 72%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_execution_engine_context_type_validation 
[gw5] [ 72%] PASSED tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_core_writer_analytics_raises_specific_exceptions 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_analyze_execution_trends_raises_specific_exception_on_failure 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_run_id_handling 
[gw1] [ 72%] PASSED tests/builder_pyspark_tests/test_marketing_pipeline.py::TestMarketingPipeline::test_complete_marketing_pipeline_execution 
tests/builder_pyspark_tests/test_marketing_pipeline.py::TestMarketingPipeline::test_incremental_marketing_processing 
[gw0] [ 72%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_analyze_execution_trends_raises_specific_exception_on_failure 
tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_storage_manager_raises_specific_exceptions 
[gw4] [ 72%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_run_id_handling 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_batch_run_ids_handling 
[gw2] [ 72%] PASSED tests/builder_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_complete_supply_chain_pipeline_execution 
tests/builder_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_incremental_supply_chain_processing 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_detect_anomalies_raises_specific_exception_on_failure 
[gw4] [ 72%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_batch_run_ids_handling 
[gw0] [ 72%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_detect_anomalies_raises_specific_exception_on_failure 
[gw8] [ 72%] PASSED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_performance_characteristics 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_display_limit_handling 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_generate_performance_report_raises_specific_exception_on_failure 
[gw4] [ 72%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_display_limit_handling 
[gw5] [ 72%] PASSED tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_storage_manager_raises_specific_exceptions 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_logger_initialization_explicit_none 
[gw0] [ 72%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_generate_performance_report_raises_specific_exception_on_failure 
tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_analytics_raises_specific_exceptions 
tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_data_quality_constraints 
[gw4] [ 72%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_logger_initialization_explicit_none 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_error_chaining_preserves_original_exception 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_logger_initialization_with_logger 
[gw0] [ 72%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_error_chaining_preserves_original_exception 
[gw4] [ 72%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_logger_initialization_with_logger 
tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_logging_still_occurs_before_exception_raising 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_step_result_step_type_handling 
[gw4] [ 72%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_step_result_step_type_handling 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_step_result_step_type_with_value 
[gw4] [ 72%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_step_result_step_type_with_value 
tests/unit/test_types.py::TestTypeAliases::test_string_type_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestTypeAliases::test_string_type_aliases 
tests/unit/test_types.py::TestTypeAliases::test_numeric_type_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestTypeAliases::test_numeric_type_aliases 
tests/unit/test_types.py::TestTypeAliases::test_dictionary_type_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestTypeAliases::test_dictionary_type_aliases 
tests/unit/test_types.py::TestTypeAliases::test_optional_type_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestTypeAliases::test_optional_type_aliases 
tests/unit/test_types.py::TestEnums::test_step_type_enum 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestEnums::test_step_type_enum 
tests/unit/test_types.py::TestEnums::test_step_status_enum 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestEnums::test_step_status_enum 
tests/unit/test_types.py::TestEnums::test_pipeline_mode_enum 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestEnums::test_pipeline_mode_enum 
tests/unit/test_types.py::TestFunctionTypes::test_transform_function_types 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestFunctionTypes::test_transform_function_types 
tests/unit/test_types.py::TestFunctionTypes::test_filter_function_type 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestFunctionTypes::test_filter_function_type 
tests/unit/test_types.py::TestDataTypeAliases::test_column_rules_type 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestDataTypeAliases::test_column_rules_type 
tests/unit/test_types.py::TestDataTypeAliases::test_result_type_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestDataTypeAliases::test_result_type_aliases 
tests/unit/test_types.py::TestDataTypeAliases::test_context_type_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestDataTypeAliases::test_context_type_aliases 
tests/unit/test_types.py::TestDataTypeAliases::test_config_type_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestDataTypeAliases::test_config_type_aliases 
tests/unit/test_types.py::TestDataTypeAliases::test_quality_thresholds_type 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestDataTypeAliases::test_quality_thresholds_type 
tests/unit/test_types.py::TestDataTypeAliases::test_error_type_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestDataTypeAliases::test_error_type_aliases 
tests/unit/test_types.py::TestProtocols::test_validatable_protocol 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestProtocols::test_validatable_protocol 
tests/unit/test_types.py::TestProtocols::test_serializable_protocol 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestProtocols::test_serializable_protocol 
tests/unit/test_types.py::TestBackwardCompatibility::test_backward_compatibility_aliases 
[gw4] [ 73%] PASSED tests/unit/test_types.py::TestBackwardCompatibility::test_backward_compatibility_aliases 
tests/unit/test_types.py::TestTypeUsage::test_pipeline_configuration_usage 
[gw4] [ 74%] PASSED tests/unit/test_types.py::TestTypeUsage::test_pipeline_configuration_usage 
tests/unit/test_types.py::TestTypeUsage::test_step_result_usage 
[gw4] [ 74%] PASSED tests/unit/test_types.py::TestTypeUsage::test_step_result_usage 
tests/unit/test_types.py::TestTypeUsage::test_validation_context_usage 
[gw4] [ 74%] PASSED tests/unit/test_types.py::TestTypeUsage::test_validation_context_usage 
tests/unit/test_types.py::TestTypeUsage::test_error_handling_usage 
[gw4] [ 74%] PASSED tests/unit/test_types.py::TestTypeUsage::test_error_handling_usage 
tests/unit/test_validation.py::TestAndAllRules::test_empty_rules 
[gw0] [ 74%] PASSED tests/unit/test_trap_8_generic_error_handling_performance.py::TestTrap8GenericErrorHandlingPerformance::test_logging_still_occurs_before_exception_raising 
[gw4] [ 74%] PASSED tests/unit/test_validation.py::TestAndAllRules::test_empty_rules 
[gw5] [ 74%] PASSED tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_analytics_raises_specific_exceptions 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_writer_config_generate_table_name_requires_parameters 
[gw0] [ 74%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_writer_config_generate_table_name_requires_parameters 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_writer_config_generate_table_name_with_pattern_requires_parameters 
[gw0] [ 74%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_writer_config_generate_table_name_with_pattern_requires_parameters 
tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_writer_config_generate_table_name_without_patterns_works_with_none 
[gw0] [ 74%] PASSED tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_writer_config_generate_table_name_without_patterns_works_with_none 
tests/unit/test_validation.py::TestGetDataframeInfo::test_basic_info 
tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_monitoring_raises_specific_exceptions 
tests/unit/test_validation.py::TestAndAllRules::test_single_rule 
[gw4] [ 74%] PASSED tests/unit/test_validation.py::TestAndAllRules::test_single_rule 
[gw0] [ 74%] ERROR tests/unit/test_validation.py::TestGetDataframeInfo::test_basic_info 
tests/unit/test_validation.py::TestAndAllRules::test_multiple_rules 
[gw5] [ 74%] PASSED tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_monitoring_raises_specific_exceptions 
tests/unit/test_validation.py::TestGetDataframeInfo::test_empty_dataframe 
[gw3] [ 74%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_incremental_sets_expected_write_modes 
tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_exception_chaining_preserves_original_error 
[gw4] [ 74%] PASSED tests/unit/test_validation.py::TestAndAllRules::test_multiple_rules 
[gw0] [ 74%] FAILED tests/unit/test_validation.py::TestGetDataframeInfo::test_empty_dataframe 
tests/unit/test_validation.py::TestValidateDataframeSchema::test_valid_schema 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_initial_load_uses_overwrite_mode 
tests/unit/test_validation.py::TestGetDataframeInfo::test_error_handling 
[gw5] [ 74%] PASSED tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_exception_chaining_preserves_original_error 
[gw4] [ 74%] ERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_valid_schema 
[gw0] [ 75%] PASSED tests/unit/test_validation.py::TestGetDataframeInfo::test_error_handling 
tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_no_generic_error_responses_returned 
tests/unit/test_validation.py::TestValidateDataframeSchema::test_missing_columns 
tests/unit/test_validation.py::TestApplyColumnRules::test_basic_validation 
[gw4] [ 75%] ERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_missing_columns 
[gw5] [ 75%] PASSED tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_no_generic_error_responses_returned 
[gw0] [ 75%] ERROR tests/unit/test_validation.py::TestApplyColumnRules::test_basic_validation 
tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_error_logging_before_raising 
tests/unit/test_validation.py::TestValidateDataframeSchema::test_extra_columns 
tests/unit/test_validation.py::TestApplyColumnRules::test_none_rules_raises_error 
[gw4] [ 75%] ERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_extra_columns 
[gw0] [ 75%] ERROR tests/unit/test_validation.py::TestApplyColumnRules::test_none_rules_raises_error 
tests/unit/test_validation.py::TestValidateDataframeSchema::test_empty_expected_columns 
[gw5] [ 75%] PASSED tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_error_logging_before_raising 
tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_silver_step_without_schema_raises_error 
[gw4] [ 75%] ERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_empty_expected_columns 
tests/unit/test_validation.py::TestApplyColumnRules::test_empty_rules 
tests/unit/test_validation.py::TestSafeDivide::test_negative_numbers 
[gw0] [ 75%] ERROR tests/unit/test_validation.py::TestApplyColumnRules::test_empty_rules 
[gw1] [ 75%] PASSED tests/builder_pyspark_tests/test_marketing_pipeline.py::TestMarketingPipeline::test_incremental_marketing_processing 
tests/builder_pyspark_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_complete_multi_source_integration_pipeline_execution 
[gw4] [ 75%] PASSED tests/unit/test_validation.py::TestSafeDivide::test_negative_numbers 
tests/unit/test_validation.py::TestApplyColumnRules::test_complex_rules 
tests/unit/test_validation.py::TestSafeDivide::test_zero_numerator 
[gw0] [ 75%] ERROR tests/unit/test_validation.py::TestApplyColumnRules::test_complex_rules 
[gw2] [ 75%] PASSED tests/builder_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_incremental_supply_chain_processing 
tests/builder_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_supply_chain_logging 
[gw4] [ 75%] PASSED tests/unit/test_validation.py::TestSafeDivide::test_zero_numerator 
tests/unit/test_validation.py::TestSafeDivide::test_normal_division 
[gw0] [ 75%] PASSED tests/unit/test_validation.py::TestSafeDivide::test_normal_division 
tests/unit/test_validation.py::TestConvertRuleToExpression::test_not_null_rule 
tests/unit/test_validation.py::TestSafeDivide::test_division_by_zero 
[gw7] [ 75%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_components_workflow 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_reporting_integration 
[gw4] [ 75%] PASSED tests/unit/test_validation.py::TestConvertRuleToExpression::test_not_null_rule 
[gw0] [ 75%] PASSED tests/unit/test_validation.py::TestSafeDivide::test_division_by_zero 
tests/unit/test_validation.py::TestConvertRuleToExpression::test_positive_rule 
[gw5] [ 76%] PASSED tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_silver_step_without_schema_raises_error 
[gw4] [ 76%] PASSED tests/unit/test_validation.py::TestConvertRuleToExpression::test_positive_rule 
tests/unit/test_validation.py::TestSafeDivide::test_division_by_zero_custom_default 
tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_gold_step_without_schema_raises_error 
tests/unit/test_validation.py::TestConvertRuleToExpression::test_non_negative_rule 
[gw0] [ 76%] PASSED tests/unit/test_validation.py::TestSafeDivide::test_division_by_zero_custom_default 
[gw4] [ 76%] PASSED tests/unit/test_validation.py::TestConvertRuleToExpression::test_non_negative_rule 
tests/unit/test_validation.py::TestSafeDivide::test_float_division 
tests/unit/test_validation.py::TestConvertRuleToExpression::test_non_zero_rule 
[gw0] [ 76%] PASSED tests/unit/test_validation.py::TestSafeDivide::test_float_division 
[gw4] [ 76%] PASSED tests/unit/test_validation.py::TestConvertRuleToExpression::test_non_zero_rule 
tests/unit/test_validation.py::TestConvertRuleToExpression::test_custom_expression_rule 
tests/unit/test_validation.py::TestApplyValidationRules::test_apply_column_rules_basic 
[gw4] [ 76%] PASSED tests/unit/test_validation.py::TestConvertRuleToExpression::test_custom_expression_rule 
[gw5] [ 76%] PASSED tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_gold_step_without_schema_raises_error 
[gw1] [ 76%] PASSED tests/builder_pyspark_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_complete_multi_source_integration_pipeline_execution 
tests/builder_pyspark_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_schema_evolution_handling 
[gw0] [ 76%] ERROR tests/unit/test_validation.py::TestApplyValidationRules::test_apply_column_rules_basic 
tests/unit/test_validation.py::TestConvertRulesToExpressions::test_string_rules_conversion 
tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_silver_step_with_schema_works_correctly 
tests/unit/test_validation.py::TestApplyValidationRules::test_apply_column_rules_empty 
[gw9] [ 76%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestComplexDependencies::test_pipeline_complex_dependencies_with_logging 
[gw4] [ 76%] PASSED tests/unit/test_validation.py::TestConvertRulesToExpressions::test_string_rules_conversion 
tests/unit/test_validation.py::TestConvertRulesToExpressions::test_mixed_rules_conversion 
[gw0] [ 76%] ERROR tests/unit/test_validation.py::TestApplyValidationRules::test_apply_column_rules_empty 
[gw4] [ 76%] PASSED tests/unit/test_validation.py::TestConvertRulesToExpressions::test_mixed_rules_conversion 
tests/unit/test_validation.py::TestAssessDataQuality::test_basic_data_quality_assessment 
tests/system/test_full_pipeline_with_logging_variations.py::TestParallelStress::test_pipeline_parallel_execution_stress 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rule_to_expression_string_handling 
[gw8] [ 76%] PASSED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_data_quality_constraints 
[gw3] [ 76%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_initial_load_uses_overwrite_mode 
[gw0] [ 76%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rule_to_expression_string_handling 
[gw2] [ 76%] PASSED tests/builder_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_supply_chain_logging 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_engine_detection 
[gw2] [ 77%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_engine_detection 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_imports 
[gw2] [ 77%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_imports 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_dataframe_operations 
[gw4] [ 77%] ERROR tests/unit/test_validation.py::TestAssessDataQuality::test_basic_data_quality_assessment 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_empty_expressions 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_empty_expressions 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_no_column_expressions 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_no_column_expressions 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_validation_predicate_true 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_validation_predicate_true 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_with_rules 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_full_refresh_uses_overwrite_mode 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_with_rules 
tests/system/test_full_pipeline_with_logging.py::TestFullPipelineWithLogging::test_full_pipeline_with_logging 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_safe_divide_edge_cases 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_safe_divide_edge_cases 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_validate_dataframe_schema_edge_cases 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_validate_dataframe_schema_edge_cases 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_get_dataframe_info_edge_cases 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_get_dataframe_info_edge_cases 
tests/unit/test_validation.py::TestAssessDataQuality::test_data_quality_with_rules 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_get_dataframe_info_error_handling 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_get_dataframe_info_error_handling 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_assess_data_quality_edge_cases 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_assess_data_quality_edge_cases 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_edge_cases 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_edge_cases 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rules_to_expressions_complex_cases 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rules_to_expressions_complex_cases 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rule_to_expression_edge_cases 
[gw2] [ 77%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_dataframe_operations 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_pipeline_building 
[gw4] [ 77%] ERROR tests/unit/test_validation.py::TestAssessDataQuality::test_data_quality_with_rules 
[gw1] [ 77%] PASSED tests/builder_pyspark_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_schema_evolution_handling 
tests/builder_pyspark_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_complex_dependency_handling 
[gw0] [ 77%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rule_to_expression_edge_cases 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_convert_rule_to_expression_with_mock_functions 
[gw2] [ 78%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_pipeline_building 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_validation 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_single_expression 
[gw4] [ 78%] PASSED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_convert_rule_to_expression_with_mock_functions 
[gw2] [ 78%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_validation 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_delta_lake_operations 
[gw0] [ 78%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_single_expression 
[gw2] [ 78%] SKIPPED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_delta_lake_operations 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_error_handling 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_convert_rule_to_expression_with_default_functions 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_multiple_expressions 
[gw0] [ 78%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_multiple_expressions 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_string_rule_conversion_edge_cases 
[gw4] [ 78%] PASSED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_convert_rule_to_expression_with_default_functions 
[gw0] [ 78%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_string_rule_conversion_edge_cases 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_convert_rules_to_expressions_with_mock_functions 
tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_validation_error_handling 
[gw0] [ 78%] PASSED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_validation_error_handling 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_error_handling_with_mock_functions 
[gw4] [ 78%] PASSED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_convert_rules_to_expressions_with_mock_functions 
[gw0] [ 78%] FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_error_handling_with_mock_functions 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_and_all_rules_with_mock_functions 
[gw2] [ 78%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_error_handling 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_performance_monitoring 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_performance_with_mock_functions 
[gw4] [ 78%] PASSED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_and_all_rules_with_mock_functions 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_apply_column_rules_with_mock_functions 
[gw0] [ 78%] FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_performance_with_mock_functions 
[gw4] [ 78%] FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_apply_column_rules_with_mock_functions 
tests/unit/test_validation_enhanced.py::TestPipelineBuilderWithFunctions::test_pipeline_builder_with_mock_functions 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_assess_data_quality_with_mock_functions 
[gw0] [ 78%] PASSED tests/unit/test_validation_enhanced.py::TestPipelineBuilderWithFunctions::test_pipeline_builder_with_mock_functions 
[gw1] [ 78%] PASSED tests/builder_pyspark_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_complex_dependency_handling 
tests/builder_pyspark_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_multi_source_logging 
[gw2] [ 78%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_performance_monitoring 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_table_operations 
tests/unit/test_validation_enhanced.py::TestPipelineBuilderWithFunctions::test_pipeline_builder_static_methods_with_mock_functions 
[gw4] [ 79%] FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_assess_data_quality_with_mock_functions 
[gw0] [ 79%] PASSED tests/unit/test_validation_enhanced.py::TestPipelineBuilderWithFunctions::test_pipeline_builder_static_methods_with_mock_functions 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validate_dataframe_schema_with_mock_functions 
tests/unit/test_validation_enhanced.py::TestPipelineBuilderWithFunctions::test_pipeline_builder_backward_compatibility 
[gw4] [ 79%] PASSED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validate_dataframe_schema_with_mock_functions 
[gw0] [ 79%] PASSED tests/unit/test_validation_enhanced.py::TestPipelineBuilderWithFunctions::test_pipeline_builder_backward_compatibility 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_functions_backward_compatibility 
tests/unit/test_validation_enhanced.py::TestPipelineBuilderWithFunctions::test_pipeline_builder_class_methods_with_mock_functions 
[gw0] [ 79%] PASSED tests/unit/test_validation_enhanced.py::TestPipelineBuilderWithFunctions::test_pipeline_builder_class_methods_with_mock_functions 
[gw4] [ 79%] PASSED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_functions_backward_compatibility 
[gw5] [ 79%] PASSED tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_silver_step_with_schema_works_correctly 
tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_mock_functions_behavior 
tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_gold_step_with_schema_works_correctly 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_mock_functions_protocol_compliance 
[gw0] [ 79%] PASSED tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_mock_functions_behavior 
[gw4] [ 79%] PASSED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_mock_functions_protocol_compliance 
[gw2] [ 79%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_table_operations 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkEngineSwitching::test_switch_to_pyspark 
[gw2] [ 79%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkEngineSwitching::test_switch_to_pyspark 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkEngineSwitching::test_switch_to_mock 
[gw2] [ 79%] SKIPPED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkEngineSwitching::test_switch_to_mock 
tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkEngineSwitching::test_auto_detection 
[gw2] [ 79%] PASSED tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkEngineSwitching::test_auto_detection 
tests/integration/test_execution_engine.py::TestExecutionMode::test_execution_mode_values 
tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_validation_with_mock_functions_end_to_end 
tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_with_complex_rules 
[gw4] [ 79%] FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_with_complex_rules 
[gw0] [ 79%] FAILED tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_validation_with_mock_functions_end_to_end 
[gw2] [ 79%] PASSED tests/integration/test_execution_engine.py::TestExecutionMode::test_execution_mode_values 
tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_mock_functions_performance 
[gw0] [ 79%] SKIPPED tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_mock_functions_performance 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_mock_functions_basic_operations 
tests/integration/test_execution_engine.py::TestExecutionMode::test_execution_mode_enumeration 
tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_mock_functions_error_handling 
[gw4] [ 79%] SKIPPED tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_mock_functions_error_handling 
tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_functions_protocol_type_checking 
[gw4] [ 80%] PASSED tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_functions_protocol_type_checking 
[gw1] [ 80%] PASSED tests/builder_pyspark_tests/test_multi_source_pipeline.py::TestMultiSourcePipeline::test_multi_source_logging 
[gw0] [ 80%] PASSED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_mock_functions_basic_operations 
[gw2] [ 80%] PASSED tests/integration/test_execution_engine.py::TestExecutionMode::test_execution_mode_enumeration 
tests/builder_pyspark_tests/test_simple.py::test_simple_pipeline_creation 
[gw1] [ 80%] PASSED tests/builder_pyspark_tests/test_simple.py::test_simple_pipeline_creation 
tests/builder_pyspark_tests/test_simple_pipeline.py::TestSimplePipeline::test_simple_pipeline_execution 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_convert_rule_to_expression_with_mock_functions 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_with_complex_rules 
tests/integration/test_execution_engine.py::TestStepStatus::test_step_status_values 
[gw2] [ 80%] PASSED tests/integration/test_execution_engine.py::TestStepStatus::test_step_status_values 
[gw4] [ 80%] PASSED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_convert_rule_to_expression_with_mock_functions 
[gw0] [ 80%] FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_with_complex_rules 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_convert_rules_to_expressions_with_mock_functions 
tests/integration/test_execution_engine.py::TestStepStatus::test_step_status_enumeration 
[gw2] [ 80%] PASSED tests/integration/test_execution_engine.py::TestStepStatus::test_step_status_enumeration 
[gw4] [ 80%] PASSED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_convert_rules_to_expressions_with_mock_functions 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_error_handling_with_mock_functions 
tests/integration/test_execution_engine.py::TestStepType::test_step_type_values 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_and_all_rules_with_mock_functions 
[gw2] [ 80%] PASSED tests/integration/test_execution_engine.py::TestStepType::test_step_type_values 
[gw0] [ 80%] FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_error_handling_with_mock_functions 
tests/integration/test_execution_engine.py::TestStepType::test_step_type_enumeration 
[gw4] [ 80%] PASSED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_and_all_rules_with_mock_functions 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_performance_with_mock_functions 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_apply_column_rules_with_mock_functions 
[gw2] [ 80%] PASSED tests/integration/test_execution_engine.py::TestStepType::test_step_type_enumeration 
tests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_creation 
[gw4] [ 80%] FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_apply_column_rules_with_mock_functions 
[gw0] [ 80%] FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_performance_with_mock_functions 
[gw2] [ 80%] PASSED tests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_creation 
tests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_with_all_fields 
tests/unit/test_validation_enhanced_simple.py::TestPipelineBuilderWithFunctionsSimple::test_pipeline_builder_with_mock_functions 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_assess_data_quality_with_mock_functions 
[gw2] [ 80%] PASSED tests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_with_all_fields 
[gw0] [ 81%] PASSED tests/unit/test_validation_enhanced_simple.py::TestPipelineBuilderWithFunctionsSimple::test_pipeline_builder_with_mock_functions 
[gw3] [ 81%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_full_refresh_uses_overwrite_mode 
[gw5] [ 81%] PASSED tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_gold_step_with_schema_works_correctly 
tests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_duration_calculation 
[gw4] [ 81%] FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_assess_data_quality_with_mock_functions 
[gw2] [ 81%] PASSED tests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_duration_calculation 
tests/unit/test_validation_enhanced_simple.py::TestPipelineBuilderWithFunctionsSimple::test_pipeline_builder_static_methods_with_mock_functions 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_pipeline_with_incremental_mode_sets_expected_write_modes 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validate_dataframe_schema_with_mock_functions 
[gw1] [ 81%] PASSED tests/builder_pyspark_tests/test_simple_pipeline.py::TestSimplePipeline::test_simple_pipeline_execution 
tests/builder_pyspark_tests/test_streaming_hybrid_pipeline.py::TestStreamingHybridPipeline::test_complete_streaming_hybrid_pipeline_execution 
[gw0] [ 81%] PASSED tests/unit/test_validation_enhanced_simple.py::TestPipelineBuilderWithFunctionsSimple::test_pipeline_builder_static_methods_with_mock_functions 
tests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_no_duration_without_end_time 
tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_pipeline_execution_logs_missing_schema_warnings 
[gw4] [ 81%] PASSED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validate_dataframe_schema_with_mock_functions 
tests/unit/test_validation_enhanced_simple.py::TestPipelineBuilderWithFunctionsSimple::test_pipeline_builder_backward_compatibility 
[gw2] [ 81%] PASSED tests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_no_duration_without_end_time 
tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_functions_backward_compatibility 
[gw0] [ 81%] PASSED tests/unit/test_validation_enhanced_simple.py::TestPipelineBuilderWithFunctionsSimple::test_pipeline_builder_backward_compatibility 
tests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_creation 
[gw4] [ 81%] PASSED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_functions_backward_compatibility 
tests/unit/test_validation_enhanced_simple.py::TestPipelineBuilderWithFunctionsSimple::test_pipeline_builder_class_methods_with_mock_functions 
[gw2] [ 81%] PASSED tests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_creation 
tests/unit/test_validation_mock.py::TestSafeDivide::test_none_numerator 
[gw0] [ 81%] PASSED tests/unit/test_validation_enhanced_simple.py::TestPipelineBuilderWithFunctionsSimple::test_pipeline_builder_class_methods_with_mock_functions 
tests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_with_all_fields 
[gw4] [ 81%] PASSED tests/unit/test_validation_mock.py::TestSafeDivide::test_none_numerator 
tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_non_zero_rule 
[gw2] [ 81%] PASSED tests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_with_all_fields 
tests/unit/test_validation_mock.py::TestSafeDivide::test_both_none 
tests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_duration_calculation 
[gw0] [ 81%] PASSED tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_non_zero_rule 
[gw2] [ 81%] PASSED tests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_duration_calculation 
[gw4] [ 81%] PASSED tests/unit/test_validation_mock.py::TestSafeDivide::test_both_none 
[gw5] [ 82%] PASSED tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_pipeline_execution_logs_missing_schema_warnings 
tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_custom_expression 
tests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_steps_initialization 
[gw0] [ 82%] PASSED tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_custom_expression 
tests/unit/test_validation_mock.py::TestGetDataframeInfo::test_basic_info 
[gw2] [ 82%] PASSED tests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_steps_initialization 
tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_mock_functions_behavior 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execution_engine_initialization_with_logger 
tests/unit/test_validation_mock.py::TestConvertRulesToExpressions::test_single_rule 
[gw4] [ 82%] FAILED tests/unit/test_validation_mock.py::TestGetDataframeInfo::test_basic_info 
[gw2] [ 82%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execution_engine_initialization_with_logger 
[gw0] [ 82%] PASSED tests/unit/test_validation_mock.py::TestConvertRulesToExpressions::test_single_rule 
tests/unit/test_validation_mock.py::TestGetDataframeInfo::test_empty_dataframe 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execution_engine_initialization_without_logger 
tests/unit/test_validation_mock.py::TestConvertRulesToExpressions::test_multiple_rules 
[gw5] [ 82%] PASSED tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_mock_functions_behavior 
[gw2] [ 82%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execution_engine_initialization_without_logger 
[gw0] [ 82%] PASSED tests/unit/test_validation_mock.py::TestConvertRulesToExpressions::test_multiple_rules 
[gw4] [ 82%] PASSED tests/unit/test_validation_mock.py::TestGetDataframeInfo::test_empty_dataframe 
tests/unit/test_validation_mock.py::TestConvertRulesToExpressions::test_empty_rules 
tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_validation_with_mock_functions_end_to_end 
tests/unit/test_validation_mock.py::TestGetDataframeInfo::test_error_handling 
[gw0] [ 82%] PASSED tests/unit/test_validation_mock.py::TestConvertRulesToExpressions::test_empty_rules 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_bronze_success 
[gw4] [ 82%] PASSED tests/unit/test_validation_mock.py::TestGetDataframeInfo::test_error_handling 
[gw2] [ 82%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_bronze_success 
tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_success 
tests/unit/test_validation_mock.py::TestAndAllRules::test_empty_rules 
tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_not_null_rule 
[gw0] [ 82%] PASSED tests/unit/test_validation_mock.py::TestAndAllRules::test_empty_rules 
[gw2] [ 82%] PASSED tests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_success 
tests/unit/test_validation_mock.py::TestAndAllRules::test_single_rule 
[gw4] [ 82%] PASSED tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_not_null_rule 
tests/unit/test_validation_mock.py::TestAndAllRules::test_multiple_rules 
[gw0] [ 82%] PASSED tests/unit/test_validation_mock.py::TestAndAllRules::test_single_rule 
tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_positive_rule 
[gw2] [ 82%] PASSED tests/unit/test_validation_mock.py::TestAndAllRules::test_multiple_rules 
tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_valid_schema 
[gw7] [ 83%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_reporting_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_alerting_integration 
tests/unit/test_validation_mock.py::TestApplyColumnRules::test_basic_validation 
[gw4] [ 83%] PASSED tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_positive_rule 
[gw0] [ 83%] PASSED tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_valid_schema 
tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_non_negative_rule 
tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_missing_columns 
[gw2] [ 83%] FAILED tests/unit/test_validation_mock.py::TestApplyColumnRules::test_basic_validation 
[gw4] [ 83%] PASSED tests/unit/test_validation_mock.py::TestConvertRuleToExpression::test_non_negative_rule 
[gw7] [ 83%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_alerting_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_metrics_integration 
[gw0] [ 83%] PASSED tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_missing_columns 
tests/unit/test_validation_mock.py::TestApplyColumnRules::test_multiple_columns 
tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_safe_divide_zero_denominator_properties 
[gw5] [ 83%] PASSED tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_validation_with_mock_functions_end_to_end 
tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_extra_columns 
[gw2] [ 83%] FAILED tests/unit/test_validation_mock.py::TestApplyColumnRules::test_multiple_columns 
tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_mock_functions_performance 
[gw5] [ 83%] SKIPPED tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_mock_functions_performance 
tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_mock_functions_error_handling 
[gw5] [ 83%] SKIPPED tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_mock_functions_error_handling 
tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_functions_protocol_compatibility 
[gw0] [ 83%] PASSED tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_extra_columns 
tests/unit/test_validation_mock.py::TestApplyColumnRules::test_empty_rules 
tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_empty_expected_columns 
[gw2] [ 83%] FAILED tests/unit/test_validation_mock.py::TestApplyColumnRules::test_empty_rules 
[gw7] [ 83%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_metrics_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_thresholds_integration 
[gw0] [ 83%] PASSED tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_empty_expected_columns 
[gw4] [ 83%] FAILED tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_safe_divide_zero_denominator_properties 
tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_validate_dataframe_schema_properties 
[gw5] [ 83%] PASSED tests/unit/test_validation_enhanced_simple.py::TestFunctionsIntegrationSimple::test_functions_protocol_compatibility 
tests/unit/test_validation_mock.py::TestAssessDataQuality::test_basic_quality_assessment 
tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_none_dataframe 
[gw4] [ 83%] PASSED tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_validate_dataframe_schema_properties 
tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_dataframe_schema_edge_cases_properties 
tests/unit/test_validation_mock.py::TestSafeDivide::test_normal_division 
[gw4] [ 84%] PASSED tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_dataframe_schema_edge_cases_properties 
tests/unit/test_validation_simple.py::TestValidationUtils::test_safe_divide_normal 
[gw2] [ 84%] FAILED tests/unit/test_validation_mock.py::TestAssessDataQuality::test_basic_quality_assessment 
[gw0] [ 84%] PASSED tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_none_dataframe 
[gw4] [ 84%] PASSED tests/unit/test_validation_simple.py::TestValidationUtils::test_safe_divide_normal 
tests/unit/test_validation_mock.py::TestAssessDataQuality::test_multiple_quality_rules 
tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_none_expected_columns 
tests/unit/test_validation_simple.py::TestValidationUtils::test_safe_divide_by_zero 
[gw5] [ 84%] PASSED tests/unit/test_validation_mock.py::TestSafeDivide::test_normal_division 
[gw4] [ 84%] PASSED tests/unit/test_validation_simple.py::TestValidationUtils::test_safe_divide_by_zero 
[gw7] [ 84%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_thresholds_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_configuration_integration 
[gw7] [ 84%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_configuration_integration 
tests/security/test_security_integration.py::TestSecurityIntegration::test_security_performance_integration 
[gw2] [ 84%] FAILED tests/unit/test_validation_mock.py::TestAssessDataQuality::test_multiple_quality_rules 
[gw0] [ 84%] PASSED tests/unit/test_validation_mock.py::TestValidateDataframeSchema::test_none_expected_columns 
tests/unit/test_validation_simple.py::TestValidationUtils::test_safe_divide_none_values 
tests/unit/test_validation_mock.py::TestAssessDataQuality::test_empty_rules 
tests/unit/test_validation_mock.py::TestSafeDivide::test_division_by_zero 
tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_safe_divide_properties 
[gw3] [ 84%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_pipeline_with_incremental_mode_sets_expected_write_modes 
[gw4] [ 84%] PASSED tests/unit/test_validation_simple.py::TestValidationUtils::test_safe_divide_none_values 
[gw2] [ 84%] FAILED tests/unit/test_validation_mock.py::TestAssessDataQuality::test_empty_rules 
[gw0] [ 84%] PASSED tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_safe_divide_properties 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_invalid_config 
[gw0] [ 84%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_invalid_config 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_initialization 
[gw5] [ 84%] PASSED tests/unit/test_validation_mock.py::TestSafeDivide::test_division_by_zero 
tests/unit/test_validation_simple.py::TestValidationUtils::test_safe_divide_edge_cases 
tests/unit/test_validation_simple.py::TestValidationErrorHandling::test_validation_error_creation 
tests/unit/test_validation_mock.py::TestSafeDivide::test_division_by_none 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_pipeline_with_initial_mode_uses_overwrite 
[gw2] [ 84%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_initialization 
[gw4] [ 84%] PASSED tests/unit/test_validation_simple.py::TestValidationUtils::test_safe_divide_edge_cases 
[gw0] [ 85%] PASSED tests/unit/test_validation_simple.py::TestValidationErrorHandling::test_validation_error_creation 
tests/unit/test_validation_simple.py::TestValidationErrorHandling::test_validation_error_with_context 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_add_validator 
tests/unit/test_validation_simple.py::TestValidationUtils::test_get_dataframe_info 
[gw0] [ 85%] PASSED tests/unit/test_validation_simple.py::TestValidationErrorHandling::test_validation_error_with_context 
[gw5] [ 85%] PASSED tests/unit/test_validation_mock.py::TestSafeDivide::test_division_by_none 
[gw2] [ 85%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_add_validator 
tests/unit/test_validation_simple.py::TestValidationErrorHandling::test_validation_error_attributes 
tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_with_pipeline_config 
[gw4] [ 85%] FAILED tests/unit/test_validation_simple.py::TestValidationUtils::test_get_dataframe_info 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_pipeline_validation 
[gw0] [ 85%] PASSED tests/unit/test_validation_simple.py::TestValidationErrorHandling::test_validation_error_attributes 
[gw2] [ 85%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_pipeline_validation 
tests/unit/test_validation_simple.py::TestValidationUtils::test_get_dataframe_info_empty 
tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_workflow_with_mock_data 
[gw4] [ 85%] PASSED tests/unit/test_validation_simple.py::TestValidationUtils::test_get_dataframe_info_empty 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_step_validation 
[gw5] [ 85%] PASSED tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_with_pipeline_config 
[gw2] [ 85%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_step_validation 
tests/unit/test_validation_simple.py::TestValidationUtils::test_get_dataframe_info_error_handling 
[gw0] [ 85%] FAILED tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_workflow_with_mock_data 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_custom_validators 
[gw4] [ 85%] PASSED tests/unit/test_validation_simple.py::TestValidationUtils::test_get_dataframe_info_error_handling 
tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_with_complex_pipeline 
[gw2] [ 85%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_custom_validators 
tests/unit/test_validation_standalone.py::TestSafeDivide::test_normal_division_float 
[gw0] [ 85%] PASSED tests/unit/test_validation_standalone.py::TestSafeDivide::test_normal_division_float 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_validation_result_creation 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_empty_pipeline 
tests/unit/test_validation_standalone.py::TestSafeDivide::test_division_with_default 
[gw5] [ 85%] PASSED tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_with_complex_pipeline 
[gw4] [ 85%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_validation_result_creation 
[gw2] [ 85%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_unified_validator_empty_pipeline 
[gw0] [ 85%] PASSED tests/unit/test_validation_standalone.py::TestSafeDivide::test_division_with_default 
tests/unit/test_validation_standalone.py::TestGetDataframeInfo::test_basic_info 
tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_error_scenarios 
tests/unit/test_validation_simple.py::TestPipelineValidation::test_validation_result_false 
tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_not_null_rule 
[gw0] [ 86%] FAILED tests/unit/test_validation_standalone.py::TestGetDataframeInfo::test_basic_info 
[gw4] [ 86%] PASSED tests/unit/test_validation_simple.py::TestPipelineValidation::test_validation_result_false 
[gw2] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_not_null_rule 
[gw5] [ 86%] PASSED tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_error_scenarios 
tests/unit/test_validation_standalone.py::TestConvertRulesToExpressions::test_single_rule 
tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_positive_rule 
tests/unit/test_validation_standalone.py::TestGetDataframeInfo::test_empty_dataframe 
tests/unit/test_validation_standalone.py::TestSafeDivide::test_normal_division 
[gw4] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestConvertRulesToExpressions::test_single_rule 
[gw2] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_positive_rule 
[gw0] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestGetDataframeInfo::test_empty_dataframe 
tests/unit/test_validation_standalone.py::TestConvertRulesToExpressions::test_multiple_rules 
tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_non_negative_rule 
tests/unit/test_validation_standalone.py::TestGetDataframeInfo::test_error_handling 
[gw5] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestSafeDivide::test_normal_division 
[gw2] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_non_negative_rule 
[gw4] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestConvertRulesToExpressions::test_multiple_rules 
[gw0] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestGetDataframeInfo::test_error_handling 
tests/unit/test_validation_standalone.py::TestSafeDivide::test_division_by_zero 
tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_non_zero_rule 
tests/unit/test_validation_standalone.py::TestConvertRulesToExpressions::test_empty_rules 
tests/unit/test_validation_standalone.py::TestAndAllRules::test_single_rule 
[gw2] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_non_zero_rule 
[gw4] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestConvertRulesToExpressions::test_empty_rules 
[gw0] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestAndAllRules::test_single_rule 
tests/unit/test_validation_standalone.py::TestAndAllRules::test_empty_rules 
tests/unit/test_validation_standalone.py::TestAndAllRules::test_multiple_rules 
[gw5] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestSafeDivide::test_division_by_zero 
[gw4] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestAndAllRules::test_empty_rules 
tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_custom_expression 
[gw0] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestAndAllRules::test_multiple_rules 
tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_valid_schema 
[gw2] [ 86%] PASSED tests/unit/test_validation_standalone.py::TestConvertRuleToExpression::test_custom_expression 
tests/unit/test_validation_standalone.py::TestApplyColumnRules::test_empty_rules 
[gw4] [ 87%] PASSED tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_valid_schema 
tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_none_dataframe 
tests/unit/test_validation_standalone.py::TestApplyColumnRules::test_basic_validation 
[gw2] [ 87%] PASSED tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_none_dataframe 
tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_missing_columns 
[gw0] [ 87%] FAILED tests/unit/test_validation_standalone.py::TestApplyColumnRules::test_basic_validation 
tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_none_expected_columns 
[gw4] [ 87%] PASSED tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_missing_columns 
[gw2] [ 87%] PASSED tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_none_expected_columns 
tests/unit/test_validation_standalone.py::TestApplyColumnRules::test_multiple_columns 
tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_extra_columns 
tests/unit/test_working_examples.py::TestWorkingExamples::test_pipeline_builder_basic 
[gw4] [ 87%] PASSED tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_extra_columns 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_pipeline_builder_basic 
[gw0] [ 87%] FAILED tests/unit/test_validation_standalone.py::TestApplyColumnRules::test_multiple_columns 
tests/unit/test_working_examples.py::TestWorkingExamples::test_validation_result_creation 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_validation_result_creation 
tests/unit/test_working_examples.py::TestWorkingExamples::test_validation_result_with_errors 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_validation_result_with_errors 
tests/unit/test_working_examples.py::TestWorkingExamples::test_log_writer_with_config 
tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_empty_expected_columns 
tests/unit/test_working_examples.py::TestWorkingExamples::test_pipeline_builder_with_quality_rates 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_log_writer_with_config 
[gw0] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_pipeline_builder_with_quality_rates 
[gw4] [ 87%] PASSED tests/unit/test_validation_standalone.py::TestValidateDataframeSchema::test_empty_expected_columns 
tests/unit/test_working_examples.py::TestWorkingExamples::test_execution_engine_with_config 
tests/unit/test_working_examples.py::TestWorkingExamples::test_enum_values 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_enum_values 
tests/unit/test_working_examples.py::TestWorkingExamples::test_step_execution_result_creation 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_step_execution_result_creation 
tests/unit/test_working_examples.py::TestWorkingExamples::test_execution_result_creation 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_execution_result_creation 
tests/unit/test_working_examples.py::TestWorkingExamples::test_validation_thresholds_creation 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_validation_thresholds_creation 
tests/unit/test_working_examples.py::TestWorkingExamples::test_parallel_config_creation 
[gw2] [ 87%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_parallel_config_creation 
tests/unit/test_working_examples.py::TestWorkingExamples::test_mock_spark_integration 
[gw2] [ 88%] SKIPPED tests/unit/test_working_examples.py::TestWorkingExamples::test_mock_spark_integration 
tests/unit/test_working_examples.py::TestWorkingExamples::test_error_handling 
[gw2] [ 88%] SKIPPED tests/unit/test_working_examples.py::TestWorkingExamples::test_error_handling 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_table_exists_function 
tests/unit/test_working_examples.py::TestWorkingExamples::test_writer_config_creation 
[gw4] [ 88%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_writer_config_creation 
tests/unit/test_working_examples.py::TestWorkingExamples::test_writer_config_with_custom_values 
[gw4] [ 88%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_writer_config_with_custom_values 
tests/unit/test_working_examples.py::TestWorkingExamples::test_pipeline_config_creation 
[gw4] [ 88%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_pipeline_config_creation 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_config_creation 
[gw0] [ 88%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_execution_engine_with_config 
[gw5] [ 88%] PASSED tests/unit/test_validation_standalone.py::TestApplyColumnRules::test_empty_rules 
[gw4] [ 88%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_config_creation 
tests/unit/test_working_examples.py::TestWorkingExamples::test_unified_validator_basic 
[gw2] [ 88%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_table_exists_function 
tests/unit/test_validation_standalone.py::TestAssessDataQuality::test_basic_quality_assessment 
[gw0] [ 88%] PASSED tests/unit/test_working_examples.py::TestWorkingExamples::test_unified_validator_basic 
[gw3] [ 88%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_run_pipeline_with_initial_mode_uses_overwrite 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_config_defaults 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_time_write_operation_function 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_initialization 
[gw4] [ 88%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_config_defaults 
[gw2] [ 88%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_time_write_operation_function 
[gw0] [ 88%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_initialization 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_invalid_config 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result_with_metadata 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_pipeline_mode_mapping_to_execution_mode 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_with_custom_logger 
[gw3] [ 88%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_pipeline_mode_mapping_to_execution_mode 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_incremental_vs_initial_write_mode_difference 
[gw4] [ 88%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_invalid_config 
[gw0] [ 88%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_with_custom_logger 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_log_rows 
[gw2] [ 88%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result_with_metadata 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_step_results 
[gw7] [ 89%] PASSED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_performance_integration 
tests/security/test_security_integration.py::TestSecurityMarkers::test_security_marker_works 
[gw4] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result 
[gw7] [ 89%] PASSED tests/security/test_security_integration.py::TestSecurityMarkers::test_security_marker_works 
tests/security/test_security_integration.py::TestSecurityMarkers::test_slow_security_test 
[gw0] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_log_rows 
[gw7] [ 89%] PASSED tests/security/test_security_integration.py::TestSecurityMarkers::test_slow_security_test 
tests/security/test_security_integration.py::test_security_cicd_integration 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_metrics_tracking 
[gw5] [ 89%] PASSED tests/unit/test_validation_standalone.py::TestAssessDataQuality::test_basic_quality_assessment 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result_batch 
[gw2] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_step_results 
tests/unit/test_validation_standalone.py::TestAssessDataQuality::test_multiple_quality_rules 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_different_log_levels 
[gw4] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_metrics_tracking 
[gw0] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result_batch 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_different_write_modes 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_compression_settings 
[gw2] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_different_log_levels 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_custom_batch_size 
[gw4] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_different_write_modes 
[gw0] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_compression_settings 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_schema_evolution_settings 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_partition_settings 
[gw2] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_custom_batch_size 
[gw4] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_schema_evolution_settings 
[gw7] [ 89%] PASSED tests/security/test_security_integration.py::test_security_cicd_integration 
tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_auto_infer_single_bronze_step 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_components_initialization 
[gw0] [ 89%] FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_partition_settings 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_error_handling 
[gw5] [ 89%] PASSED tests/unit/test_validation_standalone.py::TestAssessDataQuality::test_multiple_quality_rules 
[gw2] [ 89%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_components_initialization 
[gw4] [ 90%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_error_handling 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_table_fqn 
tests/unit/test_validation_standalone.py::TestAssessDataQuality::test_empty_rules 
tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_schema_creation 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_initialization 
[gw7] [ 90%] PASSED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_auto_infer_single_bronze_step 
[gw0] [ 90%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_table_fqn 
[gw4] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_initialization 
tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_auto_infer_multiple_bronze_steps 
[gw2] [ 90%] PASSED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_schema_creation 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_get_spark 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_initialization_with_config 
[gw2] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_get_spark 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_get_config 
[gw0] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_initialization_with_config 
[gw4] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_get_config 
[gw7] [ 90%] PASSED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_auto_infer_multiple_bronze_steps 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_table_exists_function 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_table_exists_function_invalid_parameters 
tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_explicit_source_bronze_still_works 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_write_mode_enum 
[gw0] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_table_exists_function_invalid_parameters 
[gw2] [ 90%] FAILED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_table_exists_function 
[gw4] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_write_mode_enum 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_level_enum 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_writer_config_creation 
[gw7] [ 90%] PASSED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_explicit_source_bronze_still_works 
[gw2] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_level_enum 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_writer_config_default_values 
[gw0] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_writer_config_creation 
tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_no_bronze_steps_raises_error 
[gw5] [ 90%] PASSED tests/unit/test_validation_standalone.py::TestAssessDataQuality::test_empty_rules 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_with_sample_data 
[gw4] [ 90%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_writer_config_default_values 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_error_handling 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_metrics_collection 
tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_invalid_spark_session 
[gw2] [ 90%] FAILED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_with_sample_data 
[gw7] [ 91%] PASSED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_no_bronze_steps_raises_error 
[gw0] [ 91%] FAILED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_error_handling 
[gw4] [ 91%] FAILED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_metrics_collection 
tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_invalid_source_bronze_raises_error 
tests/unit/writer/test_core.py::TestLogWriter::test_init_valid_config 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_init_valid_config 
tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_invalid_input 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_invalid_input 
tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_validation_failure 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_validation_failure 
tests/unit/writer/test_core.py::TestLogWriter::test_write_step_results 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_step_results 
tests/unit/writer/test_core.py::TestLogWriter::test_write_log_rows_success 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_log_rows_success 
tests/unit/writer/test_core.py::TestLogWriter::test_write_log_rows_validation_failure 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_log_rows_validation_failure 
tests/unit/writer/test_core.py::TestLogWriter::test_get_metrics 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_get_metrics 
tests/unit/writer/test_core.py::TestLogWriter::test_reset_metrics 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_reset_metrics 
tests/unit/writer/test_core.py::TestLogWriter::test_show_logs 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_show_logs 
tests/unit/writer/test_core.py::TestLogWriter::test_show_logs_no_limit 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_show_logs_no_limit 
tests/unit/writer/test_core.py::TestLogWriter::test_get_table_info 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_get_table_info 
tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_batch_success 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_batch_success 
tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_batch_with_failures 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_batch_with_failures 
tests/unit/writer/test_core.py::TestLogWriter::test_write_log_rows_batch 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_log_rows_batch 
tests/unit/writer/test_core.py::TestLogWriter::test_get_memory_usage_success 
[gw2] [ 91%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_get_memory_usage_success 
tests/unit/writer/test_core.py::TestLogWriter::test_get_memory_usage_psutil_not_available 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_get_memory_usage_psutil_not_available 
tests/unit/writer/test_core.py::TestLogWriter::test_get_memory_usage_exception 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_get_memory_usage_exception 
tests/unit/writer/test_core.py::TestLogWriter::test_validate_log_data_quality_success 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_validate_log_data_quality_success 
tests/unit/writer/test_core.py::TestLogWriter::test_validate_log_data_quality_failure 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_validate_log_data_quality_failure 
tests/unit/writer/test_core.py::TestLogWriter::test_detect_anomalies_success 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_detect_anomalies_success 
tests/unit/writer/test_core.py::TestLogWriter::test_detect_anomalies_disabled 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_detect_anomalies_disabled 
tests/unit/writer/test_core.py::TestLogWriter::test_optimize_table_success 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_optimize_table_success 
tests/unit/writer/test_core.py::TestLogWriter::test_optimize_table_not_exists 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_optimize_table_not_exists 
tests/unit/writer/test_core.py::TestLogWriter::test_vacuum_table_success 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_vacuum_table_success 
tests/unit/writer/test_core.py::TestLogWriter::test_analyze_quality_trends_success 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_analyze_quality_trends_success 
tests/unit/writer/test_core.py::TestLogWriter::test_analyze_execution_trends_success 
[gw2] [ 92%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_analyze_execution_trends_success 
tests/unit/writer/test_models.py::TestWriterConfig::test_valid_config 
[gw2] [ 92%] PASSED tests/unit/writer/test_models.py::TestWriterConfig::test_valid_config 
tests/unit/writer/test_models.py::TestWriterConfig::test_config_validation_success 
[gw2] [ 92%] PASSED tests/unit/writer/test_models.py::TestWriterConfig::test_config_validation_success 
tests/unit/writer/test_models.py::TestWriterConfig::test_config_validation_empty_schema 
[gw2] [ 92%] PASSED tests/unit/writer/test_models.py::TestWriterConfig::test_config_validation_empty_schema 
tests/unit/writer/test_models.py::TestWriterConfig::test_config_validation_empty_table_name 
[gw2] [ 92%] PASSED tests/unit/writer/test_models.py::TestWriterConfig::test_config_validation_empty_table_name 
tests/unit/writer/test_models.py::TestWriterConfig::test_config_validation_invalid_batch_size 
[gw2] [ 92%] PASSED tests/unit/writer/test_models.py::TestWriterConfig::test_config_validation_invalid_batch_size 
tests/unit/writer/test_models.py::TestLogSchema::test_create_log_schema 
[gw2] [ 92%] PASSED tests/unit/writer/test_models.py::TestLogSchema::test_create_log_schema 
tests/unit/writer/test_models.py::TestLogRowCreation::test_create_log_row_from_step_result 
[gw2] [ 92%] PASSED tests/unit/writer/test_models.py::TestLogRowCreation::test_create_log_row_from_step_result 
tests/unit/writer/test_models.py::TestLogRowCreation::test_create_log_rows_from_execution_result 
[gw2] [ 93%] PASSED tests/unit/writer/test_models.py::TestLogRowCreation::test_create_log_rows_from_execution_result 
tests/unit/writer/test_models.py::TestLogRowCreation::test_process_execution_result_populates_table_total_rows 
[gw2] [ 93%] PASSED tests/unit/writer/test_models.py::TestLogRowCreation::test_process_execution_result_populates_table_total_rows 
tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_valid_log_row 
[gw2] [ 93%] PASSED tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_valid_log_row 
tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_log_row_empty_run_id 
[gw2] [ 93%] PASSED tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_log_row_empty_run_id 
tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_log_row_negative_duration 
[gw2] [ 93%] PASSED tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_log_row_negative_duration 
tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_log_data_valid 
[gw2] [ 93%] PASSED tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_log_data_valid 
tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_log_data_invalid_row 
[gw2] [ 93%] PASSED tests/unit/writer/test_models.py::TestLogRowValidation::test_validate_log_data_invalid_row 
tests/unit/writer/test_core.py::TestLogWriter::test_init_default_logger 
[gw4] [ 93%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_init_default_logger 
tests/unit/writer/test_core.py::TestLogWriter::test_init_invalid_config 
[gw0] [ 93%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_init_invalid_config 
[gw5] [ 93%] PASSED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_invalid_spark_session 
[gw7] [ 93%] PASSED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_invalid_source_bronze_raises_error 
tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_logging_auto_inference 
tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_success 
[gw5] [ 93%] PASSED tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_success 
[gw6] [ 93%] PASSED tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_write_mode_consistency_across_pipeline_runs 
[gw7] [ 93%] PASSED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_logging_auto_inference 
tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_mixed_pipeline_modes_have_correct_write_modes 
tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_chaining_works_with_auto_inference 
[gw7] [ 93%] PASSED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_chaining_works_with_auto_inference 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_bronze_step_without_incremental_col 
[gw7] [ 93%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_bronze_step_without_incremental_col 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_bronze_step_with_incremental_col 
[gw7] [ 93%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_bronze_step_with_incremental_col 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_silver_step_creation 
[gw7] [ 93%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_silver_step_creation 
tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_gold_step_creation 
[gw7] [ 94%] PASSED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_gold_step_creation 
[gw1] [ 94%] PASSED tests/builder_pyspark_tests/test_streaming_hybrid_pipeline.py::TestStreamingHybridPipeline::test_complete_streaming_hybrid_pipeline_execution 
tests/builder_pyspark_tests/test_streaming_hybrid_pipeline.py::TestStreamingHybridPipeline::test_incremental_streaming_processing 
[gw3] [ 94%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_incremental_vs_initial_write_mode_difference 
tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_no_data_loss_in_incremental_mode_for_silver_steps 
[gw1] [ 94%] PASSED tests/builder_pyspark_tests/test_streaming_hybrid_pipeline.py::TestStreamingHybridPipeline::test_incremental_streaming_processing 
tests/builder_pyspark_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_complete_supply_chain_pipeline_execution 
[gw8] [ 94%] PASSED tests/system/test_full_pipeline_with_logging.py::TestFullPipelineWithLogging::test_full_pipeline_with_logging 
[gw9] [ 94%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestParallelStress::test_pipeline_parallel_execution_stress 
tests/system/test_full_pipeline_with_logging_variations.py::TestMinimalPipelines::test_minimal_pipeline_with_logging 
tests/system/test_full_pipeline_with_logging_variations.py::TestSchemaEvolution::test_pipeline_with_schema_evolution_logging 
[gw6] [ 94%] PASSED tests/integration/test_write_mode_integration.py::TestWriteModeIntegration::test_mixed_pipeline_modes_have_correct_write_modes 
[gw3] [ 94%] PASSED tests/unit/test_pipeline_runner_write_mode.py::TestPipelineRunnerWriteMode::test_no_data_loss_in_incremental_mode_for_silver_steps 
[gw1] [ 94%] PASSED tests/builder_pyspark_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_complete_supply_chain_pipeline_execution 
tests/builder_pyspark_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_incremental_supply_chain_processing 
[gw1] [ 94%] PASSED tests/builder_pyspark_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_incremental_supply_chain_processing 
tests/builder_pyspark_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_supply_chain_logging 
[gw8] [ 94%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestMinimalPipelines::test_minimal_pipeline_with_logging 
tests/system/test_full_pipeline_with_logging_variations.py::TestLargePipelines::test_large_pipeline_with_logging 
[gw1] [ 94%] PASSED tests/builder_pyspark_tests/test_supply_chain_pipeline.py::TestSupplyChainPipeline::test_supply_chain_logging 
tests/builder_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_complete_customer_360_pipeline_execution 
[gw1] [ 94%] PASSED tests/builder_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_complete_customer_360_pipeline_execution 
tests/builder_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_churn_prediction 
[gw1] [ 94%] PASSED tests/builder_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_churn_prediction 
tests/builder_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_lifetime_value_analysis 
[gw1] [ 94%] PASSED tests/builder_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_lifetime_value_analysis 
tests/builder_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_analytics_logging 
[gw1] [ 94%] PASSED tests/builder_tests/test_customer_analytics_pipeline.py::TestCustomerAnalyticsPipeline::test_customer_analytics_logging 
tests/builder_tests/test_data_quality_pipeline.py::TestDataQualityPipeline::test_complete_data_quality_pipeline_execution 
[gw1] [ 94%] PASSED tests/builder_tests/test_data_quality_pipeline.py::TestDataQualityPipeline::test_complete_data_quality_pipeline_execution 
tests/builder_tests/test_data_quality_pipeline.py::TestDataQualityPipeline::test_incremental_data_quality_processing 
[gw9] [ 94%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestSchemaEvolution::test_pipeline_with_schema_evolution_logging 
tests/system/test_full_pipeline_with_logging_variations.py::TestDataTypes::test_pipeline_data_type_variations 
[gw1] [ 95%] PASSED tests/builder_tests/test_data_quality_pipeline.py::TestDataQualityPipeline::test_incremental_data_quality_processing 
tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_complete_ecommerce_pipeline_execution 
[gw1] [ 95%] FAILED tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_complete_ecommerce_pipeline_execution 
tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_incremental_order_processing 
[gw1] [ 95%] PASSED tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_incremental_order_processing 
tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_validation_failures 
[gw1] [ 95%] FAILED tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_validation_failures 
tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_logging_and_monitoring 
[gw1] [ 95%] PASSED tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_logging_and_monitoring 
tests/builder_tests/test_financial_pipeline.py::TestFinancialPipeline::test_complete_financial_transaction_pipeline_execution 
[gw1] [ 95%] PASSED tests/builder_tests/test_financial_pipeline.py::TestFinancialPipeline::test_complete_financial_transaction_pipeline_execution 
tests/builder_tests/test_financial_pipeline.py::TestFinancialPipeline::test_fraud_detection_scenarios 
[gw1] [ 95%] PASSED tests/builder_tests/test_financial_pipeline.py::TestFinancialPipeline::test_fraud_detection_scenarios 
tests/builder_tests/test_financial_pipeline.py::TestFinancialPipeline::test_compliance_monitoring 
[gw1] [ 95%] PASSED tests/builder_tests/test_financial_pipeline.py::TestFinancialPipeline::test_compliance_monitoring 
tests/builder_tests/test_financial_pipeline.py::TestFinancialPipeline::test_financial_audit_logging 
[gw1] [ 95%] PASSED tests/builder_tests/test_financial_pipeline.py::TestFinancialPipeline::test_financial_audit_logging 
tests/builder_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_complete_healthcare_pipeline_execution 
[gw8] [ 95%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestLargePipelines::test_large_pipeline_with_logging 
tests/system/test_full_pipeline_with_logging_variations.py::TestEdgeCases::test_pipeline_with_empty_data 
[gw1] [ 95%] PASSED tests/builder_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_complete_healthcare_pipeline_execution 
tests/builder_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_incremental_healthcare_processing 
[gw9] [ 95%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestDataTypes::test_pipeline_data_type_variations 
tests/system/test_full_pipeline_with_logging_variations.py::TestValidationThresholds::test_pipeline_validation_thresholds_logging 
[gw1] [ 95%] PASSED tests/builder_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_incremental_healthcare_processing 
tests/builder_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_healthcare_logging 
[gw1] [ 95%] PASSED tests/builder_tests/test_healthcare_pipeline.py::TestHealthcarePipeline::test_healthcare_logging 
tests/builder_tests/test_iot_pipeline.py::TestIotPipeline::test_complete_iot_sensor_pipeline_execution 
[gw1] [ 95%] PASSED tests/builder_tests/test_iot_pipeline.py::TestIotPipeline::test_complete_iot_sensor_pipeline_execution 
tests/builder_tests/test_iot_pipeline.py::TestIotPipeline::test_incremental_sensor_processing 
[gw1] [ 95%] PASSED tests/builder_tests/test_iot_pipeline.py::TestIotPipeline::test_incremental_sensor_processing 
[gw8] [ 95%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestEdgeCases::test_pipeline_with_empty_data 
tests/system/test_full_pipeline_with_logging_variations.py::TestEdgeCases::test_pipeline_with_partial_validation_failures 
[gw9] [ 95%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestValidationThresholds::test_pipeline_validation_thresholds_logging 
tests/system/test_full_pipeline_with_logging_variations.py::TestWriteModes::test_pipeline_write_mode_variations 
[gw8] [ 96%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestEdgeCases::test_pipeline_with_partial_validation_failures 
tests/system/test_full_pipeline_with_logging_variations.py::TestEdgeCases::test_pipeline_all_invalid_data 
[gw8] [ 96%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestEdgeCases::test_pipeline_all_invalid_data 
tests/system/test_full_pipeline_with_logging_variations.py::TestExecutionModes::test_pipeline_sequential_execution_with_logging 
[gw9] [ 96%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestWriteModes::test_pipeline_write_mode_variations 
tests/system/test_full_pipeline_with_logging_variations.py::TestMixedSuccessFailure::test_pipeline_mixed_success_failure 
[gw8] [ 96%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestExecutionModes::test_pipeline_sequential_execution_with_logging 
tests/system/test_full_pipeline_with_logging_variations.py::TestIncrementalScenarios::test_pipeline_multiple_incremental_runs 
[gw9] [ 96%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestMixedSuccessFailure::test_pipeline_mixed_success_failure 
tests/system/test_full_pipeline_with_logging_variations.py::TestLongRunning::test_pipeline_long_running_with_logging 
[gw9] [ 96%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestLongRunning::test_pipeline_long_running_with_logging 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_source_silvers 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_source_silvers 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_source_silvers_explicit 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_source_silvers_explicit 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_no_silver_steps_error 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_no_silver_steps_error 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_development 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_development 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_production 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_production 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_testing 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_testing 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_not_null_rules 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_not_null_rules 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_positive_number_rules 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_positive_number_rules 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_string_not_empty_rules 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_string_not_empty_rules 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_timestamp_rules 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_timestamp_rules 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_detect_timestamp_columns 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_detect_timestamp_columns 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_detect_timestamp_columns_list 
[gw9] [ 96%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_detect_timestamp_columns_list 
tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_chaining_with_auto_inference 
[gw9] [ 97%] PASSED tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_chaining_with_auto_inference 
tests/system/test_logger.py::TestPipelineLogger::test_basic_logging_methods 
[gw9] [ 97%] PASSED tests/system/test_logger.py::TestPipelineLogger::test_basic_logging_methods 
tests/system/test_logger.py::TestPipelineLogger::test_log_level_management 
[gw9] [ 97%] PASSED tests/system/test_logger.py::TestPipelineLogger::test_log_level_management 
tests/system/test_logger.py::TestPipelineLogger::test_logger_creation 
[gw9] [ 97%] PASSED tests/system/test_logger.py::TestPipelineLogger::test_logger_creation 
tests/system/test_logger.py::TestPipelineLogger::test_logger_with_file 
[gw9] [ 97%] PASSED tests/system/test_logger.py::TestPipelineLogger::test_logger_with_file 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_bronze_rules_with_schema 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_bronze_rules_with_schema 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_bronze_rules_without_schema 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_bronze_rules_without_schema 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_silver_rules_with_schema 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_silver_rules_with_schema 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_silver_transform_with_schema 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_silver_transform_with_schema 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_gold_transform_with_schema 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_gold_transform_with_schema 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_success 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_success 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_failure 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_failure 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_get_effective_schema 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_get_effective_schema 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_creation 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_creation 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_creation_failure 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_creation_failure 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_cross_schema_pipeline 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_cross_schema_pipeline 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_mixed_schema_usage 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_mixed_schema_usage 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_integration 
[gw9] [ 97%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_integration 
tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_backward_compatibility 
[gw9] [ 98%] PASSED tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_backward_compatibility 
[gw8] [ 98%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestIncrementalScenarios::test_pipeline_multiple_incremental_runs 
tests/system/test_schema_evolution_without_override.py::TestSchemaEvolutionWithoutOverride::test_silver_schema_evolution_on_initial_load_rerun 
tests/system/test_full_pipeline_with_logging_variations.py::TestIncrementalScenarios::test_pipeline_incremental_with_gaps 
[gw9] [ 98%] PASSED tests/system/test_schema_evolution_without_override.py::TestSchemaEvolutionWithoutOverride::test_silver_schema_evolution_on_initial_load_rerun 
tests/system/test_schema_evolution_without_override.py::TestSchemaEvolutionWithoutOverride::test_silver_schema_evolution_incremental_should_error 
[gw9] [ 98%] PASSED tests/system/test_schema_evolution_without_override.py::TestSchemaEvolutionWithoutOverride::test_silver_schema_evolution_incremental_should_error 
tests/system/test_schema_evolution_without_override.py::TestSchemaEvolutionWithoutOverride::test_silver_schema_evolution_with_multiple_new_columns 
[gw8] [ 98%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestIncrementalScenarios::test_pipeline_incremental_with_gaps 
tests/system/test_full_pipeline_with_logging_variations.py::TestDataQuality::test_pipeline_null_handling_with_logging 
[gw9] [ 98%] PASSED tests/system/test_schema_evolution_without_override.py::TestSchemaEvolutionWithoutOverride::test_silver_schema_evolution_with_multiple_new_columns 
tests/system/test_schema_evolution_without_override.py::TestSchemaEvolutionWithoutOverride::test_gold_schema_evolution_without_override 
[gw8] [ 98%] PASSED tests/system/test_full_pipeline_with_logging_variations.py::TestDataQuality::test_pipeline_null_handling_with_logging 
tests/unit/test_constants.py::TestErrorConstants::test_max_stack_trace_lines 
[gw8] [ 98%] PASSED tests/unit/test_constants.py::TestErrorConstants::test_max_stack_trace_lines 
tests/unit/test_constants.py::TestPerformanceMonitoringConstants::test_default_metrics_interval_seconds 
[gw8] [ 98%] PASSED tests/unit/test_constants.py::TestPerformanceMonitoringConstants::test_default_metrics_interval_seconds 
tests/unit/test_constants.py::TestPerformanceMonitoringConstants::test_default_alert_threshold_percent 
[gw8] [ 98%] PASSED tests/unit/test_constants.py::TestPerformanceMonitoringConstants::test_default_alert_threshold_percent 
tests/unit/test_constants.py::TestConstantsIntegration::test_memory_calculations 
[gw8] [ 98%] PASSED tests/unit/test_constants.py::TestConstantsIntegration::test_memory_calculations 
tests/unit/test_constants.py::TestConstantsIntegration::test_threshold_percentages 
[gw8] [ 98%] PASSED tests/unit/test_constants.py::TestConstantsIntegration::test_threshold_percentages 
tests/unit/test_constants.py::TestConstantsIntegration::test_positive_values 
[gw8] [ 98%] PASSED tests/unit/test_constants.py::TestConstantsIntegration::test_positive_values 
tests/unit/test_constants.py::TestConstantsIntegration::test_string_constants_not_empty 
[gw8] [ 98%] PASSED tests/unit/test_constants.py::TestConstantsIntegration::test_string_constants_not_empty 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_import 
[gw8] [ 98%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_import 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_values 
[gw8] [ 98%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_values 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_types 
[gw8] [ 98%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_types 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_ranges 
[gw8] [ 98%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_ranges 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_consistency 
[gw8] [ 99%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_consistency 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_immutability 
[gw8] [ 99%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_immutability 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_documentation 
[gw8] [ 99%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_documentation 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_usage_examples 
[gw8] [ 99%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_usage_examples 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_error_handling 
[gw8] [ 99%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_error_handling 
tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_module_attributes 
[gw8] [ 99%] PASSED tests/unit/test_constants_coverage.py::TestConstantsCoverage::test_constants_module_attributes 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_initialization 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_initialization 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_invalid_spark_session 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_invalid_spark_session 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_get_spark 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_get_spark 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_creation 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_creation 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_failed 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_failed 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_enum 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_enum 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_with_sample_data 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_with_sample_data 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_error_handling 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_error_handling 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_metrics_collection 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_unified_validator_metrics_collection 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_with_errors 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_with_errors 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_with_warnings_only 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_with_warnings_only 
tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_empty 
[gw8] [ 99%] PASSED tests/unit/test_data_validation_simple.py::TestDataValidationSimple::test_validation_result_empty 
[gw9] [100%] PASSED tests/system/test_schema_evolution_without_override.py::TestSchemaEvolutionWithoutOverride::test_gold_schema_evolution_without_override 

==================================== ERRORS ====================================
____________ ERROR at setup of TestGetDataframeInfo.test_basic_info ____________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7080.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7060.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:53 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_______ ERROR at setup of TestValidateDataframeSchema.test_valid_schema ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11833.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o11813.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:55 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
______ ERROR at setup of TestValidateDataframeSchema.test_missing_columns ______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11907.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o11887.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:57 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_________ ERROR at setup of TestApplyColumnRules.test_basic_validation _________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7302.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7282.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:47:57 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_______ ERROR at setup of TestValidateDataframeSchema.test_extra_columns _______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11981.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o11961.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:59 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:59 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:59 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:59 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:59 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:59 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:47:59 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____ ERROR at setup of TestApplyColumnRules.test_none_rules_raises_error ______
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7376.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7356.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:59 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__ ERROR at setup of TestValidateDataframeSchema.test_empty_expected_columns ___
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o12055.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o12035.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:00 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:00 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:00 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:00 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:00 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:48:00 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___________ ERROR at setup of TestApplyColumnRules.test_empty_rules ____________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7450.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7430.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:01 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:01 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:02 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________ ERROR at setup of TestApplyColumnRules.test_complex_rules ___________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7524.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7504.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:05 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:05 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:05 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___ ERROR at setup of TestApplyValidationRules.test_apply_column_rules_basic ___
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7826.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7806.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:13 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:13 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:13 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:13 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:48:13 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___ ERROR at setup of TestApplyValidationRules.test_apply_column_rules_empty ___
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7900.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7880.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:15 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:15 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:15 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:48:15 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__ ERROR at setup of TestAssessDataQuality.test_basic_data_quality_assessment __
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o12663.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o12643.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:17 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____ ERROR at setup of TestAssessDataQuality.test_data_quality_with_rules _____
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:131: in sample_dataframe
    assert df.count() == 4, f"Expected 4 rows, got {df.count()}"
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o12737.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o12717.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:18 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:18 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:18 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:18 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:18 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:18 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
=================================== FAILURES ===================================
_______ TestPipelineExecutionFlow.test_pipeline_execution_with_mock_data _______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/integration/test_pipeline_execution.py:217: in test_pipeline_execution_with_mock_data
    assert mock_df.count() == 3
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o487.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o461.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 86 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:45:06 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:45:06 - PipelineRunner - INFO -  PipelineBuilder initialized (schema: test_schema)
18:45:06 - PipelineRunner - INFO -  Added Bronze step: events
18:45:06 - PipelineRunner - INFO -  Pipeline validation passed
18:45:06 - PipelineRunner - INFO -  Pipeline built successfully with 1 bronze, 0 silver, 0 gold steps
------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  PipelineBuilder initialized (schema: test_schema)
INFO     PipelineRunner:logging.py:82  Added Bronze step: events
INFO     PipelineRunner:logging.py:82  Pipeline validation passed
INFO     PipelineRunner:logging.py:82  Pipeline built successfully with 1 bronze, 0 silver, 0 gold steps
_________ TestPipelineExecutionFlow.test_step_execution_with_real_data _________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/integration/test_pipeline_execution.py:269: in test_step_execution_with_real_data
    assert df.count() == 5
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o679.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o655.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 86 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:09 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:09 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:09 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:09 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:09 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_________ TestRealSparkOperations.test_real_spark_dataframe_operations _________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_simple_real_spark.py:79: in test_real_spark_dataframe_operations
    assert sample_dataframe.count() == 5
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o902.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o882.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 86 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:12 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:12 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:12 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:12 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___________ TestRealSparkOperations.test_real_spark_transformations ____________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_simple_real_spark.py:112: in test_real_spark_transformations
    assert df_processed.count() == 5  # 5 users, 1 date each
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o1002.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o956.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 86 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:13 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:13 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____________ TestRealSparkOperations.test_real_spark_data_quality _____________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o1152.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more

The above exception was the direct cause of the following exception:
tests/system/test_simple_real_spark.py:135: in test_real_spark_data_quality
    quality = assess_data_quality(sample_dataframe)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o1152.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o1152.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1132.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 86 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:15 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:15 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:15 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o1152.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 62 more
_________ TestRealSparkOperations.test_real_spark_metadata_operations __________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_simple_real_spark.py:159: in test_real_spark_metadata_operations
    assert result.count() == 5
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o1228.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1206.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:16 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____________ TestRealSparkOperations.test_real_spark_performance ______________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_simple_real_spark.py:172: in test_real_spark_performance
    assert df.count() == 1000
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o1307.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1283.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:17 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:17 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:17 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
____________ TestRealSparkOperations.test_real_spark_error_handling ____________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_simple_real_spark.py:193: in test_real_spark_error_handling
    assert result.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o1384.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more

During handling of the above exception, another exception occurred:
tests/system/test_simple_real_spark.py:195: in test_real_spark_error_handling
    pytest.fail(f"Valid operation failed: {e}")
E   Failed: Valid operation failed: An error occurred while calling o1384.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1361.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:18 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________ TestRealSparkOperations.test_real_spark_schema_operations ___________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_simple_real_spark.py:214: in test_real_spark_schema_operations
    assert info["row_count"] == 5
E   assert 0 == 5
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1438.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:19 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:19 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:19 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:19 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:19 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
________________ TestRealSparkOperations.test_real_spark_joins _________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_simple_real_spark.py:247: in test_real_spark_joins
    assert joined_df.count() == 3
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o1557.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1512.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:45:20 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________________ TestDataValidation.test_apply_column_rules __________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_utils.py:112: in test_apply_column_rules
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o1778.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1758.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:24 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_________________ TestDataValidation.test_assess_data_quality __________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o1931.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more

The above exception was the direct cause of the following exception:
tests/system/test_utils.py:142: in test_assess_data_quality
    quality = assess_data_quality(sample_dataframe)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o1931.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o1931.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1911.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:26 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:26 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:26 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:26 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:26 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:26 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o1931.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 62 more
__________________ TestDataValidation.test_get_dataframe_info __________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_utils.py:157: in test_get_dataframe_info
    assert info["row_count"] == 5
E   assert 0 == 5
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o1985.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:28 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_______ TestDataTransformationUtilities.test_basic_dataframe_operations ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_utils.py:219: in test_basic_dataframe_operations
    original_count = sample_dataframe.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o2150.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o2130.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:30 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:30 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:30 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:30 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:30 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:30 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___________ TestDataTransformationUtilities.test_dataframe_filtering ___________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_utils.py:226: in test_dataframe_filtering
    original_count = sample_dataframe.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o2227.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o2207.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:31 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:31 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:31 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:31 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:31 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________ TestPerformanceWithRealData.test_large_dataset_validation ___________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_utils.py:320: in test_large_dataset_validation
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o2419.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 62 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o2395.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:33 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___ TestBronzeRulesColumnValidation.test_existing_columns_validation_success ___
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_bronze_rules_column_validation.py:107: in test_existing_columns_validation_success
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o2582.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o2562.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:35 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____ TestBronzeRulesColumnValidation.test_empty_rules_validation_success ______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_bronze_rules_column_validation.py:125: in test_empty_rules_validation_success
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:191: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o2674.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o2650.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:37 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____ TestBronzeRulesColumnValidation.test_bronze_step_with_provided_data ______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_bronze_rules_column_validation.py:172: in test_bronze_step_with_provided_data
    result_df = engine._execute_bronze_step(bronze_step, context)
src/pipeline_builder/execution.py:2311: in _execute_bronze_step
    if df.count() == 0:  # type: ignore[attr-defined]
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o2748.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o2728.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:38 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:38 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:38 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:38 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:38 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
________ TestCreateDataframeCompat.test_create_dataframe_with_dict_data ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_compat_helpers.py:26: in test_create_dataframe_with_dict_data
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o2979.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o2955.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:45:42 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_______ TestCreateDataframeCompat.test_create_dataframe_with_tuple_data ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_compat_helpers.py:34: in test_create_dataframe_with_tuple_data
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3057.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3033.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:43 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
____ TestCreateDataframeCompat.test_create_dataframe_with_structtype_schema ____
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_compat_helpers.py:65: in test_create_dataframe_with_structtype_schema
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3131.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3111.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:44 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__ TestCreateDataframeCompat.test_create_dataframe_with_tuple_and_structtype ___
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_compat_helpers.py:93: in test_create_dataframe_with_tuple_and_structtype
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3205.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3185.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:46 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_________ TestCreateTestDataframe.test_create_test_dataframe_with_dict _________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_compat_helpers.py:163: in test_create_test_dataframe_with_dict
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3547.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3523.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:50 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
________ TestCreateTestDataframe.test_create_test_dataframe_with_schema ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_compat_helpers.py:191: in test_create_test_dataframe_with_schema
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3621.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3601.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:51 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:51 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:51 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:51 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:51 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
________ TestCreateTestDataframe.test_create_test_dataframe_with_tuples ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_compat_helpers.py:201: in test_create_test_dataframe_with_tuples
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3699.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3675.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:53 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:53 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:53 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:53 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:45:53 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
________________ TestEdgeCases.test_empty_dataframe_operations _________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:82: in test_empty_dataframe_operations
    assert empty_df.count() == 0
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3773.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3753.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:54 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
____________________ TestEdgeCases.test_null_value_handling ____________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:117: in test_null_value_handling
    assert non_null_df.count() == 3
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3850.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3827.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:55 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:55 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:55 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_________________ TestEdgeCases.test_large_dataset_operations __________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:150: in test_large_dataset_operations
    assert df.count() == len(large_dataset)
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3924.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3904.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:56 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_________________ TestEdgeCases.test_complex_schema_operations _________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:190: in test_complex_schema_operations
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o3998.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o3978.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:45:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:45:57 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:45:57 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:45:57 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:45:57 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
______________________ TestEdgeCases.test_boundary_values ______________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:241: in test_boundary_values
    assert df.count() == 2
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4154.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4134.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:45:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:45:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:45:59 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___________________ TestEdgeCases.test_concurrent_operations ___________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:270: in test_concurrent_operations
    assert df1.count() == 1
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4228.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4208.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:00 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____________________ TestEdgeCases.test_memory_management _____________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:309: in test_memory_management
    assert df.count() == 10000
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4330.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4310.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:02 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____________________ TestEdgeCases.test_schema_evolution ______________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:346: in test_schema_evolution
    assert df1.count() == 1
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4404.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4384.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:04 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___________________ TestEdgeCases.test_dataframe_edge_cases ____________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:513: in test_dataframe_edge_cases
    assert empty_df.count() == 1
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4845.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4825.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:11 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
____________________ TestEdgeCases.test_session_edge_cases _____________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_edge_cases.py:559: in test_session_edge_cases
    mock_spark_session.sql("CREATE SCHEMA IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4899.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4899.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:12 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:12 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:12 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:12 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:46:12 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___________ TestExecuteStepComplete.test_execute_silver_step_success ___________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4990.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_100_coverage.py:96: in test_execute_silver_step_success
    result = engine.execute_step(silver_step, context, ExecutionMode.INITIAL)
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o4990.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4965.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:14 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:14 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:14 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:14 - PipelineRunner - INFO -  Starting SILVER step: test_silver
18:46:15 - PipelineRunner - ERROR -  Failed SILVER step: test_silver (0.45s) - An error occurred while calling o4990.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  Starting SILVER step: test_silver
ERROR    PipelineRunner:logging.py:92  Failed SILVER step: test_silver (0.45s) - An error occurred while calling o4990.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
____________ TestExecuteStepComplete.test_execute_gold_step_success ____________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5076.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_100_coverage.py:141: in test_execute_gold_step_success
    result = engine.execute_step(gold_step, context, ExecutionMode.INITIAL)
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o5076.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5056.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:15 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:16 - PipelineRunner - INFO -  Starting GOLD step: test_gold
18:46:16 - PipelineRunner - ERROR -  Failed GOLD step: test_gold (0.45s) - An error occurred while calling o5076.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  Starting GOLD step: test_gold
ERROR    PipelineRunner:logging.py:92  Failed GOLD step: test_gold (0.45s) - An error occurred while calling o5076.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_______ TestExecuteStepComplete.test_execute_step_with_rules_validation ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:646: in execute_step
    self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema}")  # type: ignore[attr-defined]
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5141.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more

The above exception was the direct cause of the following exception:
src/pipeline_builder/execution.py:656: in execute_step
    raise ExecutionError(
E   _errors_module.ExecutionError: Failed to create schema 'test_schema' before table creation: An error occurred while calling o5141.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_100_coverage.py:188: in test_execute_step_with_rules_validation
    result = engine.execute_step(silver_step, context, ExecutionMode.INITIAL)
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: Failed to create schema 'test_schema' before table creation: An error occurred while calling o5141.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5141.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:17 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:17 - PipelineRunner - INFO -  Starting SILVER step: test_silver
18:46:18 - PipelineRunner - ERROR -  Failed SILVER step: test_silver (0.57s) - Failed to create schema 'test_schema' before table creation: An error occurred while calling o5141.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  Starting SILVER step: test_silver
ERROR    PipelineRunner:logging.py:92  Failed SILVER step: test_silver (0.57s) - Failed to create schema 'test_schema' before table creation: An error occurred while calling o5141.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more
________ TestExecuteStepComplete.test_execute_step_validation_only_mode ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:1820: in execute_step
    result.rows_processed = output_df.count()  # type: ignore[attr-defined]
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5253.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_100_coverage.py:228: in test_execute_step_validation_only_mode
    result = engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o5253.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5228.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:19 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:19 - PipelineRunner - INFO -  Starting SILVER step: test_silver
18:46:19 - PipelineRunner - ERROR -  Failed SILVER step: test_silver (0.15s) - An error occurred while calling o5253.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  Starting SILVER step: test_silver
ERROR    PipelineRunner:logging.py:92  Failed SILVER step: test_silver (0.15s) - An error occurred while calling o5253.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_______ TestExecuteStepComplete.test_execute_step_silver_missing_schema ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5332.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_100_coverage.py:274: in test_execute_step_silver_missing_schema
    engine.execute_step(silver_step, context, ExecutionMode.INITIAL)
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o5332.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

During handling of the above exception, another exception occurred:
tests/unit/test_execution_100_coverage.py:274: in test_execute_step_silver_missing_schema
    engine.execute_step(silver_step, context, ExecutionMode.INITIAL)
E   AssertionError: Regex pattern did not match.
E    Regex: "Step 'test_silver' requires a schema"
E    Input: "Step execution failed: An error occurred while calling o5332.count.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3615)\n\tat jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 61 more\n"
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5307.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:21 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:21 - PipelineRunner - INFO -  Starting SILVER step: test_silver
18:46:21 - PipelineRunner - ERROR -  Failed SILVER step: test_silver (0.27s) - An error occurred while calling o5332.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  Starting SILVER step: test_silver
ERROR    PipelineRunner:logging.py:92  Failed SILVER step: test_silver (0.27s) - An error occurred while calling o5332.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
________ TestExecuteStepComplete.test_execute_step_gold_missing_schema _________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5418.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_100_coverage.py:311: in test_execute_step_gold_missing_schema
    engine.execute_step(gold_step, context, ExecutionMode.INITIAL)
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o5418.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

During handling of the above exception, another exception occurred:
tests/unit/test_execution_100_coverage.py:311: in test_execute_step_gold_missing_schema
    engine.execute_step(gold_step, context, ExecutionMode.INITIAL)
E   AssertionError: Regex pattern did not match.
E    Regex: "Step 'test_gold' requires a schema"
E    Input: "Step execution failed: An error occurred while calling o5418.count.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3615)\n\tat jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 61 more\n"
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5398.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:22 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:22 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:22 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:22 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:22 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:22 - PipelineRunner - INFO -  Starting GOLD step: test_gold
18:46:23 - PipelineRunner - ERROR -  Failed GOLD step: test_gold (0.35s) - An error occurred while calling o5418.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  Starting GOLD step: test_gold
ERROR    PipelineRunner:logging.py:92  Failed GOLD step: test_gold (0.35s) - An error occurred while calling o5418.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_______ TestExecutionEngineSimple.test_execution_engine_with_sample_data _______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_execution_engine_simple.py:197: in test_execution_engine_with_sample_data
    assert sample_dataframe.count() > 0
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5617.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5597.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:27 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
________ TestExecutionEngineSimple.test_execution_engine_error_handling ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_execution_engine_simple.py:209: in test_execution_engine_error_handling
    spark_session.table("nonexistent.table")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1667: in table
    return DataFrame(self._jsparkSession.table(tableName), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5671.table.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
E   	at scala.Option.orElse(Option.scala:447)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
E   	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
E   	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 72 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5671.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:28 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:28 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:28 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:28 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
______ TestExecutionEngineSimple.test_execution_engine_metrics_collection ______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_execution_engine_simple.py:222: in test_execution_engine_metrics_collection
    assert sample_dataframe.count() > 0
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5751.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5731.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:29 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:29 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_ TestExecutionEngineWriteMode.test_incremental_mode_uses_append_for_silver_step _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6026.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:109: in test_incremental_mode_uses_append_for_silver_step
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6026.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6004.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:34 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:34 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:34 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:46:34 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:35 - test - INFO -  Starting SILVER step: test_silver
18:46:35 - test - ERROR -  Failed SILVER step: test_silver (0.76s) - An error occurred while calling o6026.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting SILVER step: test_silver
ERROR    test:logging.py:92  Failed SILVER step: test_silver (0.76s) - An error occurred while calling o6026.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestExecutionEngineWriteMode.test_incremental_mode_uses_overwrite_for_gold_step _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6111.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:124: in test_incremental_mode_uses_overwrite_for_gold_step
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6111.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6089.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:36 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:36 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:36 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:36 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:36 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:46:36 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:36 - test - INFO -  Starting GOLD step: test_gold
18:46:37 - test - ERROR -  Failed GOLD step: test_gold (0.65s) - An error occurred while calling o6111.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting GOLD step: test_gold
ERROR    test:logging.py:92  Failed GOLD step: test_gold (0.65s) - An error occurred while calling o6111.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestExecutionEngineWriteMode.test_initial_mode_uses_overwrite_for_silver_step _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6196.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:139: in test_initial_mode_uses_overwrite_for_silver_step
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6196.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6174.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:38 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:38 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:38 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:39 - test - INFO -  Starting SILVER step: test_silver
18:46:39 - test - ERROR -  Failed SILVER step: test_silver (0.61s) - An error occurred while calling o6196.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting SILVER step: test_silver
ERROR    test:logging.py:92  Failed SILVER step: test_silver (0.61s) - An error occurred while calling o6196.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestExecutionEngineWriteMode.test_initial_mode_uses_overwrite_for_gold_step __
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6281.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:154: in test_initial_mode_uses_overwrite_for_gold_step
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6281.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6259.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:41 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:41 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:41 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:41 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:41 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:41 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:41 - test - INFO -  Starting GOLD step: test_gold
18:46:41 - test - ERROR -  Failed GOLD step: test_gold (0.30s) - An error occurred while calling o6281.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting GOLD step: test_gold
ERROR    test:logging.py:92  Failed GOLD step: test_gold (0.30s) - An error occurred while calling o6281.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestExecutionEngineWriteMode.test_full_refresh_mode_uses_overwrite_for_silver_step _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6366.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:169: in test_full_refresh_mode_uses_overwrite_for_silver_step
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6366.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6344.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:42 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:42 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:42 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:42 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:42 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:42 - test - INFO -  Starting SILVER step: test_silver
18:46:42 - test - ERROR -  Failed SILVER step: test_silver (0.40s) - An error occurred while calling o6366.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting SILVER step: test_silver
ERROR    test:logging.py:92  Failed SILVER step: test_silver (0.40s) - An error occurred while calling o6366.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestExecutionEngineWriteMode.test_full_refresh_mode_uses_overwrite_for_gold_step _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6451.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:184: in test_full_refresh_mode_uses_overwrite_for_gold_step
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6451.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6429.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:43 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:43 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:43 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:43 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:44 - test - INFO -  Starting GOLD step: test_gold
18:46:44 - test - ERROR -  Failed GOLD step: test_gold (0.40s) - An error occurred while calling o6451.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting GOLD step: test_gold
ERROR    test:logging.py:92  Failed GOLD step: test_gold (0.40s) - An error occurred while calling o6451.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestExecutionEngineWriteMode.test_validation_only_mode_has_no_write_mode_for_silver_step _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:1820: in execute_step
    result.rows_processed = output_df.count()  # type: ignore[attr-defined]
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6536.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:199: in test_validation_only_mode_has_no_write_mode_for_silver_step
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6536.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6514.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:44 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:44 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:44 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:45 - test - INFO -  Starting SILVER step: test_silver
18:46:45 - test - ERROR -  Failed SILVER step: test_silver (0.08s) - An error occurred while calling o6536.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting SILVER step: test_silver
ERROR    test:logging.py:92  Failed SILVER step: test_silver (0.08s) - An error occurred while calling o6536.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestExecutionEngineWriteMode.test_validation_only_mode_has_no_write_mode_for_gold_step _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:1820: in execute_step
    result.rows_processed = output_df.count()  # type: ignore[attr-defined]
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6612.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:272: in test_validation_only_mode_has_no_write_mode_for_gold_step
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6612.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6590.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:46 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:46 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:46 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:46 - test - INFO -  Starting GOLD step: test_gold
18:46:46 - test - ERROR -  Failed GOLD step: test_gold (0.28s) - An error occurred while calling o6612.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting GOLD step: test_gold
ERROR    test:logging.py:92  Failed GOLD step: test_gold (0.28s) - An error occurred while calling o6612.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestExecutionEngineWriteMode.test_spark_write_mode_matches_result_write_mode _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6690.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:287: in test_spark_write_mode_matches_result_write_mode
    result_inc_silver = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6690.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6666.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:47 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:47 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:47 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:47 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:47 - test - INFO -  Starting SILVER step: test_silver
18:46:47 - test - ERROR -  Failed SILVER step: test_silver (0.20s) - An error occurred while calling o6690.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting SILVER step: test_silver
ERROR    test:logging.py:92  Failed SILVER step: test_silver (0.20s) - An error occurred while calling o6690.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
__ TestExecutionEngineWriteMode.test_write_mode_consistency_across_step_types __
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6777.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:346: in test_write_mode_consistency_across_step_types
    silver_result = execution_engine.execute_step(silver_step, context, mode)
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6777.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6753.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:48 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:48 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:48 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:48 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:48 - test - INFO -  Starting SILVER step: test_silver
18:46:48 - test - ERROR -  Failed SILVER step: test_silver (0.23s) - An error occurred while calling o6777.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting SILVER step: test_silver
ERROR    test:logging.py:92  Failed SILVER step: test_silver (0.23s) - An error occurred while calling o6777.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
___________ TestPerformanceWithRealData.test_complex_transformations ___________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/system/test_utils.py:350: in test_complex_transformations
    assert result.count() == 2  # 2 users, 1 date each
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o2298.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Warning: Delta extensions not automatically set, setting manually
 Delta Lake verification issue (non-fatal): Cannot modify the value of a static config: spark.sql.extensions.
 Could not create test_schema database: An error occurred while calling o137.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
_______ TestExecutionEngineWriteMode.test_bronze_step_has_no_write_mode ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:589: in execute_step
    output_df = self._execute_bronze_step(step, context)
src/pipeline_builder/execution.py:2311: in _execute_bronze_step
    if df.count() == 0:  # type: ignore[attr-defined]
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6860.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:371: in test_bronze_step_has_no_write_mode
    result = execution_engine.execute_step(bronze_step, context, mode)
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6860.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6840.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:49 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:49 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:49 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:49 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:49 - test - INFO -  Starting BRONZE step: test_bronze
18:46:49 - test - ERROR -  Failed BRONZE step: test_bronze (0.11s) - An error occurred while calling o6860.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting BRONZE step: test_bronze
ERROR    test:logging.py:92  Failed BRONZE step: test_bronze (0.11s) - An error occurred while calling o6860.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
__ TestWriteModeRegression.test_incremental_mode_uses_append_for_silver_steps __
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6942.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:415: in test_incremental_mode_uses_append_for_silver_steps
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o6942.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6916.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:50 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:50 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:50 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:50 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:50 - test - INFO -  Starting SILVER step: test_silver
18:46:51 - test - ERROR -  Failed SILVER step: test_silver (0.28s) - An error occurred while calling o6942.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting SILVER step: test_silver
ERROR    test:logging.py:92  Failed SILVER step: test_silver (0.28s) - An error occurred while calling o6942.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
______ TestWriteModeRegression.test_gold_incremental_mode_uses_overwrite _______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:610: in execute_step
    output_df, _, validation_stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7031.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_execution_write_mode.py:449: in test_gold_incremental_mode_uses_overwrite
    result = execution_engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o7031.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7005.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:46:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:46:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:46:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:46:51 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:46:51 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:46:51 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:46:51 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:46:51 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:46:51 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:46:51 - test - INFO -  Starting GOLD step: test_gold
18:46:52 - test - ERROR -  Failed GOLD step: test_gold (0.68s) - An error occurred while calling o7031.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more

------------------------------ Captured log call -------------------------------
INFO     test:logging.py:82  Starting GOLD step: test_gold
ERROR    test:logging.py:92  Failed GOLD step: test_gold (0.68s) - An error occurred while calling o7031.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_______________ TestHelperMethods.test_validate_schema_existing ________________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_pipeline_builder_comprehensive.py:664: in test_validate_schema_existing
    mock_spark_session.sql("CREATE SCHEMA IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4396.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4396.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:47:15 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
______________ TestHelperMethods.test_create_schema_if_not_exists ______________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/pipeline/builder.py:1363: in _create_schema_if_not_exists
    self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema}")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o4518.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more

The above exception was the direct cause of the following exception:
tests/unit/test_pipeline_builder_comprehensive.py:690: in test_create_schema_if_not_exists
    builder._create_schema_if_not_exists("new_schema_to_test")
src/pipeline_builder/pipeline/builder.py:1366: in _create_schema_if_not_exists
    raise StepError(
E   _errors_module.ExecutionError: Failed to create schema 'new_schema_to_test': An error occurred while calling o4518.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
E    | Context: step_name=schema_creation, step_type=validation | Suggestions: Check schema permissions; Verify schema name is valid; Check for naming conflicts
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o4518.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:17 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:17 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:47:17 - PipelineRunner - INFO -  PipelineBuilder initialized (schema: test_schema)
------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  PipelineBuilder initialized (schema: test_schema)
_______________ TestHelperMethods.test_validate_schema_existing ________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_pipeline_builder_simple.py:715: in test_validate_schema_existing
    mock_spark_session.sql("CREATE SCHEMA IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o9869.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o9869.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:28 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
______________ TestHelperMethods.test_create_schema_if_not_exists ______________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/pipeline/builder.py:1363: in _create_schema_if_not_exists
    self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema}")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o9991.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more

The above exception was the direct cause of the following exception:
tests/unit/test_pipeline_builder_simple.py:747: in test_create_schema_if_not_exists
    builder._create_schema_if_not_exists("new_schema_simple_test")
src/pipeline_builder/pipeline/builder.py:1366: in _create_schema_if_not_exists
    raise StepError(
E   _errors_module.ExecutionError: Failed to create schema 'new_schema_simple_test': An error occurred while calling o9991.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
E    | Context: step_name=schema_creation, step_type=validation | Suggestions: Check schema permissions; Verify schema name is valid; Check for naming conflicts
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o9991.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:30 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:47:30 - PipelineRunner - INFO -  PipelineBuilder initialized (schema: test_schema)
------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  PipelineBuilder initialized (schema: test_schema)
_____________ TestSparkForgeWorking.test_pipeline_builder_working ______________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_sparkforge_working.py:97: in test_pipeline_builder_working
    mock_spark_session.sql("CREATE SCHEMA IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5638.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5638.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:32 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:32 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:32 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:32 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:47:32 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:47:32 - PipelineRunner - INFO -  PipelineBuilder initialized (schema: test_schema)
------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  PipelineBuilder initialized (schema: test_schema)
_____ TestTrap1SilentExceptionHandling.test_validation_error_is_re_raised ______
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_trap_1_silent_exception_handling.py:45: in test_validation_error_is_re_raised
    assert "Columns referenced in validation rules do not exist" in error_msg
E   assert 'Columns referenced in validation rules do not exist' in "Data quality assessment failed: An error occurred while calling o10228.count.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)\n\tat org.apache.spa...t.scala:3615)\n\tat jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 61 more\n"
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o10204.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:33 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:33 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:33 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:33 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:33 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:33 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o10228.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_______________ TestSparkForgeWorking.test_writer_system_working _______________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_sparkforge_working.py:228: in test_writer_system_working
    mock_spark_session.sql("CREATE SCHEMA IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5814.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5814.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:34 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:47:34 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_logs
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_logs
_ TestTrap1SilentExceptionHandling.test_successful_assessment_returns_correct_metrics _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o10385.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_trap_1_silent_exception_handling.py:81: in test_successful_assessment_returns_correct_metrics
    result = assess_data_quality(df, None)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o10385.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o10385.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o10361.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:34 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:34 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:34 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:34 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o10385.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestTrap1SilentExceptionHandling.test_successful_assessment_with_rules_returns_correct_metrics _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o10463.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_trap_1_silent_exception_handling.py:104: in test_successful_assessment_with_rules_returns_correct_metrics
    result = assess_data_quality(df, rules)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o10463.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o10463.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o10439.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:36 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o10463.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestTrap1SilentExceptionHandling.test_empty_dataframe_returns_correct_metrics _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o10541.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_trap_1_silent_exception_handling.py:131: in test_empty_dataframe_returns_correct_metrics
    result = assess_data_quality(df, None)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o10541.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o10541.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o10521.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:37 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o10541.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_____________ TestSparkForgeWorking.test_table_operations_working ______________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_sparkforge_working.py:322: in test_table_operations_working
    mock_spark_session.sql("CREATE SCHEMA IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6047.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6047.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:37 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:37 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:37 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:37 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:47:37 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____________ TestSparkForgeWorking.test_validation_utils_working ______________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_sparkforge_working.py:401: in test_validation_utils_working
    assert info["column_count"] == 2
E   assert 0 == 2
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6166.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:39 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
________________ TestSparkForgeWorking.test_edge_cases_working _________________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_sparkforge_working.py:427: in test_edge_cases_working
    assert empty_df.count() == 1
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6317.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6297.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:41 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________ TestSparkForgeWorking.test_comprehensive_coverage_working ___________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_sparkforge_working.py:596: in test_comprehensive_coverage_working
    mock_spark_session.sql("CREATE SCHEMA IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6485.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6485.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:44 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:47:44 - PipelineRunner - INFO -  PipelineBuilder initialized (schema: test_schema)
18:47:44 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_logs
------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  PipelineBuilder initialized (schema: test_schema)
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_logs
_ TestTrap5DefaultSchemaFallbacks.test_validation_mode_skips_schema_validation _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/execution.py:1820: in execute_step
    result.rows_processed = output_df.count()  # type: ignore[attr-defined]
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11014.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_trap_5_default_schema_fallbacks.py:341: in test_validation_mode_skips_schema_validation
    result = engine.execute_step(
src/pipeline_builder/execution.py:1861: in execute_step
    raise ExecutionError(f"Step execution failed: {e}") from e
E   _errors_module.ExecutionError: Step execution failed: An error occurred while calling o11014.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o10994.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:44 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:44 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________________ TestGetDataframeInfo.test_empty_dataframe ___________________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation.py:216: in test_empty_dataframe
    assert info["column_count"] == 1
E   assert 0 == 1
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7134.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:47:54 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:47:54 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_ TestValidationWithFunctions.test_validation_error_handling_with_mock_functions _
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o8391.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced.py:263: in test_validation_error_handling_with_mock_functions
    result = assess_data_quality(self.mock_df, empty_rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o8391.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o8391.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o8371.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:22 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:48:22 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o8391.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestValidationWithFunctions.test_validation_performance_with_mock_functions __
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o8479.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced.py:296: in test_validation_performance_with_mock_functions
    result = assess_data_quality(large_df, rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o8479.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o8479.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o8445.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:24 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:24 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:24 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:24 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:25 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o8479.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
___ TestValidationWithFunctions.test_apply_column_rules_with_mock_functions ____
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_enhanced.py:180: in test_apply_column_rules_with_mock_functions
    result = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o13119.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o13099.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:25 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:25 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:25 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:25 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:25 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___ TestValidationWithFunctions.test_assess_data_quality_with_mock_functions ___
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o13204.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced.py:193: in test_assess_data_quality_with_mock_functions
    result = assess_data_quality(self.mock_df, rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o13204.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o13204.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o13184.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:27 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:27 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:27 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:27 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:27 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:28 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o13204.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
________ TestValidationWithFunctions.test_validation_with_complex_rules ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o13495.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced.py:251: in test_validation_with_complex_rules
    result = assess_data_quality(self.mock_df, complex_rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o13495.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o13495.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o13475.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:35 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o13495.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
___ TestFunctionsIntegration.test_validation_with_mock_functions_end_to_end ____
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o8862.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced.py:465: in test_validation_with_mock_functions_end_to_end
    result = assess_data_quality(df, rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o8862.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o8862.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o8842.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:35 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:48:35 - PipelineRunner - INFO -  PipelineBuilder initialized (schema: test_schema)
------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  PipelineBuilder initialized (schema: test_schema)
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o8862.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_____ TestValidationWithFunctionsSimple.test_validation_with_complex_rules _____
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o9010.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced_simple.py:239: in test_validation_with_complex_rules
    result = assess_data_quality(self.mock_df, complex_rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o9010.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o9010.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o8990.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:38 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:38 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:38 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:38 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:38 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o9010.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestValidationWithFunctionsSimple.test_validation_error_handling_with_mock_functions _
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o9084.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced_simple.py:248: in test_validation_error_handling_with_mock_functions
    result = assess_data_quality(self.mock_df, empty_rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o9084.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o9084.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o9064.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:41 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:41 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:41 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:41 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:41 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:41 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:41 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o9084.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestValidationWithFunctionsSimple.test_apply_column_rules_with_mock_functions _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_enhanced_simple.py:168: in test_apply_column_rules_with_mock_functions
    result = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o13862.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o13842.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:48:43 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_ TestValidationWithFunctionsSimple.test_validation_performance_with_mock_functions _
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o9172.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced_simple.py:281: in test_validation_performance_with_mock_functions
    result = assess_data_quality(large_df, rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o9172.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o9172.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o9138.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:43 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:43 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o9172.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_ TestValidationWithFunctionsSimple.test_assess_data_quality_with_mock_functions _
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o13947.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_enhanced_simple.py:182: in test_assess_data_quality_with_mock_functions
    result = assess_data_quality(self.mock_df, rules, self.mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o13947.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o13947.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o13927.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:45 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:45 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:45 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:45 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:45 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:45 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:48:45 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o13947.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_____________________ TestGetDataframeInfo.test_basic_info _____________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_mock.py:157: in test_basic_info
    assert info["row_count"] == 4
E   assert 0 == 4
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o14261.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:52 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________________ TestApplyColumnRules.test_basic_validation __________________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_mock.py:272: in test_basic_validation
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5302.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5282.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:48:58 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
25/12/15 18:48:58 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________________ TestApplyColumnRules.test_multiple_columns __________________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_mock.py:283: in test_multiple_columns
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5381.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5361.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:00 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
____________________ TestApplyColumnRules.test_empty_rules _____________________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_mock.py:293: in test_empty_rules
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:191: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5463.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5443.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:01 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:01 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:01 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:01 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:02 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
___ TestValidationPropertyBased.test_safe_divide_zero_denominator_properties ___
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_property_based.py:44: in test_safe_divide_zero_denominator_properties
    numerator=st.floats(
E   hypothesis.errors.FailedHealthCheck: Input generation is slow: Hypothesis only generated 7 valid inputs after 3.84 seconds.
E   
E                     count | fraction |    slowest draws (seconds)
E     default_value |    7  |    100%  |      --      --      --      --   3.844
E       numerator   |    7  |      0%  |      --      --      --      --      -- 
E   
E   This could be for a few reasons:
E   1. This strategy could be generating too much data per input. Try decreasing the amount of data generated, for example by decreasing the minimum size of collection strategies like st.lists().
E   2. Some other expensive computation could be running during input generation. For example, if @st.composite or st.data() is interspersed with an expensive computation, HealthCheck.too_slow is likely to trigger. If this computation is unrelated to input generation, move it elsewhere. Otherwise, try making it more efficient, or disable this health check if that is not possible.
E   
E   If you expect input generation to take this long, you can disable this health check with @settings(suppress_health_check=[HealthCheck.too_slow]). See https://hypothesis.readthedocs.io/en/latest/reference/api.html#hypothesis.HealthCheck for details.
---------------------------------- Hypothesis ----------------------------------
You can add @seed(162399460385529054696508870231838412833) to this test or run pytest with --hypothesis-seed=162399460385529054696508870231838412833 to reproduce this failure.
_____________ TestAssessDataQuality.test_basic_quality_assessment ______________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5537.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_mock.py:307: in test_basic_quality_assessment
    result = assess_data_quality(sample_dataframe, rules, mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o5537.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o5537.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5517.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:05 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o5537.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
______________ TestAssessDataQuality.test_multiple_quality_rules _______________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5611.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_mock.py:320: in test_multiple_quality_rules
    result = assess_data_quality(sample_dataframe, rules, mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o5611.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o5611.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5591.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:07 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o5611.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
____________________ TestAssessDataQuality.test_empty_rules ____________________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/validation/data_validation.py:315: in assess_data_quality
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o5685.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more

The above exception was the direct cause of the following exception:
tests/unit/test_validation_mock.py:326: in test_empty_rules
    result = assess_data_quality(sample_dataframe, {}, mock_functions)
src/pipeline_builder/validation/data_validation.py:354: in assess_data_quality
    raise ValidationError(
E   _errors_module.ValidationError: Data quality assessment failed: An error occurred while calling o5685.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
E    | Context: function=assess_data_quality, original_error=An error occurred while calling o5685.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o5665.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:08 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:08 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:08 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:08 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
ERROR    pipeline_builder.validation.data_validation:data_validation.py:353 Unexpected error in assess_data_quality: An error occurred while calling o5685.count.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 61 more
_________________ TestValidationUtils.test_get_dataframe_info __________________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_simple.py:129: in test_get_dataframe_info
    assert info["row_count"] == 2
E   assert 0 == 2
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o14871.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:11 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:11 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:11 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:11 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:11 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
______ TestValidationIntegration.test_validation_workflow_with_mock_data _______
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_simple.py:379: in test_validation_workflow_with_mock_data
    assert info["row_count"] == 3
E   assert 0 == 3
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o10527.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:13 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:13 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:13 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
_____________________ TestGetDataframeInfo.test_basic_info _____________________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_standalone.py:132: in test_basic_info
    assert info["row_count"] == 4
E   assert 0 == 4
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o10715.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:16 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________________ TestApplyColumnRules.test_basic_validation __________________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_standalone.py:358: in test_basic_validation
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11061.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o11041.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:21 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:21 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:21 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
__________________ TestApplyColumnRules.test_multiple_columns __________________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_validation_standalone.py:395: in test_multiple_columns
    valid_df, invalid_df, stats = apply_column_rules(
src/pipeline_builder/validation/data_validation.py:244: in apply_column_rules
    total_rows = df.count()
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11140.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o11120.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:23 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:23 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
______________ TestWriterComprehensive.test_table_exists_function ______________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_writer_comprehensive.py:116: in test_table_exists_function
    mock_spark_session.sql("CREATE SCHEMA IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6617.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6617.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:26 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:26 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:26 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:26 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:26 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
------------------------------ Captured log call -------------------------------
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.non_existent_table exists: An error occurred while calling o6617.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more
______ TestWriterComprehensive.test_write_execution_result_with_metadata _______
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6779.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:311: in test_write_execution_result_with_metadata
    result = writer.write_execution_result(
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o6779.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6739.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:28 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:28 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:28 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:28 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:29 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:29 - LogWriter - INFO - Started monitoring write_execution_result operation: 00f24aaa-bdd2-43db-b924-34eb3e0a0a65
18:49:29 - LogWriter - INFO - Writing execution result for run custom-run-123
18:49:29 - LogWriter - INFO - Processing execution result for run custom-run-123
18:49:29 - LogWriter - INFO - Successfully processed 1 log rows
18:49:29 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:29 - LogWriter - INFO - Completed monitoring 00f24aaa-bdd2-43db-b924-34eb3e0a0a65: 0.63s, 0 rows
18:49:29 - LogWriter - ERROR - Failed to write execution result for run custom-run-123: Failed to create table test_schema.test_table: An error occurred while calling o6779.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: 00f24aaa-bdd2-43db-b924-34eb3e0a0a65
INFO     LogWriter:logging.py:82 Writing execution result for run custom-run-123
INFO     LogWriter:logging.py:82 Processing execution result for run custom-run-123
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o6739.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o6739.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

INFO     LogWriter:logging.py:82 Completed monitoring 00f24aaa-bdd2-43db-b924-34eb3e0a0a65: 0.63s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run custom-run-123: Failed to create table test_schema.test_table: An error occurred while calling o6779.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
_____________ TestWriterComprehensive.test_write_execution_result ______________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o15921.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:287: in test_write_execution_result
    result = writer.write_execution_result(execution_result)
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o15921.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o15881.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:29 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:29 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:29 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:30 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:30 - LogWriter - INFO - Started monitoring write_execution_result operation: 7551bc47-85ab-4d21-9ee6-b94d49448042
18:49:30 - LogWriter - INFO - Writing execution result for run ab9ad72c-0560-4362-a7b5-ec04431f8b05
18:49:30 - LogWriter - INFO - Processing execution result for run ab9ad72c-0560-4362-a7b5-ec04431f8b05
18:49:30 - LogWriter - INFO - Successfully processed 1 log rows
18:49:30 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:31 - LogWriter - INFO - Completed monitoring 7551bc47-85ab-4d21-9ee6-b94d49448042: 1.10s, 0 rows
18:49:31 - LogWriter - ERROR - Failed to write execution result for run ab9ad72c-0560-4362-a7b5-ec04431f8b05: Failed to create table test_schema.test_table: An error occurred while calling o15921.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: 7551bc47-85ab-4d21-9ee6-b94d49448042
INFO     LogWriter:logging.py:82 Writing execution result for run ab9ad72c-0560-4362-a7b5-ec04431f8b05
INFO     LogWriter:logging.py:82 Processing execution result for run ab9ad72c-0560-4362-a7b5-ec04431f8b05
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o15881.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 72 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o15881.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 72 more

INFO     LogWriter:logging.py:82 Completed monitoring 7551bc47-85ab-4d21-9ee6-b94d49448042: 1.10s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run ab9ad72c-0560-4362-a7b5-ec04431f8b05: Failed to create table test_schema.test_table: An error occurred while calling o15921.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
_________________ TestWriterComprehensive.test_write_log_rows __________________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11527.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:407: in test_write_log_rows
    result = writer.write_log_rows(log_rows)
src/pipeline_builder/writer/core.py:563: in write_log_rows
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o11527.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o11487.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:30 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:30 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:30 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:30 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:30 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:49:30 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:30 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:30 - LogWriter - INFO - Started monitoring write_log_rows operation: 895d5df5-5e36-4445-b7b2-145396e2b876
18:49:30 - LogWriter - INFO - Writing 1 log rows for run 0c9e6ad0-166d-4593-9bb7-ecb06ec38121
18:49:30 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:31 - LogWriter - INFO - Completed monitoring 895d5df5-5e36-4445-b7b2-145396e2b876: 1.05s, 0 rows
18:49:31 - LogWriter - ERROR - Failed to write log rows for run 0c9e6ad0-166d-4593-9bb7-ecb06ec38121: Failed to create table test_schema.test_table: An error occurred while calling o11527.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_log_rows operation: 895d5df5-5e36-4445-b7b2-145396e2b876
INFO     LogWriter:logging.py:82 Writing 1 log rows for run 0c9e6ad0-166d-4593-9bb7-ecb06ec38121
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o11487.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o11487.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

INFO     LogWriter:logging.py:82 Completed monitoring 895d5df5-5e36-4445-b7b2-145396e2b876: 1.05s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write log rows for run 0c9e6ad0-166d-4593-9bb7-ecb06ec38121: Failed to create table test_schema.test_table: An error occurred while calling o11527.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
_______________ TestWriterComprehensive.test_write_step_results ________________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o6981.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:358: in test_write_step_results
    result = writer.write_step_results(step_results)
src/pipeline_builder/writer/core.py:493: in write_step_results
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o6981.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o6941.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:31 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:31 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:31 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:31 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:31 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:31 - LogWriter - INFO - Started monitoring write_step_results operation: 6826b939-c6a0-453d-8e2b-70845bd6994c
18:49:31 - LogWriter - INFO - Writing 2 step results for run 3270511b-5946-41e8-94a3-2fa62db8e35f
18:49:31 - LogWriter - INFO - Processing 2 step results for run 3270511b-5946-41e8-94a3-2fa62db8e35f
18:49:31 - LogWriter - INFO - Successfully processed 2 step log rows
18:49:31 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:32 - LogWriter - INFO - Completed monitoring 6826b939-c6a0-453d-8e2b-70845bd6994c: 1.04s, 0 rows
18:49:32 - LogWriter - ERROR - Failed to write step results for run 3270511b-5946-41e8-94a3-2fa62db8e35f: Failed to create table test_schema.test_table: An error occurred while calling o6981.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_step_results operation: 6826b939-c6a0-453d-8e2b-70845bd6994c
INFO     LogWriter:logging.py:82 Writing 2 step results for run 3270511b-5946-41e8-94a3-2fa62db8e35f
INFO     LogWriter:logging.py:82 Processing 2 step results for run 3270511b-5946-41e8-94a3-2fa62db8e35f
INFO     LogWriter:logging.py:82 Successfully processed 2 step log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o6941.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o6941.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

INFO     LogWriter:logging.py:82 Completed monitoring 6826b939-c6a0-453d-8e2b-70845bd6994c: 1.04s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write step results for run 3270511b-5946-41e8-94a3-2fa62db8e35f: Failed to create table test_schema.test_table: An error occurred while calling o6981.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
_____________ TestWriterComprehensive.test_writer_metrics_tracking _____________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o16123.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:459: in test_writer_metrics_tracking
    writer.write_execution_result(execution_result)
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o16123.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o16083.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:32 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:32 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:32 - LogWriter - INFO - Started monitoring write_execution_result operation: b08b39b6-f78f-41c6-ad55-e0e201888f36
18:49:32 - LogWriter - INFO - Writing execution result for run 5d4b58a0-5e7b-4450-a110-b0b689b19c72
18:49:32 - LogWriter - INFO - Processing execution result for run 5d4b58a0-5e7b-4450-a110-b0b689b19c72
18:49:32 - LogWriter - INFO - Successfully processed 1 log rows
18:49:32 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:33 - LogWriter - INFO - Completed monitoring b08b39b6-f78f-41c6-ad55-e0e201888f36: 0.89s, 0 rows
18:49:33 - LogWriter - ERROR - Failed to write execution result for run 5d4b58a0-5e7b-4450-a110-b0b689b19c72: Failed to create table test_schema.test_table: An error occurred while calling o16123.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: b08b39b6-f78f-41c6-ad55-e0e201888f36
INFO     LogWriter:logging.py:82 Writing execution result for run 5d4b58a0-5e7b-4450-a110-b0b689b19c72
INFO     LogWriter:logging.py:82 Processing execution result for run 5d4b58a0-5e7b-4450-a110-b0b689b19c72
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o16083.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 72 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o16083.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 72 more

INFO     LogWriter:logging.py:82 Completed monitoring b08b39b6-f78f-41c6-ad55-e0e201888f36: 0.89s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run 5d4b58a0-5e7b-4450-a110-b0b689b19c72: Failed to create table test_schema.test_table: An error occurred while calling o16123.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
__________ TestWriterComprehensive.test_write_execution_result_batch ___________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11729.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:429: in test_write_execution_result_batch
    result = writer.write_execution_result_batch(execution_results)
src/pipeline_builder/writer/core.py:655: in write_execution_result_batch
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o11729.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o11689.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:49:32 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:33 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:33 - LogWriter - INFO - Started monitoring write_execution_result_batch operation: 8914be72-0197-4aae-9cbd-99a4d9a0b2b3
18:49:33 - LogWriter - INFO - Writing batch of 2 execution results
18:49:33 - LogWriter - INFO - Processing execution result for run 4683d3c0-d8af-4d12-adac-67be0bb8891e
18:49:33 - LogWriter - INFO - Successfully processed 1 log rows
18:49:33 - LogWriter - INFO - Processing execution result for run 46a82d56-ab78-4b6b-b8c0-f1a02057b6c0
18:49:33 - LogWriter - INFO - Successfully processed 1 log rows
18:49:33 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:34 - LogWriter - INFO - Completed monitoring 8914be72-0197-4aae-9cbd-99a4d9a0b2b3: 0.88s, 0 rows
18:49:34 - LogWriter - ERROR - Failed to write execution result batch: Failed to create table test_schema.test_table: An error occurred while calling o11729.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result_batch operation: 8914be72-0197-4aae-9cbd-99a4d9a0b2b3
INFO     LogWriter:logging.py:82 Writing batch of 2 execution results
INFO     LogWriter:logging.py:82 Processing execution result for run 4683d3c0-d8af-4d12-adac-67be0bb8891e
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Processing execution result for run 46a82d56-ab78-4b6b-b8c0-f1a02057b6c0
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o11689.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o11689.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

INFO     LogWriter:logging.py:82 Completed monitoring 8914be72-0197-4aae-9cbd-99a4d9a0b2b3: 0.88s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result batch: Failed to create table test_schema.test_table: An error occurred while calling o11729.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
________ TestWriterComprehensive.test_writer_with_different_log_levels _________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7183.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:527: in test_writer_with_different_log_levels
    result = writer.write_execution_result(execution_result)
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table_DEBUG: An error occurred while calling o7183.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7143.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:33 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:33 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:33 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:33 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:33 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:34 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table_DEBUG
18:49:34 - LogWriter - INFO - Started monitoring write_execution_result operation: 75e849df-6c2b-45e7-a681-915b58727b52
18:49:34 - LogWriter - INFO - Writing execution result for run f26bef5a-a6f8-4d4b-86bd-1db918ee53c0
18:49:34 - LogWriter - INFO - Processing execution result for run f26bef5a-a6f8-4d4b-86bd-1db918ee53c0
18:49:34 - LogWriter - INFO - Successfully processed 1 log rows
18:49:34 - LogWriter - INFO - Creating table if not exists: test_schema.test_table_DEBUG
18:49:35 - LogWriter - INFO - Completed monitoring 75e849df-6c2b-45e7-a681-915b58727b52: 0.84s, 0 rows
18:49:35 - LogWriter - ERROR - Failed to write execution result for run f26bef5a-a6f8-4d4b-86bd-1db918ee53c0: Failed to create table test_schema.test_table_DEBUG: An error occurred while calling o7183.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table_DEBUG
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: 75e849df-6c2b-45e7-a681-915b58727b52
INFO     LogWriter:logging.py:82 Writing execution result for run f26bef5a-a6f8-4d4b-86bd-1db918ee53c0
INFO     LogWriter:logging.py:82 Processing execution result for run f26bef5a-a6f8-4d4b-86bd-1db918ee53c0
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table_DEBUG
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table_DEBUG exists: An error occurred while calling o7143.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table_DEBUG exists: An error occurred while calling o7143.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

INFO     LogWriter:logging.py:82 Completed monitoring 75e849df-6c2b-45e7-a681-915b58727b52: 0.84s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run f26bef5a-a6f8-4d4b-86bd-1db918ee53c0: Failed to create table test_schema.test_table_DEBUG: An error occurred while calling o7183.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
________ TestWriterComprehensive.test_writer_with_different_write_modes ________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o16325.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:488: in test_writer_with_different_write_modes
    result_append = writer_append.write_execution_result(execution_result)
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table_append: An error occurred while calling o16325.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o16285.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:34 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:35 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table_append
18:49:35 - LogWriter - INFO - Started monitoring write_execution_result operation: e5042112-98d8-4ddb-a770-4f03df7ff7c5
18:49:35 - LogWriter - INFO - Writing execution result for run 97dafb0d-0627-4f49-b3c0-f337a755d2e2
18:49:35 - LogWriter - INFO - Processing execution result for run 97dafb0d-0627-4f49-b3c0-f337a755d2e2
18:49:35 - LogWriter - INFO - Successfully processed 1 log rows
18:49:35 - LogWriter - INFO - Creating table if not exists: test_schema.test_table_append
18:49:35 - LogWriter - INFO - Completed monitoring e5042112-98d8-4ddb-a770-4f03df7ff7c5: 0.60s, 0 rows
18:49:35 - LogWriter - ERROR - Failed to write execution result for run 97dafb0d-0627-4f49-b3c0-f337a755d2e2: Failed to create table test_schema.test_table_append: An error occurred while calling o16325.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table_append
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: e5042112-98d8-4ddb-a770-4f03df7ff7c5
INFO     LogWriter:logging.py:82 Writing execution result for run 97dafb0d-0627-4f49-b3c0-f337a755d2e2
INFO     LogWriter:logging.py:82 Processing execution result for run 97dafb0d-0627-4f49-b3c0-f337a755d2e2
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table_append
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table_append exists: An error occurred while calling o16285.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 72 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table_append exists: An error occurred while calling o16285.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 72 more

INFO     LogWriter:logging.py:82 Completed monitoring e5042112-98d8-4ddb-a770-4f03df7ff7c5: 0.60s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run 97dafb0d-0627-4f49-b3c0-f337a755d2e2: Failed to create table test_schema.test_table_append: An error occurred while calling o16325.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
________ TestWriterComprehensive.test_writer_with_compression_settings _________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o11931.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:572: in test_writer_with_compression_settings
    result = writer.write_execution_result(execution_result)
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table_snappy: An error occurred while calling o11931.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o11891.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:34 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:49:34 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:35 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table_snappy
18:49:35 - LogWriter - INFO - Started monitoring write_execution_result operation: a7bfccea-9518-4ee3-991a-c593b386ca95
18:49:35 - LogWriter - INFO - Writing execution result for run a1c6b0ea-4328-4be7-875c-8b726047c686
18:49:35 - LogWriter - INFO - Processing execution result for run a1c6b0ea-4328-4be7-875c-8b726047c686
18:49:35 - LogWriter - INFO - Successfully processed 1 log rows
18:49:35 - LogWriter - INFO - Creating table if not exists: test_schema.test_table_snappy
18:49:35 - LogWriter - INFO - Completed monitoring a7bfccea-9518-4ee3-991a-c593b386ca95: 0.65s, 0 rows
18:49:35 - LogWriter - ERROR - Failed to write execution result for run a1c6b0ea-4328-4be7-875c-8b726047c686: Failed to create table test_schema.test_table_snappy: An error occurred while calling o11931.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table_snappy
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: a7bfccea-9518-4ee3-991a-c593b386ca95
INFO     LogWriter:logging.py:82 Writing execution result for run a1c6b0ea-4328-4be7-875c-8b726047c686
INFO     LogWriter:logging.py:82 Processing execution result for run a1c6b0ea-4328-4be7-875c-8b726047c686
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table_snappy
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table_snappy exists: An error occurred while calling o11891.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table_snappy exists: An error occurred while calling o11891.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

INFO     LogWriter:logging.py:82 Completed monitoring a7bfccea-9518-4ee3-991a-c593b386ca95: 0.65s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run a1c6b0ea-4328-4be7-875c-8b726047c686: Failed to create table test_schema.test_table_snappy: An error occurred while calling o11931.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
__________ TestWriterComprehensive.test_writer_with_custom_batch_size __________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7385.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:548: in test_writer_with_custom_batch_size
    result = writer.write_execution_result(execution_result)
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o7385.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7345.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:35 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:35 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:35 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:36 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:36 - LogWriter - INFO - Started monitoring write_execution_result operation: 33535596-dd38-4709-834f-6d40f0ec0abd
18:49:36 - LogWriter - INFO - Writing execution result for run 63f39dda-8875-4aeb-a97e-41a1eabb627a
18:49:36 - LogWriter - INFO - Processing execution result for run 63f39dda-8875-4aeb-a97e-41a1eabb627a
18:49:36 - LogWriter - INFO - Successfully processed 1 log rows
18:49:36 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:37 - LogWriter - INFO - Completed monitoring 33535596-dd38-4709-834f-6d40f0ec0abd: 1.01s, 0 rows
18:49:37 - LogWriter - ERROR - Failed to write execution result for run 63f39dda-8875-4aeb-a97e-41a1eabb627a: Failed to create table test_schema.test_table: An error occurred while calling o7385.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: 33535596-dd38-4709-834f-6d40f0ec0abd
INFO     LogWriter:logging.py:82 Writing execution result for run 63f39dda-8875-4aeb-a97e-41a1eabb627a
INFO     LogWriter:logging.py:82 Processing execution result for run 63f39dda-8875-4aeb-a97e-41a1eabb627a
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o7345.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o7345.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

INFO     LogWriter:logging.py:82 Completed monitoring 33535596-dd38-4709-834f-6d40f0ec0abd: 1.01s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run 63f39dda-8875-4aeb-a97e-41a1eabb627a: Failed to create table test_schema.test_table: An error occurred while calling o7385.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor192.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
________ TestWriterComprehensive.test_writer_schema_evolution_settings _________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o16527.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:620: in test_writer_schema_evolution_settings
    result = writer.write_execution_result(execution_result)
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o16527.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 15 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o16487.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:36 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:36 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:36 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:36 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:36 - LogWriter - INFO - Started monitoring write_execution_result operation: 921e0759-df47-4c70-b4bb-c1b99824a91f
18:49:36 - LogWriter - INFO - Writing execution result for run b16d50d3-5167-466a-becb-28bd6c36d87b
18:49:36 - LogWriter - INFO - Processing execution result for run b16d50d3-5167-466a-becb-28bd6c36d87b
18:49:36 - LogWriter - INFO - Successfully processed 1 log rows
18:49:36 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:38 - LogWriter - INFO - Completed monitoring 921e0759-df47-4c70-b4bb-c1b99824a91f: 1.17s, 0 rows
18:49:38 - LogWriter - ERROR - Failed to write execution result for run b16d50d3-5167-466a-becb-28bd6c36d87b: Failed to create table test_schema.test_table: An error occurred while calling o16527.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: 921e0759-df47-4c70-b4bb-c1b99824a91f
INFO     LogWriter:logging.py:82 Writing execution result for run b16d50d3-5167-466a-becb-28bd6c36d87b
INFO     LogWriter:logging.py:82 Processing execution result for run b16d50d3-5167-466a-becb-28bd6c36d87b
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o16487.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 72 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o16487.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 72 more

INFO     LogWriter:logging.py:82 Completed monitoring 921e0759-df47-4c70-b4bb-c1b99824a91f: 1.17s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run b16d50d3-5167-466a-becb-28bd6c36d87b: Failed to create table test_schema.test_table: An error occurred while calling o16527.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 15 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
_________ TestWriterComprehensive.test_writer_with_partition_settings __________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
src/pipeline_builder/writer/storage.py:330: in create_table_if_not_exists
    empty_df.write.format("delta")
.venv39/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1586: in saveAsTable
    self._jwrite.saveAsTable(name)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o12133.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more

The above exception was the direct cause of the following exception:
tests/unit/test_writer_comprehensive.py:595: in test_writer_with_partition_settings
    result = writer.write_execution_result(execution_result)
src/pipeline_builder/writer/core.py:402: in write_execution_result
    self.storage_manager.create_table_if_not_exists(self.schema)
src/pipeline_builder/writer/storage.py:399: in create_table_if_not_exists
    raise WriterTableError(
E   pipeline_builder.writer.exceptions.WriterTableError: Failed to create table test_schema.test_table: An error occurred while calling o12133.saveAsTable.
E   : org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
E   	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
E   	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
E   	at jdk.internal.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
E   	at scala.util.Try$.apply(Try.scala:213)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
E   	at scala.util.Failure.orElse(Try.scala:224)
E   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
E   	... 14 more
E   
E   Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
E   Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o12093.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:37 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:37 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:37 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:49:37 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:37 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_table
18:49:37 - LogWriter - INFO - Started monitoring write_execution_result operation: f0d9e9f1-ccc9-4772-812c-e3d078cd7685
18:49:37 - LogWriter - INFO - Writing execution result for run 0e536b29-4a01-4f76-9800-1f57a58f7370
18:49:37 - LogWriter - INFO - Processing execution result for run 0e536b29-4a01-4f76-9800-1f57a58f7370
18:49:37 - LogWriter - INFO - Successfully processed 1 log rows
18:49:37 - LogWriter - INFO - Creating table if not exists: test_schema.test_table
18:49:38 - LogWriter - INFO - Completed monitoring f0d9e9f1-ccc9-4772-812c-e3d078cd7685: 1.20s, 0 rows
18:49:38 - LogWriter - ERROR - Failed to write execution result for run 0e536b29-4a01-4f76-9800-1f57a58f7370: Failed to create table test_schema.test_table: An error occurred while calling o12133.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_table
INFO     LogWriter:logging.py:82 Started monitoring write_execution_result operation: f0d9e9f1-ccc9-4772-812c-e3d078cd7685
INFO     LogWriter:logging.py:82 Writing execution result for run 0e536b29-4a01-4f76-9800-1f57a58f7370
INFO     LogWriter:logging.py:82 Processing execution result for run 0e536b29-4a01-4f76-9800-1f57a58f7370
INFO     LogWriter:logging.py:82 Successfully processed 1 log rows
INFO     LogWriter:logging.py:82 Creating table if not exists: test_schema.test_table
WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o12093.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

WARNING  pipeline_builder.table_operations:table_operations.py:185 Error checking if table test_schema.test_table exists: An error occurred while calling o12093.table.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 71 more

INFO     LogWriter:logging.py:82 Completed monitoring f0d9e9f1-ccc9-4772-812c-e3d078cd7685: 1.20s, 0 rows
ERROR    LogWriter:logging.py:92 Failed to write execution result for run 0e536b29-4a01-4f76-9800-1f57a58f7370: Failed to create table test_schema.test_table: An error occurred while calling o12133.saveAsTable.
: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:568)
	at jdk.internal.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)
	... 14 more

Context: {'schema': "StructType([StructField('run_id', StringType(), False), StructField('run_mode', StringType(), False), StructField('run_started_at', TimestampType(), True), StructField('run_ended_at', TimestampType(), True), StructField('execution_id', StringType(), False), StructField('pipeline_id', StringType(), False), StructField('schema', StringType(), False), StructField('phase', StringType(), False), StructField('step_name', StringType(), False), StructField('step_type', StringType(), False), StructField('start_time', TimestampType(), True), StructField('end_time', TimestampType(), True), StructField('duration_secs', FloatType(), False), StructField('table_fqn', StringType(), True), StructField('write_mode', StringType(), True), StructField('input_rows', IntegerType(), True), StructField('output_rows', IntegerType(), True), StructField('rows_written', IntegerType(), True), StructField('rows_processed', IntegerType(), False), StructField('table_total_rows', IntegerType(), True), StructField('valid_rows', IntegerType(), False), StructField('invalid_rows', IntegerType(), False), StructField('validation_rate', FloatType(), False), StructField('success', BooleanType(), False), StructField('error_message', StringType(), True), StructField('memory_usage_mb', FloatType(), True), StructField('cpu_usage_percent', FloatType(), True), StructField('metadata', StringType(), True), StructField('created_at', StringType(), True), StructField('updated_at', StringType(), True)])"}
Suggestions: Check table permissions; Verify schema configuration; Ensure Delta Lake is properly configured
_______________ TestWriterCoreSimple.test_table_exists_function ________________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_writer_core_simple.py:125: in test_table_exists_function
    spark_session.sql("CREATE DATABASE IF NOT EXISTS test_schema")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1631: in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7718.sql.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
E   	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
E   	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
E   	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
E   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
E   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
E   	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 85 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7718.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:42 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
____________ TestWriterCoreSimple.test_log_writer_with_sample_data _____________
[gw2] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_writer_core_simple.py:223: in test_log_writer_with_sample_data
    assert sample_dataframe.count() > 0
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o7857.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o7837.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:45 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:45 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:45 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:45 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:45 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor147.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:45 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_logs
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_logs
_____________ TestWriterCoreSimple.test_log_writer_error_handling ______________
[gw0] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_writer_core_simple.py:235: in test_log_writer_error_handling
    spark_session.table("nonexistent.table")
.venv39/lib/python3.9/site-packages/pyspark/sql/session.py:1667: in table
    return DataFrame(self._jsparkSession.table(tableName), self)
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o12525.table.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.currentCatalog(Analyzer.scala:202)
E   	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:125)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1301)
E   	at scala.Option.orElse(Option.scala:447)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1300)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1157)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1121)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
E   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
E   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1121)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1080)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
E   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
E   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
E   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
E   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
E   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
E   	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)
E   	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:602)
E   	at jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 71 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o12525.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:45 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor133.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:46 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_logs
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_logs
___________ TestWriterCoreSimple.test_log_writer_metrics_collection ____________
[gw4] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/unit/test_writer_core_simple.py:248: in test_log_writer_metrics_collection
    assert sample_dataframe.count() > 0
.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1240: in count
    return int(self._jdf.count())
.venv39/lib/python3.9/site-packages/py4j/java_gateway.py:1322: in __call__
    return_value = get_return_value(
.venv39/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179: in deco
    return f(*a, **kw)
.venv39/lib/python3.9/site-packages/py4j/protocol.py:326: in get_return_value
    raise Py4JJavaError(
E   py4j.protocol.Py4JJavaError: An error occurred while calling o17102.count.
E   : org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
E   	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
E   	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
E   	at scala.Option.map(Option.scala:230)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
E   	at org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)
E   	at org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)
E   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
E   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
E   	at scala.collection.immutable.List.foldLeft(List.scala:91)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)
E   	at org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
E   	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
E   	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
E   	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
E   	at scala.collection.immutable.List.foreach(List.scala:431)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
E   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
E   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
E   	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
E   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
E   	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
E   	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
E   	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
E   	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
E   	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
E   	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
E   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
E   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
E   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
E   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
E   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
E   	at org.apache.spark.sql.Dataset.count(Dataset.scala:3615)
E   	at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
E   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
E   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
E   	at py4j.Gateway.invoke(Gateway.java:282)
E   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
E   	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E   	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E   	at java.base/java.lang.Thread.run(Thread.java:829)
E   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
E   	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
E   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
E   	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
E   	... 61 more
---------------------------- Captured stdout setup -----------------------------
 Using Python at: /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 PYSPARK_DRIVER_PYTHON=/Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
 Using Java at: /opt/homebrew/opt/openjdk@11
 Configuring real Spark with Delta Lake support for all tests
 Delta catalog configured correctly
 Delta Lake configuration completed
 Could not create test_schema database: An error occurred while calling o17082.sql.
: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1926)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)
	at org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:68)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:87)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:87)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:52)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:34)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:89)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)
	... 85 more

---------------------------- Captured stderr setup -----------------------------
25/12/15 18:49:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/12/15 18:49:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/12/15 18:49:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/12/15 18:49:46 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/12/15 18:49:46 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/12/15 18:49:46 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
25/12/15 18:49:46 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
25/12/15 18:49:46 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.
java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)
	at org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:94)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)
	at org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)
	at jdk.internal.reflect.GeneratedConstructorAccessor47.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
----------------------------- Captured stdout call -----------------------------
18:49:47 - LogWriter - INFO - LogWriter initialized for table: test_schema.test_logs
------------------------------ Captured log call -------------------------------
INFO     LogWriter:logging.py:82 LogWriter initialized for table: test_schema.test_logs
_______ TestEcommercePipeline.test_complete_ecommerce_pipeline_execution _______
[gw1] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/builder_tests/test_ecommerce_pipeline.py:48: in test_complete_ecommerce_pipeline_execution
    builder.with_bronze_rules(
src/pipeline_builder/pipeline/builder.py:408: in with_bronze_rules
    converted_rules = _convert_rules_to_expressions(rules, self.functions)
src/pipeline_builder/validation/data_validation.py:114: in _convert_rules_to_expressions
    _convert_rule_to_expression(rule, column_name, functions)
src/pipeline_builder/validation/data_validation.py:97: in _convert_rule_to_expression
    return functions.expr(rule)
.venv39/lib/python3.9/site-packages/sparkless/functions/functions.py:1338: in expr
    Functions._require_active_session(f"expression '{expression}'")
.venv39/lib/python3.9/site-packages/sparkless/functions/functions.py:92: in _require_active_session
    raise RuntimeError(
E   RuntimeError: Cannot perform expression 'gt': No active SparkSession found. This operation requires an active SparkSession, similar to PySpark. Create a SparkSession first: spark = SparkSession('app_name')
---------------------------- Captured stderr setup -----------------------------
25/12/15 18:51:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
----------------------------- Captured stdout call -----------------------------
18:51:06 - PipelineRunner - INFO -  PipelineBuilder initialized (schema: bronze)
------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  PipelineBuilder initialized (schema: bronze)
________________ TestEcommercePipeline.test_validation_failures ________________
[gw1] darwin -- Python 3.9.23 /Users/odosmatthews/Documents/coding/sparkforge/.venv39/bin/python
tests/builder_tests/test_ecommerce_pipeline.py:374: in test_validation_failures
    builder.with_bronze_rules(
src/pipeline_builder/pipeline/builder.py:408: in with_bronze_rules
    converted_rules = _convert_rules_to_expressions(rules, self.functions)
src/pipeline_builder/validation/data_validation.py:114: in _convert_rules_to_expressions
    _convert_rule_to_expression(rule, column_name, functions)
src/pipeline_builder/validation/data_validation.py:97: in _convert_rule_to_expression
    return functions.expr(rule)
.venv39/lib/python3.9/site-packages/sparkless/functions/functions.py:1338: in expr
    Functions._require_active_session(f"expression '{expression}'")
.venv39/lib/python3.9/site-packages/sparkless/functions/functions.py:92: in _require_active_session
    raise RuntimeError(
E   RuntimeError: Cannot perform expression 'gt': No active SparkSession found. This operation requires an active SparkSession, similar to PySpark. Create a SparkSession first: spark = SparkSession('app_name')
---------------------------- Captured stderr setup -----------------------------
25/12/15 18:51:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
----------------------------- Captured stdout call -----------------------------
18:51:08 - PipelineRunner - INFO -  PipelineBuilder initialized (schema: bronze)
------------------------------ Captured log call -------------------------------
INFO     PipelineRunner:logging.py:82  PipelineBuilder initialized (schema: bronze)
=============================== warnings summary ===============================
src/sql_pipeline_builder/tests/test_full_pipeline.py:28: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/src/sql_pipeline_builder/tests/test_full_pipeline.py:28: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base: Any = declarative_base()

src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py:20: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/src/sql_pipeline_builder/tests/test_sql_pipeline_builder.py:20: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base: Any = declarative_base()

src/sql_pipeline_builder/tests/test_validation_and_table_ops.py:17: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/src/sql_pipeline_builder/tests/test_validation_and_table_ops.py:17: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
    Base: Any = declarative_base()

tests/builder_pyspark_tests/test_healthcare_pipeline.py:582: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/builder_pyspark_tests/test_healthcare_pipeline.py:582: PytestUnknownMarkWarning: Unknown pytest.mark.pyspark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.pyspark

tests/builder_pyspark_tests/test_supply_chain_pipeline.py:516: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/builder_pyspark_tests/test_supply_chain_pipeline.py:516: PytestUnknownMarkWarning: Unknown pytest.mark.pyspark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.pyspark

tests/compat_pyspark/test_pyspark_compatibility.py:15: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/compat_pyspark/test_pyspark_compatibility.py:15: PytestUnknownMarkWarning: Unknown pytest.mark.pyspark_compat - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytestmark = pytest.mark.pyspark_compat

tests/security/test_security_integration.py:498: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/security/test_security_integration.py:498: PytestUnknownMarkWarning: Unknown pytest.mark.security - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.security

tests/system/test_delta_lake.py:78: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_delta_lake.py:78: PytestUnknownMarkWarning: Unknown pytest.mark.delta - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.delta

tests/system/test_simple_real_spark.py:75: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:75: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:94: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:94: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:115: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:115: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:131: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:131: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:145: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:145: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:161: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:161: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:186: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:186: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:201: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:201: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:219: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:219: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_simple_real_spark.py:220: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_simple_real_spark.py:220: PytestUnknownMarkWarning: Unknown pytest.mark.pyspark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.pyspark

tests/system/test_utils.py:81: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:81: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:95: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:95: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:104: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:104: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:125: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:125: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:139: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:139: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:152: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:152: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:162: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:162: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:205: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:205: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:222: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:222: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:250: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:250: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:276: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:276: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:304: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:304: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

tests/system/test_utils.py:329: 10 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/system/test_utils.py:329: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.spark

scripts/test_python38_environment.py::test_python_version
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but scripts/test_python38_environment.py::test_python_version returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

scripts/test_python38_environment.py::test_java_environment
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but scripts/test_python38_environment.py::test_java_environment returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

scripts/test_python38_environment.py::test_spark_session
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but scripts/test_python38_environment.py::test_spark_session returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

scripts/test_python38_environment.py::test_sparkforge_imports
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but scripts/test_python38_environment.py::test_sparkforge_imports returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

scripts/test_python38_environment.py::test_type_annotations
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but scripts/test_python38_environment.py::test_type_annotations returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

scripts/test_python38_environment.py::test_sparkforge_functionality
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but scripts/test_python38_environment.py::test_sparkforge_functionality returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

scripts/test_python38_environment.py::test_dict_annotation_checker
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but scripts/test_python38_environment.py::test_dict_annotation_checker returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

src/sql_pipeline_builder/tests/test_full_pipeline.py: 16 warnings
src/sql_pipeline_builder/tests/test_validation_and_table_ops.py: 1 warning
  /Users/odosmatthews/Documents/coding/sparkforge/src/sql_pipeline_builder/table_operations.py:171: SADeprecationWarning: Table.tometadata() is renamed to Table.to_metadata() (deprecated since: 1.4)
    table_copy = model_class.__table__.tometadata(metadata, schema=ddl_schema)

test_environment.py::test_python_version
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but test_environment.py::test_python_version returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

test_environment.py::test_pyspark
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but test_environment.py::test_pyspark returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

test_environment.py::test_spark_session
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but test_environment.py::test_spark_session returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

test_environment.py::test_delta_lake
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but test_environment.py::test_delta_lake returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

test_environment.py::test_sparkforge
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but test_environment.py::test_sparkforge returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

test_environment.py::test_testing_tools
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but test_environment.py::test_testing_tools returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

test_environment.py::test_dev_tools
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but test_environment.py::test_dev_tools returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/unit/test_edge_cases.py::TestEdgeCases::test_writer_edge_cases
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_edge_cases.py:416: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=minimal_config)

tests/unit/test_edge_cases.py::TestEdgeCases::test_writer_edge_cases
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_edge_cases.py:432: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer_max = LogWriter(spark=mock_spark_session, config=max_config)

tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_writer_system_working
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_sparkforge_working.py:203: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_comprehensive_coverage_working
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_sparkforge_working.py:583: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=writer_config)

tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_core_writer_raises_specific_exceptions
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_trap_4_broad_exception_catching.py:31: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(

tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_core_writer_analytics_raises_specific_exceptions
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_trap_4_broad_exception_catching.py:57: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(

tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_run_id_handling
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_trap_9_default_value_fallbacks.py:146: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=spark_session, config=config)

tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_batch_run_ids_handling
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_trap_9_default_value_fallbacks.py:185: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=spark_session, config=config)

tests/unit/test_trap_9_default_value_fallbacks.py::TestTrap9DefaultValueFallbacks::test_log_writer_display_limit_handling
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_trap_9_default_value_fallbacks.py:223: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=spark_session, config=config)

tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_exception_chaining_preserves_original_error
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_trap_4_broad_exception_catching.py:155: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(

tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_no_generic_error_responses_returned
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_trap_4_broad_exception_catching.py:178: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(

tests/unit/test_trap_4_broad_exception_catching.py::TestTrap4BroadExceptionCatching::test_error_logging_before_raising
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_trap_4_broad_exception_catching.py:216: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(

tests/compat_pyspark/test_pyspark_compatibility.py::TestPySparkCompatibility::test_pyspark_table_operations
  /Users/odosmatthews/Documents/coding/sparkforge/.venv39/lib/python3.9/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.
    warnings.warn(

tests/unit/test_working_examples.py::TestWorkingExamples::test_log_writer_with_config
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_working_examples.py:107: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_initialization
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:197: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_invalid_config
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:235: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_log_writer_with_custom_logger
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:218: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result_with_metadata
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:304: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:247: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_log_rows
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:372: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_step_results
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:330: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_metrics_tracking
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:443: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result_batch
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:421: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_different_log_levels
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:521: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_different_write_modes
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:482: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer_append = LogWriter(spark=mock_spark_session, config=config_append)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_compression_settings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:564: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_custom_batch_size
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:540: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_schema_evolution_settings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:610: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_partition_settings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:586: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_components_initialization
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:645: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_error_handling
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:631: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_schema_creation
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:671: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_table_fqn
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:690: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=mock_spark_session, config=config)

tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_table_fqn
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_comprehensive.py:701: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer2 = LogWriter(spark=mock_spark_session, config=config2)

tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_initialization
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_core_simple.py:63: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=spark_session, config=config)

tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_get_spark
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_core_simple.py:95: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=spark_session, config=config)

tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_initialization_with_config
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_core_simple.py:75: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=spark_session, config=config)

tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_get_config
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_core_simple.py:107: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=spark_session, config=config)

tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_with_sample_data
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_core_simple.py:220: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    LogWriter(spark=spark_session, config=config)

tests/unit/writer/test_core.py::TestLogWriter::test_init_valid_config
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:85: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_invalid_input
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:166: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_validation_failure
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:186: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_write_step_results
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:216: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_write_log_rows_success
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:240: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_write_log_rows_validation_failure
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:270: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_get_metrics
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:281: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_reset_metrics
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:299: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_show_logs
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:319: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_show_logs_no_limit
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:335: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py::TestLogWriter::test_get_table_info
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:346: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/writer/test_core.py: 15 warnings
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:66: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    return LogWriter(mock_spark, config=valid_config, logger=mock_logger)

tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_metrics_collection
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_core_simple.py:240: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    LogWriter(spark=spark_session, config=config)

tests/unit/writer/test_core.py::TestLogWriter::test_init_default_logger
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:104: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config)

tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_error_handling
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_core_simple.py:231: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    LogWriter(spark=spark_session, config=config)

tests/unit/writer/test_core.py::TestLogWriter::test_init_invalid_config
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:96: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    LogWriter(mock_spark, config=invalid_config, logger=mock_logger)

tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_invalid_spark_session
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/test_writer_core_simple.py:85: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(spark=None, config=config)

tests/unit/writer/test_core.py::TestLogWriter::test_write_execution_result_success
  /Users/odosmatthews/Documents/coding/sparkforge/tests/unit/writer/test_core.py:127: DeprecationWarning: Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.
    writer = LogWriter(mock_spark, config=valid_config, logger=mock_logger)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_execution_with_mock_data
FAILED tests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_step_execution_with_real_data
FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_dataframe_operations
FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_transformations
FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_data_quality
FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_metadata_operations
FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_performance
FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_error_handling
FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_schema_operations
FAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_joins
FAILED tests/system/test_utils.py::TestDataValidation::test_apply_column_rules
FAILED tests/system/test_utils.py::TestDataValidation::test_assess_data_quality
FAILED tests/system/test_utils.py::TestDataValidation::test_get_dataframe_info
FAILED tests/system/test_utils.py::TestDataTransformationUtilities::test_basic_dataframe_operations
FAILED tests/system/test_utils.py::TestDataTransformationUtilities::test_dataframe_filtering
FAILED tests/system/test_utils.py::TestPerformanceWithRealData::test_large_dataset_validation
FAILED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_existing_columns_validation_success
FAILED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_empty_rules_validation_success
FAILED tests/unit/test_bronze_rules_column_validation.py::TestBronzeRulesColumnValidation::test_bronze_step_with_provided_data
FAILED tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_dict_data
FAILED tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_tuple_data
FAILED tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_structtype_schema
FAILED tests/unit/test_compat_helpers.py::TestCreateDataframeCompat::test_create_dataframe_with_tuple_and_structtype
FAILED tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_dict
FAILED tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_schema
FAILED tests/unit/test_compat_helpers.py::TestCreateTestDataframe::test_create_test_dataframe_with_tuples
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_empty_dataframe_operations
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_null_value_handling
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_large_dataset_operations
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_complex_schema_operations
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_boundary_values - p...
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_concurrent_operations
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_memory_management
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_schema_evolution - ...
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_dataframe_edge_cases
FAILED tests/unit/test_edge_cases.py::TestEdgeCases::test_session_edge_cases
FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_silver_step_success
FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_gold_step_success
FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_with_rules_validation
FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_validation_only_mode
FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_silver_missing_schema
FAILED tests/unit/test_execution_100_coverage.py::TestExecuteStepComplete::test_execute_step_gold_missing_schema
FAILED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_with_sample_data
FAILED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_error_handling
FAILED tests/unit/test_execution_engine_simple.py::TestExecutionEngineSimple::test_execution_engine_metrics_collection
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_mode_uses_append_for_silver_step
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_incremental_mode_uses_overwrite_for_gold_step
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_initial_mode_uses_overwrite_for_silver_step
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_initial_mode_uses_overwrite_for_gold_step
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_full_refresh_mode_uses_overwrite_for_silver_step
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_full_refresh_mode_uses_overwrite_for_gold_step
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_validation_only_mode_has_no_write_mode_for_silver_step
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_validation_only_mode_has_no_write_mode_for_gold_step
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_spark_write_mode_matches_result_write_mode
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_write_mode_consistency_across_step_types
FAILED tests/system/test_utils.py::TestPerformanceWithRealData::test_complex_transformations
FAILED tests/unit/test_execution_write_mode.py::TestExecutionEngineWriteMode::test_bronze_step_has_no_write_mode
FAILED tests/unit/test_execution_write_mode.py::TestWriteModeRegression::test_incremental_mode_uses_append_for_silver_steps
FAILED tests/unit/test_execution_write_mode.py::TestWriteModeRegression::test_gold_incremental_mode_uses_overwrite
FAILED tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_validate_schema_existing
FAILED tests/unit/test_pipeline_builder_comprehensive.py::TestHelperMethods::test_create_schema_if_not_exists
FAILED tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_validate_schema_existing
FAILED tests/unit/test_pipeline_builder_simple.py::TestHelperMethods::test_create_schema_if_not_exists
FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_pipeline_builder_working
FAILED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_validation_error_is_re_raised
FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_writer_system_working
FAILED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_successful_assessment_returns_correct_metrics
FAILED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_successful_assessment_with_rules_returns_correct_metrics
FAILED tests/unit/test_trap_1_silent_exception_handling.py::TestTrap1SilentExceptionHandling::test_empty_dataframe_returns_correct_metrics
FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_table_operations_working
FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_validation_utils_working
FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_edge_cases_working
FAILED tests/unit/test_sparkforge_working.py::TestSparkForgeWorking::test_comprehensive_coverage_working
FAILED tests/unit/test_trap_5_default_schema_fallbacks.py::TestTrap5DefaultSchemaFallbacks::test_validation_mode_skips_schema_validation
FAILED tests/unit/test_validation.py::TestGetDataframeInfo::test_empty_dataframe
FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_error_handling_with_mock_functions
FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_performance_with_mock_functions
FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_apply_column_rules_with_mock_functions
FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_assess_data_quality_with_mock_functions
FAILED tests/unit/test_validation_enhanced.py::TestValidationWithFunctions::test_validation_with_complex_rules
FAILED tests/unit/test_validation_enhanced.py::TestFunctionsIntegration::test_validation_with_mock_functions_end_to_end
FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_with_complex_rules
FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_error_handling_with_mock_functions
FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_apply_column_rules_with_mock_functions
FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_validation_performance_with_mock_functions
FAILED tests/unit/test_validation_enhanced_simple.py::TestValidationWithFunctionsSimple::test_assess_data_quality_with_mock_functions
FAILED tests/unit/test_validation_mock.py::TestGetDataframeInfo::test_basic_info
FAILED tests/unit/test_validation_mock.py::TestApplyColumnRules::test_basic_validation
FAILED tests/unit/test_validation_mock.py::TestApplyColumnRules::test_multiple_columns
FAILED tests/unit/test_validation_mock.py::TestApplyColumnRules::test_empty_rules
FAILED tests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_safe_divide_zero_denominator_properties
FAILED tests/unit/test_validation_mock.py::TestAssessDataQuality::test_basic_quality_assessment
FAILED tests/unit/test_validation_mock.py::TestAssessDataQuality::test_multiple_quality_rules
FAILED tests/unit/test_validation_mock.py::TestAssessDataQuality::test_empty_rules
FAILED tests/unit/test_validation_simple.py::TestValidationUtils::test_get_dataframe_info
FAILED tests/unit/test_validation_simple.py::TestValidationIntegration::test_validation_workflow_with_mock_data
FAILED tests/unit/test_validation_standalone.py::TestGetDataframeInfo::test_basic_info
FAILED tests/unit/test_validation_standalone.py::TestApplyColumnRules::test_basic_validation
FAILED tests/unit/test_validation_standalone.py::TestApplyColumnRules::test_multiple_columns
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_table_exists_function
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result_with_metadata
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_log_rows
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_step_results
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_metrics_tracking
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_write_execution_result_batch
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_different_log_levels
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_different_write_modes
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_compression_settings
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_custom_batch_size
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_schema_evolution_settings
FAILED tests/unit/test_writer_comprehensive.py::TestWriterComprehensive::test_writer_with_partition_settings
FAILED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_table_exists_function
FAILED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_with_sample_data
FAILED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_error_handling
FAILED tests/unit/test_writer_core_simple.py::TestWriterCoreSimple::test_log_writer_metrics_collection
FAILED tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_complete_ecommerce_pipeline_execution
FAILED tests/builder_tests/test_ecommerce_pipeline.py::TestEcommercePipeline::test_validation_failures
ERROR tests/unit/test_validation.py::TestGetDataframeInfo::test_basic_info - ...
ERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_valid_schema
ERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_missing_columns
ERROR tests/unit/test_validation.py::TestApplyColumnRules::test_basic_validation
ERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_extra_columns
ERROR tests/unit/test_validation.py::TestApplyColumnRules::test_none_rules_raises_error
ERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_empty_expected_columns
ERROR tests/unit/test_validation.py::TestApplyColumnRules::test_empty_rules
ERROR tests/unit/test_validation.py::TestApplyColumnRules::test_complex_rules
ERROR tests/unit/test_validation.py::TestApplyValidationRules::test_apply_column_rules_basic
ERROR tests/unit/test_validation.py::TestApplyValidationRules::test_apply_column_rules_empty
ERROR tests/unit/test_validation.py::TestAssessDataQuality::test_basic_data_quality_assessment
ERROR tests/unit/test_validation.py::TestAssessDataQuality::test_data_quality_with_rules
= 118 failed, 1642 passed, 30 skipped, 412 warnings, 13 errors in 588.28s (0:09:48) =
