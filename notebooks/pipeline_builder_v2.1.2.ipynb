{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PipelineBuilder & LogWriter v2.1.2 - Standalone Notebook\n",
    "#\n",
    "# This notebook contains the complete PipelineBuilder and LogWriter implementation\n",
    "# as a standalone, executable notebook. All dependencies are included as cells\n",
    "# in the correct order.\n",
    "#\n",
    "# Usage:\n",
    "# 1. Run all cells from top to bottom\n",
    "# 2. The PipelineBuilder and LogWriter classes will be available after all cells execute\n",
    "# 3. Use PipelineBuilder to build and execute data pipelines\n",
    "# 4. Use LogWriter to log and analyze pipeline execution results\n",
    "#\n",
    "# Note: This is generated from version 2.1.2. Module dependencies are\n",
    "# resolved automatically from source code analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# External imports (PySpark, standard library)\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, deque\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from functools import wraps\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Generator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Protocol,\n",
    "    Tuple,\n",
    "    TypedDict,\n",
    "    TypeVar,\n",
    "    Union,\n",
    "    cast,\n",
    ")\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import Column, DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Delta Lake imports\n",
    "try:\n",
    "    from delta.tables import DeltaTable\n",
    "except ImportError:\n",
    "    print(\"⚠️  Delta Lake not available. Some features may not work.\")\n",
    "    DeltaTable = None\n",
    "\n",
    "# Optional imports\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"⚠️  psutil not available. Memory monitoring disabled.\")\n",
    "    psutil = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.logging (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from datetime import timezone\n",
    "\n",
    "\n",
    "class PipelineLogger:\n",
    "    \"\"\"\n",
    "    Simple, focused logging for pipeline operations.\n",
    "\n",
    "    Features:\n",
    "    - Basic logging levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    - Console and file output\n",
    "    - Simple context management\n",
    "    - Performance timing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = \"PipelineRunner\",\n",
    "        level: int = logging.INFO,\n",
    "        log_file: Optional[str] = None,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.level = level\n",
    "        self.log_file = log_file\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Create logger\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(level)\n",
    "\n",
    "        # Clear existing handlers\n",
    "        self.logger.handlers.clear()\n",
    "\n",
    "        # Setup handlers\n",
    "        self._setup_handlers()\n",
    "\n",
    "        # Performance tracking\n",
    "        self._timers: Dict[str, datetime] = {}\n",
    "\n",
    "    def _setup_handlers(self) -> None:\n",
    "        \"\"\"Setup logging handlers.\"\"\"\n",
    "        # Console handler\n",
    "        if self.verbose:\n",
    "            console_handler = logging.StreamHandler(sys.stdout)\n",
    "            console_formatter = logging.Formatter(\n",
    "                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "                datefmt=\"%H:%M:%S\",\n",
    "            )\n",
    "            console_handler.setFormatter(console_formatter)\n",
    "            console_handler.setLevel(self.level)\n",
    "            self.logger.addHandler(console_handler)\n",
    "\n",
    "        # File handler\n",
    "        if self.log_file:\n",
    "            file_handler = logging.FileHandler(self.log_file)\n",
    "            file_formatter = logging.Formatter(\n",
    "                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "                datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "            )\n",
    "            file_handler.setFormatter(file_formatter)\n",
    "            file_handler.setLevel(self.level)\n",
    "            self.logger.addHandler(file_handler)\n",
    "\n",
    "    # Basic logging methods\n",
    "    def debug(self, message: str, **kwargs: Union[str, int, float, bool, None]) -> None:\n",
    "        \"\"\"Log debug message.\"\"\"\n",
    "        self.logger.debug(self._format_message(message, kwargs))\n",
    "\n",
    "    def info(self, message: str, **kwargs: Union[str, int, float, bool, None]) -> None:\n",
    "        \"\"\"Log info message.\"\"\"\n",
    "        self.logger.info(self._format_message(message, kwargs))\n",
    "\n",
    "    def warning(\n",
    "        self, message: str, **kwargs: Union[str, int, float, bool, None]\n",
    "    ) -> None:\n",
    "        \"\"\"Log warning message.\"\"\"\n",
    "        self.logger.warning(self._format_message(message, kwargs))\n",
    "\n",
    "    def error(self, message: str, **kwargs: Union[str, int, float, bool, None]) -> None:\n",
    "        \"\"\"Log error message.\"\"\"\n",
    "        self.logger.error(self._format_message(message, kwargs))\n",
    "\n",
    "    def critical(\n",
    "        self, message: str, **kwargs: Union[str, int, float, bool, None]\n",
    "    ) -> None:\n",
    "        \"\"\"Log critical message.\"\"\"\n",
    "        self.logger.critical(self._format_message(message, kwargs))\n",
    "\n",
    "    def _format_message(\n",
    "        self, message: str, kwargs: Dict[str, Union[str, int, float, bool, None]]\n",
    "    ) -> str:\n",
    "        \"\"\"Format message with keyword arguments.\"\"\"\n",
    "        if not kwargs:\n",
    "            return message\n",
    "        kwargs_str = \", \".join(f\"{k}={v}\" for k, v in kwargs.items())\n",
    "        return f\"{message} ({kwargs_str})\"\n",
    "\n",
    "    # Performance timing\n",
    "    @contextmanager\n",
    "    def time_operation(self, operation_name: str) -> Generator[None, None, None]:\n",
    "        \"\"\"Context manager for timing operations.\"\"\"\n",
    "        start_time = datetime.now(timezone.utc)\n",
    "        self._timers[operation_name] = start_time\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            end_time = datetime.now(timezone.utc)\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            self.info(f\"Operation '{operation_name}' took {duration:.2f}s\")\n",
    "            # Clean up timer after operation completes\n",
    "            if operation_name in self._timers:\n",
    "                del self._timers[operation_name]\n",
    "\n",
    "    def start_timer(self, timer_name: str) -> None:\n",
    "        \"\"\"Start a named timer.\"\"\"\n",
    "        self._timers[timer_name] = datetime.now(timezone.utc)\n",
    "\n",
    "    def stop_timer(self, timer_name: str) -> float:\n",
    "        \"\"\"Stop a named timer and return duration in seconds.\"\"\"\n",
    "        if timer_name not in self._timers:\n",
    "            self.warning(f\"Timer '{timer_name}' was not started\")\n",
    "            return 0.0\n",
    "        start_time = self._timers[timer_name]\n",
    "        end_time = datetime.now(timezone.utc)\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        del self._timers[timer_name]\n",
    "        return duration\n",
    "\n",
    "    def get_timer_duration(self, timer_name: str) -> float:\n",
    "        \"\"\"Get current duration of a running timer without stopping it.\"\"\"\n",
    "        if timer_name not in self._timers:\n",
    "            return 0.0\n",
    "        start_time = self._timers[timer_name]\n",
    "        end_time = datetime.now(timezone.utc)\n",
    "        return (end_time - start_time).total_seconds()\n",
    "\n",
    "    # Context management\n",
    "    @contextmanager\n",
    "    def log_context(self, context_name: str) -> Generator[None, None, None]:\n",
    "        \"\"\"Context manager for logging context.\"\"\"\n",
    "        self.info(f\"Starting: {context_name}\")\n",
    "        try:\n",
    "            yield\n",
    "            self.info(f\"Completed: {context_name}\")\n",
    "        except Exception as e:\n",
    "            self.error(f\"Failed: {context_name}\", error=str(e))\n",
    "            raise\n",
    "\n",
    "    # Step execution logging\n",
    "    def step_start(self, step_type: str, step_name: str) -> None:\n",
    "        \"\"\"Log step start.\"\"\"\n",
    "        self.info(f\"▶️ Starting {step_type.upper()} step: {step_name}\")\n",
    "\n",
    "    def step_complete(\n",
    "        self,\n",
    "        step_type: str,\n",
    "        step_name: str,\n",
    "        duration: float,\n",
    "        rows_processed: int = 0,\n",
    "        rows_written: int = 0,\n",
    "        invalid_rows: int = 0,\n",
    "        validation_rate: float = 100.0,\n",
    "    ) -> None:\n",
    "        \"\"\"Log step completion.\"\"\"\n",
    "        self.info(\n",
    "            f\"✅ Completed {step_type.upper()} step: {step_name} ({duration:.2f}s) - \"\n",
    "            f\"{rows_processed} rows processed, {rows_written} rows written, \"\n",
    "            f\"{invalid_rows} invalid, {validation_rate:.1f}% valid\"\n",
    "        )\n",
    "\n",
    "    # Utility methods\n",
    "    def set_level(self, level: int) -> None:\n",
    "        \"\"\"Set logging level.\"\"\"\n",
    "        self.level = level\n",
    "        self.logger.setLevel(level)\n",
    "        for handler in self.logger.handlers:\n",
    "            handler.setLevel(level)\n",
    "\n",
    "    def add_handler(self, handler: logging.Handler) -> None:\n",
    "        \"\"\"Add a custom logging handler.\"\"\"\n",
    "        self.logger.addHandler(handler)\n",
    "\n",
    "    def remove_handler(self, handler: logging.Handler) -> None:\n",
    "        \"\"\"Remove a logging handler.\"\"\"\n",
    "        self.logger.removeHandler(handler)\n",
    "\n",
    "    def clear_handlers(self) -> None:\n",
    "        \"\"\"Clear all logging handlers.\"\"\"\n",
    "        self.logger.handlers.clear()\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close all logging handlers, especially file handlers.\"\"\"\n",
    "        for handler in self.logger.handlers[\n",
    "            :\n",
    "        ]:  # Copy list to avoid modification during iteration\n",
    "            handler.close()\n",
    "            self.logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.errors (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "class ErrorSeverity(Enum):\n",
    "    \"\"\"Severity levels for errors.\"\"\"\n",
    "\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "\n",
    "class ErrorCategory(Enum):\n",
    "    \"\"\"Categories of errors.\"\"\"\n",
    "\n",
    "    CONFIGURATION = \"configuration\"\n",
    "    VALIDATION = \"validation\"\n",
    "    EXECUTION = \"execution\"\n",
    "    DATA = \"data\"\n",
    "    SYSTEM = \"system\"\n",
    "    PERFORMANCE = \"performance\"\n",
    "    RESOURCE = \"resource\"\n",
    "\n",
    "\n",
    "# Type definitions for error context\n",
    "ErrorContextValue = Union[str, int, float, bool, List[str], Dict[str, str], None]\n",
    "ErrorContext = Dict[str, ErrorContextValue]\n",
    "ErrorSuggestions = List[str]\n",
    "\n",
    "\n",
    "class SparkForgeError(Exception):\n",
    "    \"\"\"\n",
    "    Base exception for all framework errors.\n",
    "\n",
    "    This is the root exception class that all other framework exceptions\n",
    "    inherit from, providing consistent error handling patterns and rich context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        error_code: str | None = None,\n",
    "        category: ErrorCategory | None = None,\n",
    "        severity: ErrorSeverity = ErrorSeverity.MEDIUM,\n",
    "        context: ErrorContext | None = None,\n",
    "        suggestions: ErrorSuggestions | None = None,\n",
    "        timestamp: datetime | None = None,\n",
    "        cause: Exception | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a framework error.\n",
    "\n",
    "        Args:\n",
    "            message: Human-readable error message\n",
    "            error_code: Optional error code for programmatic handling\n",
    "            category: Error category for classification\n",
    "            severity: Error severity level\n",
    "            context: Additional context information\n",
    "            suggestions: Suggested actions to resolve the error\n",
    "            timestamp: When the error occurred (defaults to now)\n",
    "            cause: The underlying exception that caused this error\n",
    "        \"\"\"\n",
    "        super().__init__(message)\n",
    "        self.message = message\n",
    "        self.error_code = error_code\n",
    "        self.category = category\n",
    "        self.severity = severity\n",
    "        self.context = context or {}\n",
    "        self.suggestions = suggestions or []\n",
    "        self.timestamp = timestamp or datetime.now(timezone.utc)\n",
    "        self.cause = cause\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return string representation of the error.\"\"\"\n",
    "        parts = [self.message]\n",
    "\n",
    "        if self.error_code:\n",
    "            parts.append(f\"[{self.error_code}]\")\n",
    "\n",
    "        if self.context:\n",
    "            context_str = \", \".join(f\"{k}={v}\" for k, v in self.context.items())\n",
    "            parts.append(f\"Context: {context_str}\")\n",
    "\n",
    "        if self.suggestions:\n",
    "            parts.append(f\"Suggestions: {'; '.join(self.suggestions)}\")\n",
    "\n",
    "        return \" | \".join(parts)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert error to dictionary for serialization.\"\"\"\n",
    "        return {\n",
    "            \"message\": self.message,\n",
    "            \"error_code\": self.error_code,\n",
    "            \"category\": self.category.value if self.category else None,\n",
    "            \"severity\": self.severity.value if self.severity else None,\n",
    "            \"context\": self.context,\n",
    "            \"suggestions\": self.suggestions,\n",
    "            \"timestamp\": self.timestamp.isoformat() if self.timestamp else None,\n",
    "            \"cause\": str(self.cause) if self.cause else None,\n",
    "        }\n",
    "\n",
    "\n",
    "class ValidationError(SparkForgeError):\n",
    "    \"\"\"Raised when validation fails.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        field: str | None = None,\n",
    "        value: Any = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message,\n",
    "            category=ErrorCategory.VALIDATION,\n",
    "            severity=ErrorSeverity.MEDIUM,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.field = field\n",
    "        self.value = value\n",
    "        if field:\n",
    "            self.context[\"field\"] = field\n",
    "        if value is not None:\n",
    "            self.context[\"value\"] = str(value)\n",
    "\n",
    "\n",
    "class PipelineValidationError(ValidationError):\n",
    "    \"\"\"Raised when pipeline validation fails.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        step_name: str | None = None,\n",
    "        phase: str | None = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__(message, **kwargs)\n",
    "        self.step_name = step_name\n",
    "        self.phase = phase\n",
    "        if step_name:\n",
    "            self.context[\"step_name\"] = step_name\n",
    "        if phase:\n",
    "            self.context[\"phase\"] = phase\n",
    "\n",
    "\n",
    "class ConfigurationError(SparkForgeError):\n",
    "    \"\"\"Raised when configuration is invalid.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, **kwargs: Any):\n",
    "        # Only set default severity if not provided in kwargs\n",
    "        if \"severity\" not in kwargs:\n",
    "            kwargs[\"severity\"] = ErrorSeverity.MEDIUM\n",
    "        super().__init__(\n",
    "            message,\n",
    "            category=ErrorCategory.CONFIGURATION,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class ExecutionError(SparkForgeError):\n",
    "    \"\"\"Raised when execution fails.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        step_name: str | None = None,\n",
    "        phase: str | None = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message,\n",
    "            category=ErrorCategory.EXECUTION,\n",
    "            severity=ErrorSeverity.HIGH,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.step_name = step_name\n",
    "        self.phase = phase\n",
    "        if step_name:\n",
    "            self.context[\"step_name\"] = step_name\n",
    "        if phase:\n",
    "            self.context[\"phase\"] = phase\n",
    "\n",
    "\n",
    "class DataError(SparkForgeError):\n",
    "    \"\"\"Raised when data operations fail.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, **kwargs: Any):\n",
    "        super().__init__(\n",
    "            message,\n",
    "            category=ErrorCategory.DATA,\n",
    "            severity=ErrorSeverity.MEDIUM,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class SystemError(SparkForgeError):\n",
    "    \"\"\"Raised when system operations fail.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, **kwargs: Any):\n",
    "        super().__init__(\n",
    "            message,\n",
    "            category=ErrorCategory.SYSTEM,\n",
    "            severity=ErrorSeverity.CRITICAL,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class PerformanceError(SparkForgeError):\n",
    "    \"\"\"Raised when performance issues are detected.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, **kwargs: Any):\n",
    "        super().__init__(\n",
    "            message,\n",
    "            category=ErrorCategory.PERFORMANCE,\n",
    "            severity=ErrorSeverity.LOW,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class ResourceError(SparkForgeError):\n",
    "    \"\"\"Raised when resource operations fail.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, **kwargs: Any):\n",
    "        super().__init__(\n",
    "            message,\n",
    "            category=ErrorCategory.RESOURCE,\n",
    "            severity=ErrorSeverity.HIGH,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.dependencies.graph (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from enum import Enum\n",
    "from typing import Dict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class StepType(Enum):\n",
    "    \"\"\"Types of pipeline steps.\"\"\"\n",
    "\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepNode:\n",
    "    \"\"\"Represents a single step in the dependency graph.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    step_type: StepType\n",
    "    dependencies: set[str] = field(default_factory=set)\n",
    "    dependents: set[str] = field(default_factory=set)\n",
    "    execution_group: int = 0\n",
    "    can_run_parallel: bool = True\n",
    "    estimated_duration: float = 0.0\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class DependencyGraph:\n",
    "    \"\"\"\n",
    "    Represents the dependency graph of a pipeline.\n",
    "\n",
    "    This class provides efficient operations for dependency analysis,\n",
    "    cycle detection, and execution planning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nodes: Dict[str, StepNode] = {}\n",
    "        self._adjacency_list: Dict[str, set[str]] = defaultdict(set)\n",
    "        self._reverse_adjacency_list: Dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    def add_node(self, node: StepNode) -> None:\n",
    "        \"\"\"Add a node to the dependency graph.\"\"\"\n",
    "        self.nodes[node.name] = node\n",
    "        self._adjacency_list[node.name] = set()\n",
    "        self._reverse_adjacency_list[node.name] = set()\n",
    "\n",
    "    def add_dependency(self, from_step: str, to_step: str) -> None:\n",
    "        \"\"\"Add a dependency from one step to another.\"\"\"\n",
    "        if from_step not in self.nodes or to_step not in self.nodes:\n",
    "            raise ValueError(f\"Steps {from_step} or {to_step} not found in graph\")\n",
    "\n",
    "        self._adjacency_list[from_step].add(to_step)\n",
    "        self._reverse_adjacency_list[to_step].add(from_step)\n",
    "\n",
    "        # Update node dependencies\n",
    "        self.nodes[from_step].dependencies.add(to_step)\n",
    "        self.nodes[to_step].dependents.add(from_step)\n",
    "\n",
    "    def get_dependencies(self, step_name: str) -> set[str]:\n",
    "        \"\"\"Get all dependencies for a step.\"\"\"\n",
    "        return self.nodes.get(\n",
    "            step_name, StepNode(\"\", StepType.BRONZE)\n",
    "        ).dependencies.copy()\n",
    "\n",
    "    def get_dependents(self, step_name: str) -> set[str]:\n",
    "        \"\"\"Get all dependents for a step.\"\"\"\n",
    "        return self.nodes.get(\n",
    "            step_name, StepNode(\"\", StepType.BRONZE)\n",
    "        ).dependents.copy()\n",
    "\n",
    "    def detect_cycles(self) -> list[list[str]]:\n",
    "        \"\"\"Detect cycles in the dependency graph using DFS.\"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = set()\n",
    "        cycles = []\n",
    "\n",
    "        def dfs(node: str, path: list[str]) -> None:\n",
    "            if node in rec_stack:\n",
    "                # Found a cycle\n",
    "                cycle_start = path.index(node)\n",
    "                cycle = path[cycle_start:] + [node]\n",
    "                cycles.append(cycle)\n",
    "                return\n",
    "\n",
    "            if node in visited:\n",
    "                return\n",
    "\n",
    "            visited.add(node)\n",
    "            rec_stack.add(node)\n",
    "            path.append(node)\n",
    "\n",
    "            for neighbor in self._adjacency_list[node]:\n",
    "                dfs(neighbor, path)\n",
    "\n",
    "            rec_stack.remove(node)\n",
    "            path.pop()\n",
    "\n",
    "        for node in self.nodes:\n",
    "            if node not in visited:\n",
    "                dfs(node, [])\n",
    "\n",
    "        return cycles\n",
    "\n",
    "    def topological_sort(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Perform topological sort of the dependency graph.\n",
    "\n",
    "        Returns nodes in an order such that dependencies come before dependents.\n",
    "        Uses reverse adjacency list since add_dependency(A, B) means A depends on B,\n",
    "        so B must come before A in the sort.\n",
    "        \"\"\"\n",
    "        in_degree = dict.fromkeys(self.nodes, 0)\n",
    "\n",
    "        # Calculate in-degrees using reverse adjacency\n",
    "        # If A depends on B, then B->A edge exists in reverse list\n",
    "        for node in self.nodes:\n",
    "            for dependent in self._reverse_adjacency_list[node]:\n",
    "                in_degree[dependent] += 1\n",
    "\n",
    "        # Find nodes with no incoming edges (no dependencies)\n",
    "        queue = deque([node for node, degree in in_degree.items() if degree == 0])\n",
    "        result = []\n",
    "\n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            result.append(node)\n",
    "\n",
    "            # Process nodes that depend on this one\n",
    "            for dependent in self._reverse_adjacency_list[node]:\n",
    "                in_degree[dependent] -= 1\n",
    "                if in_degree[dependent] == 0:\n",
    "                    queue.append(dependent)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_execution_groups(self) -> list[list[str]]:\n",
    "        \"\"\"Get execution groups for parallel execution.\"\"\"\n",
    "        # Use topological sort to determine execution order\n",
    "        sorted_nodes = self.topological_sort()\n",
    "\n",
    "        # Group nodes by their level in the dependency tree\n",
    "        levels = {}\n",
    "        for node in sorted_nodes:\n",
    "            if not self.nodes[node].dependencies:\n",
    "                levels[node] = 0\n",
    "            else:\n",
    "                # Ensure all dependencies have been processed\n",
    "                max_dep_level = 0\n",
    "                for dep in self.nodes[node].dependencies:\n",
    "                    if dep in levels:\n",
    "                        max_dep_level = max(max_dep_level, levels[dep])\n",
    "                    else:\n",
    "                        # If dependency not found, it might be missing from the graph\n",
    "                        # This could happen if the dependency graph is incomplete\n",
    "                        logger.warning(\n",
    "                            f\"Dependency {dep} not found in levels for node {node}\"\n",
    "                        )\n",
    "                        max_dep_level = max(max_dep_level, 0)\n",
    "                levels[node] = max_dep_level + 1\n",
    "\n",
    "        # Group nodes by level\n",
    "        groups = defaultdict(list)\n",
    "        for node, level in levels.items():\n",
    "            groups[level].append(node)\n",
    "\n",
    "        return [groups[level] for level in sorted(groups.keys())]\n",
    "\n",
    "    def get_parallel_candidates(self) -> list[list[str]]:\n",
    "        \"\"\"Get groups of steps that can run in parallel.\"\"\"\n",
    "        execution_groups = self.get_execution_groups()\n",
    "        return execution_groups\n",
    "\n",
    "    def validate(self) -> list[str]:\n",
    "        \"\"\"Validate the dependency graph and return any issues.\"\"\"\n",
    "        issues = []\n",
    "\n",
    "        # Check for cycles\n",
    "        cycles = self.detect_cycles()\n",
    "        if cycles:\n",
    "            for cycle in cycles:\n",
    "                issues.append(f\"Circular dependency detected: {' -> '.join(cycle)}\")\n",
    "\n",
    "        # Check for missing dependencies\n",
    "        for node_name, node in self.nodes.items():\n",
    "            for dep in node.dependencies:\n",
    "                if dep not in self.nodes:\n",
    "                    issues.append(f\"Node {node_name} depends on missing node {dep}\")\n",
    "\n",
    "        return issues\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the dependency graph.\"\"\"\n",
    "        total_nodes = len(self.nodes)\n",
    "        total_edges = sum(len(deps) for deps in self._adjacency_list.values())\n",
    "\n",
    "        # Count by step type\n",
    "        type_counts: Dict[str, int] = defaultdict(int)\n",
    "        for node in self.nodes.values():\n",
    "            type_counts[node.step_type.value] += 1\n",
    "\n",
    "        # Calculate average dependencies\n",
    "        avg_dependencies = total_edges / total_nodes if total_nodes > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"total_nodes\": total_nodes,\n",
    "            \"total_edges\": total_edges,\n",
    "            \"type_counts\": dict(type_counts),\n",
    "            \"average_dependencies\": avg_dependencies,\n",
    "            \"has_cycles\": len(self.detect_cycles()) > 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.dependencies.exceptions (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DependencyError(Exception):\n",
    "    \"\"\"Base exception for dependency-related errors.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, step_name: Optional[str] = None):\n",
    "        super().__init__(message)\n",
    "        self.step_name = step_name\n",
    "\n",
    "\n",
    "class DependencyAnalysisError(DependencyError):\n",
    "    \"\"\"Raised when dependency analysis fails.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, analysis_step: Optional[str] = None):\n",
    "        super().__init__(message, analysis_step)\n",
    "        self.analysis_step = analysis_step\n",
    "\n",
    "\n",
    "class CircularDependencyError(DependencyError):\n",
    "    \"\"\"Raised when circular dependencies are detected.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, cycle: List[str]):\n",
    "        super().__init__(message)\n",
    "        self.cycle = cycle\n",
    "\n",
    "\n",
    "class InvalidDependencyError(DependencyError):\n",
    "    \"\"\"Raised when invalid dependencies are detected.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, invalid_dependencies: List[str]):\n",
    "        super().__init__(message)\n",
    "        self.invalid_dependencies = invalid_dependencies\n",
    "\n",
    "\n",
    "class DependencyConflictError(DependencyError):\n",
    "    \"\"\"Raised when dependency conflicts are detected.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, conflicting_steps: List[str]):\n",
    "        super().__init__(message)\n",
    "        self.conflicting_steps = conflicting_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.models.steps (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class StepProtocol(Protocol):\n",
    "    \"\"\"Protocol for all pipeline steps.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    rules: Dict[str, Any]\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the step configuration.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class BronzeStepProtocol(StepProtocol, Protocol):\n",
    "    \"\"\"Protocol for bronze layer steps.\"\"\"\n",
    "\n",
    "    incremental_col: str | None\n",
    "\n",
    "\n",
    "class SilverStepProtocol(StepProtocol, Protocol):\n",
    "    \"\"\"Protocol for silver layer steps.\"\"\"\n",
    "\n",
    "    source_bronze: str\n",
    "    table_name: str\n",
    "\n",
    "\n",
    "class GoldStepProtocol(StepProtocol, Protocol):\n",
    "    \"\"\"Protocol for gold layer steps.\"\"\"\n",
    "\n",
    "    source_silvers: list[str] | None\n",
    "    table_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.models.enums (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class PipelinePhase(Enum):\n",
    "    \"\"\"Enumeration of pipeline phases.\"\"\"\n",
    "\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "\n",
    "class ExecutionMode(Enum):\n",
    "    \"\"\"Enumeration of execution modes.\"\"\"\n",
    "\n",
    "    INITIAL = \"initial\"\n",
    "    INCREMENTAL = \"incremental\"\n",
    "    FULL_REFRESH = \"full_refresh\"\n",
    "    VALIDATION_ONLY = \"validation_only\"\n",
    "\n",
    "\n",
    "class WriteMode(Enum):\n",
    "    \"\"\"Enumeration of write modes.\"\"\"\n",
    "\n",
    "    OVERWRITE = \"overwrite\"\n",
    "    APPEND = \"append\"\n",
    "\n",
    "\n",
    "class ValidationResult(Enum):\n",
    "    \"\"\"Enumeration of validation results.\"\"\"\n",
    "\n",
    "    PASSED = \"passed\"\n",
    "    FAILED = \"failed\"\n",
    "    WARNING = \"warning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.models.execution (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict\n",
    "\n",
    "# from .base import BaseModel  # Removed: defined in notebook cells above\n",
    "# from .enums import ExecutionMode, PipelinePhase  # Removed: defined in notebook cells above\n",
    "# from .exceptions import PipelineConfigurationError  # Removed: defined in notebook cells above\n",
    "# from .pipeline import PipelineMetrics  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExecutionContext(BaseModel):\n",
    "    \"\"\"\n",
    "    Context for pipeline execution.\n",
    "\n",
    "    Attributes:\n",
    "        mode: Execution mode (initial/incremental)\n",
    "        start_time: When execution started\n",
    "        end_time: When execution ended\n",
    "        duration_secs: Total execution duration\n",
    "        run_id: Unique run identifier\n",
    "        execution_id: Unique identifier for this execution\n",
    "        pipeline_id: Identifier for the pipeline being executed\n",
    "        schema: Target schema for data storage\n",
    "        started_at: When execution started (alias for start_time)\n",
    "        ended_at: When execution ended (alias for end_time)\n",
    "        run_mode: Mode of execution (alias for mode)\n",
    "        config: Pipeline configuration as dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    mode: ExecutionMode\n",
    "    start_time: datetime\n",
    "    end_time: datetime | None = None\n",
    "    duration_secs: float | None = None\n",
    "    run_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n",
    "\n",
    "    # Additional fields for writer compatibility\n",
    "    execution_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n",
    "    pipeline_id: str = \"unknown\"\n",
    "    schema: str = \"default\"\n",
    "    started_at: datetime | None = None\n",
    "    ended_at: datetime | None = None\n",
    "    run_mode: str = \"initial\"\n",
    "    config: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize aliases and defaults.\"\"\"\n",
    "        if self.started_at is None:\n",
    "            self.started_at = self.start_time\n",
    "        if self.ended_at is None:\n",
    "            self.ended_at = self.end_time\n",
    "        if self.run_mode == \"initial\":\n",
    "            # Map mode to run_mode string\n",
    "            if hasattr(self.mode, \"value\"):\n",
    "                self.run_mode = self.mode.value\n",
    "            elif hasattr(self.mode, \"name\"):\n",
    "                self.run_mode = self.mode.name.lower()\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the execution context.\"\"\"\n",
    "        if not self.run_id:\n",
    "            raise ValueError(\"Run ID cannot be empty\")\n",
    "        if self.duration_secs is not None and self.duration_secs < 0:\n",
    "            raise ValueError(\"Duration cannot be negative\")\n",
    "\n",
    "    def finish(self) -> None:\n",
    "        \"\"\"Mark execution as finished and calculate duration.\"\"\"\n",
    "        self.end_time = datetime.now(timezone.utc)\n",
    "        if self.start_time:\n",
    "            self.duration_secs = (self.end_time - self.start_time).total_seconds()\n",
    "\n",
    "    @property\n",
    "    def is_finished(self) -> bool:\n",
    "        \"\"\"Check if execution is finished.\"\"\"\n",
    "        return self.end_time is not None\n",
    "\n",
    "    @property\n",
    "    def is_running(self) -> bool:\n",
    "        \"\"\"Check if execution is currently running.\"\"\"\n",
    "        return not self.is_finished\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StageStats(BaseModel):\n",
    "    \"\"\"\n",
    "    Statistics for a pipeline stage.\n",
    "\n",
    "    Attributes:\n",
    "        stage: Stage name (bronze/silver/gold)\n",
    "        step: Step name\n",
    "        total_rows: Total number of rows processed\n",
    "        valid_rows: Number of valid rows\n",
    "        invalid_rows: Number of invalid rows\n",
    "        validation_rate: Validation success rate (0-100)\n",
    "        duration_secs: Processing duration in seconds\n",
    "        start_time: When processing started\n",
    "        end_time: When processing ended\n",
    "    \"\"\"\n",
    "\n",
    "    stage: str\n",
    "    step: str\n",
    "    total_rows: int\n",
    "    valid_rows: int\n",
    "    invalid_rows: int\n",
    "    validation_rate: float\n",
    "    duration_secs: float\n",
    "    start_time: datetime | None = None\n",
    "    end_time: datetime | None = None\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate stage statistics.\"\"\"\n",
    "        if self.total_rows != self.valid_rows + self.invalid_rows:\n",
    "            raise PipelineConfigurationError(\n",
    "                f\"Total rows ({self.total_rows}) must equal valid ({self.valid_rows}) + invalid ({self.invalid_rows})\"\n",
    "            )\n",
    "        if not 0 <= self.validation_rate <= 100:\n",
    "            raise PipelineConfigurationError(\n",
    "                f\"Validation rate must be between 0 and 100, got {self.validation_rate}\"\n",
    "            )\n",
    "        if self.duration_secs < 0:\n",
    "            raise PipelineConfigurationError(\n",
    "                f\"Duration must be non-negative, got {self.duration_secs}\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"Check if the stage passed validation.\"\"\"\n",
    "        return self.validation_rate >= 95.0  # Default threshold\n",
    "\n",
    "    @property\n",
    "    def error_rate(self) -> float:\n",
    "        \"\"\"Calculate error rate.\"\"\"\n",
    "        if self.total_rows == 0:\n",
    "            return 0.0\n",
    "        return (self.invalid_rows / self.total_rows) * 100\n",
    "\n",
    "    @property\n",
    "    def throughput_rows_per_sec(self) -> float:\n",
    "        \"\"\"Calculate throughput in rows per second.\"\"\"\n",
    "        if self.duration_secs == 0:\n",
    "            return 0.0\n",
    "        return self.total_rows / self.duration_secs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepResult(BaseModel):\n",
    "    \"\"\"\n",
    "    Result of a pipeline step execution.\n",
    "\n",
    "    Attributes:\n",
    "        step_name: Name of the step\n",
    "        phase: Pipeline phase\n",
    "        success: Whether the step succeeded\n",
    "        start_time: When execution started\n",
    "        end_time: When execution ended\n",
    "        duration_secs: Execution duration in seconds\n",
    "        rows_processed: Number of rows processed\n",
    "        rows_written: Number of rows written\n",
    "        validation_rate: Validation success rate\n",
    "        error_message: Error message if failed\n",
    "        step_type: Type of step (bronze, silver, gold)\n",
    "        table_fqn: Fully qualified table name if step writes to table\n",
    "        write_mode: Write mode used (overwrite, append)\n",
    "        input_rows: Number of input rows processed\n",
    "    \"\"\"\n",
    "\n",
    "    step_name: str\n",
    "    phase: PipelinePhase\n",
    "    success: bool\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    duration_secs: float\n",
    "    rows_processed: int\n",
    "    rows_written: int\n",
    "    validation_rate: float\n",
    "    error_message: str | None = None\n",
    "    step_type: str | None = None\n",
    "    table_fqn: str | None = None\n",
    "    write_mode: str | None = None\n",
    "    input_rows: int | None = None\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the step result.\"\"\"\n",
    "        if not self.step_name:\n",
    "            raise ValueError(\"Step name cannot be empty\")\n",
    "        if self.duration_secs < 0:\n",
    "            raise ValueError(\"Duration cannot be negative\")\n",
    "        if self.rows_processed < 0:\n",
    "            raise ValueError(\"Rows processed cannot be negative\")\n",
    "        if self.rows_written < 0:\n",
    "            raise ValueError(\"Rows written cannot be negative\")\n",
    "        if not 0 <= self.validation_rate <= 100:\n",
    "            raise ValueError(\"Validation rate must be between 0 and 100\")\n",
    "\n",
    "    @property\n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"Check if the step result is valid.\"\"\"\n",
    "        return self.success and self.validation_rate >= 95.0\n",
    "\n",
    "    @property\n",
    "    def is_high_quality(self) -> bool:\n",
    "        \"\"\"Check if the step result is high quality.\"\"\"\n",
    "        return self.success and self.validation_rate >= 98.0\n",
    "\n",
    "    @property\n",
    "    def throughput_rows_per_sec(self) -> float:\n",
    "        \"\"\"Calculate throughput in rows per second.\"\"\"\n",
    "        if self.duration_secs == 0:\n",
    "            return 0.0\n",
    "        return self.rows_processed / self.duration_secs\n",
    "\n",
    "    @classmethod\n",
    "    def create_success(\n",
    "        cls,\n",
    "        step_name: str,\n",
    "        phase: PipelinePhase,\n",
    "        start_time: datetime,\n",
    "        end_time: datetime,\n",
    "        rows_processed: int,\n",
    "        rows_written: int,\n",
    "        validation_rate: float,\n",
    "        step_type: str | None = None,\n",
    "        table_fqn: str | None = None,\n",
    "        write_mode: str | None = None,\n",
    "        input_rows: int | None = None,\n",
    "    ) -> StepResult:\n",
    "        \"\"\"Create a successful step result.\"\"\"\n",
    "        duration_secs = (end_time - start_time).total_seconds()\n",
    "        return cls(\n",
    "            step_name=step_name,\n",
    "            phase=phase,\n",
    "            success=True,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            duration_secs=duration_secs,\n",
    "            rows_processed=rows_processed,\n",
    "            rows_written=rows_written,\n",
    "            validation_rate=validation_rate,\n",
    "            error_message=None,\n",
    "            step_type=step_type,\n",
    "            table_fqn=table_fqn,\n",
    "            write_mode=write_mode,\n",
    "            input_rows=input_rows,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def create_failure(\n",
    "        cls,\n",
    "        step_name: str,\n",
    "        phase: PipelinePhase,\n",
    "        start_time: datetime,\n",
    "        end_time: datetime,\n",
    "        error_message: str,\n",
    "        step_type: str | None = None,\n",
    "        table_fqn: str | None = None,\n",
    "        write_mode: str | None = None,\n",
    "        input_rows: int | None = None,\n",
    "    ) -> StepResult:\n",
    "        \"\"\"Create a failed step result.\"\"\"\n",
    "        duration_secs = (end_time - start_time).total_seconds()\n",
    "        return cls(\n",
    "            step_name=step_name,\n",
    "            phase=phase,\n",
    "            success=False,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            duration_secs=duration_secs,\n",
    "            rows_processed=0,\n",
    "            rows_written=0,\n",
    "            validation_rate=0.0,\n",
    "            error_message=error_message,\n",
    "            step_type=step_type,\n",
    "            table_fqn=table_fqn,\n",
    "            write_mode=write_mode,\n",
    "            input_rows=input_rows,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def error_rate(self) -> float:\n",
    "        \"\"\"Calculate error rate.\"\"\"\n",
    "        if self.rows_processed == 0:\n",
    "            return 0.0\n",
    "        return 100.0 - self.validation_rate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExecutionResult(BaseModel):\n",
    "    \"\"\"\n",
    "    Result of pipeline execution.\n",
    "\n",
    "    Attributes:\n",
    "        context: Execution context\n",
    "        step_results: Results for each step\n",
    "        metrics: Overall execution metrics\n",
    "        success: Whether the entire pipeline succeeded\n",
    "    \"\"\"\n",
    "\n",
    "    context: ExecutionContext\n",
    "    step_results: list[StepResult]\n",
    "    metrics: PipelineMetrics\n",
    "    success: bool\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate execution result.\"\"\"\n",
    "        if not isinstance(self.context, ExecutionContext):\n",
    "            raise PipelineConfigurationError(\n",
    "                \"Context must be an ExecutionContext instance\"\n",
    "            )\n",
    "        if not isinstance(self.step_results, list):\n",
    "            raise PipelineConfigurationError(\"Step results must be a list\")\n",
    "        if not isinstance(self.metrics, PipelineMetrics):\n",
    "            raise PipelineConfigurationError(\n",
    "                \"Metrics must be a PipelineMetrics instance\"\n",
    "            )\n",
    "        if not isinstance(self.success, bool):\n",
    "            raise PipelineConfigurationError(\"Success must be a boolean\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_context_and_results(\n",
    "        cls, context: ExecutionContext, step_results: list[StepResult]\n",
    "    ) -> ExecutionResult:\n",
    "        \"\"\"Create execution result from context and step results.\"\"\"\n",
    "        metrics = PipelineMetrics.from_step_results(step_results)\n",
    "        success = all(result.success for result in step_results)\n",
    "        return cls(\n",
    "            context=context, step_results=step_results, metrics=metrics, success=success\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.models.types (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from typing import Dict, List, Protocol, Union\n",
    "\n",
    "# Specific types for model values instead of Any\n",
    "ModelValue = Union[str, int, float, bool, List[str], Dict[str, str], None]\n",
    "ResourceValue = Union[str, int, float, bool, List[str], Dict[str, str]]\n",
    "\n",
    "# Generic type for pipeline results\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Validatable(Protocol):\n",
    "    \"\"\"Protocol for objects that can be validated.\"\"\"\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the object and raise ValidationError if invalid.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class Serializable(Protocol):\n",
    "    \"\"\"Protocol for objects that can be serialized.\"\"\"\n",
    "\n",
    "    def to_dict(self) -> Dict[str, ModelValue]:\n",
    "        \"\"\"Convert object to dictionary.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Convert object to JSON string.\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.models.exceptions (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "\n",
    "class PipelineConfigurationError(ValueError):\n",
    "    \"\"\"Raised when pipeline configuration is invalid.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class PipelineExecutionError(RuntimeError):\n",
    "    \"\"\"Raised when pipeline execution fails.\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.writer.models (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Dict, Literal\n",
    "\n",
    "# from ..models import ExecutionResult, StepResult  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Enums\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class WriteMode(Enum):\n",
    "    \"\"\"Write mode for log operations.\"\"\"\n",
    "\n",
    "    OVERWRITE = \"overwrite\"\n",
    "    APPEND = \"append\"\n",
    "    MERGE = \"merge\"\n",
    "    IGNORE = \"ignore\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TypedDict Definitions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class LogRow(TypedDict):\n",
    "    \"\"\"\n",
    "    Enhanced log row with full type safety and framework integration.\n",
    "\n",
    "    This is an engine-agnostic log row structure that can be used\n",
    "    by both Spark and SQL implementations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run-level information\n",
    "    run_id: str\n",
    "    run_mode: Literal[\"initial\", \"incremental\", \"full_refresh\", \"validation_only\"]\n",
    "    run_started_at: datetime | None\n",
    "    run_ended_at: datetime | None\n",
    "\n",
    "    # Execution context\n",
    "    execution_id: str\n",
    "    pipeline_id: str\n",
    "    schema: str\n",
    "\n",
    "    # Step-level information\n",
    "    phase: Literal[\"bronze\", \"silver\", \"gold\", \"pipeline\"]\n",
    "    step_name: str\n",
    "    step_type: str\n",
    "\n",
    "    # Timing information\n",
    "    start_time: datetime | None\n",
    "    end_time: datetime | None\n",
    "    duration_secs: float\n",
    "\n",
    "    # Table information\n",
    "    table_fqn: str | None\n",
    "    write_mode: Literal[\"overwrite\", \"append\"] | None\n",
    "\n",
    "    # Data metrics\n",
    "    input_rows: int | None\n",
    "    output_rows: int | None\n",
    "    rows_written: int | None\n",
    "    rows_processed: int\n",
    "    table_total_rows: int | None  # Total rows in table after this write\n",
    "\n",
    "    # Validation metrics\n",
    "    valid_rows: int\n",
    "    invalid_rows: int\n",
    "    validation_rate: float\n",
    "\n",
    "    # Execution status\n",
    "    success: bool\n",
    "    error_message: str | None\n",
    "\n",
    "    # Performance metrics\n",
    "    memory_usage_mb: float | None\n",
    "    cpu_usage_percent: float | None\n",
    "\n",
    "    # Metadata\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "\n",
    "class WriterMetrics(TypedDict):\n",
    "    \"\"\"Metrics for writer operations.\"\"\"\n",
    "\n",
    "    total_writes: int\n",
    "    successful_writes: int\n",
    "    failed_writes: int\n",
    "    total_duration_secs: float\n",
    "    avg_write_duration_secs: float\n",
    "    total_rows_written: int\n",
    "    memory_usage_peak_mb: float\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Models\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WriterConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the LogWriter.\n",
    "\n",
    "    Provides comprehensive configuration options for the writer module\n",
    "    including table settings, performance tuning, and feature flags.\n",
    "    \"\"\"\n",
    "\n",
    "    table_schema: str\n",
    "    table_name: str\n",
    "    write_mode: WriteMode = WriteMode.APPEND\n",
    "    enable_analytics: bool = True\n",
    "    enable_monitoring: bool = True\n",
    "    enable_quality_checks: bool = True\n",
    "    batch_size: int = 1000\n",
    "    max_retries: int = 3\n",
    "    retry_delay_secs: float = 1.0\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the writer configuration.\"\"\"\n",
    "        if not self.table_schema or not isinstance(self.table_schema, str):\n",
    "            raise ValueError(\"table_schema must be a non-empty string\")\n",
    "        if not self.table_name or not isinstance(self.table_name, str):\n",
    "            raise ValueError(\"table_name must be a non-empty string\")\n",
    "        if self.batch_size < 1:\n",
    "            raise ValueError(\"batch_size must be at least 1\")\n",
    "        if self.max_retries < 0:\n",
    "            raise ValueError(\"max_retries must be non-negative\")\n",
    "        if self.retry_delay_secs < 0:\n",
    "            raise ValueError(\"retry_delay_secs must be non-negative\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Utility Functions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def create_log_rows_from_execution_result(\n",
    "    execution_result: ExecutionResult,\n",
    "    run_id: str,\n",
    "    run_mode: str = \"initial\",\n",
    "    metadata: Dict[str, Any] | None = None,\n",
    ") -> list[LogRow]:\n",
    "    \"\"\"\n",
    "    Create log rows from an execution result.\n",
    "\n",
    "    This is an engine-agnostic function that creates log rows from\n",
    "    execution results. Engine-specific implementations can use this\n",
    "    as a base and extend it as needed.\n",
    "\n",
    "    Args:\n",
    "        execution_result: The execution result\n",
    "        run_id: Run identifier\n",
    "        run_mode: Mode of the run\n",
    "        metadata: Additional metadata\n",
    "\n",
    "    Returns:\n",
    "        List of log rows\n",
    "    \"\"\"\n",
    "    log_rows = []\n",
    "\n",
    "    # Create a main log row for the execution\n",
    "    context = execution_result.context\n",
    "    main_row: LogRow = {\n",
    "        \"run_id\": run_id,\n",
    "        \"run_mode\": run_mode,  # type: ignore[typeddict-item]\n",
    "        \"run_started_at\": context.start_time,\n",
    "        \"run_ended_at\": context.end_time,\n",
    "        \"execution_id\": context.execution_id,\n",
    "        \"pipeline_id\": context.pipeline_id,\n",
    "        \"schema\": context.schema,\n",
    "        \"phase\": \"pipeline\",\n",
    "        \"step_name\": \"pipeline_execution\",\n",
    "        \"step_type\": \"pipeline\",\n",
    "        \"start_time\": context.start_time,\n",
    "        \"end_time\": context.end_time,\n",
    "        \"duration_secs\": context.duration_secs or 0.0,\n",
    "        \"table_fqn\": None,\n",
    "        \"write_mode\": None,\n",
    "        \"input_rows\": None,\n",
    "        \"output_rows\": None,\n",
    "        \"rows_written\": None,\n",
    "        \"rows_processed\": 0,\n",
    "        \"table_total_rows\": None,\n",
    "        \"valid_rows\": 0,\n",
    "        \"invalid_rows\": 0,\n",
    "        \"validation_rate\": 100.0,\n",
    "        \"success\": execution_result.success,\n",
    "        \"error_message\": None,\n",
    "        \"memory_usage_mb\": None,\n",
    "        \"cpu_usage_percent\": None,\n",
    "        \"metadata\": metadata or {},\n",
    "    }\n",
    "\n",
    "    log_rows.append(main_row)\n",
    "\n",
    "    # Add step results\n",
    "    for step_result in execution_result.step_results:\n",
    "        step_row: LogRow = {\n",
    "            \"run_id\": run_id,\n",
    "            \"run_mode\": run_mode,  # type: ignore[typeddict-item]\n",
    "            \"run_started_at\": context.start_time,\n",
    "            \"run_ended_at\": context.end_time,\n",
    "            \"execution_id\": context.execution_id,\n",
    "            \"pipeline_id\": context.pipeline_id,\n",
    "            \"schema\": context.schema,\n",
    "            \"phase\": step_result.phase.value,  # type: ignore[typeddict-item]\n",
    "            \"step_name\": step_result.step_name,\n",
    "            \"step_type\": step_result.step_type or \"unknown\",\n",
    "            \"start_time\": step_result.start_time,\n",
    "            \"end_time\": step_result.end_time,\n",
    "            \"duration_secs\": step_result.duration_secs,\n",
    "            \"table_fqn\": step_result.table_fqn,\n",
    "            \"write_mode\": step_result.write_mode,  # type: ignore[typeddict-item]\n",
    "            \"input_rows\": step_result.input_rows,\n",
    "            \"output_rows\": step_result.rows_written,\n",
    "            \"rows_written\": step_result.rows_written,\n",
    "            \"rows_processed\": step_result.rows_processed,\n",
    "            \"table_total_rows\": None,\n",
    "            \"valid_rows\": step_result.rows_processed,\n",
    "            \"invalid_rows\": 0,\n",
    "            \"validation_rate\": step_result.validation_rate,\n",
    "            \"success\": step_result.success,\n",
    "            \"error_message\": step_result.error_message,\n",
    "            \"memory_usage_mb\": None,\n",
    "            \"cpu_usage_percent\": None,\n",
    "            \"metadata\": {},\n",
    "        }\n",
    "        log_rows.append(step_row)\n",
    "\n",
    "    return log_rows\n",
    "\n",
    "\n",
    "def validate_log_data(log_rows: list[LogRow]) -> None:\n",
    "    \"\"\"\n",
    "    Validate log data for quality and consistency.\n",
    "\n",
    "    Args:\n",
    "        log_rows: List of log rows to validate\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    if not log_rows:\n",
    "        return\n",
    "\n",
    "    # Basic validation - check required fields\n",
    "    required_fields = {\"run_id\", \"phase\", \"step_name\"}\n",
    "    for i, row in enumerate(log_rows):\n",
    "        missing_fields = required_fields - set(row.keys())\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"Log row {i} missing required fields: {missing_fields}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.writer.exceptions (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "# from ..errors import SparkForgeError  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class WriterError(SparkForgeError):\n",
    "    \"\"\"Base exception for writer errors.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class WriterConfigurationError(WriterError):\n",
    "    \"\"\"Raised when writer configuration is invalid.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class WriterValidationError(WriterError):\n",
    "    \"\"\"Raised when writer validation fails.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class WriterTableError(WriterError):\n",
    "    \"\"\"Raised when table operations fail.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class WriterDataQualityError(WriterError):\n",
    "    \"\"\"Raised when data quality checks fail.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class WriterPerformanceError(WriterError):\n",
    "    \"\"\"Raised when performance issues are detected.\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.writer.base (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "# from ..logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from ..models import ExecutionResult  # Removed: defined in notebook cells above\n",
    "# from .models import LogRow, WriteMode, WriterConfig, WriterMetrics, create_log_rows_from_execution_result  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class BaseLogWriter(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for LogWriter implementations.\n",
    "\n",
    "    This class defines the interface that all LogWriter implementations\n",
    "    must follow, while allowing engine-specific implementations for\n",
    "    storage operations.\n",
    "\n",
    "    Subclasses must implement:\n",
    "    - _write_log_rows() - Engine-specific write operation\n",
    "    - _read_log_table() - Engine-specific read operation\n",
    "    - _table_exists() - Engine-specific table existence check\n",
    "    - _create_table() - Engine-specific table creation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: str,\n",
    "        table_name: str,\n",
    "        config: WriterConfig | None = None,\n",
    "        logger: PipelineLogger | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the base LogWriter.\n",
    "\n",
    "        Args:\n",
    "            schema: Database schema name\n",
    "            table_name: Table name\n",
    "            config: Writer configuration (optional)\n",
    "            logger: Pipeline logger (optional)\n",
    "        \"\"\"\n",
    "        self.schema = schema\n",
    "        self.table_name = table_name\n",
    "        self.logger = logger or PipelineLogger()\n",
    "\n",
    "        # Create config from schema/table_name if not provided\n",
    "        if config is None:\n",
    "            # from .models import WriteMode  # Removed: defined in notebook cells above\n",
    "            config = WriterConfig(\n",
    "                table_schema=schema,\n",
    "                table_name=table_name,\n",
    "                write_mode=WriteMode.APPEND,\n",
    "            )\n",
    "        self.config = config\n",
    "        self.config.validate()\n",
    "\n",
    "    @property\n",
    "    def table_fqn(self) -> str:\n",
    "        \"\"\"Get fully qualified table name.\"\"\"\n",
    "        return f\"{self.schema}.{self.table_name}\"\n",
    "\n",
    "    def create_table(self, execution_result: ExecutionResult) -> None:\n",
    "        \"\"\"\n",
    "        Create the log table from the first execution result.\n",
    "\n",
    "        Args:\n",
    "            execution_result: The execution result to create table from\n",
    "        \"\"\"\n",
    "        if self._table_exists():\n",
    "            self.logger.warning(\n",
    "                f\"Table {self.table_fqn} already exists, skipping creation\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        self.logger.info(f\"Creating log table {self.table_fqn}\")\n",
    "        log_rows = create_log_rows_from_execution_result(\n",
    "            execution_result,\n",
    "            run_id=execution_result.context.run_id,\n",
    "            run_mode=execution_result.context.run_mode,\n",
    "        )\n",
    "\n",
    "        if not log_rows:\n",
    "            self.logger.warning(\"No log rows to create table from\")\n",
    "            return\n",
    "\n",
    "        self._create_table(log_rows)\n",
    "        self._write_log_rows(log_rows, WriteMode.APPEND)\n",
    "\n",
    "    def append(self, execution_result: ExecutionResult) -> WriterMetrics:\n",
    "        \"\"\"\n",
    "        Append execution result to the log table.\n",
    "\n",
    "        Args:\n",
    "            execution_result: The execution result to append\n",
    "\n",
    "        Returns:\n",
    "            Writer metrics\n",
    "        \"\"\"\n",
    "        if not self._table_exists():\n",
    "            self.logger.warning(f\"Table {self.table_fqn} does not exist, creating it\")\n",
    "            self.create_table(execution_result)\n",
    "            return self._get_metrics()\n",
    "\n",
    "        log_rows = create_log_rows_from_execution_result(\n",
    "            execution_result,\n",
    "            run_id=execution_result.context.run_id,\n",
    "            run_mode=execution_result.context.run_mode,\n",
    "        )\n",
    "\n",
    "        if not log_rows:\n",
    "            self.logger.warning(\"No log rows to append\")\n",
    "            return self._get_metrics()\n",
    "\n",
    "        self._write_log_rows(log_rows, WriteMode.APPEND)\n",
    "        return self._get_metrics()\n",
    "\n",
    "    def write(\n",
    "        self, execution_result: ExecutionResult, mode: WriteMode = WriteMode.APPEND\n",
    "    ) -> WriterMetrics:\n",
    "        \"\"\"\n",
    "        Write execution result to the log table.\n",
    "\n",
    "        Args:\n",
    "            execution_result: The execution result to write\n",
    "            mode: Write mode (APPEND or OVERWRITE)\n",
    "\n",
    "        Returns:\n",
    "            Writer metrics\n",
    "        \"\"\"\n",
    "        if mode == WriteMode.OVERWRITE or not self._table_exists():\n",
    "            if not self._table_exists():\n",
    "                self.logger.info(f\"Table {self.table_fqn} does not exist, creating it\")\n",
    "                self.create_table(execution_result)\n",
    "            else:\n",
    "                self.logger.info(f\"Overwriting table {self.table_fqn}\")\n",
    "                # For overwrite, we need to clear the table first\n",
    "                # This is engine-specific, so subclasses should override if needed\n",
    "                self._write_log_rows([], WriteMode.OVERWRITE)\n",
    "\n",
    "        log_rows = create_log_rows_from_execution_result(\n",
    "            execution_result,\n",
    "            run_id=execution_result.context.run_id,\n",
    "            run_mode=execution_result.context.run_mode,\n",
    "        )\n",
    "\n",
    "        if not log_rows:\n",
    "            self.logger.warning(\"No log rows to write\")\n",
    "            return self._get_metrics()\n",
    "\n",
    "        self._write_log_rows(log_rows, mode)\n",
    "        return self._get_metrics()\n",
    "\n",
    "    def read(self, limit: int | None = None) -> list[LogRow]:\n",
    "        \"\"\"\n",
    "        Read log rows from the table.\n",
    "\n",
    "        Args:\n",
    "            limit: Maximum number of rows to read (None for all)\n",
    "\n",
    "        Returns:\n",
    "            List of log rows\n",
    "        \"\"\"\n",
    "        if not self._table_exists():\n",
    "            self.logger.warning(f\"Table {self.table_fqn} does not exist\")\n",
    "            return []\n",
    "\n",
    "        return self._read_log_table(limit)\n",
    "\n",
    "    # Abstract methods that must be implemented by subclasses\n",
    "\n",
    "    @abstractmethod\n",
    "    def _write_log_rows(self, log_rows: list[LogRow], mode: WriteMode) -> None:\n",
    "        \"\"\"\n",
    "        Write log rows to the storage system.\n",
    "\n",
    "        This is an engine-specific operation that must be implemented\n",
    "        by subclasses.\n",
    "\n",
    "        Args:\n",
    "            log_rows: List of log rows to write\n",
    "            mode: Write mode\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _read_log_table(self, limit: int | None = None) -> list[LogRow]:\n",
    "        \"\"\"\n",
    "        Read log rows from the storage system.\n",
    "\n",
    "        This is an engine-specific operation that must be implemented\n",
    "        by subclasses.\n",
    "\n",
    "        Args:\n",
    "            limit: Maximum number of rows to read (None for all)\n",
    "\n",
    "        Returns:\n",
    "            List of log rows\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _table_exists(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the log table exists.\n",
    "\n",
    "        This is an engine-specific operation that must be implemented\n",
    "        by subclasses.\n",
    "\n",
    "        Returns:\n",
    "            True if table exists, False otherwise\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_table(self, sample_rows: list[LogRow]) -> None:\n",
    "        \"\"\"\n",
    "        Create the log table with appropriate schema.\n",
    "\n",
    "        This is an engine-specific operation that must be implemented\n",
    "        by subclasses.\n",
    "\n",
    "        Args:\n",
    "            sample_rows: Sample log rows to infer schema from\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_metrics(self) -> WriterMetrics:\n",
    "        \"\"\"\n",
    "        Get writer metrics.\n",
    "\n",
    "        This is a default implementation that can be overridden\n",
    "        by subclasses for more detailed metrics.\n",
    "\n",
    "        Returns:\n",
    "            Writer metrics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"total_writes\": 1,\n",
    "            \"successful_writes\": 1,\n",
    "            \"failed_writes\": 0,\n",
    "            \"total_duration_secs\": 0.0,\n",
    "            \"avg_write_duration_secs\": 0.0,\n",
    "            \"total_rows_written\": 0,\n",
    "            \"memory_usage_peak_mb\": 0.0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.validation.utils (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "def safe_divide(numerator: float, denominator: float, default: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    Safely divide two numbers, returning default if denominator is zero or None.\n",
    "\n",
    "    Args:\n",
    "        numerator: The numerator\n",
    "        denominator: The denominator\n",
    "        default: Default value to return if denominator is zero or None\n",
    "\n",
    "    Returns:\n",
    "        The division result or default value\n",
    "    \"\"\"\n",
    "    if denominator is None or numerator is None or denominator == 0:\n",
    "        return default\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.logging (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# Re-export from base for backward compatibility\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "\n",
    "__all__ = [\"PipelineLogger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.compat (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Type\n",
    "\n",
    "_ENGINE = os.getenv(\"SPARKFORGE_ENGINE\", \"auto\").lower()\n",
    "\n",
    "\n",
    "def _try_import_pyspark() -> Optional[\n",
    "    Tuple[Type[Any], Type[Any], Type[Any], Any, Any, Type[Exception]]\n",
    "]:\n",
    "    \"\"\"Try to import PySpark modules.\"\"\"\n",
    "    try:\n",
    "        from pyspark.sql import Column as _Column\n",
    "        from pyspark.sql import DataFrame as _DataFrame\n",
    "        from pyspark.sql import SparkSession as _SparkSession\n",
    "        from pyspark.sql import functions as _F\n",
    "        from pyspark.sql import types as _types\n",
    "        from pyspark.sql.utils import (\n",
    "            AnalysisException as _AnalysisException,\n",
    "        )\n",
    "\n",
    "        return _DataFrame, _SparkSession, _Column, _F, _types, _AnalysisException\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _try_import_mockspark() -> Optional[\n",
    "    Tuple[Type[Any], Type[Any], Type[Any], Any, Any, Type[Exception]]\n",
    "]:\n",
    "    \"\"\"Try to import mock-spark modules.\"\"\"\n",
    "    try:\n",
    "        return _DataFrame, _SparkSession, _Column, _F, _types, _AnalysisException\n",
    "    except Exception:\n",
    "        # Log the error for debugging but don't fail\n",
    "        # Note: mock-spark 3.1.0 has Python 3.8 compatibility issues\n",
    "        # This is a known issue with the mock-spark package\n",
    "        return None\n",
    "\n",
    "\n",
    "def _select_engine() -> Tuple[\n",
    "    str, Tuple[Type[Any], Type[Any], Type[Any], Any, Any, Type[Exception]]\n",
    "]:\n",
    "    \"\"\"Select the appropriate engine based on environment and availability.\"\"\"\n",
    "    if _ENGINE in (\"pyspark\", \"spark\", \"real\"):\n",
    "        ps = _try_import_pyspark()\n",
    "        if ps is None:\n",
    "            raise ImportError(\n",
    "                \"SPARKFORGE_ENGINE=pyspark but pyspark is not importable. \"\n",
    "                \"Install with: pip install sparkforge[pyspark]\"\n",
    "            )\n",
    "        return \"pyspark\", ps\n",
    "    if _ENGINE in (\"mock\", \"mockspark\"):\n",
    "        ms = _try_import_mockspark()\n",
    "        if ms is None:\n",
    "            raise ImportError(\n",
    "                \"SPARKFORGE_ENGINE=mock but mock-spark is not importable. \"\n",
    "                \"Install with: pip install sparkforge[mock]\"\n",
    "            )\n",
    "        return \"mock\", ms\n",
    "\n",
    "    # auto mode: prefer PySpark if available, otherwise mock-spark\n",
    "    ps = _try_import_pyspark()\n",
    "    if ps is not None:\n",
    "        return \"pyspark\", ps\n",
    "    ms = _try_import_mockspark()\n",
    "    if ms is not None:\n",
    "        return \"mock\", ms\n",
    "\n",
    "    raise ImportError(\n",
    "        \"Neither pyspark nor mock-spark could be imported. \"\n",
    "        \"Install with: pip install sparkforge[pyspark] or pip install sparkforge[mock]\"\n",
    "    )\n",
    "\n",
    "\n",
    "_ENGINE_NAME, (DataFrame, SparkSession, Column, F, types, AnalysisException) = (\n",
    "    _select_engine()\n",
    ")\n",
    "\n",
    "\n",
    "def is_mock_spark() -> bool:\n",
    "    \"\"\"Check if currently using mock-spark.\"\"\"\n",
    "    return bool(_ENGINE_NAME == \"mock\")\n",
    "\n",
    "\n",
    "def compat_name() -> str:\n",
    "    \"\"\"Get the name of the current compatibility engine.\"\"\"\n",
    "    return str(_ENGINE_NAME)\n",
    "\n",
    "\n",
    "def require_pyspark(message: str | None = None) -> None:\n",
    "    \"\"\"Raise an error if not using PySpark.\"\"\"\n",
    "    if is_mock_spark():\n",
    "        raise RuntimeError(\n",
    "            message\n",
    "            or \"This operation requires PySpark and is not supported in mock mode\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Function shims when running in mock mode (no-op fallbacks)\n",
    "def desc(col_name: str) -> Any:\n",
    "    \"\"\"Get descending order expression for a column.\"\"\"\n",
    "    if _ENGINE_NAME == \"pyspark\":\n",
    "        # Delegate to PySpark's desc via functions\n",
    "        return F.desc(col_name)\n",
    "    # mock-spark: return a tuple understood by orderBy implementation if present\n",
    "    return (col_name, False)\n",
    "\n",
    "\n",
    "def col(col_name: str) -> Any:\n",
    "    \"\"\"Get a column by name.\"\"\"\n",
    "    return F.col(col_name)\n",
    "\n",
    "\n",
    "def lit(value: Any) -> Any:\n",
    "    \"\"\"Create a literal column.\"\"\"\n",
    "    return F.lit(value)\n",
    "\n",
    "\n",
    "def current_timestamp() -> Any:\n",
    "    \"\"\"Get current timestamp.\"\"\"\n",
    "    ct = getattr(F, \"current_timestamp\", None)\n",
    "    if callable(ct):\n",
    "        return ct()\n",
    "    # Fallback: literal current timestamp string\n",
    "    import datetime as _dt\n",
    "\n",
    "    return lit(_dt.datetime.now().isoformat())\n",
    "\n",
    "\n",
    "# Export Window if available\n",
    "if _ENGINE_NAME == \"pyspark\":\n",
    "    try:\n",
    "        from pyspark.sql import Window  # type: ignore[import-untyped]\n",
    "    except ImportError:\n",
    "        # Fallback Window for mock-spark\n",
    "        class Window:\n",
    "            @staticmethod\n",
    "            def orderBy(*cols: Any) -> Any:\n",
    "                return None\n",
    "else:\n",
    "    # Mock Window for mock-spark\n",
    "    class Window:\n",
    "        @staticmethod\n",
    "        def orderBy(*cols: Any) -> Any:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.constants (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "# Memory and Size Constants\n",
    "BYTES_PER_KB = 1024\n",
    "BYTES_PER_MB = BYTES_PER_KB * 1024\n",
    "BYTES_PER_GB = BYTES_PER_MB * 1024\n",
    "\n",
    "# Default Memory Limits\n",
    "DEFAULT_MAX_MEMORY_MB = 1024\n",
    "DEFAULT_CACHE_MEMORY_MB = 512\n",
    "\n",
    "# File Size Constants\n",
    "DEFAULT_MAX_FILE_SIZE_MB = 10\n",
    "DEFAULT_BACKUP_COUNT = 5\n",
    "\n",
    "# Performance Constants\n",
    "DEFAULT_CACHE_PARTITIONS = 200\n",
    "DEFAULT_SHUFFLE_PARTITIONS = 200\n",
    "\n",
    "# Validation Constants\n",
    "DEFAULT_BRONZE_THRESHOLD = 95.0\n",
    "DEFAULT_SILVER_THRESHOLD = 98.0\n",
    "DEFAULT_GOLD_THRESHOLD = 99.0\n",
    "\n",
    "# Timeout Constants (in seconds)\n",
    "DEFAULT_TIMEOUT_SECONDS = 300\n",
    "DEFAULT_RETRY_TIMEOUT_SECONDS = 60\n",
    "\n",
    "# Logging Constants\n",
    "DEFAULT_LOG_LEVEL = \"INFO\"\n",
    "DEFAULT_VERBOSE = True\n",
    "\n",
    "# Schema Constants\n",
    "DEFAULT_SCHEMA = \"default\"\n",
    "TEST_SCHEMA = \"test_schema\"\n",
    "\n",
    "# Error Constants\n",
    "MAX_ERROR_MESSAGE_LENGTH = 1000\n",
    "MAX_STACK_TRACE_LINES = 50\n",
    "\n",
    "# Performance Monitoring Constants\n",
    "DEFAULT_METRICS_INTERVAL_SECONDS = 30\n",
    "DEFAULT_ALERT_THRESHOLD_PERCENT = 80.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.errors (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# Re-export from base for backward compatibility\n",
    "# from .errors import (  # Removed: defined in notebook cells above\n",
    "# ConfigurationError,\n",
    "# DataError,\n",
    "# ErrorCategory,\n",
    "# ErrorContext,\n",
    "# ErrorContextValue,\n",
    "# ErrorSeverity,\n",
    "# ErrorSuggestions,\n",
    "# ExecutionError,\n",
    "# PerformanceError,\n",
    "# PipelineValidationError,\n",
    "# ResourceError,\n",
    "# SparkForgeError,\n",
    "# SystemError,\n",
    "# ValidationError,\n",
    "# )\n",
    "\n",
    "__all__ = [\n",
    "    \"SparkForgeError\",\n",
    "    \"ValidationError\",\n",
    "    \"PipelineValidationError\",\n",
    "    \"ConfigurationError\",\n",
    "    \"ExecutionError\",\n",
    "    \"DataError\",\n",
    "    \"SystemError\",\n",
    "    \"PerformanceError\",\n",
    "    \"ResourceError\",\n",
    "    \"ErrorSeverity\",\n",
    "    \"ErrorCategory\",\n",
    "    \"ErrorContext\",\n",
    "    \"ErrorContextValue\",\n",
    "    \"ErrorSuggestions\",\n",
    "]\n",
    "\n",
    "# Backward compatibility aliases\n",
    "PipelineValidationError = ValidationError\n",
    "PipelineConfigurationError = ConfigurationError\n",
    "PipelineExecutionError = ExecutionError\n",
    "TableOperationError = DataError\n",
    "DependencyError = ValidationError\n",
    "StepError = ExecutionError\n",
    "PipelineError = ExecutionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.reporting (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# TypedDict is available in typing for Python 3.8+\n",
    "try:\n",
    "    from typing import TypedDict\n",
    "except ImportError:\n",
    "    from typing_extensions import TypedDict\n",
    "\n",
    "# Re-export from base\n",
    "# from .reporting import (  # Removed: defined in notebook cells above\n",
    "# format_duration,\n",
    "# safe_divide,\n",
    "# )\n",
    "# from .models import StageStats  # Removed: defined in notebook cells above\n",
    "\n",
    "# ============================================================================\n",
    "# TypedDict Definitions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class ValidationReport(TypedDict):\n",
    "    \"\"\"Validation report structure.\"\"\"\n",
    "\n",
    "    stage: str | None\n",
    "    step: str | None\n",
    "    total_rows: int\n",
    "    valid_rows: int\n",
    "    invalid_rows: int\n",
    "    validation_rate: float\n",
    "    duration_secs: float\n",
    "    start_at: datetime\n",
    "    end_at: datetime\n",
    "\n",
    "\n",
    "class TransformReport(TypedDict):\n",
    "    \"\"\"Transform operation report structure.\"\"\"\n",
    "\n",
    "    input_rows: int\n",
    "    output_rows: int\n",
    "    duration_secs: float\n",
    "    skipped: bool\n",
    "    start_at: datetime\n",
    "    end_at: datetime\n",
    "\n",
    "\n",
    "class WriteReport(TypedDict):\n",
    "    \"\"\"Write operation report structure.\"\"\"\n",
    "\n",
    "    mode: str\n",
    "    rows_written: int\n",
    "    duration_secs: float\n",
    "    table_fqn: str\n",
    "    skipped: bool\n",
    "    start_at: datetime\n",
    "    end_at: datetime\n",
    "\n",
    "\n",
    "class ExecutionSummary(TypedDict):\n",
    "    \"\"\"Execution summary nested structure.\"\"\"\n",
    "\n",
    "    total_steps: int\n",
    "    successful_steps: int\n",
    "    failed_steps: int\n",
    "    success_rate: float\n",
    "    failure_rate: float\n",
    "\n",
    "\n",
    "class PerformanceMetrics(TypedDict):\n",
    "    \"\"\"Performance metrics nested structure.\"\"\"\n",
    "\n",
    "    total_duration_secs: float\n",
    "    formatted_duration: str\n",
    "    avg_validation_rate: float\n",
    "\n",
    "\n",
    "class DataMetrics(TypedDict):\n",
    "    \"\"\"Data metrics nested structure.\"\"\"\n",
    "\n",
    "    total_rows_processed: int\n",
    "    total_rows_written: int\n",
    "    processing_efficiency: float\n",
    "\n",
    "\n",
    "class SummaryReport(TypedDict):\n",
    "    \"\"\"Complete summary report structure.\"\"\"\n",
    "\n",
    "    execution_summary: ExecutionSummary\n",
    "    performance_metrics: PerformanceMetrics\n",
    "    data_metrics: DataMetrics\n",
    "\n",
    "\n",
    "def create_validation_dict(\n",
    "    stats: StageStats | None, *, start_at: datetime, end_at: datetime\n",
    ") -> ValidationReport:\n",
    "    \"\"\"\n",
    "    Create validation dictionary for reporting.\n",
    "\n",
    "    Args:\n",
    "        stats: Stage statistics\n",
    "        start_at: Start time\n",
    "        end_at: End time\n",
    "\n",
    "    Returns:\n",
    "        Validation dictionary\n",
    "    \"\"\"\n",
    "    if stats is None:\n",
    "        return {\n",
    "            \"stage\": None,\n",
    "            \"step\": None,\n",
    "            \"total_rows\": 0,\n",
    "            \"valid_rows\": 0,\n",
    "            \"invalid_rows\": 0,\n",
    "            \"validation_rate\": 100.0,\n",
    "            \"duration_secs\": 0.0,\n",
    "            \"start_at\": start_at,\n",
    "            \"end_at\": end_at,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"stage\": stats.stage,\n",
    "        \"step\": stats.step,\n",
    "        \"total_rows\": stats.total_rows,\n",
    "        \"valid_rows\": stats.valid_rows,\n",
    "        \"invalid_rows\": stats.invalid_rows,\n",
    "        \"validation_rate\": round(stats.validation_rate, 2),\n",
    "        \"duration_secs\": round(stats.duration_secs, 3),\n",
    "        \"start_at\": start_at,\n",
    "        \"end_at\": end_at,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_transform_dict(\n",
    "    input_rows: int,\n",
    "    output_rows: int,\n",
    "    duration_secs: float,\n",
    "    skipped: bool,\n",
    "    *,\n",
    "    start_at: datetime,\n",
    "    end_at: datetime,\n",
    ") -> TransformReport:\n",
    "    \"\"\"\n",
    "    Create transform dictionary for reporting.\n",
    "\n",
    "    Args:\n",
    "        input_rows: Number of input rows\n",
    "        output_rows: Number of output rows\n",
    "        duration_secs: Duration in seconds\n",
    "        skipped: Whether operation was skipped\n",
    "        start_at: Start time\n",
    "        end_at: End time\n",
    "\n",
    "    Returns:\n",
    "        Transform dictionary\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"input_rows\": int(input_rows),\n",
    "        \"output_rows\": int(output_rows),\n",
    "        \"duration_secs\": round(duration_secs, 3),\n",
    "        \"skipped\": bool(skipped),\n",
    "        \"start_at\": start_at,\n",
    "        \"end_at\": end_at,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_write_dict(\n",
    "    mode: str,\n",
    "    rows: int,\n",
    "    duration_secs: float,\n",
    "    table_fqn: str,\n",
    "    skipped: bool,\n",
    "    *,\n",
    "    start_at: datetime,\n",
    "    end_at: datetime,\n",
    ") -> WriteReport:\n",
    "    \"\"\"\n",
    "    Create write dictionary for reporting.\n",
    "\n",
    "    Args:\n",
    "        mode: Write mode\n",
    "        rows: Number of rows written\n",
    "        duration_secs: Duration in seconds\n",
    "        table_fqn: Fully qualified table name\n",
    "        skipped: Whether operation was skipped\n",
    "        start_at: Start time\n",
    "        end_at: End time\n",
    "\n",
    "    Returns:\n",
    "        Write dictionary\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"rows_written\": int(rows),\n",
    "        \"duration_secs\": round(duration_secs, 3),\n",
    "        \"table_fqn\": table_fqn,\n",
    "        \"skipped\": bool(skipped),\n",
    "        \"start_at\": start_at,\n",
    "        \"end_at\": end_at,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_summary_report(\n",
    "    total_steps: int,\n",
    "    successful_steps: int,\n",
    "    failed_steps: int,\n",
    "    total_duration: float,\n",
    "    total_rows_processed: int,\n",
    "    total_rows_written: int,\n",
    "    avg_validation_rate: float,\n",
    ") -> SummaryReport:\n",
    "    \"\"\"\n",
    "    Create a summary report for pipeline execution.\n",
    "\n",
    "    Args:\n",
    "        total_steps: Total number of steps\n",
    "        successful_steps: Number of successful steps\n",
    "        failed_steps: Number of failed steps\n",
    "        total_duration: Total duration in seconds\n",
    "        total_rows_processed: Total rows processed\n",
    "        total_rows_written: Total rows written\n",
    "        avg_validation_rate: Average validation rate\n",
    "\n",
    "    Returns:\n",
    "        Summary report dictionary\n",
    "    \"\"\"\n",
    "    if total_steps == 0:\n",
    "        success_rate = 0.0\n",
    "        failure_rate = 0.0\n",
    "    else:\n",
    "        success_rate = safe_divide(successful_steps * 100.0, total_steps, 0.0)\n",
    "        failure_rate = 100.0 - success_rate\n",
    "\n",
    "    return {\n",
    "        \"execution_summary\": {\n",
    "            \"total_steps\": total_steps,\n",
    "            \"successful_steps\": successful_steps,\n",
    "            \"failed_steps\": failed_steps,\n",
    "            \"success_rate\": round(success_rate, 2),\n",
    "            \"failure_rate\": round(failure_rate, 2),\n",
    "        },\n",
    "        \"performance_metrics\": {\n",
    "            \"total_duration_secs\": round(total_duration, 3),\n",
    "            \"formatted_duration\": format_duration(total_duration),\n",
    "            \"avg_validation_rate\": round(avg_validation_rate, 2),\n",
    "        },\n",
    "        \"data_metrics\": {\n",
    "            \"total_rows_processed\": total_rows_processed,\n",
    "            \"total_rows_written\": total_rows_written,\n",
    "            \"processing_efficiency\": round(\n",
    "                safe_divide(total_rows_written * 100.0, total_rows_processed, 0.0), 2\n",
    "            ),\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.dependencies.graph (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Dict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class StepType(Enum):\n",
    "    \"\"\"Types of pipeline steps.\"\"\"\n",
    "\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepNode:\n",
    "    \"\"\"Represents a single step in the dependency graph.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    step_type: StepType\n",
    "    dependencies: set[str] = field(default_factory=set)\n",
    "    dependents: set[str] = field(default_factory=set)\n",
    "    execution_group: int = 0\n",
    "    can_run_parallel: bool = True\n",
    "    estimated_duration: float = 0.0\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class DependencyGraph:\n",
    "    \"\"\"\n",
    "    Represents the dependency graph of a pipeline.\n",
    "\n",
    "    This class provides efficient operations for dependency analysis,\n",
    "    cycle detection, and execution planning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nodes: Dict[str, StepNode] = {}\n",
    "        self._adjacency_list: Dict[str, set[str]] = defaultdict(set)\n",
    "        self._reverse_adjacency_list: Dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    def add_node(self, node: StepNode) -> None:\n",
    "        \"\"\"Add a node to the dependency graph.\"\"\"\n",
    "        self.nodes[node.name] = node\n",
    "        self._adjacency_list[node.name] = set()\n",
    "        self._reverse_adjacency_list[node.name] = set()\n",
    "\n",
    "    def add_dependency(self, from_step: str, to_step: str) -> None:\n",
    "        \"\"\"Add a dependency from one step to another.\"\"\"\n",
    "        if from_step not in self.nodes or to_step not in self.nodes:\n",
    "            raise ValueError(f\"Steps {from_step} or {to_step} not found in graph\")\n",
    "\n",
    "        self._adjacency_list[from_step].add(to_step)\n",
    "        self._reverse_adjacency_list[to_step].add(from_step)\n",
    "\n",
    "        # Update node dependencies\n",
    "        self.nodes[from_step].dependencies.add(to_step)\n",
    "        self.nodes[to_step].dependents.add(from_step)\n",
    "\n",
    "    def get_dependencies(self, step_name: str) -> set[str]:\n",
    "        \"\"\"Get all dependencies for a step.\"\"\"\n",
    "        return self.nodes.get(\n",
    "            step_name, StepNode(\"\", StepType.BRONZE)\n",
    "        ).dependencies.copy()\n",
    "\n",
    "    def get_dependents(self, step_name: str) -> set[str]:\n",
    "        \"\"\"Get all dependents for a step.\"\"\"\n",
    "        return self.nodes.get(\n",
    "            step_name, StepNode(\"\", StepType.BRONZE)\n",
    "        ).dependents.copy()\n",
    "\n",
    "    def detect_cycles(self) -> list[list[str]]:\n",
    "        \"\"\"Detect cycles in the dependency graph using DFS.\"\"\"\n",
    "        visited = set()\n",
    "        rec_stack = set()\n",
    "        cycles = []\n",
    "\n",
    "        def dfs(node: str, path: list[str]) -> None:\n",
    "            if node in rec_stack:\n",
    "                # Found a cycle\n",
    "                cycle_start = path.index(node)\n",
    "                cycle = path[cycle_start:] + [node]\n",
    "                cycles.append(cycle)\n",
    "                return\n",
    "\n",
    "            if node in visited:\n",
    "                return\n",
    "\n",
    "            visited.add(node)\n",
    "            rec_stack.add(node)\n",
    "            path.append(node)\n",
    "\n",
    "            for neighbor in self._adjacency_list[node]:\n",
    "                dfs(neighbor, path)\n",
    "\n",
    "            rec_stack.remove(node)\n",
    "            path.pop()\n",
    "\n",
    "        for node in self.nodes:\n",
    "            if node not in visited:\n",
    "                dfs(node, [])\n",
    "\n",
    "        return cycles\n",
    "\n",
    "    def topological_sort(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Perform topological sort of the dependency graph.\n",
    "\n",
    "        Returns nodes in an order such that dependencies come before dependents.\n",
    "        Uses reverse adjacency list since add_dependency(A, B) means A depends on B,\n",
    "        so B must come before A in the sort.\n",
    "        \"\"\"\n",
    "        in_degree = dict.fromkeys(self.nodes, 0)\n",
    "\n",
    "        # Calculate in-degrees using reverse adjacency\n",
    "        # If A depends on B, then B->A edge exists in reverse list\n",
    "        for node in self.nodes:\n",
    "            for dependent in self._reverse_adjacency_list[node]:\n",
    "                in_degree[dependent] += 1\n",
    "\n",
    "        # Find nodes with no incoming edges (no dependencies)\n",
    "        queue = deque([node for node, degree in in_degree.items() if degree == 0])\n",
    "        result = []\n",
    "\n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            result.append(node)\n",
    "\n",
    "            # Process nodes that depend on this one\n",
    "            for dependent in self._reverse_adjacency_list[node]:\n",
    "                in_degree[dependent] -= 1\n",
    "                if in_degree[dependent] == 0:\n",
    "                    queue.append(dependent)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_execution_groups(self) -> list[list[str]]:\n",
    "        \"\"\"Get execution groups for parallel execution.\"\"\"\n",
    "        # Use topological sort to determine execution order\n",
    "        sorted_nodes = self.topological_sort()\n",
    "\n",
    "        # Group nodes by their level in the dependency tree\n",
    "        levels = {}\n",
    "        for node in sorted_nodes:\n",
    "            if not self.nodes[node].dependencies:\n",
    "                levels[node] = 0\n",
    "            else:\n",
    "                # Ensure all dependencies have been processed\n",
    "                max_dep_level = 0\n",
    "                for dep in self.nodes[node].dependencies:\n",
    "                    if dep in levels:\n",
    "                        max_dep_level = max(max_dep_level, levels[dep])\n",
    "                    else:\n",
    "                        # If dependency not found, it might be missing from the graph\n",
    "                        # This could happen if the dependency graph is incomplete\n",
    "                        logger.warning(\n",
    "                            f\"Dependency {dep} not found in levels for node {node}\"\n",
    "                        )\n",
    "                        max_dep_level = max(max_dep_level, 0)\n",
    "                levels[node] = max_dep_level + 1\n",
    "\n",
    "        # Group nodes by level\n",
    "        groups = defaultdict(list)\n",
    "        for node, level in levels.items():\n",
    "            groups[level].append(node)\n",
    "\n",
    "        return [groups[level] for level in sorted(groups.keys())]\n",
    "\n",
    "    def get_parallel_candidates(self) -> list[list[str]]:\n",
    "        \"\"\"Get groups of steps that can run in parallel.\"\"\"\n",
    "        execution_groups = self.get_execution_groups()\n",
    "        return execution_groups\n",
    "\n",
    "    def validate(self) -> list[str]:\n",
    "        \"\"\"Validate the dependency graph and return any issues.\"\"\"\n",
    "        issues = []\n",
    "\n",
    "        # Check for cycles\n",
    "        cycles = self.detect_cycles()\n",
    "        if cycles:\n",
    "            for cycle in cycles:\n",
    "                issues.append(f\"Circular dependency detected: {' -> '.join(cycle)}\")\n",
    "\n",
    "        # Check for missing dependencies\n",
    "        for node_name, node in self.nodes.items():\n",
    "            for dep in node.dependencies:\n",
    "                if dep not in self.nodes:\n",
    "                    issues.append(f\"Node {node_name} depends on missing node {dep}\")\n",
    "\n",
    "        return issues\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the dependency graph.\"\"\"\n",
    "        total_nodes = len(self.nodes)\n",
    "        total_edges = sum(len(deps) for deps in self._adjacency_list.values())\n",
    "\n",
    "        # Count by step type\n",
    "        type_counts: Dict[str, int] = defaultdict(int)\n",
    "        for node in self.nodes.values():\n",
    "            type_counts[node.step_type.value] += 1\n",
    "\n",
    "        # Calculate average dependencies\n",
    "        avg_dependencies = total_edges / total_nodes if total_nodes > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"total_nodes\": total_nodes,\n",
    "            \"total_edges\": total_edges,\n",
    "            \"type_counts\": dict(type_counts),\n",
    "            \"average_dependencies\": avg_dependencies,\n",
    "            \"has_cycles\": len(self.detect_cycles()) > 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.dependencies.analyzer (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# Re-export from base - the base analyzer uses protocols so it works with Spark steps\n",
    "# from .dependencies import (  # Removed: defined in notebook cells above\n",
    "# AnalysisStrategy,\n",
    "# DependencyAnalysisResult,\n",
    "# DependencyAnalyzer,\n",
    "# DependencyError,\n",
    "# DependencyGraph,\n",
    "# StepNode,\n",
    "# StepType,\n",
    "# )\n",
    "\n",
    "# Keep for backward compatibility - the base analyzer works with any step type via protocols\n",
    "__all__ = [\n",
    "    \"DependencyAnalyzer\",\n",
    "    \"DependencyAnalysisResult\",\n",
    "    \"AnalysisStrategy\",\n",
    "    \"DependencyGraph\",\n",
    "    \"StepNode\",\n",
    "    \"StepType\",\n",
    "    \"DependencyError\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.dependencies.exceptions (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DependencyError(Exception):\n",
    "    \"\"\"Base exception for dependency-related errors.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, step_name: Optional[str] = None):\n",
    "        super().__init__(message)\n",
    "        self.step_name = step_name\n",
    "\n",
    "\n",
    "class DependencyAnalysisError(DependencyError):\n",
    "    \"\"\"Raised when dependency analysis fails.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, analysis_step: Optional[str] = None):\n",
    "        super().__init__(message, analysis_step)\n",
    "        self.analysis_step = analysis_step\n",
    "\n",
    "\n",
    "class CircularDependencyError(DependencyError):\n",
    "    \"\"\"Raised when circular dependencies are detected.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, cycle: List[str]):\n",
    "        super().__init__(message)\n",
    "        self.cycle = cycle\n",
    "\n",
    "\n",
    "class InvalidDependencyError(DependencyError):\n",
    "    \"\"\"Raised when invalid dependencies are detected.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, invalid_dependencies: List[str]):\n",
    "        super().__init__(message)\n",
    "        self.invalid_dependencies = invalid_dependencies\n",
    "\n",
    "\n",
    "class DependencyConflictError(DependencyError):\n",
    "    \"\"\"Raised when dependency conflicts are detected.\"\"\"\n",
    "\n",
    "    def __init__(self, message: str, conflicting_steps: List[str]):\n",
    "        super().__init__(message)\n",
    "        self.conflicting_steps = conflicting_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.enums (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class PipelinePhase(Enum):\n",
    "    \"\"\"Enumeration of pipeline phases.\"\"\"\n",
    "\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "\n",
    "class ExecutionMode(Enum):\n",
    "    \"\"\"Enumeration of execution modes.\"\"\"\n",
    "\n",
    "    INITIAL = \"initial\"\n",
    "    INCREMENTAL = \"incremental\"\n",
    "\n",
    "\n",
    "class WriteMode(Enum):\n",
    "    \"\"\"Enumeration of write modes.\"\"\"\n",
    "\n",
    "    OVERWRITE = \"overwrite\"\n",
    "    APPEND = \"append\"\n",
    "\n",
    "\n",
    "class ValidationResult(Enum):\n",
    "    \"\"\"Enumeration of validation results.\"\"\"\n",
    "\n",
    "    PASSED = \"passed\"\n",
    "    FAILED = \"failed\"\n",
    "    WARNING = \"warning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.exceptions (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "\n",
    "class PipelineConfigurationError(ValueError):\n",
    "    \"\"\"Raised when pipeline configuration is invalid.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class PipelineExecutionError(RuntimeError):\n",
    "    \"\"\"Raised when pipeline execution fails.\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.writer.exceptions (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class WriterError(Exception):\n",
    "    \"\"\"\n",
    "    Base exception for all writer-related errors.\n",
    "\n",
    "    Provides a common base class for all writer exceptions with\n",
    "    enhanced error context and suggestions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        context: Dict[str, Any] | None = None,\n",
    "        suggestions: list[str] | None = None,\n",
    "        cause: Exception | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the writer error.\n",
    "\n",
    "        Args:\n",
    "            message: Error message\n",
    "            context: Additional context information\n",
    "            suggestions: List of suggestions to resolve the error\n",
    "            cause: The underlying exception that caused this error\n",
    "        \"\"\"\n",
    "        super().__init__(message)\n",
    "        self.message = message\n",
    "        self.context = context or {}\n",
    "        self.suggestions = suggestions or []\n",
    "        self.cause = cause\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return formatted error message.\"\"\"\n",
    "        msg = self.message\n",
    "        if self.context:\n",
    "            msg += f\"\\nContext: {self.context}\"\n",
    "        if self.suggestions:\n",
    "            msg += f\"\\nSuggestions: {'; '.join(self.suggestions)}\"\n",
    "        return msg\n",
    "\n",
    "\n",
    "class WriterValidationError(WriterError):\n",
    "    \"\"\"\n",
    "    Raised when writer validation fails.\n",
    "\n",
    "    This exception is raised when data validation fails during\n",
    "    the writing process, such as invalid log rows or schema mismatches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        validation_errors: list[str] | None = None,\n",
    "        context: Dict[str, Any] | None = None,\n",
    "        suggestions: list[str] | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize validation error.\n",
    "\n",
    "        Args:\n",
    "            message: Error message\n",
    "            validation_errors: List of specific validation errors\n",
    "            context: Additional context information\n",
    "            suggestions: List of suggestions to resolve the error\n",
    "        \"\"\"\n",
    "        super().__init__(message, context, suggestions)\n",
    "        self.validation_errors = validation_errors or []\n",
    "\n",
    "\n",
    "class WriterConfigurationError(WriterError):\n",
    "    \"\"\"\n",
    "    Raised when writer configuration is invalid.\n",
    "\n",
    "    This exception is raised when the WriterConfig contains\n",
    "    invalid values or conflicting settings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        config_errors: list[str] | None = None,\n",
    "        context: Dict[str, Any] | None = None,\n",
    "        suggestions: list[str] | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize configuration error.\n",
    "\n",
    "        Args:\n",
    "            message: Error message\n",
    "            config_errors: List of specific configuration errors\n",
    "            context: Additional context information\n",
    "            suggestions: List of suggestions to resolve the error\n",
    "        \"\"\"\n",
    "        super().__init__(message, context, suggestions)\n",
    "        self.config_errors = config_errors or []\n",
    "\n",
    "\n",
    "class WriterTableError(WriterError):\n",
    "    \"\"\"\n",
    "    Raised when table operations fail.\n",
    "\n",
    "    This exception is raised when there are issues with Delta table\n",
    "    operations, such as table creation, writing, or schema evolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        table_name: str | None = None,\n",
    "        operation: str | None = None,\n",
    "        context: Dict[str, Any] | None = None,\n",
    "        suggestions: list[str] | None = None,\n",
    "        cause: Exception | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize table error.\n",
    "\n",
    "        Args:\n",
    "            message: Error message\n",
    "            table_name: Name of the table that caused the error\n",
    "            operation: The operation that failed\n",
    "            context: Additional context information\n",
    "            suggestions: List of suggestions to resolve the error\n",
    "            cause: The underlying exception that caused this error\n",
    "        \"\"\"\n",
    "        super().__init__(message, context, suggestions, cause)\n",
    "        self.table_name = table_name\n",
    "        self.operation = operation\n",
    "\n",
    "\n",
    "class WriterPerformanceError(WriterError):\n",
    "    \"\"\"\n",
    "    Raised when performance thresholds are exceeded.\n",
    "\n",
    "    This exception is raised when operations take longer than expected\n",
    "    or consume more resources than configured limits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        actual_duration: float | None = None,\n",
    "        expected_duration: float | None = None,\n",
    "        actual_memory: float | None = None,\n",
    "        expected_memory: float | None = None,\n",
    "        context: Dict[str, Any] | None = None,\n",
    "        suggestions: list[str] | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize performance error.\n",
    "\n",
    "        Args:\n",
    "            message: Error message\n",
    "            actual_duration: Actual duration in seconds\n",
    "            expected_duration: Expected duration in seconds\n",
    "            actual_memory: Actual memory usage in MB\n",
    "            expected_memory: Expected memory usage in MB\n",
    "            context: Additional context information\n",
    "            suggestions: List of suggestions to resolve the error\n",
    "        \"\"\"\n",
    "        super().__init__(message, context, suggestions)\n",
    "        self.actual_duration = actual_duration\n",
    "        self.expected_duration = expected_duration\n",
    "        self.actual_memory = actual_memory\n",
    "        self.expected_memory = expected_memory\n",
    "\n",
    "\n",
    "class WriterSchemaError(WriterError):\n",
    "    \"\"\"\n",
    "    Raised when schema operations fail.\n",
    "\n",
    "    This exception is raised when there are issues with schema\n",
    "    validation, evolution, or compatibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        schema_errors: list[str] | None = None,\n",
    "        expected_schema: str | None = None,\n",
    "        actual_schema: str | None = None,\n",
    "        context: Dict[str, Any] | None = None,\n",
    "        suggestions: list[str] | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize schema error.\n",
    "\n",
    "        Args:\n",
    "            message: Error message\n",
    "            schema_errors: List of specific schema errors\n",
    "            expected_schema: Expected schema definition\n",
    "            actual_schema: Actual schema definition\n",
    "            context: Additional context information\n",
    "            suggestions: List of suggestions to resolve the error\n",
    "        \"\"\"\n",
    "        super().__init__(message, context, suggestions)\n",
    "        self.schema_errors = schema_errors or []\n",
    "        self.expected_schema = expected_schema\n",
    "        self.actual_schema = actual_schema\n",
    "\n",
    "\n",
    "class WriterDataQualityError(WriterError):\n",
    "    \"\"\"\n",
    "    Raised when data quality checks fail.\n",
    "\n",
    "    This exception is raised when data quality validation fails,\n",
    "    such as when validation rates are too low or data anomalies are detected.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        quality_issues: list[str] | None = None,\n",
    "        validation_rate: float | None = None,\n",
    "        threshold: float | None = None,\n",
    "        context: Dict[str, Any] | None = None,\n",
    "        suggestions: list[str] | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize data quality error.\n",
    "\n",
    "        Args:\n",
    "            message: Error message\n",
    "            quality_issues: List of specific quality issues\n",
    "            validation_rate: Actual validation rate\n",
    "            threshold: Expected validation threshold\n",
    "            context: Additional context information\n",
    "            suggestions: List of suggestions to resolve the error\n",
    "        \"\"\"\n",
    "        super().__init__(message, context, suggestions)\n",
    "        self.quality_issues = quality_issues or []\n",
    "        self.validation_rate = validation_rate\n",
    "        self.threshold = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.engine.spark_engine (pipeline_builder)\n",
    "#\n",
    "# Dependencies: None (base module)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abstracts.engine import Engine\n",
    "from abstracts.reports.transform import TransformReport\n",
    "from abstracts.reports.validation import ValidationReport\n",
    "from abstracts.reports.write import WriteReport\n",
    "from abstracts.source import Source\n",
    "from abstracts.step import Step\n",
    "\n",
    "# from ..compat import DataFrame, SparkSession  # Removed: defined in notebook cells above\n",
    "# from ..execution import ExecutionEngine  # Removed: defined in notebook cells above\n",
    "# from ..functions import FunctionsProtocol  # Removed: defined in notebook cells above\n",
    "# from ..models import BronzeStep, GoldStep, SilverStep  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from ..table_operations import fqn  # Removed: defined in notebook cells above\n",
    "# from ..validation import apply_column_rules  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class SparkEngine(Engine):\n",
    "    \"\"\"\n",
    "    SparkEngine implements abstracts.Engine using ExecutionEngine.\n",
    "\n",
    "    This engine adapts between the abstracts interface (Step, Source protocols)\n",
    "    and the concrete pipeline_builder types (BronzeStep/SilverStep/GoldStep, DataFrame).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        config: Any,  # PipelineConfig\n",
    "        logger: Optional[PipelineLogger] = None,\n",
    "        functions: Optional[FunctionsProtocol] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize SparkEngine.\n",
    "\n",
    "        Args:\n",
    "            spark: SparkSession instance\n",
    "            config: PipelineConfig instance\n",
    "            logger: Optional logger instance\n",
    "            functions: Optional functions protocol for PySpark operations\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.config = config\n",
    "        self.logger = logger or PipelineLogger()\n",
    "        self.functions = functions\n",
    "        self._execution_engine = ExecutionEngine(spark, config, self.logger, functions)\n",
    "\n",
    "    def validate_source(self, step: Step, source: Source) -> ValidationReport:\n",
    "        \"\"\"\n",
    "        Validate a data source according to step rules.\n",
    "\n",
    "        Args:\n",
    "            step: Step with validation rules\n",
    "            source: Source data to validate (DataFrame)\n",
    "\n",
    "        Returns:\n",
    "            ValidationReport with validation results\n",
    "        \"\"\"\n",
    "        # Type check: source should be a DataFrame\n",
    "        if not isinstance(source, DataFrame):\n",
    "            raise TypeError(f\"Source must be a DataFrame, got {type(source)}\")\n",
    "\n",
    "        df: DataFrame = source\n",
    "\n",
    "        # Type check: step should be a concrete step type\n",
    "        if not isinstance(step, (BronzeStep, SilverStep, GoldStep)):\n",
    "            raise TypeError(\n",
    "                f\"Step must be BronzeStep, SilverStep, or GoldStep, got {type(step)}\"\n",
    "            )\n",
    "\n",
    "        # Apply validation rules\n",
    "        try:\n",
    "            # Rules type compatibility - Step Protocol uses Rules, concrete steps use ColumnRules\n",
    "            # mypy doesn't understand Protocol structural typing here, so we use Any\n",
    "            rules: Any = step.rules\n",
    "            valid_df, invalid_df, validation_stats = apply_column_rules(\n",
    "                df,\n",
    "                rules,\n",
    "                \"pipeline\",\n",
    "                step.name,\n",
    "                functions=self.functions,\n",
    "            )\n",
    "\n",
    "            valid_rows = valid_df.count()\n",
    "            invalid_rows = invalid_df.count()\n",
    "\n",
    "            return ValidationReport(\n",
    "                source=valid_df,  # Return validated source\n",
    "                valid_rows=valid_rows,\n",
    "                invalid_rows=invalid_rows,\n",
    "                error=None,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationReport(\n",
    "                source=df,\n",
    "                valid_rows=0,\n",
    "                invalid_rows=df.count() if df is not None else 0,\n",
    "                error=e,\n",
    "            )\n",
    "\n",
    "    def transform_source(self, step: Step, source: Source) -> TransformReport:\n",
    "        \"\"\"\n",
    "        Transform a data source according to step transformation logic.\n",
    "\n",
    "        Args:\n",
    "            step: Step with transformation function\n",
    "            source: Source data to transform (DataFrame)\n",
    "\n",
    "        Returns:\n",
    "            TransformReport with transformed source\n",
    "        \"\"\"\n",
    "        # Type check: source should be a DataFrame\n",
    "        if not isinstance(source, DataFrame):\n",
    "            raise TypeError(f\"Source must be a DataFrame, got {type(source)}\")\n",
    "\n",
    "        df: DataFrame = source\n",
    "\n",
    "        # Type check: step should be a concrete step type\n",
    "        if not isinstance(step, (BronzeStep, SilverStep, GoldStep)):\n",
    "            raise TypeError(\n",
    "                f\"Step must be BronzeStep, SilverStep, or GoldStep, got {type(step)}\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Bronze steps: no transformation, just return source\n",
    "            if isinstance(step, BronzeStep):\n",
    "                return TransformReport(source=df, error=None)\n",
    "\n",
    "            # Silver steps: transform with bronze data and empty silvers dict\n",
    "            elif isinstance(step, SilverStep):\n",
    "                if step.transform is None:\n",
    "                    raise ValueError(\n",
    "                        f\"Silver step '{step.name}' requires a transform function\"\n",
    "                    )\n",
    "                transformed_df = step.transform(self.spark, df, {})\n",
    "                return TransformReport(source=transformed_df, error=None)\n",
    "\n",
    "            # Gold steps: transform with silvers dict\n",
    "            # Note: For gold steps, the \"source\" parameter is actually a dict of silvers\n",
    "            # This is a limitation of the abstracts.Engine interface for gold steps\n",
    "            elif isinstance(step, GoldStep):\n",
    "                if step.transform is None:\n",
    "                    raise ValueError(\n",
    "                        f\"Gold step '{step.name}' requires a transform function\"\n",
    "                    )\n",
    "                # For gold steps, source should be a dict of silvers (Dict[str, DataFrame])\n",
    "                # The abstracts interface expects Source, but we accept dict for gold steps\n",
    "                if isinstance(source, dict):\n",
    "                    silvers = source\n",
    "                else:\n",
    "                    # If single DataFrame, this is an error for gold steps\n",
    "                    raise TypeError(\n",
    "                        f\"Gold step '{step.name}' requires a dict of silvers, got {type(source)}\"\n",
    "                    )\n",
    "                transformed_df = step.transform(self.spark, silvers)\n",
    "                return TransformReport(source=transformed_df, error=None)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown step type: {type(step)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            return TransformReport(source=df, error=e)\n",
    "\n",
    "    def write_target(self, step: Step, source: Source) -> WriteReport:\n",
    "        \"\"\"\n",
    "        Write a data source to target table.\n",
    "\n",
    "        Args:\n",
    "            step: Step with target configuration\n",
    "            source: Source data to write (DataFrame)\n",
    "\n",
    "        Returns:\n",
    "            WriteReport with write results\n",
    "        \"\"\"\n",
    "        # Type check: source should be a DataFrame\n",
    "        if not isinstance(source, DataFrame):\n",
    "            raise TypeError(f\"Source must be a DataFrame, got {type(source)}\")\n",
    "\n",
    "        df: DataFrame = source\n",
    "\n",
    "        # Type check: step should be a concrete step type\n",
    "        if not isinstance(step, (BronzeStep, SilverStep, GoldStep)):\n",
    "            raise TypeError(\n",
    "                f\"Step must be BronzeStep, SilverStep, or GoldStep, got {type(step)}\"\n",
    "            )\n",
    "\n",
    "        # Bronze steps don't write to tables\n",
    "        if isinstance(step, BronzeStep):\n",
    "            rows_written = df.count()\n",
    "            return WriteReport(\n",
    "                source=df,\n",
    "                written_rows=rows_written,\n",
    "                failed_rows=0,\n",
    "                error=None,\n",
    "            )\n",
    "\n",
    "        # Get table name and schema\n",
    "        table_name = getattr(step, \"table_name\", None) or getattr(\n",
    "            step, \"target\", step.name\n",
    "        )\n",
    "        schema = getattr(step, \"schema\", None) or getattr(step, \"write_schema\", None)\n",
    "\n",
    "        if schema is None:\n",
    "            raise ValueError(\n",
    "                f\"Step '{step.name}' requires a schema to be specified for writing\"\n",
    "            )\n",
    "\n",
    "        output_table = fqn(schema, table_name)\n",
    "\n",
    "        # Determine write mode\n",
    "        write_mode = getattr(step, \"write_mode\", \"overwrite\")\n",
    "        if write_mode is None:\n",
    "            write_mode = \"overwrite\"\n",
    "\n",
    "        # Create schema if needed\n",
    "        try:\n",
    "            self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to create schema '{schema}': {e}\") from e\n",
    "\n",
    "        # Write to table\n",
    "        try:\n",
    "            rows_before = df.count()\n",
    "            df.write.mode(write_mode).saveAsTable(output_table)\n",
    "            rows_written = rows_before  # Assuming all rows were written successfully\n",
    "            return WriteReport(\n",
    "                source=df,\n",
    "                written_rows=rows_written,\n",
    "                failed_rows=0,\n",
    "                error=None,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return WriteReport(\n",
    "                source=df,\n",
    "                written_rows=0,\n",
    "                failed_rows=df.count() if df is not None else 0,\n",
    "                error=e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.dependencies.analyzer (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: pipeline_builder_base.logging\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Dict, Protocol\n",
    "\n",
    "# from ..logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .exceptions import DependencyError  # Removed: defined in notebook cells above\n",
    "# from .graph import DependencyGraph, StepNode, StepType  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class AnalysisStrategy(Enum):\n",
    "    \"\"\"Strategies for dependency analysis.\"\"\"\n",
    "\n",
    "    CONSERVATIVE = \"conservative\"  # Assume all dependencies exist\n",
    "    OPTIMISTIC = \"optimistic\"  # Assume minimal dependencies\n",
    "    HYBRID = \"hybrid\"  # Balance between conservative and optimistic\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DependencyAnalysisResult:\n",
    "    \"\"\"Result of dependency analysis.\"\"\"\n",
    "\n",
    "    graph: DependencyGraph\n",
    "    execution_groups: list[list[str]]\n",
    "    cycles: list[list[str]]\n",
    "    conflicts: list[str]\n",
    "    recommendations: list[str]\n",
    "    stats: Dict[str, Any]\n",
    "    analysis_duration: float\n",
    "\n",
    "\n",
    "# Protocol for step objects that can be analyzed\n",
    "class StepProtocol(Protocol):\n",
    "    \"\"\"Protocol for steps that can be analyzed for dependencies.\"\"\"\n",
    "\n",
    "    name: str\n",
    "\n",
    "\n",
    "class BronzeStepProtocol(StepProtocol, Protocol):\n",
    "    \"\"\"Protocol for bronze steps.\"\"\"\n",
    "\n",
    "    incremental_col: str | None\n",
    "\n",
    "\n",
    "class SilverStepProtocol(StepProtocol, Protocol):\n",
    "    \"\"\"Protocol for silver steps.\"\"\"\n",
    "\n",
    "    source_bronze: str\n",
    "\n",
    "\n",
    "class GoldStepProtocol(StepProtocol, Protocol):\n",
    "    \"\"\"Protocol for gold steps.\"\"\"\n",
    "\n",
    "    source_silvers: list[str] | None\n",
    "\n",
    "\n",
    "class DependencyAnalyzer:\n",
    "    \"\"\"\n",
    "    Unified dependency analyzer for all pipeline step types.\n",
    "\n",
    "    This analyzer works with any step implementation that follows the step protocols.\n",
    "    It analyzes dependencies across bronze, silver, and gold steps.\n",
    "\n",
    "    Features:\n",
    "    - Single analyzer for all step types (Bronze, Silver, Gold)\n",
    "    - Multiple analysis strategies\n",
    "    - Cycle detection and resolution\n",
    "    - Execution group optimization\n",
    "    - Performance analysis and recommendations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        strategy: AnalysisStrategy = AnalysisStrategy.HYBRID,\n",
    "        logger: PipelineLogger | None = None,\n",
    "    ):\n",
    "        self.strategy = strategy\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger()\n",
    "        else:\n",
    "            self.logger = logger\n",
    "        self._analysis_cache: Dict[str, DependencyAnalysisResult] = {}\n",
    "\n",
    "    def analyze_dependencies(\n",
    "        self,\n",
    "        bronze_steps: Dict[str, BronzeStepProtocol] | None = None,\n",
    "        silver_steps: Dict[str, SilverStepProtocol] | None = None,\n",
    "        gold_steps: Dict[str, GoldStepProtocol] | None = None,\n",
    "        force_refresh: bool = False,\n",
    "    ) -> DependencyAnalysisResult:\n",
    "        \"\"\"\n",
    "        Analyze dependencies across all step types.\n",
    "\n",
    "        Args:\n",
    "            bronze_steps: Dictionary of bronze steps (any object with name and incremental_col)\n",
    "            silver_steps: Dictionary of silver steps (any object with name and source_bronze)\n",
    "            gold_steps: Dictionary of gold steps (any object with name and source_silvers)\n",
    "            force_refresh: Whether to force refresh of cached results\n",
    "\n",
    "        Returns:\n",
    "            DependencyAnalysisResult containing analysis results\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create cache key\n",
    "        cache_key = self._create_cache_key(bronze_steps, silver_steps, gold_steps)\n",
    "\n",
    "        if not force_refresh and cache_key in self._analysis_cache:\n",
    "            self.logger.info(f\"Using cached dependency analysis: {cache_key}\")\n",
    "            return self._analysis_cache[cache_key]\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Starting dependency analysis with strategy: {self.strategy.value}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Step 1: Build dependency graph\n",
    "            graph = self._build_dependency_graph(bronze_steps, silver_steps, gold_steps)\n",
    "\n",
    "            # Step 2: Detect cycles\n",
    "            cycles = graph.detect_cycles()\n",
    "            if cycles:\n",
    "                self.logger.warning(f\"Detected {len(cycles)} circular dependencies\")\n",
    "                graph = self._resolve_cycles(graph, cycles)\n",
    "\n",
    "            # Step 3: Detect conflicts\n",
    "            conflicts = self._detect_conflicts(graph)\n",
    "            if conflicts:\n",
    "                self.logger.warning(f\"Detected {len(conflicts)} dependency conflicts\")\n",
    "\n",
    "            # Step 4: Generate execution groups\n",
    "            execution_groups = graph.get_execution_groups()\n",
    "\n",
    "            # Step 5: Generate recommendations\n",
    "            recommendations = self._generate_recommendations(graph, cycles, conflicts)\n",
    "\n",
    "            # Step 6: Calculate statistics\n",
    "            stats = graph.get_stats()\n",
    "\n",
    "            # Create result\n",
    "            result = DependencyAnalysisResult(\n",
    "                graph=graph,\n",
    "                execution_groups=execution_groups,\n",
    "                cycles=cycles,\n",
    "                conflicts=conflicts,\n",
    "                recommendations=recommendations,\n",
    "                stats=stats,\n",
    "                analysis_duration=time.time() - start_time,\n",
    "            )\n",
    "\n",
    "            # Cache result\n",
    "            self._analysis_cache[cache_key] = result\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Dependency analysis completed in {result.analysis_duration:.2f}s\"\n",
    "            )\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Dependency analysis failed: {str(e)}\")\n",
    "            raise DependencyError(f\"Dependency analysis failed: {str(e)}\") from e\n",
    "\n",
    "    def _build_dependency_graph(\n",
    "        self,\n",
    "        bronze_steps: Dict[str, BronzeStepProtocol] | None,\n",
    "        silver_steps: Dict[str, SilverStepProtocol] | None,\n",
    "        gold_steps: Dict[str, GoldStepProtocol] | None,\n",
    "    ) -> DependencyGraph:\n",
    "        \"\"\"Build the dependency graph from all step types.\"\"\"\n",
    "        graph = DependencyGraph()\n",
    "\n",
    "        # Add bronze steps\n",
    "        if bronze_steps:\n",
    "            for name, step in bronze_steps.items():\n",
    "                node = StepNode(\n",
    "                    name=name, step_type=StepType.BRONZE, metadata={\"step\": step}\n",
    "                )\n",
    "                graph.add_node(node)\n",
    "\n",
    "        # Add silver steps\n",
    "        if silver_steps:\n",
    "            for name, silver_step in silver_steps.items():\n",
    "                node = StepNode(\n",
    "                    name=name, step_type=StepType.SILVER, metadata={\"step\": silver_step}\n",
    "                )\n",
    "                graph.add_node(node)\n",
    "\n",
    "                # Add dependencies\n",
    "                # SilverStep always has source_bronze attribute\n",
    "                source_bronze = getattr(silver_step, \"source_bronze\", None)\n",
    "                if source_bronze:\n",
    "                    # Check if the source bronze step exists\n",
    "                    if source_bronze in graph.nodes:\n",
    "                        graph.add_dependency(name, source_bronze)\n",
    "                    else:\n",
    "                        # Log warning about missing dependency\n",
    "                        self.logger.warning(\n",
    "                            f\"Silver step {name} references non-existent bronze step {source_bronze}\"\n",
    "                        )\n",
    "\n",
    "                # Check for additional dependencies\n",
    "                if hasattr(silver_step, \"depends_on\"):\n",
    "                    depends_on = getattr(silver_step, \"depends_on\", None)\n",
    "                    if depends_on and isinstance(depends_on, (list, tuple, set)):\n",
    "                        for dep in depends_on:\n",
    "                            if dep in graph.nodes:\n",
    "                                graph.add_dependency(name, dep)\n",
    "                            else:\n",
    "                                self.logger.warning(\n",
    "                                    f\"Silver step {name} references non-existent dependency {dep}\"\n",
    "                                )\n",
    "\n",
    "        # Add gold steps\n",
    "        if gold_steps:\n",
    "            for name, gold_step in gold_steps.items():\n",
    "                node = StepNode(\n",
    "                    name=name, step_type=StepType.GOLD, metadata={\"step\": gold_step}\n",
    "                )\n",
    "                graph.add_node(node)\n",
    "\n",
    "                # Add dependencies\n",
    "                # GoldStep always has source_silvers attribute (can be None)\n",
    "                source_silvers = getattr(gold_step, \"source_silvers\", None)\n",
    "                if source_silvers:\n",
    "                    for dep in source_silvers:\n",
    "                        if dep in graph.nodes:\n",
    "                            graph.add_dependency(name, dep)\n",
    "                        else:\n",
    "                            self.logger.warning(\n",
    "                                f\"Gold step {name} references non-existent silver step {dep}\"\n",
    "                            )\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _resolve_cycles(\n",
    "        self, graph: DependencyGraph, cycles: list[list[str]]\n",
    "    ) -> DependencyGraph:\n",
    "        \"\"\"Resolve cycles in the dependency graph.\"\"\"\n",
    "        # Simple cycle resolution: break cycles by removing the last dependency\n",
    "        for cycle in cycles:\n",
    "            if len(cycle) > 1:\n",
    "                # Remove the last dependency in the cycle\n",
    "                from_step = cycle[-2]\n",
    "                to_step = cycle[-1]\n",
    "\n",
    "                self.logger.warning(\n",
    "                    f\"Breaking cycle by removing dependency: {from_step} -> {to_step}\"\n",
    "                )\n",
    "\n",
    "                # Remove from adjacency lists\n",
    "                if to_step in graph._adjacency_list[from_step]:\n",
    "                    graph._adjacency_list[from_step].remove(to_step)\n",
    "                if from_step in graph._reverse_adjacency_list[to_step]:\n",
    "                    graph._reverse_adjacency_list[to_step].remove(from_step)\n",
    "\n",
    "                # Update node dependencies\n",
    "                if to_step in graph.nodes[from_step].dependencies:\n",
    "                    graph.nodes[from_step].dependencies.remove(to_step)\n",
    "                if from_step in graph.nodes[to_step].dependents:\n",
    "                    graph.nodes[to_step].dependents.remove(from_step)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _detect_conflicts(self, graph: DependencyGraph) -> list[str]:\n",
    "        \"\"\"Detect dependency conflicts.\"\"\"\n",
    "        conflicts = []\n",
    "\n",
    "        # Check for conflicting step names\n",
    "        step_names = list(graph.nodes.keys())\n",
    "        seen_names = set()\n",
    "        for node_name in step_names:\n",
    "            if node_name in seen_names:\n",
    "                conflicts.append(f\"Conflicting step name: {node_name}\")\n",
    "            seen_names.add(node_name)\n",
    "\n",
    "        # Check for missing dependencies\n",
    "        for node_name, node in graph.nodes.items():\n",
    "            for dep in node.dependencies:\n",
    "                if dep not in graph.nodes:\n",
    "                    conflicts.append(f\"Node {node_name} depends on missing node {dep}\")\n",
    "\n",
    "        return conflicts\n",
    "\n",
    "    def _generate_recommendations(\n",
    "        self, graph: DependencyGraph, cycles: list[list[str]], conflicts: list[str]\n",
    "    ) -> list[str]:\n",
    "        \"\"\"Generate optimization recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        # Cycle recommendations\n",
    "        if cycles:\n",
    "            recommendations.append(\n",
    "                \"Consider refactoring to eliminate circular dependencies\"\n",
    "            )\n",
    "\n",
    "        # Conflict recommendations\n",
    "        if conflicts:\n",
    "            recommendations.append(\"Resolve dependency conflicts before execution\")\n",
    "\n",
    "        # Performance recommendations\n",
    "        stats = graph.get_stats()\n",
    "        if stats[\"average_dependencies\"] > 3:\n",
    "            recommendations.append(\n",
    "                \"Consider reducing step dependencies for better parallelization\"\n",
    "            )\n",
    "\n",
    "        if len(graph.nodes) > 10:\n",
    "            recommendations.append(\n",
    "                \"Consider breaking large pipelines into smaller, focused pipelines\"\n",
    "            )\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def _create_cache_key(\n",
    "        self,\n",
    "        bronze_steps: Dict[str, BronzeStepProtocol] | None,\n",
    "        silver_steps: Dict[str, SilverStepProtocol] | None,\n",
    "        gold_steps: Dict[str, GoldStepProtocol] | None,\n",
    "    ) -> str:\n",
    "        \"\"\"Create a cache key for the analysis.\"\"\"\n",
    "        # Create a simple hash of the step configurations\n",
    "        key_parts = []\n",
    "\n",
    "        if bronze_steps:\n",
    "            key_parts.extend(sorted(bronze_steps.keys()))\n",
    "        if silver_steps:\n",
    "            key_parts.extend(sorted(silver_steps.keys()))\n",
    "        if gold_steps:\n",
    "            key_parts.extend(sorted(gold_steps.keys()))\n",
    "\n",
    "        key_string = f\"{self.strategy.value}:{':'.join(key_parts)}\"\n",
    "        return hashlib.sha256(key_string.encode()).hexdigest()\n",
    "\n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"Clear the analysis cache.\"\"\"\n",
    "        self._analysis_cache.clear()\n",
    "        self.logger.info(\"Dependency analysis cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.models.pipeline (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: pipeline_builder_base.errors\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# from ..errors import PipelineValidationError  # Removed: defined in notebook cells above\n",
    "# from .base import BaseModel, ParallelConfig, ValidationThresholds  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Main pipeline configuration.\n",
    "\n",
    "    Attributes:\n",
    "        schema: Database schema name\n",
    "        thresholds: Validation thresholds for each phase\n",
    "        parallel: Parallel execution configuration\n",
    "        verbose: Whether to enable verbose logging\n",
    "    \"\"\"\n",
    "\n",
    "    schema: str\n",
    "    thresholds: ValidationThresholds\n",
    "    parallel: ParallelConfig | bool\n",
    "    verbose: bool = True\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Post-initialization to convert boolean parallel to ParallelConfig.\"\"\"\n",
    "        # Convert boolean parallel to ParallelConfig for backward compatibility\n",
    "        if isinstance(self.parallel, bool):\n",
    "            if self.parallel:\n",
    "                # If True, create default parallel config\n",
    "                object.__setattr__(self, \"parallel\", ParallelConfig.create_default())\n",
    "            else:\n",
    "                # If False, create sequential config\n",
    "                object.__setattr__(self, \"parallel\", ParallelConfig.create_sequential())\n",
    "\n",
    "    @property\n",
    "    def min_bronze_rate(self) -> float:\n",
    "        \"\"\"Get bronze validation threshold.\"\"\"\n",
    "        return self.thresholds.bronze\n",
    "\n",
    "    @property\n",
    "    def min_silver_rate(self) -> float:\n",
    "        \"\"\"Get silver validation threshold.\"\"\"\n",
    "        return self.thresholds.silver\n",
    "\n",
    "    @property\n",
    "    def min_gold_rate(self) -> float:\n",
    "        \"\"\"Get gold validation threshold.\"\"\"\n",
    "        return self.thresholds.gold\n",
    "\n",
    "    @property\n",
    "    def enable_parallel_silver(self) -> bool:\n",
    "        \"\"\"Get parallel silver execution setting.\"\"\"\n",
    "        # After __post_init__, parallel is always ParallelConfig\n",
    "        if isinstance(self.parallel, ParallelConfig):\n",
    "            return self.parallel.enabled\n",
    "        # Fallback for mock configs in tests\n",
    "        return bool(self.parallel)\n",
    "\n",
    "    @property\n",
    "    def max_parallel_workers(self) -> int:\n",
    "        \"\"\"Get max parallel workers setting.\"\"\"\n",
    "        # After __post_init__, parallel is always ParallelConfig\n",
    "        if isinstance(self.parallel, ParallelConfig):\n",
    "            return self.parallel.max_workers\n",
    "        # Fallback for mock configs in tests\n",
    "        return 4\n",
    "\n",
    "    @property\n",
    "    def enable_caching(self) -> bool:\n",
    "        \"\"\"Get caching setting.\"\"\"\n",
    "        return getattr(self.parallel, \"enable_caching\", True)\n",
    "\n",
    "    @property\n",
    "    def enable_monitoring(self) -> bool:\n",
    "        \"\"\"Get monitoring setting.\"\"\"\n",
    "        return getattr(self.parallel, \"enable_monitoring\", True)\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate pipeline configuration.\"\"\"\n",
    "        if not self.schema or not isinstance(self.schema, str):\n",
    "            raise PipelineValidationError(\"Schema name must be a non-empty string\")\n",
    "        self.thresholds.validate()\n",
    "        # After __post_init__, parallel is always ParallelConfig\n",
    "        if isinstance(self.parallel, ParallelConfig):\n",
    "            self.parallel.validate()\n",
    "\n",
    "    @classmethod\n",
    "    def create_default(cls, schema: str) -> PipelineConfig:\n",
    "        \"\"\"Create default pipeline configuration.\"\"\"\n",
    "        return cls(\n",
    "            schema=schema,\n",
    "            thresholds=ValidationThresholds.create_default(),\n",
    "            parallel=ParallelConfig.create_default(),\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def create_high_performance(cls, schema: str) -> PipelineConfig:\n",
    "        \"\"\"Create high-performance pipeline configuration.\"\"\"\n",
    "        return cls(\n",
    "            schema=schema,\n",
    "            thresholds=ValidationThresholds.create_strict(),\n",
    "            parallel=ParallelConfig.create_high_performance(),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def create_conservative(cls, schema: str) -> PipelineConfig:\n",
    "        \"\"\"Create conservative pipeline configuration.\"\"\"\n",
    "        return cls(\n",
    "            schema=schema,\n",
    "            thresholds=ValidationThresholds.create_strict(),\n",
    "            parallel=ParallelConfig.create_sequential(),\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineMetrics(BaseModel):\n",
    "    \"\"\"\n",
    "    Overall pipeline execution metrics.\n",
    "\n",
    "    Attributes:\n",
    "        total_steps: Total number of steps\n",
    "        successful_steps: Number of successful steps\n",
    "        failed_steps: Number of failed steps\n",
    "        skipped_steps: Number of skipped steps\n",
    "        total_duration: Total execution duration\n",
    "        bronze_duration: Bronze layer duration\n",
    "        silver_duration: Silver layer duration\n",
    "        gold_duration: Gold layer duration\n",
    "        total_rows_processed: Total rows processed\n",
    "        total_rows_written: Total rows written\n",
    "        avg_validation_rate: Average validation rate\n",
    "        parallel_efficiency: Parallel execution efficiency\n",
    "        cache_hit_rate: Cache hit rate\n",
    "        error_count: Number of errors\n",
    "        retry_count: Number of retries\n",
    "    \"\"\"\n",
    "\n",
    "    total_steps: int = 0\n",
    "    successful_steps: int = 0\n",
    "    failed_steps: int = 0\n",
    "    skipped_steps: int = 0\n",
    "    total_duration: float = 0.0\n",
    "    bronze_duration: float = 0.0\n",
    "    silver_duration: float = 0.0\n",
    "    gold_duration: float = 0.0\n",
    "    total_rows_processed: int = 0\n",
    "    total_rows_written: int = 0\n",
    "    avg_validation_rate: float = 0.0\n",
    "    parallel_efficiency: float = 0.0\n",
    "    cache_hit_rate: float = 0.0\n",
    "    error_count: int = 0\n",
    "    retry_count: int = 0\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the pipeline metrics.\"\"\"\n",
    "        if self.total_steps < 0:\n",
    "            raise ValueError(\"Total steps cannot be negative\")\n",
    "        if self.successful_steps < 0:\n",
    "            raise ValueError(\"Successful steps cannot be negative\")\n",
    "        if self.failed_steps < 0:\n",
    "            raise ValueError(\"Failed steps cannot be negative\")\n",
    "        if self.skipped_steps < 0:\n",
    "            raise ValueError(\"Skipped steps cannot be negative\")\n",
    "        if self.total_duration < 0:\n",
    "            raise ValueError(\"Total duration cannot be negative\")\n",
    "        if not 0 <= self.avg_validation_rate <= 100:\n",
    "            raise ValueError(\"Average validation rate must be between 0 and 100\")\n",
    "\n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        \"\"\"Calculate success rate.\"\"\"\n",
    "        return (\n",
    "            (self.successful_steps / self.total_steps * 100)\n",
    "            if self.total_steps > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def failure_rate(self) -> float:\n",
    "        \"\"\"Calculate failure rate.\"\"\"\n",
    "        return 100.0 - self.success_rate\n",
    "\n",
    "    @classmethod\n",
    "    def from_step_results(cls, step_results: list[Any]) -> PipelineMetrics:\n",
    "        \"\"\"Create metrics from step results.\"\"\"\n",
    "        total_steps = len(step_results)\n",
    "        successful_steps = sum(1 for result in step_results if result.success)\n",
    "        failed_steps = total_steps - successful_steps\n",
    "        total_duration_secs = sum(result.duration_secs for result in step_results)\n",
    "        total_rows_processed = sum(result.rows_processed for result in step_results)\n",
    "        total_rows_written = sum(result.rows_written for result in step_results)\n",
    "        avg_validation_rate = (\n",
    "            sum(result.validation_rate for result in step_results) / total_steps\n",
    "            if total_steps > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        return cls(\n",
    "            total_steps=total_steps,\n",
    "            successful_steps=successful_steps,\n",
    "            failed_steps=failed_steps,\n",
    "            total_duration=total_duration_secs,\n",
    "            total_rows_processed=total_rows_processed,\n",
    "            total_rows_written=total_rows_written,\n",
    "            avg_validation_rate=avg_validation_rate,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.models.base (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: pipeline_builder_base.errors\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "# from ..errors import PipelineValidationError  # Removed: defined in notebook cells above\n",
    "# from .enums import PipelinePhase  # Removed: defined in notebook cells above\n",
    "# from .types import ModelValue  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"\n",
    "    Base class for all pipeline models with common functionality.\n",
    "\n",
    "    Provides standard validation, serialization, and representation methods\n",
    "    for all pipeline data models. All models in the pipeline system inherit\n",
    "    from this base class to ensure consistent behavior.\n",
    "\n",
    "    Features:\n",
    "    - Automatic validation support\n",
    "    - JSON serialization and deserialization\n",
    "    - Dictionary conversion for easy data exchange\n",
    "    - String representation for debugging\n",
    "    - Type-safe field access\n",
    "\n",
    "    Example:\n",
    "        >>> @dataclass\n",
    "        >>> class MyStep(BaseModel):\n",
    "        ...     name: str\n",
    "        ...     rules: Dict[str, List[ColumnRule]]\n",
    "        ...\n",
    "        ...     def validate(self) -> None:\n",
    "        ...         if not self.name:\n",
    "        ...             raise ValueError(\"Name cannot be empty\")\n",
    "        ...         if not self.rules:\n",
    "        ...             raise ValueError(\"Rules cannot be empty\")\n",
    "        >>>\n",
    "        >>> step = MyStep(name=\"test\", rules={\"id\": [F.col(\"id\").isNotNull()]})\n",
    "        >>> step.validate()\n",
    "        >>> print(step.to_json())\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the model. Override in subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def to_dict(self) -> Dict[str, ModelValue]:\n",
    "        \"\"\"Convert model to dictionary.\"\"\"\n",
    "        result: Dict[str, ModelValue] = {}\n",
    "        for field_info in self.__dataclass_fields__.values():\n",
    "            value = getattr(self, field_info.name)\n",
    "            if hasattr(value, \"to_dict\"):\n",
    "                result[field_info.name] = value.to_dict()\n",
    "            else:\n",
    "                result[field_info.name] = value\n",
    "        return result\n",
    "\n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Convert model to JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), default=str, indent=2)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"String representation of the model.\"\"\"\n",
    "        return f\"{self.__class__.__name__}({', '.join(f'{k}={v}' for k, v in self.to_dict().items())})\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationThresholds(BaseModel):\n",
    "    \"\"\"\n",
    "    Validation thresholds for different pipeline phases.\n",
    "\n",
    "    Attributes:\n",
    "        bronze: Bronze layer validation threshold (0-100)\n",
    "        silver: Silver layer validation threshold (0-100)\n",
    "        gold: Gold layer validation threshold (0-100)\n",
    "    \"\"\"\n",
    "\n",
    "    bronze: float\n",
    "    silver: float\n",
    "    gold: float\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate threshold values.\"\"\"\n",
    "        for phase, threshold in [\n",
    "            (\"bronze\", self.bronze),\n",
    "            (\"silver\", self.silver),\n",
    "            (\"gold\", self.gold),\n",
    "        ]:\n",
    "            if not 0 <= threshold <= 100:\n",
    "                raise PipelineValidationError(\n",
    "                    f\"{phase} threshold must be between 0 and 100, got {threshold}\"\n",
    "                )\n",
    "\n",
    "    def get_threshold(self, phase: PipelinePhase) -> float:\n",
    "        \"\"\"Get threshold for a specific phase.\"\"\"\n",
    "        phase_map = {\n",
    "            PipelinePhase.BRONZE: self.bronze,\n",
    "            PipelinePhase.SILVER: self.silver,\n",
    "            PipelinePhase.GOLD: self.gold,\n",
    "        }\n",
    "        return phase_map[phase]\n",
    "\n",
    "    @classmethod\n",
    "    def create_default(cls) -> ValidationThresholds:\n",
    "        \"\"\"Create default validation thresholds.\"\"\"\n",
    "        return cls(bronze=95.0, silver=98.0, gold=99.0)\n",
    "\n",
    "    @classmethod\n",
    "    def create_strict(cls) -> ValidationThresholds:\n",
    "        \"\"\"Create strict validation thresholds.\"\"\"\n",
    "        return cls(bronze=99.0, silver=99.5, gold=99.9)\n",
    "\n",
    "    @classmethod\n",
    "    def create_loose(cls) -> ValidationThresholds:\n",
    "        \"\"\"Create loose validation thresholds.\"\"\"\n",
    "        return cls(bronze=80.0, silver=85.0, gold=90.0)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParallelConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration for parallel execution.\n",
    "\n",
    "    Attributes:\n",
    "        enabled: Whether parallel execution is enabled\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        timeout_secs: Timeout for parallel operations in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    enabled: bool\n",
    "    max_workers: int\n",
    "    timeout_secs: int = 300\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate parallel configuration.\"\"\"\n",
    "        if self.max_workers < 1:\n",
    "            raise PipelineValidationError(\n",
    "                f\"max_workers must be at least 1, got {self.max_workers}\"\n",
    "            )\n",
    "        if self.max_workers > 32:\n",
    "            raise PipelineValidationError(\n",
    "                f\"max_workers should not exceed 32, got {self.max_workers}\"\n",
    "            )\n",
    "        if self.timeout_secs < 1:\n",
    "            raise PipelineValidationError(\n",
    "                f\"timeout_secs must be at least 1, got {self.timeout_secs}\"\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def create_default(cls) -> ParallelConfig:\n",
    "        \"\"\"Create default parallel configuration.\"\"\"\n",
    "        return cls(enabled=True, max_workers=4, timeout_secs=300)\n",
    "\n",
    "    @classmethod\n",
    "    def create_sequential(cls) -> ParallelConfig:\n",
    "        \"\"\"Create sequential execution configuration.\"\"\"\n",
    "        return cls(enabled=False, max_workers=1, timeout_secs=600)\n",
    "\n",
    "    @classmethod\n",
    "    def create_high_performance(cls) -> ParallelConfig:\n",
    "        \"\"\"Create high-performance parallel configuration.\"\"\"\n",
    "        return cls(enabled=True, max_workers=16, timeout_secs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.functions (pipeline_builder)\n",
    "#\n",
    "# Dependencies: compat\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Protocol\n",
    "\n",
    "# from .compat import Column  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class FunctionsProtocol(Protocol):\n",
    "    \"\"\"Protocol for PySpark functions interface.\"\"\"\n",
    "\n",
    "    def col(self, col_name: str) -> Column:\n",
    "        \"\"\"Create a column reference.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def expr(self, expr: str) -> Column:\n",
    "        \"\"\"Create an expression from a string.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def lit(self, value: str | int | float | bool | None) -> Column:\n",
    "        \"\"\"Create a literal column.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def when(self, condition: Column, value: str | int | float | bool | None) -> Column:\n",
    "        \"\"\"Create a conditional expression.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def count(self, col: str | Column = \"*\") -> Column:\n",
    "        \"\"\"Create a count aggregation.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def countDistinct(self, *cols: str | Column) -> Column:\n",
    "        \"\"\"Create a count distinct aggregation.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def sum(self, col: str | Column) -> Column:\n",
    "        \"\"\"Create a sum aggregation.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def max(self, col: str | Column) -> Column:\n",
    "        \"\"\"Create a max aggregation.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def min(self, col: str | Column) -> Column:\n",
    "        \"\"\"Create a min aggregation.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def avg(self, col: str | Column) -> Column:\n",
    "        \"\"\"Create an average aggregation.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def length(self, col: str | Column) -> Column:\n",
    "        \"\"\"Create a length function.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def date_trunc(self, format: str, col: str | Column) -> Column:\n",
    "        \"\"\"Create a date truncation function.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def dayofweek(self, col: str | Column) -> Column:\n",
    "        \"\"\"Create a day of week function.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def current_timestamp(self) -> Column:\n",
    "        \"\"\"Create a current timestamp function.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "def get_default_functions() -> FunctionsProtocol:\n",
    "    \"\"\"Get the default PySpark functions implementation.\n",
    "\n",
    "    Returns the functions from the current compatibility layer.\n",
    "    \"\"\"\n",
    "    # from .compat import F  # Removed: defined in notebook cells above\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.types (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Protocol, Union\n",
    "\n",
    "# from .compat import Column, DataFrame, SparkSession  # Removed: defined in notebook cells above\n",
    "\n",
    "# ============================================================================\n",
    "# Basic Type Aliases\n",
    "# ============================================================================\n",
    "\n",
    "# String types\n",
    "StepName = str\n",
    "PipelineId = str\n",
    "ExecutionId = str\n",
    "TableName = str\n",
    "SchemaName = str\n",
    "ErrorCode = str\n",
    "\n",
    "# Numeric types\n",
    "QualityRate = float\n",
    "Duration = float\n",
    "RowCount = int\n",
    "\n",
    "# Dictionary types\n",
    "StringDict = Dict[str, str]\n",
    "NumericDict = Dict[str, Union[int, float]]\n",
    "GenericDict = Dict[str, Any]\n",
    "OptionalDict = Optional[Dict[str, Any]]\n",
    "OptionalList = Optional[List[Any]]\n",
    "\n",
    "# ============================================================================\n",
    "# Enums\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class StepType(Enum):\n",
    "    \"\"\"Types of pipeline steps.\"\"\"\n",
    "\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "\n",
    "class StepStatus(Enum):\n",
    "    \"\"\"Step execution status.\"\"\"\n",
    "\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    SKIPPED = \"skipped\"\n",
    "\n",
    "\n",
    "# PipelineMode moved to pipeline/models.py to avoid duplication\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Function Types\n",
    "# ============================================================================\n",
    "\n",
    "# Transform function types\n",
    "TransformFunction = Callable[[SparkSession, DataFrame], DataFrame]\n",
    "BronzeTransformFunction = Callable[[SparkSession, DataFrame], DataFrame]\n",
    "SilverTransformFunction = Callable[\n",
    "    [SparkSession, DataFrame, Dict[str, DataFrame]], DataFrame\n",
    "]\n",
    "GoldTransformFunction = Callable[[SparkSession, Dict[str, DataFrame]], DataFrame]\n",
    "\n",
    "# Filter function type\n",
    "FilterFunction = Callable[[DataFrame], DataFrame]\n",
    "\n",
    "# ============================================================================\n",
    "# Data Types\n",
    "# ============================================================================\n",
    "\n",
    "# Column rules type\n",
    "ColumnRules = Dict[str, List[Union[str, Column]]]\n",
    "\n",
    "# Result types\n",
    "StepResult = Dict[str, Any]\n",
    "PipelineResult = Dict[str, Any]\n",
    "ExecutionResultDict = Dict[str, Any]\n",
    "ValidationResultDict = Dict[str, Any]\n",
    "\n",
    "# Context types\n",
    "StepContext = Dict[str, Any]\n",
    "ExecutionContext = Dict[str, Any]\n",
    "\n",
    "# Configuration types\n",
    "PipelineConfigDict = Dict[str, Any]\n",
    "ExecutionConfig = Dict[str, Any]\n",
    "ValidationConfig = Dict[str, Any]\n",
    "MonitoringConfig = Dict[str, Any]\n",
    "\n",
    "# Quality types\n",
    "QualityThresholds = Dict[str, float]\n",
    "\n",
    "# Error types\n",
    "ErrorContext = Dict[str, Any]\n",
    "ErrorSuggestions = List[str]\n",
    "\n",
    "# ============================================================================\n",
    "# Protocols (Simplified)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class Validatable(Protocol):\n",
    "    \"\"\"Protocol for objects that can be validated.\"\"\"\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the object and raise ValidationError if invalid.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class Serializable(Protocol):\n",
    "    \"\"\"Protocol for objects that can be serialized.\"\"\"\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert object to dictionary.\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.types (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat\n",
    "\n",
    "from typing import Callable, Dict, List, Protocol, TypeVar, Union\n",
    "\n",
    "# from ..compat import Column, DataFrame, SparkSession  # Removed: defined in notebook cells above\n",
    "\n",
    "# Specific types for model values instead of Any\n",
    "ModelValue = Union[str, int, float, bool, List[str], Dict[str, str], None]\n",
    "ColumnRule = Union[DataFrame, str, bool]  # PySpark Column, string, or boolean\n",
    "ResourceValue = Union[str, int, float, bool, List[str], Dict[str, str]]\n",
    "\n",
    "# Type aliases for better readability\n",
    "ColumnRules = Dict[str, List[Union[str, Column]]]\n",
    "TransformFunction = Callable[[DataFrame], DataFrame]\n",
    "SilverTransformFunction = Callable[\n",
    "    [SparkSession, DataFrame, Dict[str, DataFrame]], DataFrame\n",
    "]\n",
    "GoldTransformFunction = Callable[[SparkSession, Dict[str, DataFrame]], DataFrame]\n",
    "\n",
    "# Generic type for pipeline results\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Validatable(Protocol):\n",
    "    \"\"\"Protocol for objects that can be validated.\"\"\"\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the object and raise ValidationError if invalid.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class Serializable(Protocol):\n",
    "    \"\"\"Protocol for objects that can be serialized.\"\"\"\n",
    "\n",
    "    def to_dict(self) -> Dict[str, ModelValue]:\n",
    "        \"\"\"Convert object to dictionary.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Convert object to JSON string.\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.writer.query_builder (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from ..compat import DataFrame  # Removed: defined in notebook cells above\n",
    "\n",
    "# Import specific functions for convenience\n",
    "# from ..compat import F as functions  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class QueryBuilder:\n",
    "    \"\"\"Builder class for common PySpark DataFrame operations.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_by_date_range(df: DataFrame, days: int = 30) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Filter DataFrame by date range.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            days: Number of days to look back\n",
    "\n",
    "        Returns:\n",
    "            Filtered DataFrame\n",
    "        \"\"\"\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        return df.filter(\n",
    "            functions.col(\"created_at\")\n",
    "            >= functions.lit(start_date.strftime(\"%Y-%m-%d\"))\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def add_date_column(\n",
    "        df: DataFrame,\n",
    "        date_column: str = \"created_at\",\n",
    "        output_column: str = \"date\",\n",
    "        format: str = \"yyyy-MM-dd\",\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Add formatted date column to DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            date_column: Source date column name\n",
    "            output_column: Output column name\n",
    "            format: Date format string\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with added date column\n",
    "        \"\"\"\n",
    "        return df.withColumn(\n",
    "            output_column, functions.date_format(functions.col(date_column), format)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_common_aggregations() -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get common aggregation functions.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of common aggregations\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"count_all\": functions.count(\"*\").alias(\"total_executions\"),\n",
    "            \"count_rows\": functions.count(\"*\").alias(\"execution_count\"),\n",
    "            \"avg_validation_rate\": functions.avg(\"validation_rate\").alias(\n",
    "                \"avg_validation_rate\"\n",
    "            ),\n",
    "            \"min_validation_rate\": functions.min(\"validation_rate\").alias(\n",
    "                \"min_validation_rate\"\n",
    "            ),\n",
    "            \"max_validation_rate\": functions.max(\"validation_rate\").alias(\n",
    "                \"max_validation_rate\"\n",
    "            ),\n",
    "            \"stddev_validation_rate\": functions.stddev(\"validation_rate\").alias(\n",
    "                \"stddev_validation_rate\"\n",
    "            ),\n",
    "            \"avg_execution_time\": functions.avg(\"execution_time\").alias(\n",
    "                \"avg_execution_time\"\n",
    "            ),\n",
    "            \"min_execution_time\": functions.min(\"execution_time\").alias(\n",
    "                \"min_execution_time\"\n",
    "            ),\n",
    "            \"max_execution_time\": functions.max(\"execution_time\").alias(\n",
    "                \"max_execution_time\"\n",
    "            ),\n",
    "            \"stddev_execution_time\": functions.stddev(\"execution_time\").alias(\n",
    "                \"stddev_execution_time\"\n",
    "            ),\n",
    "            \"sum_rows_written\": functions.sum(\"rows_written\").alias(\n",
    "                \"total_rows_written\"\n",
    "            ),\n",
    "            \"successful_executions\": functions.sum(\n",
    "                functions.when(functions.col(\"success\"), 1).otherwise(0)\n",
    "            ).alias(\"successful_executions\"),\n",
    "            \"failed_executions\": functions.sum(\n",
    "                functions.when(~functions.col(\"success\"), 1).otherwise(0)\n",
    "            ).alias(\"failed_executions\"),\n",
    "            \"high_quality_executions\": functions.sum(\n",
    "                functions.when(functions.col(\"validation_rate\") >= 95.0, 1).otherwise(0)\n",
    "            ).alias(\"high_quality_executions\"),\n",
    "            \"low_quality_executions\": functions.sum(\n",
    "                functions.when(functions.col(\"validation_rate\") < 80.0, 1).otherwise(0)\n",
    "            ).alias(\"low_quality_executions\"),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_quality_aggregations() -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get quality-specific aggregations.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of quality aggregations\n",
    "        \"\"\"\n",
    "        aggs = QueryBuilder.get_common_aggregations()\n",
    "        return {\n",
    "            \"total_executions\": aggs[\"count_all\"],\n",
    "            \"avg_validation_rate\": aggs[\"avg_validation_rate\"],\n",
    "            \"min_validation_rate\": aggs[\"min_validation_rate\"],\n",
    "            \"max_validation_rate\": aggs[\"max_validation_rate\"],\n",
    "            \"stddev_validation_rate\": aggs[\"stddev_validation_rate\"],\n",
    "            \"high_quality_executions\": aggs[\"high_quality_executions\"],\n",
    "            \"low_quality_executions\": aggs[\"low_quality_executions\"],\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_performance_aggregations() -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get performance-specific aggregations.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of performance aggregations\n",
    "        \"\"\"\n",
    "        aggs = QueryBuilder.get_common_aggregations()\n",
    "        return {\n",
    "            \"execution_count\": aggs[\"count_rows\"],\n",
    "            \"avg_execution_time\": aggs[\"avg_execution_time\"],\n",
    "            \"min_execution_time\": aggs[\"min_execution_time\"],\n",
    "            \"max_execution_time\": aggs[\"max_execution_time\"],\n",
    "            \"stddev_execution_time\": aggs[\"stddev_execution_time\"],\n",
    "            \"avg_validation_rate\": aggs[\"avg_validation_rate\"],\n",
    "            \"total_rows_written\": aggs[\"sum_rows_written\"],\n",
    "            \"successful_executions\": aggs[\"successful_executions\"],\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_trend_aggregations() -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get trend-specific aggregations.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of trend aggregations\n",
    "        \"\"\"\n",
    "        aggs = QueryBuilder.get_common_aggregations()\n",
    "        return {\n",
    "            \"daily_executions\": aggs[\"count_all\"],\n",
    "            \"successful_executions\": aggs[\"successful_executions\"],\n",
    "            \"failed_executions\": aggs[\"failed_executions\"],\n",
    "            \"avg_execution_time\": aggs[\"avg_execution_time\"],\n",
    "            \"total_rows_written\": aggs[\"sum_rows_written\"],\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def build_daily_trends_query(df: DataFrame, days: int = 30) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build daily trends query with common aggregations.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with daily trends\n",
    "        \"\"\"\n",
    "        filtered_df = QueryBuilder.filter_by_date_range(df, days)\n",
    "        aggs = QueryBuilder.get_trend_aggregations()\n",
    "\n",
    "        return (\n",
    "            filtered_df.transform(lambda df: QueryBuilder.add_date_column(df))\n",
    "            .groupBy(\"date\")\n",
    "            .agg(**aggs)\n",
    "            .orderBy(\"date\")\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_phase_trends_query(df: DataFrame, days: int = 30) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build phase trends query with common aggregations.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with phase trends\n",
    "        \"\"\"\n",
    "        filtered_df = QueryBuilder.filter_by_date_range(df, days)\n",
    "        aggs = QueryBuilder.get_performance_aggregations()\n",
    "\n",
    "        return filtered_df.groupBy(\"phase\").agg(**aggs).orderBy(\"phase\")\n",
    "\n",
    "    @staticmethod\n",
    "    def build_step_trends_query(df: DataFrame, days: int = 30) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build step trends query with common aggregations.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with step trends\n",
    "        \"\"\"\n",
    "        filtered_df = QueryBuilder.filter_by_date_range(df, days)\n",
    "        aggs = QueryBuilder.get_performance_aggregations()\n",
    "\n",
    "        return (\n",
    "            filtered_df.groupBy(\"step\")\n",
    "            .agg(**aggs)\n",
    "            .orderBy(functions.desc(\"avg_execution_time\"))\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_quality_trends_query(df: DataFrame, days: int = 30) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build quality trends query with common aggregations.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with quality trends\n",
    "        \"\"\"\n",
    "        filtered_df = QueryBuilder.filter_by_date_range(df, days)\n",
    "        aggs = QueryBuilder.get_quality_aggregations()\n",
    "\n",
    "        return (\n",
    "            filtered_df.transform(lambda df: QueryBuilder.add_date_column(df))\n",
    "            .groupBy(\"date\")\n",
    "            .agg(**aggs)\n",
    "            .orderBy(\"date\")\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_overall_metrics_query(df: DataFrame, days: int = 30) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build overall metrics query.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with overall metrics\n",
    "        \"\"\"\n",
    "        filtered_df = QueryBuilder.filter_by_date_range(df, days)\n",
    "        aggs = QueryBuilder.get_quality_aggregations()\n",
    "\n",
    "        return filtered_df.agg(**aggs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_anomaly_detection_query(\n",
    "        df: DataFrame, threshold_column: str, threshold_value: float\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build anomaly detection query.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            threshold_column: Column to check against threshold\n",
    "            threshold_value: Threshold value\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with anomalies\n",
    "        \"\"\"\n",
    "        return df.filter(functions.col(threshold_column) < threshold_value)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_performance_anomaly_query(\n",
    "        df: DataFrame, performance_threshold: float\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build performance anomaly detection query.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            performance_threshold: Performance threshold value\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with performance anomalies\n",
    "        \"\"\"\n",
    "        return df.filter(\n",
    "            (functions.col(\"execution_time\") > performance_threshold)\n",
    "            | (functions.col(\"validation_rate\") < 80.0)\n",
    "            | (~functions.col(\"success\"))\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_quality_anomaly_query(\n",
    "        df: DataFrame, quality_threshold: float = 90.0\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build quality anomaly detection query.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            quality_threshold: Quality threshold value\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with quality anomalies\n",
    "        \"\"\"\n",
    "        return df.filter(functions.col(\"validation_rate\") < quality_threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_temporal_anomaly_query(\n",
    "        df: DataFrame, change_threshold: float = -10.0\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build temporal anomaly detection query.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            change_threshold: Change threshold value\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with temporal anomalies\n",
    "        \"\"\"\n",
    "        # First, calculate daily quality metrics\n",
    "        daily_quality_df = (\n",
    "            df.transform(lambda df: QueryBuilder.add_date_column(df))\n",
    "            .groupBy(\"date\")\n",
    "            .agg(functions.avg(\"validation_rate\").alias(\"daily_avg_validation_rate\"))\n",
    "            .orderBy(\"date\")\n",
    "        )\n",
    "\n",
    "        # Use window function to calculate lag and quality change\n",
    "        # from ..compat import Window  # Removed: defined in notebook cells above\n",
    "\n",
    "        window_spec = Window.orderBy(\"date\")\n",
    "        return (\n",
    "            daily_quality_df.withColumn(\n",
    "                \"prev_avg_validation_rate\",\n",
    "                functions.lag(\"daily_avg_validation_rate\", 1).over(window_spec),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"quality_change\",\n",
    "                functions.col(\"daily_avg_validation_rate\")\n",
    "                - functions.col(\"prev_avg_validation_rate\"),\n",
    "            )\n",
    "            .filter(functions.col(\"quality_change\") < change_threshold)\n",
    "            .orderBy(\"quality_change\")\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_statistics(df: DataFrame, column: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate basic statistics for a column.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            column: Column name to calculate statistics for\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with statistics\n",
    "        \"\"\"\n",
    "        stats_df = df.agg(\n",
    "            functions.avg(column).alias(\"avg\"),\n",
    "            functions.stddev(column).alias(\"stddev\"),\n",
    "            functions.min(column).alias(\"min\"),\n",
    "            functions.max(column).alias(\"max\"),\n",
    "        )\n",
    "\n",
    "        result = stats_df.collect()[0]\n",
    "        return {\n",
    "            \"avg\": result[\"avg\"],\n",
    "            \"stddev\": result[\"stddev\"],\n",
    "            \"min\": result[\"min\"],\n",
    "            \"max\": result[\"max\"],\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def build_recent_performance_query(df: DataFrame, days: int = 7) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Build recent performance query.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            days: Number of recent days to analyze\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with recent performance\n",
    "        \"\"\"\n",
    "        filtered_df = QueryBuilder.filter_by_date_range(df, days)\n",
    "        aggs = {\n",
    "            \"daily_executions\": functions.count(\"*\").alias(\"daily_executions\"),\n",
    "            \"avg_execution_time\": functions.avg(\"execution_time\").alias(\n",
    "                \"avg_execution_time\"\n",
    "            ),\n",
    "            \"avg_validation_rate\": functions.avg(\"validation_rate\").alias(\n",
    "                \"avg_validation_rate\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        return (\n",
    "            filtered_df.transform(lambda df: QueryBuilder.add_date_column(df))\n",
    "            .groupBy(\"date\")\n",
    "            .agg(**aggs)\n",
    "            .orderBy(\"date\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.validation.utils (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from ..compat import DataFrame  # Removed: defined in notebook cells above\n",
    "# Re-export safe_divide from base for backward compatibility\n",
    "# from .validation import safe_divide  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "def safe_divide(numerator: float, denominator: float, default: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    Safely divide two numbers, returning default if denominator is zero or None.\n",
    "\n",
    "    Args:\n",
    "        numerator: The numerator\n",
    "        denominator: The denominator\n",
    "        default: Default value to return if denominator is zero or None\n",
    "\n",
    "    Returns:\n",
    "        The division result or default value\n",
    "    \"\"\"\n",
    "    if denominator is None or numerator is None or denominator == 0:\n",
    "        return default\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def get_dataframe_info(df: DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get basic information about a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with DataFrame information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row_count = df.count()\n",
    "        column_count = len(df.columns)\n",
    "        schema = df.schema\n",
    "\n",
    "        return {\n",
    "            \"row_count\": row_count,\n",
    "            \"column_count\": column_count,\n",
    "            \"columns\": df.columns,\n",
    "            \"schema\": str(schema),\n",
    "            \"is_empty\": row_count == 0,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"row_count\": 0,\n",
    "            \"column_count\": 0,\n",
    "            \"columns\": [],\n",
    "            \"schema\": \"unknown\",\n",
    "            \"is_empty\": True,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.pipeline.models (pipeline_builder)\n",
    "#\n",
    "# Dependencies: models.pipeline\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from .models import PipelineMetrics  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class PipelineMode(Enum):\n",
    "    \"\"\"Pipeline execution modes.\"\"\"\n",
    "\n",
    "    INITIAL = \"initial\"\n",
    "    INCREMENTAL = \"incremental\"\n",
    "    FULL_REFRESH = \"full_refresh\"\n",
    "    VALIDATION_ONLY = \"validation_only\"\n",
    "\n",
    "\n",
    "class PipelineStatus(Enum):\n",
    "    \"\"\"Pipeline execution status.\"\"\"\n",
    "\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    CANCELLED = \"cancelled\"\n",
    "    PAUSED = \"paused\"\n",
    "\n",
    "\n",
    "# PipelineMetrics moved to main models.py to avoid duplication\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineReport:\n",
    "    \"\"\"Comprehensive pipeline execution report.\"\"\"\n",
    "\n",
    "    pipeline_id: str\n",
    "    execution_id: str\n",
    "    mode: PipelineMode\n",
    "    status: PipelineStatus  # Protocol expects str, but we use enum - structural typing allows this\n",
    "    start_time: datetime\n",
    "    end_time: datetime | None = None\n",
    "    duration_seconds: float = 0.0\n",
    "    metrics: PipelineMetrics = field(default_factory=PipelineMetrics)\n",
    "    bronze_results: Dict[str, Any] = field(default_factory=dict)\n",
    "    silver_results: Dict[str, Any] = field(default_factory=dict)\n",
    "    gold_results: Dict[str, Any] = field(default_factory=dict)\n",
    "    errors: list[str] = field(default_factory=list)\n",
    "    warnings: list[str] = field(default_factory=list)\n",
    "    recommendations: list[str] = field(default_factory=list)\n",
    "    execution_groups_count: int = 0\n",
    "    max_group_size: int = 0\n",
    "\n",
    "    @property\n",
    "    def success(self) -> bool:\n",
    "        \"\"\"Whether the pipeline executed successfully.\"\"\"\n",
    "        return self.status == PipelineStatus.COMPLETED and len(self.errors) == 0\n",
    "\n",
    "    @property\n",
    "    def status_str(self) -> str:\n",
    "        \"\"\"Return status as string for Protocol compatibility.\"\"\"\n",
    "        return self.status.value\n",
    "\n",
    "    @property\n",
    "    def successful_steps(self) -> int:\n",
    "        \"\"\"Number of successful steps.\"\"\"\n",
    "        return self.metrics.successful_steps\n",
    "\n",
    "    @property\n",
    "    def failed_steps(self) -> int:\n",
    "        \"\"\"Number of failed steps.\"\"\"\n",
    "        return self.metrics.failed_steps\n",
    "\n",
    "    @property\n",
    "    def parallel_efficiency(self) -> float:\n",
    "        \"\"\"Parallel execution efficiency percentage.\"\"\"\n",
    "        return self.metrics.parallel_efficiency\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert report to dictionary.\"\"\"\n",
    "        return {\n",
    "            \"pipeline_id\": self.pipeline_id,\n",
    "            \"execution_id\": self.execution_id,\n",
    "            \"mode\": self.mode.value,\n",
    "            \"status\": self.status.value,\n",
    "            \"start_time\": self.start_time.isoformat(),\n",
    "            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n",
    "            \"duration_seconds\": self.duration_seconds,\n",
    "            \"metrics\": {\n",
    "                \"total_steps\": self.metrics.total_steps,\n",
    "                \"successful_steps\": self.metrics.successful_steps,\n",
    "                \"failed_steps\": self.metrics.failed_steps,\n",
    "                \"skipped_steps\": self.metrics.skipped_steps,\n",
    "                \"total_duration\": self.metrics.total_duration,\n",
    "                \"bronze_duration\": self.metrics.bronze_duration,\n",
    "                \"silver_duration\": self.metrics.silver_duration,\n",
    "                \"gold_duration\": self.metrics.gold_duration,\n",
    "                \"total_rows_processed\": self.metrics.total_rows_processed,\n",
    "                \"total_rows_written\": self.metrics.total_rows_written,\n",
    "                \"parallel_efficiency\": self.metrics.parallel_efficiency,\n",
    "                \"cache_hit_rate\": self.metrics.cache_hit_rate,\n",
    "                \"error_count\": self.metrics.error_count,\n",
    "                \"retry_count\": self.metrics.retry_count,\n",
    "            },\n",
    "            \"bronze_results\": self.bronze_results,\n",
    "            \"silver_results\": self.silver_results,\n",
    "            \"gold_results\": self.gold_results,\n",
    "            \"errors\": self.errors,\n",
    "            \"warnings\": self.warnings,\n",
    "            \"recommendations\": self.recommendations,\n",
    "        }\n",
    "\n",
    "\n",
    "# ParallelConfig and PipelineConfig moved to main models.py to avoid duplication\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepExecutionContext:\n",
    "    \"\"\"Context for step execution.\"\"\"\n",
    "\n",
    "    step_name: str\n",
    "    step_type: str\n",
    "    mode: PipelineMode\n",
    "    start_time: datetime\n",
    "    dependencies: list[str] = field(default_factory=list)\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        \"\"\"Duration of step execution in seconds.\"\"\"\n",
    "        return (datetime.now() - self.start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.steps (pipeline_builder)\n",
    "#\n",
    "# Dependencies: models.base, models.types, pipeline_builder_base.errors\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# from .errors import PipelineValidationError, ValidationError  # Removed: defined in notebook cells above\n",
    "# from .base import BaseModel  # Removed: defined in notebook cells above\n",
    "# from .types import ColumnRules, GoldTransformFunction, SilverTransformFunction  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BronzeStep(BaseModel):\n",
    "    \"\"\"\n",
    "    Bronze layer step configuration for raw data validation and ingestion.\n",
    "\n",
    "    Bronze steps represent the first layer of the Medallion Architecture,\n",
    "    handling raw data validation and establishing the foundation for downstream\n",
    "    processing. They define validation rules and incremental processing capabilities.\n",
    "\n",
    "    **Validation Requirements:**\n",
    "        - `name`: Must be a non-empty string\n",
    "        - `rules`: Must be a non-empty dictionary with validation rules\n",
    "        - `incremental_col`: Must be a string if provided\n",
    "\n",
    "    Attributes:\n",
    "        name: Unique identifier for this Bronze step\n",
    "        rules: Dictionary mapping column names to validation rule lists.\n",
    "               Each rule should be a PySpark Column expression.\n",
    "        incremental_col: Column name for incremental processing (e.g., \"timestamp\").\n",
    "                        If provided, enables watermarking for efficient updates.\n",
    "                        If None, forces full refresh mode for downstream steps.\n",
    "        schema: Optional schema name for reading bronze data\n",
    "\n",
    "    Raises:\n",
    "        ValidationError: If validation requirements are not met during construction\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import functions as F\n",
    "        >>>\n",
    "        >>> # Valid Bronze step with PySpark expressions\n",
    "        >>> bronze_step = BronzeStep(\n",
    "        ...     name=\"user_events\",\n",
    "        ...     rules={\n",
    "        ...         \"user_id\": [F.col(\"user_id\").isNotNull()],\n",
    "        ...         \"event_type\": [F.col(\"event_type\").isin([\"click\", \"view\", \"purchase\"])],\n",
    "        ...         \"timestamp\": [F.col(\"timestamp\").isNotNull(), F.col(\"timestamp\") > \"2020-01-01\"]\n",
    "        ...     },\n",
    "        ...     incremental_col=\"timestamp\"\n",
    "        ... )\n",
    "        >>>\n",
    "        >>> # Validate configuration\n",
    "        >>> bronze_step.validate()\n",
    "        >>> print(f\"Supports incremental: {bronze_step.has_incremental_capability}\")\n",
    "\n",
    "        >>> # Invalid Bronze step (will raise ValidationError)\n",
    "        >>> try:\n",
    "        ...     BronzeStep(name=\"\", rules={})  # Empty name and rules\n",
    "        ... except ValidationError as e:\n",
    "        ...     print(f\"Validation failed: {e}\")\n",
    "        ...     # Output: \"Step name must be a non-empty string\"\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    rules: ColumnRules\n",
    "    incremental_col: str | None = None\n",
    "    schema: str | None = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Validate required fields after initialization.\"\"\"\n",
    "        if not self.name or not isinstance(self.name, str):\n",
    "            raise ValidationError(\"Step name must be a non-empty string\")\n",
    "        if not isinstance(self.rules, dict) or not self.rules:\n",
    "            raise ValidationError(\"Rules must be a non-empty dictionary\")\n",
    "        if self.incremental_col is not None and not isinstance(\n",
    "            self.incremental_col, str\n",
    "        ):\n",
    "            raise ValidationError(\"Incremental column must be a string\")\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate bronze step configuration.\"\"\"\n",
    "        if not self.name or not isinstance(self.name, str):\n",
    "            raise PipelineValidationError(\"Step name must be a non-empty string\")\n",
    "        if not isinstance(self.rules, dict):\n",
    "            raise PipelineValidationError(\"Rules must be a dictionary\")\n",
    "        if self.incremental_col is not None and not isinstance(\n",
    "            self.incremental_col, str\n",
    "        ):\n",
    "            raise PipelineValidationError(\"Incremental column must be a string\")\n",
    "\n",
    "    @property\n",
    "    def has_incremental_capability(self) -> bool:\n",
    "        \"\"\"Check if this Bronze step supports incremental processing.\"\"\"\n",
    "        return self.incremental_col is not None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SilverStep(BaseModel):\n",
    "    \"\"\"\n",
    "    Silver layer step configuration for data cleaning and enrichment.\n",
    "\n",
    "    Silver steps represent the second layer of the Medallion Architecture,\n",
    "    transforming raw Bronze data into clean, business-ready datasets.\n",
    "    They apply data quality rules, business logic, and data transformations.\n",
    "\n",
    "    **Validation Requirements:**\n",
    "        - `name`: Must be a non-empty string\n",
    "        - `source_bronze`: Must be a non-empty string (except for existing tables)\n",
    "        - `transform`: Must be callable and cannot be None\n",
    "        - `rules`: Must be a non-empty dictionary with validation rules\n",
    "        - `table_name`: Must be a non-empty string\n",
    "\n",
    "    Attributes:\n",
    "        name: Unique identifier for this Silver step\n",
    "        source_bronze: Name of the Bronze step providing input data\n",
    "        transform: Transformation function with signature:\n",
    "                 (spark: SparkSession, bronze_df: DataFrame, prior_silvers: Dict[str, DataFrame]) -> DataFrame\n",
    "                 Must be callable and cannot be None.\n",
    "        rules: Dictionary mapping column names to validation rule lists.\n",
    "               Each rule should be a PySpark Column expression.\n",
    "        table_name: Target Delta table name where results will be stored\n",
    "        watermark_col: Column name for watermarking (e.g., \"timestamp\", \"updated_at\").\n",
    "                      If provided, enables incremental processing with append mode.\n",
    "                      If None, uses overwrite mode for full refresh.\n",
    "        existing: Whether this represents an existing table (for validation-only steps)\n",
    "        schema: Optional schema name for writing silver data\n",
    "\n",
    "    Raises:\n",
    "        ValidationError: If validation requirements are not met during construction\n",
    "\n",
    "    Example:\n",
    "        >>> def clean_user_events(spark, bronze_df, prior_silvers):\n",
    "        ...     return (bronze_df\n",
    "        ...         .filter(F.col(\"user_id\").isNotNull())\n",
    "        ...         .withColumn(\"event_date\", F.date_trunc(\"day\", \"timestamp\"))\n",
    "        ...         .withColumn(\"is_weekend\", F.dayofweek(\"timestamp\").isin([1, 7]))\n",
    "        ...     )\n",
    "        >>>\n",
    "        >>> # Valid Silver step\n",
    "        >>> silver_step = SilverStep(\n",
    "        ...     name=\"clean_events\",\n",
    "        ...     source_bronze=\"user_events\",\n",
    "        ...     transform=clean_user_events,\n",
    "        ...     rules={\n",
    "        ...         \"user_id\": [F.col(\"user_id\").isNotNull()],\n",
    "        ...         \"event_date\": [F.col(\"event_date\").isNotNull()]\n",
    "        ...     },\n",
    "        ...     table_name=\"clean_user_events\",\n",
    "        ...     watermark_col=\"timestamp\"\n",
    "        ... )\n",
    "\n",
    "        >>> # Invalid Silver step (will raise ValidationError)\n",
    "        >>> try:\n",
    "        ...     SilverStep(name=\"clean_events\", source_bronze=\"\", transform=None, rules={}, table_name=\"\")\n",
    "        ... except ValidationError as e:\n",
    "        ...     print(f\"Validation failed: {e}\")\n",
    "        ...     # Output: \"Transform function is required and must be callable\"\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    source_bronze: str\n",
    "    transform: SilverTransformFunction\n",
    "    rules: ColumnRules\n",
    "    table_name: str\n",
    "    watermark_col: str | None = None\n",
    "    existing: bool = False\n",
    "    schema: str | None = None\n",
    "    source_incremental_col: str | None = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Validate required fields after initialization.\"\"\"\n",
    "        if not self.name or not isinstance(self.name, str):\n",
    "            raise ValidationError(\"Step name must be a non-empty string\")\n",
    "        if not self.existing and (\n",
    "            not self.source_bronze or not isinstance(self.source_bronze, str)\n",
    "        ):\n",
    "            raise ValidationError(\"Source bronze step name must be a non-empty string\")\n",
    "        if self.transform is None or not callable(self.transform):\n",
    "            raise ValidationError(\"Transform function is required and must be callable\")\n",
    "        if not self.table_name or not isinstance(self.table_name, str):\n",
    "            raise ValidationError(\"Table name must be a non-empty string\")\n",
    "        if self.source_incremental_col is not None and not isinstance(\n",
    "            self.source_incremental_col, str\n",
    "        ):\n",
    "            raise ValidationError(\"source_incremental_col must be a string\")\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate silver step configuration.\"\"\"\n",
    "        if not self.name or not isinstance(self.name, str):\n",
    "            raise PipelineValidationError(\"Step name must be a non-empty string\")\n",
    "        if not self.source_bronze or not isinstance(self.source_bronze, str):\n",
    "            raise PipelineValidationError(\n",
    "                \"Source bronze step name must be a non-empty string\"\n",
    "            )\n",
    "        if not callable(self.transform):\n",
    "            raise PipelineValidationError(\"Transform must be a callable function\")\n",
    "        if not isinstance(self.rules, dict):\n",
    "            raise PipelineValidationError(\"Rules must be a dictionary\")\n",
    "        if not self.table_name or not isinstance(self.table_name, str):\n",
    "            raise PipelineValidationError(\"Table name must be a non-empty string\")\n",
    "        if self.source_incremental_col is not None and not isinstance(\n",
    "            self.source_incremental_col, str\n",
    "        ):\n",
    "            raise PipelineValidationError(\n",
    "                \"source_incremental_col must be a string when provided\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GoldStep(BaseModel):\n",
    "    \"\"\"\n",
    "    Gold layer step configuration for business analytics and reporting.\n",
    "\n",
    "    Gold steps represent the third layer of the Medallion Architecture,\n",
    "    creating business-ready datasets for analytics, reporting, and dashboards.\n",
    "    They aggregate and transform Silver layer data into meaningful business insights.\n",
    "\n",
    "    **Validation Requirements:**\n",
    "        - `name`: Must be a non-empty string\n",
    "        - `transform`: Must be callable and cannot be None\n",
    "        - `rules`: Must be a non-empty dictionary with validation rules\n",
    "        - `table_name`: Must be a non-empty string\n",
    "        - `source_silvers`: Must be a non-empty list if provided\n",
    "\n",
    "    Attributes:\n",
    "        name: Unique identifier for this Gold step\n",
    "        transform: Transformation function with signature:\n",
    "                 (spark: SparkSession, silvers: Dict[str, DataFrame]) -> DataFrame\n",
    "                 - spark: Active SparkSession for operations\n",
    "                 - silvers: Dictionary of all Silver DataFrames by step name\n",
    "                 Must be callable and cannot be None.\n",
    "        rules: Dictionary mapping column names to validation rule lists.\n",
    "               Each rule should be a PySpark Column expression.\n",
    "        table_name: Target Delta table name where results will be stored\n",
    "        source_silvers: List of Silver step names to use as input sources.\n",
    "                       If None, uses all available Silver steps.\n",
    "                       Allows selective consumption of Silver data.\n",
    "        schema: Optional schema name for writing gold data\n",
    "\n",
    "    Raises:\n",
    "        ValidationError: If validation requirements are not met during construction\n",
    "\n",
    "    Example:\n",
    "        >>> def user_daily_metrics(spark, silvers):\n",
    "        ...     events_df = silvers[\"clean_events\"]\n",
    "        ...     return (events_df\n",
    "        ...         .groupBy(\"user_id\", \"event_date\")\n",
    "        ...         .agg(\n",
    "        ...             F.count(\"*\").alias(\"total_events\"),\n",
    "        ...             F.countDistinct(\"event_type\").alias(\"unique_event_types\"),\n",
    "        ...             F.max(\"timestamp\").alias(\"last_activity\"),\n",
    "        ...             F.sum(F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "        ...         )\n",
    "        ...         .withColumn(\"is_active_user\", F.col(\"total_events\") > 5)\n",
    "        ...     )\n",
    "        >>>\n",
    "        >>> # Valid Gold step\n",
    "        >>> gold_step = GoldStep(\n",
    "        ...     name=\"user_metrics\",\n",
    "        ...     transform=user_daily_metrics,\n",
    "        ...     rules={\n",
    "        ...         \"user_id\": [F.col(\"user_id\").isNotNull()],\n",
    "        ...         \"total_events\": [F.col(\"total_events\") > 0]\n",
    "        ...     },\n",
    "        ...     table_name=\"user_daily_metrics\",\n",
    "        ...     source_silvers=[\"clean_events\"]\n",
    "        ... )\n",
    "\n",
    "        >>> # Invalid Gold step (will raise ValidationError)\n",
    "        >>> try:\n",
    "        ...     GoldStep(name=\"\", transform=None, rules={}, table_name=\"\", source_silvers=[])\n",
    "        ... except ValidationError as e:\n",
    "        ...     print(f\"Validation failed: {e}\")\n",
    "        ...     # Output: \"Step name must be a non-empty string\"\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    transform: GoldTransformFunction\n",
    "    rules: ColumnRules\n",
    "    table_name: str\n",
    "    source_silvers: list[str] | None = None\n",
    "    schema: str | None = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Validate required fields after initialization.\"\"\"\n",
    "        if not self.name or not isinstance(self.name, str):\n",
    "            raise ValidationError(\"Step name must be a non-empty string\")\n",
    "        if self.transform is None or not callable(self.transform):\n",
    "            raise ValidationError(\"Transform function is required and must be callable\")\n",
    "        if not self.table_name or not isinstance(self.table_name, str):\n",
    "            raise ValidationError(\"Table name must be a non-empty string\")\n",
    "        if not isinstance(self.rules, dict) or not self.rules:\n",
    "            raise ValidationError(\"Rules must be a non-empty dictionary\")\n",
    "        if self.source_silvers is not None and (\n",
    "            not isinstance(self.source_silvers, list) or not self.source_silvers\n",
    "        ):\n",
    "            raise ValidationError(\"Source silvers must be a non-empty list\")\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate gold step configuration.\"\"\"\n",
    "        if not self.name or not isinstance(self.name, str):\n",
    "            raise PipelineValidationError(\"Step name must be a non-empty string\")\n",
    "        if not callable(self.transform):\n",
    "            raise PipelineValidationError(\"Transform must be a callable function\")\n",
    "        if not isinstance(self.rules, dict):\n",
    "            raise PipelineValidationError(\"Rules must be a dictionary\")\n",
    "        if not self.table_name or not isinstance(self.table_name, str):\n",
    "            raise PipelineValidationError(\"Table name must be a non-empty string\")\n",
    "        if self.source_silvers is not None and not isinstance(\n",
    "            self.source_silvers, list\n",
    "        ):\n",
    "            raise PipelineValidationError(\"Source silvers must be a list or None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.execution (pipeline_builder)\n",
    "#\n",
    "# Dependencies: models.base, models.exceptions, models.pipeline, pipeline_builder.models.enums\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from .base import BaseModel  # Removed: defined in notebook cells above\n",
    "# from .enums import ExecutionMode, PipelinePhase  # Removed: defined in notebook cells above\n",
    "# from .exceptions import PipelineConfigurationError  # Removed: defined in notebook cells above\n",
    "# from .pipeline import PipelineMetrics  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExecutionContext(BaseModel):\n",
    "    \"\"\"\n",
    "    Context for pipeline execution.\n",
    "\n",
    "    Attributes:\n",
    "        mode: Execution mode (initial/incremental)\n",
    "        start_time: When execution started\n",
    "        end_time: When execution ended\n",
    "        duration_secs: Total execution duration\n",
    "        run_id: Unique run identifier\n",
    "        execution_id: Unique identifier for this execution\n",
    "        pipeline_id: Identifier for the pipeline being executed\n",
    "        schema: Target schema for data storage\n",
    "        started_at: When execution started (alias for start_time)\n",
    "        ended_at: When execution ended (alias for end_time)\n",
    "        run_mode: Mode of execution (alias for mode)\n",
    "        config: Pipeline configuration as dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    mode: ExecutionMode\n",
    "    start_time: datetime\n",
    "    end_time: datetime | None = None\n",
    "    duration_secs: float | None = None\n",
    "    run_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n",
    "\n",
    "    # Additional fields for writer compatibility\n",
    "    execution_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n",
    "    pipeline_id: str = \"unknown\"\n",
    "    schema: str = \"default\"\n",
    "    started_at: datetime | None = None\n",
    "    ended_at: datetime | None = None\n",
    "    run_mode: str = \"initial\"\n",
    "    config: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize aliases and defaults.\"\"\"\n",
    "        if self.started_at is None:\n",
    "            self.started_at = self.start_time\n",
    "        if self.ended_at is None:\n",
    "            self.ended_at = self.end_time\n",
    "        if self.run_mode == \"initial\":\n",
    "            # Map mode to run_mode string\n",
    "            if hasattr(self.mode, \"value\"):\n",
    "                self.run_mode = self.mode.value\n",
    "            elif hasattr(self.mode, \"name\"):\n",
    "                self.run_mode = self.mode.name.lower()\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the execution context.\"\"\"\n",
    "        if not self.run_id:\n",
    "            raise ValueError(\"Run ID cannot be empty\")\n",
    "        if self.duration_secs is not None and self.duration_secs < 0:\n",
    "            raise ValueError(\"Duration cannot be negative\")\n",
    "\n",
    "    def finish(self) -> None:\n",
    "        \"\"\"Mark execution as finished and calculate duration.\"\"\"\n",
    "        self.end_time = datetime.now(timezone.utc)\n",
    "        if self.start_time:\n",
    "            self.duration_secs = (self.end_time - self.start_time).total_seconds()\n",
    "\n",
    "    @property\n",
    "    def is_finished(self) -> bool:\n",
    "        \"\"\"Check if execution is finished.\"\"\"\n",
    "        return self.end_time is not None\n",
    "\n",
    "    @property\n",
    "    def is_running(self) -> bool:\n",
    "        \"\"\"Check if execution is currently running.\"\"\"\n",
    "        return not self.is_finished\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StageStats(BaseModel):\n",
    "    \"\"\"\n",
    "    Statistics for a pipeline stage.\n",
    "\n",
    "    Attributes:\n",
    "        stage: Stage name (bronze/silver/gold)\n",
    "        step: Step name\n",
    "        total_rows: Total number of rows processed\n",
    "        valid_rows: Number of valid rows\n",
    "        invalid_rows: Number of invalid rows\n",
    "        validation_rate: Validation success rate (0-100)\n",
    "        duration_secs: Processing duration in seconds\n",
    "        start_time: When processing started\n",
    "        end_time: When processing ended\n",
    "    \"\"\"\n",
    "\n",
    "    stage: str\n",
    "    step: str\n",
    "    total_rows: int\n",
    "    valid_rows: int\n",
    "    invalid_rows: int\n",
    "    validation_rate: float\n",
    "    duration_secs: float\n",
    "    start_time: datetime | None = None\n",
    "    end_time: datetime | None = None\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate stage statistics.\"\"\"\n",
    "        if self.total_rows != self.valid_rows + self.invalid_rows:\n",
    "            raise PipelineConfigurationError(\n",
    "                f\"Total rows ({self.total_rows}) must equal valid ({self.valid_rows}) + invalid ({self.invalid_rows})\"\n",
    "            )\n",
    "        if not 0 <= self.validation_rate <= 100:\n",
    "            raise PipelineConfigurationError(\n",
    "                f\"Validation rate must be between 0 and 100, got {self.validation_rate}\"\n",
    "            )\n",
    "        if self.duration_secs < 0:\n",
    "            raise PipelineConfigurationError(\n",
    "                f\"Duration must be non-negative, got {self.duration_secs}\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"Check if the stage passed validation.\"\"\"\n",
    "        return self.validation_rate >= 95.0  # Default threshold\n",
    "\n",
    "    @property\n",
    "    def error_rate(self) -> float:\n",
    "        \"\"\"Calculate error rate.\"\"\"\n",
    "        if self.total_rows == 0:\n",
    "            return 0.0\n",
    "        return (self.invalid_rows / self.total_rows) * 100\n",
    "\n",
    "    @property\n",
    "    def throughput_rows_per_sec(self) -> float:\n",
    "        \"\"\"Calculate throughput in rows per second.\"\"\"\n",
    "        if self.duration_secs == 0:\n",
    "            return 0.0\n",
    "        return self.total_rows / self.duration_secs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepResult(BaseModel):\n",
    "    \"\"\"\n",
    "    Result of a pipeline step execution.\n",
    "\n",
    "    Attributes:\n",
    "        step_name: Name of the step\n",
    "        phase: Pipeline phase\n",
    "        success: Whether the step succeeded\n",
    "        start_time: When execution started\n",
    "        end_time: When execution ended\n",
    "        duration_secs: Execution duration in seconds\n",
    "        rows_processed: Number of rows processed\n",
    "        rows_written: Number of rows written\n",
    "        validation_rate: Validation success rate\n",
    "        error_message: Error message if failed\n",
    "        step_type: Type of step (bronze, silver, gold)\n",
    "        table_fqn: Fully qualified table name if step writes to table\n",
    "        write_mode: Write mode used (overwrite, append)\n",
    "        input_rows: Number of input rows processed\n",
    "    \"\"\"\n",
    "\n",
    "    step_name: str\n",
    "    phase: PipelinePhase\n",
    "    success: bool\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    duration_secs: float\n",
    "    rows_processed: int\n",
    "    rows_written: int\n",
    "    validation_rate: float\n",
    "    error_message: str | None = None\n",
    "    step_type: str | None = None\n",
    "    table_fqn: str | None = None\n",
    "    write_mode: str | None = None\n",
    "    input_rows: int | None = None\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the step result.\"\"\"\n",
    "        if not self.step_name:\n",
    "            raise ValueError(\"Step name cannot be empty\")\n",
    "        if self.duration_secs < 0:\n",
    "            raise ValueError(\"Duration cannot be negative\")\n",
    "        if self.rows_processed < 0:\n",
    "            raise ValueError(\"Rows processed cannot be negative\")\n",
    "        if self.rows_written < 0:\n",
    "            raise ValueError(\"Rows written cannot be negative\")\n",
    "        if not 0 <= self.validation_rate <= 100:\n",
    "            raise ValueError(\"Validation rate must be between 0 and 100\")\n",
    "\n",
    "    @property\n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"Check if the step result is valid.\"\"\"\n",
    "        return self.success and self.validation_rate >= 95.0\n",
    "\n",
    "    @property\n",
    "    def is_high_quality(self) -> bool:\n",
    "        \"\"\"Check if the step result is high quality.\"\"\"\n",
    "        return self.success and self.validation_rate >= 98.0\n",
    "\n",
    "    @property\n",
    "    def throughput_rows_per_sec(self) -> float:\n",
    "        \"\"\"Calculate throughput in rows per second.\"\"\"\n",
    "        if self.duration_secs == 0:\n",
    "            return 0.0\n",
    "        return self.rows_processed / self.duration_secs\n",
    "\n",
    "    @classmethod\n",
    "    def create_success(\n",
    "        cls,\n",
    "        step_name: str,\n",
    "        phase: PipelinePhase,\n",
    "        start_time: datetime,\n",
    "        end_time: datetime,\n",
    "        rows_processed: int,\n",
    "        rows_written: int,\n",
    "        validation_rate: float,\n",
    "        step_type: str | None = None,\n",
    "        table_fqn: str | None = None,\n",
    "        write_mode: str | None = None,\n",
    "        input_rows: int | None = None,\n",
    "    ) -> StepResult:\n",
    "        \"\"\"Create a successful step result.\"\"\"\n",
    "        duration_secs = (end_time - start_time).total_seconds()\n",
    "        return cls(\n",
    "            step_name=step_name,\n",
    "            phase=phase,\n",
    "            success=True,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            duration_secs=duration_secs,\n",
    "            rows_processed=rows_processed,\n",
    "            rows_written=rows_written,\n",
    "            validation_rate=validation_rate,\n",
    "            error_message=None,\n",
    "            step_type=step_type,\n",
    "            table_fqn=table_fqn,\n",
    "            write_mode=write_mode,\n",
    "            input_rows=input_rows,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def create_failure(\n",
    "        cls,\n",
    "        step_name: str,\n",
    "        phase: PipelinePhase,\n",
    "        start_time: datetime,\n",
    "        end_time: datetime,\n",
    "        error_message: str,\n",
    "        step_type: str | None = None,\n",
    "        table_fqn: str | None = None,\n",
    "        write_mode: str | None = None,\n",
    "        input_rows: int | None = None,\n",
    "    ) -> StepResult:\n",
    "        \"\"\"Create a failed step result.\"\"\"\n",
    "        duration_secs = (end_time - start_time).total_seconds()\n",
    "        return cls(\n",
    "            step_name=step_name,\n",
    "            phase=phase,\n",
    "            success=False,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            duration_secs=duration_secs,\n",
    "            rows_processed=0,\n",
    "            rows_written=0,\n",
    "            validation_rate=0.0,\n",
    "            error_message=error_message,\n",
    "            step_type=step_type,\n",
    "            table_fqn=table_fqn,\n",
    "            write_mode=write_mode,\n",
    "            input_rows=input_rows,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def error_rate(self) -> float:\n",
    "        \"\"\"Calculate error rate.\"\"\"\n",
    "        if self.rows_processed == 0:\n",
    "            return 0.0\n",
    "        return 100.0 - self.validation_rate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExecutionResult(BaseModel):\n",
    "    \"\"\"\n",
    "    Result of pipeline execution.\n",
    "\n",
    "    Attributes:\n",
    "        context: Execution context\n",
    "        step_results: Results for each step\n",
    "        metrics: Overall execution metrics\n",
    "        success: Whether the entire pipeline succeeded\n",
    "    \"\"\"\n",
    "\n",
    "    context: ExecutionContext\n",
    "    step_results: list[StepResult]\n",
    "    metrics: PipelineMetrics\n",
    "    success: bool\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate execution result.\"\"\"\n",
    "        if not isinstance(self.context, ExecutionContext):\n",
    "            raise PipelineConfigurationError(\n",
    "                \"Context must be an ExecutionContext instance\"\n",
    "            )\n",
    "        if not isinstance(self.step_results, list):\n",
    "            raise PipelineConfigurationError(\"Step results must be a list\")\n",
    "        if not isinstance(self.metrics, PipelineMetrics):\n",
    "            raise PipelineConfigurationError(\n",
    "                \"Metrics must be a PipelineMetrics instance\"\n",
    "            )\n",
    "        if not isinstance(self.success, bool):\n",
    "            raise PipelineConfigurationError(\"Success must be a boolean\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_context_and_results(\n",
    "        cls, context: ExecutionContext, step_results: list[StepResult]\n",
    "    ) -> ExecutionResult:\n",
    "        \"\"\"Create execution result from context and step results.\"\"\"\n",
    "        metrics = PipelineMetrics.from_step_results(step_results)\n",
    "        success = all(result.success for result in step_results)\n",
    "        return cls(\n",
    "            context=context, step_results=step_results, metrics=metrics, success=success\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.pipeline (pipeline_builder)\n",
    "#\n",
    "# Dependencies: models.base, pipeline_builder_base.errors\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "# from ..errors import PipelineValidationError  # Removed: defined in notebook cells above\n",
    "# from .base import BaseModel, ParallelConfig, ValidationThresholds  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Main pipeline configuration.\n",
    "\n",
    "    Attributes:\n",
    "        schema: Database schema name\n",
    "        thresholds: Validation thresholds for each phase\n",
    "        parallel: Parallel execution configuration\n",
    "        verbose: Whether to enable verbose logging\n",
    "    \"\"\"\n",
    "\n",
    "    schema: str\n",
    "    thresholds: ValidationThresholds\n",
    "    parallel: ParallelConfig | bool\n",
    "    verbose: bool = True\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Post-initialization to convert boolean parallel to ParallelConfig.\"\"\"\n",
    "        # Convert boolean parallel to ParallelConfig for backward compatibility\n",
    "        if isinstance(self.parallel, bool):\n",
    "            if self.parallel:\n",
    "                # If True, create default parallel config\n",
    "                object.__setattr__(self, \"parallel\", ParallelConfig.create_default())\n",
    "            else:\n",
    "                # If False, create sequential config\n",
    "                object.__setattr__(self, \"parallel\", ParallelConfig.create_sequential())\n",
    "\n",
    "    @property\n",
    "    def min_bronze_rate(self) -> float:\n",
    "        \"\"\"Get bronze validation threshold.\"\"\"\n",
    "        return self.thresholds.bronze\n",
    "\n",
    "    @property\n",
    "    def min_silver_rate(self) -> float:\n",
    "        \"\"\"Get silver validation threshold.\"\"\"\n",
    "        return self.thresholds.silver\n",
    "\n",
    "    @property\n",
    "    def min_gold_rate(self) -> float:\n",
    "        \"\"\"Get gold validation threshold.\"\"\"\n",
    "        return self.thresholds.gold\n",
    "\n",
    "    @property\n",
    "    def enable_parallel_silver(self) -> bool:\n",
    "        \"\"\"Get parallel silver execution setting.\"\"\"\n",
    "        # After __post_init__, parallel is always ParallelConfig\n",
    "        if isinstance(self.parallel, ParallelConfig):\n",
    "            return self.parallel.enabled\n",
    "        # Fallback for mock configs in tests\n",
    "        return bool(self.parallel)\n",
    "\n",
    "    @property\n",
    "    def max_parallel_workers(self) -> int:\n",
    "        \"\"\"Get max parallel workers setting.\"\"\"\n",
    "        # After __post_init__, parallel is always ParallelConfig\n",
    "        if isinstance(self.parallel, ParallelConfig):\n",
    "            return self.parallel.max_workers\n",
    "        # Fallback for mock configs in tests\n",
    "        return 4\n",
    "\n",
    "    @property\n",
    "    def enable_caching(self) -> bool:\n",
    "        \"\"\"Get caching setting.\"\"\"\n",
    "        return getattr(self.parallel, \"enable_caching\", True)\n",
    "\n",
    "    @property\n",
    "    def enable_monitoring(self) -> bool:\n",
    "        \"\"\"Get monitoring setting.\"\"\"\n",
    "        return getattr(self.parallel, \"enable_monitoring\", True)\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate pipeline configuration.\"\"\"\n",
    "        if not self.schema or not isinstance(self.schema, str):\n",
    "            raise PipelineValidationError(\"Schema name must be a non-empty string\")\n",
    "        self.thresholds.validate()\n",
    "        # After __post_init__, parallel is always ParallelConfig\n",
    "        if isinstance(self.parallel, ParallelConfig):\n",
    "            self.parallel.validate()\n",
    "\n",
    "    @classmethod\n",
    "    def create_default(cls, schema: str) -> PipelineConfig:\n",
    "        \"\"\"Create default pipeline configuration.\"\"\"\n",
    "        return cls(\n",
    "            schema=schema,\n",
    "            thresholds=ValidationThresholds.create_default(),\n",
    "            parallel=ParallelConfig.create_default(),\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def create_high_performance(cls, schema: str) -> PipelineConfig:\n",
    "        \"\"\"Create high-performance pipeline configuration.\"\"\"\n",
    "        return cls(\n",
    "            schema=schema,\n",
    "            thresholds=ValidationThresholds.create_strict(),\n",
    "            parallel=ParallelConfig.create_high_performance(),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def create_conservative(cls, schema: str) -> PipelineConfig:\n",
    "        \"\"\"Create conservative pipeline configuration.\"\"\"\n",
    "        return cls(\n",
    "            schema=schema,\n",
    "            thresholds=ValidationThresholds.create_strict(),\n",
    "            parallel=ParallelConfig.create_sequential(),\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineMetrics(BaseModel):\n",
    "    \"\"\"\n",
    "    Overall pipeline execution metrics.\n",
    "\n",
    "    Attributes:\n",
    "        total_steps: Total number of steps\n",
    "        successful_steps: Number of successful steps\n",
    "        failed_steps: Number of failed steps\n",
    "        skipped_steps: Number of skipped steps\n",
    "        total_duration: Total execution duration\n",
    "        bronze_duration: Bronze layer duration\n",
    "        silver_duration: Silver layer duration\n",
    "        gold_duration: Gold layer duration\n",
    "        total_rows_processed: Total rows processed\n",
    "        total_rows_written: Total rows written\n",
    "        avg_validation_rate: Average validation rate\n",
    "        parallel_efficiency: Parallel execution efficiency\n",
    "        cache_hit_rate: Cache hit rate\n",
    "        error_count: Number of errors\n",
    "        retry_count: Number of retries\n",
    "    \"\"\"\n",
    "\n",
    "    total_steps: int = 0\n",
    "    successful_steps: int = 0\n",
    "    failed_steps: int = 0\n",
    "    skipped_steps: int = 0\n",
    "    total_duration: float = 0.0\n",
    "    bronze_duration: float = 0.0\n",
    "    silver_duration: float = 0.0\n",
    "    gold_duration: float = 0.0\n",
    "    total_rows_processed: int = 0\n",
    "    total_rows_written: int = 0\n",
    "    avg_validation_rate: float = 0.0\n",
    "    parallel_efficiency: float = 0.0\n",
    "    cache_hit_rate: float = 0.0\n",
    "    error_count: int = 0\n",
    "    retry_count: int = 0\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the pipeline metrics.\"\"\"\n",
    "        if self.total_steps < 0:\n",
    "            raise ValueError(\"Total steps cannot be negative\")\n",
    "        if self.successful_steps < 0:\n",
    "            raise ValueError(\"Successful steps cannot be negative\")\n",
    "        if self.failed_steps < 0:\n",
    "            raise ValueError(\"Failed steps cannot be negative\")\n",
    "        if self.skipped_steps < 0:\n",
    "            raise ValueError(\"Skipped steps cannot be negative\")\n",
    "        if self.total_duration < 0:\n",
    "            raise ValueError(\"Total duration cannot be negative\")\n",
    "        if not 0 <= self.avg_validation_rate <= 100:\n",
    "            raise ValueError(\"Average validation rate must be between 0 and 100\")\n",
    "\n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        \"\"\"Calculate success rate.\"\"\"\n",
    "        return (\n",
    "            (self.successful_steps / self.total_steps * 100)\n",
    "            if self.total_steps > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def failure_rate(self) -> float:\n",
    "        \"\"\"Calculate failure rate.\"\"\"\n",
    "        return 100.0 - self.success_rate\n",
    "\n",
    "    @classmethod\n",
    "    def from_step_results(cls, step_results: list[Any]) -> PipelineMetrics:\n",
    "        \"\"\"Create metrics from step results.\"\"\"\n",
    "        total_steps = len(step_results)\n",
    "        successful_steps = sum(1 for result in step_results if result.success)\n",
    "        failed_steps = total_steps - successful_steps\n",
    "        total_duration_secs = sum(result.duration_secs for result in step_results)\n",
    "        total_rows_processed = sum(result.rows_processed for result in step_results)\n",
    "        total_rows_written = sum(result.rows_written for result in step_results)\n",
    "        avg_validation_rate = (\n",
    "            sum(result.validation_rate for result in step_results) / total_steps\n",
    "            if total_steps > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        return cls(\n",
    "            total_steps=total_steps,\n",
    "            successful_steps=successful_steps,\n",
    "            failed_steps=failed_steps,\n",
    "            total_duration=total_duration_secs,\n",
    "            total_rows_processed=total_rows_processed,\n",
    "            total_rows_written=total_rows_written,\n",
    "            avg_validation_rate=avg_validation_rate,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.base (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.models.enums, pipeline_builder.models.types, pipeline_builder_base.errors\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "# from ..errors import PipelineValidationError  # Removed: defined in notebook cells above\n",
    "# from .enums import PipelinePhase  # Removed: defined in notebook cells above\n",
    "# from .types import ModelValue  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"\n",
    "    Base class for all pipeline models with common functionality.\n",
    "\n",
    "    Provides standard validation, serialization, and representation methods\n",
    "    for all pipeline data models. All models in the pipeline system inherit\n",
    "    from this base class to ensure consistent behavior.\n",
    "\n",
    "    Features:\n",
    "    - Automatic validation support\n",
    "    - JSON serialization and deserialization\n",
    "    - Dictionary conversion for easy data exchange\n",
    "    - String representation for debugging\n",
    "    - Type-safe field access\n",
    "\n",
    "    Example:\n",
    "        >>> @dataclass\n",
    "        >>> class MyStep(BaseModel):\n",
    "        ...     name: str\n",
    "        ...     rules: Dict[str, List[ColumnRule]]\n",
    "        ...\n",
    "        ...     def validate(self) -> None:\n",
    "        ...         if not self.name:\n",
    "        ...             raise ValueError(\"Name cannot be empty\")\n",
    "        ...         if not self.rules:\n",
    "        ...             raise ValueError(\"Rules cannot be empty\")\n",
    "        >>>\n",
    "        >>> step = MyStep(name=\"test\", rules={\"id\": [F.col(\"id\").isNotNull()]})\n",
    "        >>> step.validate()\n",
    "        >>> print(step.to_json())\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the model. Override in subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def to_dict(self) -> Dict[str, ModelValue]:\n",
    "        \"\"\"Convert model to dictionary.\"\"\"\n",
    "        result: Dict[str, ModelValue] = {}\n",
    "        for field_info in self.__dataclass_fields__.values():\n",
    "            value = getattr(self, field_info.name)\n",
    "            if hasattr(value, \"to_dict\"):\n",
    "                result[field_info.name] = value.to_dict()\n",
    "            else:\n",
    "                result[field_info.name] = value\n",
    "        return result\n",
    "\n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Convert model to JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), default=str, indent=2)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"String representation of the model.\"\"\"\n",
    "        return f\"{self.__class__.__name__}({', '.join(f'{k}={v}' for k, v in self.to_dict().items())})\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationThresholds(BaseModel):\n",
    "    \"\"\"\n",
    "    Validation thresholds for different pipeline phases.\n",
    "\n",
    "    Attributes:\n",
    "        bronze: Bronze layer validation threshold (0-100)\n",
    "        silver: Silver layer validation threshold (0-100)\n",
    "        gold: Gold layer validation threshold (0-100)\n",
    "    \"\"\"\n",
    "\n",
    "    bronze: float\n",
    "    silver: float\n",
    "    gold: float\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate threshold values.\"\"\"\n",
    "        for phase, threshold in [\n",
    "            (\"bronze\", self.bronze),\n",
    "            (\"silver\", self.silver),\n",
    "            (\"gold\", self.gold),\n",
    "        ]:\n",
    "            if not 0 <= threshold <= 100:\n",
    "                raise PipelineValidationError(\n",
    "                    f\"{phase} threshold must be between 0 and 100, got {threshold}\"\n",
    "                )\n",
    "\n",
    "    def get_threshold(self, phase: PipelinePhase) -> float:\n",
    "        \"\"\"Get threshold for a specific phase.\"\"\"\n",
    "        phase_map = {\n",
    "            PipelinePhase.BRONZE: self.bronze,\n",
    "            PipelinePhase.SILVER: self.silver,\n",
    "            PipelinePhase.GOLD: self.gold,\n",
    "        }\n",
    "        return phase_map[phase]\n",
    "\n",
    "    @classmethod\n",
    "    def create_default(cls) -> ValidationThresholds:\n",
    "        \"\"\"Create default validation thresholds.\"\"\"\n",
    "        return cls(bronze=95.0, silver=98.0, gold=99.0)\n",
    "\n",
    "    @classmethod\n",
    "    def create_strict(cls) -> ValidationThresholds:\n",
    "        \"\"\"Create strict validation thresholds.\"\"\"\n",
    "        return cls(bronze=99.0, silver=99.5, gold=99.9)\n",
    "\n",
    "    @classmethod\n",
    "    def create_loose(cls) -> ValidationThresholds:\n",
    "        \"\"\"Create loose validation thresholds.\"\"\"\n",
    "        return cls(bronze=80.0, silver=85.0, gold=90.0)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParallelConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration for parallel execution.\n",
    "\n",
    "    Attributes:\n",
    "        enabled: Whether parallel execution is enabled\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        timeout_secs: Timeout for parallel operations in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    enabled: bool\n",
    "    max_workers: int\n",
    "    timeout_secs: int = 300\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate parallel configuration.\"\"\"\n",
    "        if self.max_workers < 1:\n",
    "            raise PipelineValidationError(\n",
    "                f\"max_workers must be at least 1, got {self.max_workers}\"\n",
    "            )\n",
    "        if self.max_workers > 32:\n",
    "            raise PipelineValidationError(\n",
    "                f\"max_workers should not exceed 32, got {self.max_workers}\"\n",
    "            )\n",
    "        if self.timeout_secs < 1:\n",
    "            raise PipelineValidationError(\n",
    "                f\"timeout_secs must be at least 1, got {self.timeout_secs}\"\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def create_default(cls) -> ParallelConfig:\n",
    "        \"\"\"Create default parallel configuration.\"\"\"\n",
    "        return cls(enabled=True, max_workers=4, timeout_secs=300)\n",
    "\n",
    "    @classmethod\n",
    "    def create_sequential(cls) -> ParallelConfig:\n",
    "        \"\"\"Create sequential execution configuration.\"\"\"\n",
    "        return cls(enabled=False, max_workers=1, timeout_secs=600)\n",
    "\n",
    "    @classmethod\n",
    "    def create_high_performance(cls) -> ParallelConfig:\n",
    "        \"\"\"Create high-performance parallel configuration.\"\"\"\n",
    "        return cls(enabled=True, max_workers=16, timeout_secs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.writer.analytics (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, pipeline_builder.writer.exceptions, pipeline_builder.writer.query_builder, pipeline_builder_base.logging\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, TypedDict, Union\n",
    "\n",
    "# from ..compat import DataFrame, F, SparkSession  # Removed: defined in notebook cells above\n",
    "# from ..logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .exceptions import WriterError  # Removed: defined in notebook cells above\n",
    "# from .query_builder import QueryBuilder  # Removed: defined in notebook cells above\n",
    "\n",
    "# Alias for convenience\n",
    "col = F.col\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TypedDict Definitions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class AnalysisPeriod(TypedDict):\n",
    "    \"\"\"Analysis period structure.\"\"\"\n",
    "\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    days_analyzed: int\n",
    "\n",
    "\n",
    "class DailyQualityTrend(TypedDict):\n",
    "    \"\"\"Daily quality trend data point.\"\"\"\n",
    "\n",
    "    date: str\n",
    "    total_executions: int\n",
    "    avg_validation_rate: float\n",
    "    min_validation_rate: float\n",
    "    max_validation_rate: float\n",
    "    stddev_validation_rate: float\n",
    "    high_quality_executions: int\n",
    "    low_quality_executions: int\n",
    "    quality_score: str\n",
    "\n",
    "\n",
    "class OverallQualityMetrics(TypedDict):\n",
    "    \"\"\"Overall quality metrics.\"\"\"\n",
    "\n",
    "    total_executions: int\n",
    "    avg_validation_rate: float\n",
    "    min_validation_rate: float\n",
    "    max_validation_rate: float\n",
    "    stddev_validation_rate: float\n",
    "\n",
    "\n",
    "class DegradationAlert(TypedDict):\n",
    "    \"\"\"Quality degradation alert.\"\"\"\n",
    "\n",
    "    type: str\n",
    "    message: str\n",
    "    severity: Literal[\"high\", \"medium\", \"low\"]\n",
    "\n",
    "\n",
    "class QualityTrends(TypedDict):\n",
    "    \"\"\"Quality trends analysis structure.\"\"\"\n",
    "\n",
    "    analysis_period: AnalysisPeriod\n",
    "    daily_trends: list[DailyQualityTrend]\n",
    "    overall_metrics: OverallQualityMetrics\n",
    "    degradation_alerts: list[DegradationAlert]\n",
    "    quality_grade: str\n",
    "\n",
    "\n",
    "class ValidationAnomaly(TypedDict):\n",
    "    \"\"\"Validation anomaly data point.\"\"\"\n",
    "\n",
    "    step: str\n",
    "    phase: str\n",
    "    validation_rate: float\n",
    "    valid_rows: int\n",
    "    invalid_rows: int\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "class StepAnomaly(TypedDict):\n",
    "    \"\"\"Step-level anomaly data point.\"\"\"\n",
    "\n",
    "    step: str\n",
    "    execution_count: int\n",
    "    avg_validation_rate: float\n",
    "    min_validation_rate: float\n",
    "    stddev_validation_rate: float\n",
    "    anomaly_score: float\n",
    "\n",
    "\n",
    "class TemporalAnomaly(TypedDict):\n",
    "    \"\"\"Temporal anomaly data point.\"\"\"\n",
    "\n",
    "    date: str\n",
    "    daily_avg_validation_rate: float\n",
    "    prev_avg_validation_rate: float\n",
    "    quality_change: float\n",
    "\n",
    "\n",
    "class AnomalySummary(TypedDict):\n",
    "    \"\"\"Anomaly summary statistics.\"\"\"\n",
    "\n",
    "    total_validation_anomalies: int\n",
    "    total_step_anomalies: int\n",
    "    total_temporal_anomalies: int\n",
    "    overall_anomaly_score: float\n",
    "\n",
    "\n",
    "class QualityAnomalies(TypedDict):\n",
    "    \"\"\"Quality anomalies analysis structure.\"\"\"\n",
    "\n",
    "    validation_anomalies: list[ValidationAnomaly]\n",
    "    step_anomalies: list[StepAnomaly]\n",
    "    temporal_anomalies: list[TemporalAnomaly]\n",
    "    anomaly_summary: AnomalySummary\n",
    "\n",
    "\n",
    "class VolumeTrendPoint(TypedDict):\n",
    "    \"\"\"Volume trend data point.\"\"\"\n",
    "\n",
    "    date: str\n",
    "    daily_executions: int\n",
    "    successful_executions: int\n",
    "    failed_executions: int\n",
    "    success_rate: float\n",
    "    avg_execution_time: float\n",
    "    total_rows_written: int\n",
    "\n",
    "\n",
    "class PhaseTrendPoint(TypedDict):\n",
    "    \"\"\"Phase trend data point.\"\"\"\n",
    "\n",
    "    phase: str\n",
    "    execution_count: int\n",
    "    avg_execution_time: float\n",
    "    avg_validation_rate: float\n",
    "    total_rows_written: int\n",
    "    success_rate: float\n",
    "\n",
    "\n",
    "class StepTrendPoint(TypedDict):\n",
    "    \"\"\"Step trend data point.\"\"\"\n",
    "\n",
    "    step: str\n",
    "    execution_count: int\n",
    "    avg_execution_time: float\n",
    "    avg_validation_rate: float\n",
    "    stddev_execution_time: float\n",
    "    min_execution_time: float\n",
    "    max_execution_time: float\n",
    "    performance_grade: str\n",
    "\n",
    "\n",
    "class TrendIndicators(TypedDict):\n",
    "    \"\"\"Trend indicators.\"\"\"\n",
    "\n",
    "    execution_volume_trend: str\n",
    "    success_rate_trend: str\n",
    "    recent_executions: int\n",
    "    historical_avg_executions: float\n",
    "    recent_success_rate: float\n",
    "    historical_success_rate: float\n",
    "\n",
    "\n",
    "class ExecutionTrends(TypedDict):\n",
    "    \"\"\"Execution trends analysis structure.\"\"\"\n",
    "\n",
    "    analysis_period: AnalysisPeriod\n",
    "    volume_trends: list[VolumeTrendPoint]\n",
    "    phase_trends: list[PhaseTrendPoint]\n",
    "    step_trends: list[StepTrendPoint]\n",
    "    trend_indicators: TrendIndicators\n",
    "\n",
    "\n",
    "class DataQualityAnalyzer:\n",
    "    \"\"\"Analyzes data quality metrics and trends.\"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession, logger: PipelineLogger | None = None):\n",
    "        \"\"\"Initialize the data quality analyzer.\"\"\"\n",
    "        self.spark = spark\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger(\"DataQualityAnalyzer\")\n",
    "        else:\n",
    "            self.logger = logger\n",
    "\n",
    "    def analyze_quality_trends(self, df: DataFrame, days: int = 30) -> QualityTrends:\n",
    "        \"\"\"\n",
    "        Analyze data quality trends over time.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing log data\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing quality trend analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Analyzing data quality trends for last {days} days\")\n",
    "\n",
    "            # Use query builder for quality trends\n",
    "            quality_trends_df = QueryBuilder.build_quality_trends_query(df, days)\n",
    "            quality_trends = quality_trends_df.collect()\n",
    "\n",
    "            # Use query builder for overall metrics\n",
    "            overall_metrics_df = QueryBuilder.build_overall_metrics_query(df, days)\n",
    "            overall_metrics = overall_metrics_df.collect()[0]\n",
    "\n",
    "            # Detect quality degradation\n",
    "            degradation_alerts = []\n",
    "            if len(quality_trends) > 1:\n",
    "                recent_avg = quality_trends[-1][\"avg_validation_rate\"]\n",
    "                historical_avg = sum(\n",
    "                    row[\"avg_validation_rate\"] for row in quality_trends[:-1]\n",
    "                ) / len(quality_trends[:-1])\n",
    "\n",
    "                if recent_avg < historical_avg - 5.0:  # 5% degradation threshold\n",
    "                    degradation_alerts.append(\n",
    "                        {\n",
    "                            \"type\": \"quality_degradation\",\n",
    "                            \"message\": f\"Recent validation rate ({recent_avg:.1f}%) is significantly lower than historical average ({historical_avg:.1f}%)\",\n",
    "                            \"severity\": (\n",
    "                                \"high\"\n",
    "                                if recent_avg < historical_avg - 10.0\n",
    "                                else \"medium\"\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # Get date range for analysis period\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=days)\n",
    "\n",
    "            analysis_result = {\n",
    "                \"analysis_period\": {\n",
    "                    \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"days_analyzed\": days,\n",
    "                },\n",
    "                \"daily_trends\": [\n",
    "                    {\n",
    "                        \"date\": row[\"date\"].strftime(\"%Y-%m-%d\"),\n",
    "                        \"total_executions\": row[\"total_executions\"],\n",
    "                        \"avg_validation_rate\": round(row[\"avg_validation_rate\"], 2),\n",
    "                        \"min_validation_rate\": round(row[\"min_validation_rate\"], 2),\n",
    "                        \"max_validation_rate\": round(row[\"max_validation_rate\"], 2),\n",
    "                        \"stddev_validation_rate\": round(\n",
    "                            row[\"stddev_validation_rate\"], 2\n",
    "                        ),\n",
    "                        \"high_quality_executions\": row[\"high_quality_executions\"],\n",
    "                        \"low_quality_executions\": row[\"low_quality_executions\"],\n",
    "                        \"quality_score\": self._calculate_quality_score(row.asDict()),\n",
    "                    }\n",
    "                    for row in quality_trends\n",
    "                ],\n",
    "                \"overall_metrics\": {\n",
    "                    \"total_executions\": overall_metrics[\"total_executions\"],\n",
    "                    \"avg_validation_rate\": round(\n",
    "                        overall_metrics[\"overall_avg_validation_rate\"], 2\n",
    "                    ),\n",
    "                    \"min_validation_rate\": round(\n",
    "                        overall_metrics[\"overall_min_validation_rate\"], 2\n",
    "                    ),\n",
    "                    \"max_validation_rate\": round(\n",
    "                        overall_metrics[\"overall_max_validation_rate\"], 2\n",
    "                    ),\n",
    "                    \"stddev_validation_rate\": round(\n",
    "                        overall_metrics[\"overall_stddev_validation_rate\"], 2\n",
    "                    ),\n",
    "                },\n",
    "                \"degradation_alerts\": degradation_alerts,\n",
    "                \"quality_grade\": self._calculate_quality_grade(\n",
    "                    overall_metrics[\"overall_avg_validation_rate\"]\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            self.logger.info(\"Data quality trends analysis completed\")\n",
    "            return cast(QualityTrends, analysis_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to analyze quality trends: {e}\")\n",
    "            raise WriterError(f\"Failed to analyze quality trends: {e}\") from e\n",
    "\n",
    "    def detect_quality_anomalies(self, df: DataFrame) -> QualityAnomalies:\n",
    "        \"\"\"\n",
    "        Detect data quality anomalies.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing log data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing anomaly detection results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Detecting data quality anomalies\")\n",
    "\n",
    "            # Calculate overall statistics for anomaly detection\n",
    "            overall_stats = QueryBuilder.calculate_statistics(df, \"validation_rate\")\n",
    "            threshold = overall_stats[\"avg\"] - (2 * overall_stats[\"stddev\"])\n",
    "\n",
    "            # Detect validation rate anomalies using query builder\n",
    "            validation_anomalies_df = (\n",
    "                QueryBuilder.build_anomaly_detection_query(\n",
    "                    df, \"validation_rate\", threshold\n",
    "                )\n",
    "                .select(\n",
    "                    \"step\",\n",
    "                    \"phase\",\n",
    "                    \"validation_rate\",\n",
    "                    \"valid_rows\",\n",
    "                    \"invalid_rows\",\n",
    "                    \"created_at\",\n",
    "                )\n",
    "                .orderBy(\"validation_rate\")\n",
    "            )\n",
    "\n",
    "            validation_anomalies = validation_anomalies_df.collect()\n",
    "\n",
    "            # Detect step-specific anomalies using query builder\n",
    "            step_anomalies_df = (\n",
    "                df.groupBy(\"step\")\n",
    "                .agg(**QueryBuilder.get_performance_aggregations())\n",
    "                .filter(\n",
    "                    (col(\"avg_validation_rate\") < 90.0)\n",
    "                    | (col(\"stddev_validation_rate\") > 10.0)\n",
    "                )\n",
    "                .orderBy(\"avg_validation_rate\")\n",
    "            )\n",
    "\n",
    "            step_anomalies = step_anomalies_df.collect()\n",
    "\n",
    "            # Detect temporal anomalies using query builder\n",
    "            temporal_anomalies_df = QueryBuilder.build_temporal_anomaly_query(df)\n",
    "            temporal_anomalies = temporal_anomalies_df.collect()\n",
    "\n",
    "            anomaly_result = {\n",
    "                \"validation_anomalies\": [\n",
    "                    {\n",
    "                        \"step\": row[\"step\"],\n",
    "                        \"phase\": row[\"phase\"],\n",
    "                        \"validation_rate\": round(row[\"validation_rate\"], 2),\n",
    "                        \"valid_rows\": row[\"valid_rows\"],\n",
    "                        \"invalid_rows\": row[\"invalid_rows\"],\n",
    "                        \"timestamp\": row[\"created_at\"].strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    }\n",
    "                    for row in validation_anomalies\n",
    "                ],\n",
    "                \"step_anomalies\": [\n",
    "                    {\n",
    "                        \"step\": row[\"step\"],\n",
    "                        \"execution_count\": row[\"execution_count\"],\n",
    "                        \"avg_validation_rate\": round(row[\"avg_validation_rate\"], 2),\n",
    "                        \"min_validation_rate\": round(row[\"min_validation_rate\"], 2),\n",
    "                        \"stddev_validation_rate\": round(\n",
    "                            row[\"stddev_validation_rate\"], 2\n",
    "                        ),\n",
    "                        \"anomaly_score\": self._calculate_anomaly_score(row.asDict()),\n",
    "                    }\n",
    "                    for row in step_anomalies\n",
    "                ],\n",
    "                \"temporal_anomalies\": [\n",
    "                    {\n",
    "                        \"date\": row[\"date\"].strftime(\"%Y-%m-%d\"),\n",
    "                        \"daily_avg_validation_rate\": round(\n",
    "                            row[\"daily_avg_validation_rate\"], 2\n",
    "                        ),\n",
    "                        \"prev_avg_validation_rate\": round(\n",
    "                            row[\"prev_avg_validation_rate\"], 2\n",
    "                        ),\n",
    "                        \"quality_change\": round(row[\"quality_change\"], 2),\n",
    "                    }\n",
    "                    for row in temporal_anomalies\n",
    "                ],\n",
    "                \"anomaly_summary\": {\n",
    "                    \"total_validation_anomalies\": len(validation_anomalies),\n",
    "                    \"total_step_anomalies\": len(step_anomalies),\n",
    "                    \"total_temporal_anomalies\": len(temporal_anomalies),\n",
    "                    \"overall_anomaly_score\": self._calculate_overall_anomaly_score(\n",
    "                        len(validation_anomalies),\n",
    "                        len(step_anomalies),\n",
    "                        len(temporal_anomalies),\n",
    "                    ),\n",
    "                },\n",
    "            }\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Quality anomaly detection completed: {len(validation_anomalies)} validation anomalies found\"\n",
    "            )\n",
    "            return cast(QualityAnomalies, anomaly_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to detect quality anomalies: {e}\")\n",
    "            raise WriterError(f\"Failed to detect quality anomalies: {e}\") from e\n",
    "\n",
    "    def _calculate_quality_score(self, row: Dict[str, Union[int, float]]) -> str:\n",
    "        \"\"\"Calculate quality score for a row.\"\"\"\n",
    "        avg_rate = row[\"avg_validation_rate\"]\n",
    "        if avg_rate >= 95.0:\n",
    "            return \"A\"\n",
    "        elif avg_rate >= 90.0:\n",
    "            return \"B\"\n",
    "        elif avg_rate >= 80.0:\n",
    "            return \"C\"\n",
    "        else:\n",
    "            return \"D\"\n",
    "\n",
    "    def _calculate_quality_grade(self, avg_validation_rate: float) -> str:\n",
    "        \"\"\"Calculate overall quality grade.\"\"\"\n",
    "        if avg_validation_rate >= 95.0:\n",
    "            return \"A\"\n",
    "        elif avg_validation_rate >= 90.0:\n",
    "            return \"B\"\n",
    "        elif avg_validation_rate >= 80.0:\n",
    "            return \"C\"\n",
    "        else:\n",
    "            return \"D\"\n",
    "\n",
    "    def _calculate_anomaly_score(self, row: Dict[str, Union[int, float]]) -> float:\n",
    "        \"\"\"Calculate anomaly score for a step.\"\"\"\n",
    "        avg_rate = row[\"avg_validation_rate\"]\n",
    "        stddev_rate = row[\"stddev_validation_rate\"]\n",
    "\n",
    "        # Lower average rate and higher standard deviation = higher anomaly score\n",
    "        anomaly_score = (100 - avg_rate) + (stddev_rate * 2)\n",
    "        return float(round(min(anomaly_score, 100.0), 2))\n",
    "\n",
    "    def _calculate_overall_anomaly_score(\n",
    "        self, validation_anomalies: int, step_anomalies: int, temporal_anomalies: int\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate overall anomaly score.\"\"\"\n",
    "        total_anomalies = validation_anomalies + step_anomalies + temporal_anomalies\n",
    "\n",
    "        if total_anomalies == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Weight different types of anomalies\n",
    "        weighted_score = (\n",
    "            (validation_anomalies * 1.0)\n",
    "            + (step_anomalies * 0.8)\n",
    "            + (temporal_anomalies * 1.2)\n",
    "        )\n",
    "        return round(min(weighted_score, 100.0), 2)\n",
    "\n",
    "\n",
    "class TrendAnalyzer:\n",
    "    \"\"\"Analyzes execution trends and patterns.\"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession, logger: PipelineLogger | None = None):\n",
    "        \"\"\"Initialize the trend analyzer.\"\"\"\n",
    "        self.spark = spark\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger(\"TrendAnalyzer\")\n",
    "        else:\n",
    "            self.logger = logger\n",
    "\n",
    "    def analyze_execution_trends(\n",
    "        self, df: DataFrame, days: int = 30\n",
    "    ) -> ExecutionTrends:\n",
    "        \"\"\"\n",
    "        Analyze execution trends over time.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing log data\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing trend analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Analyzing execution trends for last {days} days\")\n",
    "\n",
    "            # Use query builder for all trend analyses\n",
    "            volume_trends_df = QueryBuilder.build_daily_trends_query(df, days)\n",
    "            volume_trends = volume_trends_df.collect()\n",
    "\n",
    "            phase_trends_df = QueryBuilder.build_phase_trends_query(df, days)\n",
    "            phase_trends = phase_trends_df.collect()\n",
    "\n",
    "            step_trends_df = QueryBuilder.build_step_trends_query(df, days)\n",
    "            step_trends = step_trends_df.collect()\n",
    "\n",
    "            # Calculate trend indicators\n",
    "            trend_indicators = self._calculate_trend_indicators(\n",
    "                [row.asDict() for row in volume_trends]\n",
    "            )\n",
    "\n",
    "            # Get date range for analysis period\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=days)\n",
    "\n",
    "            analysis_result = {\n",
    "                \"analysis_period\": {\n",
    "                    \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"days_analyzed\": days,\n",
    "                },\n",
    "                \"volume_trends\": [\n",
    "                    {\n",
    "                        \"date\": row[\"date\"].strftime(\"%Y-%m-%d\"),\n",
    "                        \"daily_executions\": row[\"daily_executions\"],\n",
    "                        \"successful_executions\": row[\"successful_executions\"],\n",
    "                        \"failed_executions\": row[\"failed_executions\"],\n",
    "                        \"success_rate\": (\n",
    "                            round(\n",
    "                                (row[\"successful_executions\"] / row[\"daily_executions\"])\n",
    "                                * 100,\n",
    "                                2,\n",
    "                            )\n",
    "                            if row[\"daily_executions\"] > 0\n",
    "                            else 0\n",
    "                        ),\n",
    "                        \"avg_execution_time\": round(row[\"avg_execution_time\"], 2),\n",
    "                        \"total_rows_written\": row[\"total_rows_written\"],\n",
    "                    }\n",
    "                    for row in volume_trends\n",
    "                ],\n",
    "                \"phase_trends\": [\n",
    "                    {\n",
    "                        \"phase\": row[\"phase\"],\n",
    "                        \"execution_count\": row[\"execution_count\"],\n",
    "                        \"avg_execution_time\": round(row[\"avg_execution_time\"], 2),\n",
    "                        \"avg_validation_rate\": round(row[\"avg_validation_rate\"], 2),\n",
    "                        \"total_rows_written\": row[\"total_rows_written\"],\n",
    "                        \"success_rate\": round(\n",
    "                            (row[\"successful_executions\"] / row[\"execution_count\"])\n",
    "                            * 100,\n",
    "                            2,\n",
    "                        ),\n",
    "                    }\n",
    "                    for row in phase_trends\n",
    "                ],\n",
    "                \"step_trends\": [\n",
    "                    {\n",
    "                        \"step\": row[\"step\"],\n",
    "                        \"execution_count\": row[\"execution_count\"],\n",
    "                        \"avg_execution_time\": round(row[\"avg_execution_time\"], 2),\n",
    "                        \"avg_validation_rate\": round(row[\"avg_validation_rate\"], 2),\n",
    "                        \"stddev_execution_time\": round(row[\"stddev_execution_time\"], 2),\n",
    "                        \"min_execution_time\": round(row[\"min_execution_time\"], 2),\n",
    "                        \"max_execution_time\": round(row[\"max_execution_time\"], 2),\n",
    "                        \"performance_grade\": self._calculate_performance_grade(\n",
    "                            row.asDict()\n",
    "                        ),\n",
    "                    }\n",
    "                    for row in step_trends\n",
    "                ],\n",
    "                \"trend_indicators\": trend_indicators,\n",
    "            }\n",
    "\n",
    "            self.logger.info(\"Execution trends analysis completed\")\n",
    "            return cast(ExecutionTrends, analysis_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to analyze execution trends: {e}\")\n",
    "            raise WriterError(f\"Failed to analyze execution trends: {e}\") from e\n",
    "\n",
    "    def _calculate_trend_indicators(\n",
    "        self, volume_trends: list[Dict[str, Union[int, float]]]\n",
    "    ) -> TrendIndicators:\n",
    "        \"\"\"Calculate trend indicators from volume trends.\"\"\"\n",
    "        if len(volume_trends) < 2:\n",
    "            return {\n",
    "                \"execution_volume_trend\": \"insufficient_data\",\n",
    "                \"success_rate_trend\": \"insufficient_data\",\n",
    "                \"recent_executions\": 0,\n",
    "                \"historical_avg_executions\": 0.0,\n",
    "                \"recent_success_rate\": 0.0,\n",
    "                \"historical_success_rate\": 0.0,\n",
    "            }\n",
    "\n",
    "        # Calculate execution volume trend\n",
    "        recent_executions = volume_trends[-1][\"daily_executions\"]\n",
    "        historical_avg = sum(\n",
    "            row[\"daily_executions\"] for row in volume_trends[:-1]\n",
    "        ) / len(volume_trends[:-1])\n",
    "\n",
    "        execution_trend = (\n",
    "            \"increasing\"\n",
    "            if recent_executions > historical_avg * 1.1\n",
    "            else \"decreasing\"\n",
    "            if recent_executions < historical_avg * 0.9\n",
    "            else \"stable\"\n",
    "        )\n",
    "\n",
    "        # Calculate success rate trend\n",
    "        recent_success_rate = (\n",
    "            (\n",
    "                volume_trends[-1][\"successful_executions\"]\n",
    "                / volume_trends[-1][\"daily_executions\"]\n",
    "            )\n",
    "            * 100\n",
    "            if volume_trends[-1][\"daily_executions\"] > 0\n",
    "            else 0\n",
    "        )\n",
    "        historical_success_rate = sum(\n",
    "            (row[\"successful_executions\"] / row[\"daily_executions\"]) * 100\n",
    "            for row in volume_trends[:-1]\n",
    "            if row[\"daily_executions\"] > 0\n",
    "        ) / len([row for row in volume_trends[:-1] if row[\"daily_executions\"] > 0])\n",
    "\n",
    "        success_trend = (\n",
    "            \"improving\"\n",
    "            if recent_success_rate > historical_success_rate + 2\n",
    "            else (\n",
    "                \"declining\"\n",
    "                if recent_success_rate < historical_success_rate - 2\n",
    "                else \"stable\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"execution_volume_trend\": execution_trend,\n",
    "            \"success_rate_trend\": success_trend,\n",
    "            \"recent_executions\": int(recent_executions),\n",
    "            \"historical_avg_executions\": round(historical_avg, 2),\n",
    "            \"recent_success_rate\": round(recent_success_rate, 2),\n",
    "            \"historical_success_rate\": round(historical_success_rate, 2),\n",
    "        }\n",
    "\n",
    "    def _calculate_performance_grade(self, row: Dict[str, Union[int, float]]) -> str:\n",
    "        \"\"\"Calculate performance grade for a step.\"\"\"\n",
    "        avg_time = row[\"avg_execution_time\"]\n",
    "        stddev_time = row[\"stddev_execution_time\"]\n",
    "\n",
    "        # Consider both average time and consistency (low stddev)\n",
    "        if avg_time < 60 and stddev_time < 30:  # Fast and consistent\n",
    "            return \"A\"\n",
    "        elif avg_time < 120 and stddev_time < 60:  # Reasonable and somewhat consistent\n",
    "            return \"B\"\n",
    "        elif avg_time < 300:  # Acceptable\n",
    "            return \"C\"\n",
    "        else:  # Slow\n",
    "            return \"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.pipeline.monitor (pipeline_builder)\n",
    "#\n",
    "# Dependencies: models.pipeline, pipeline.models, pipeline_builder_base.logging\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .models import PipelineMetrics  # Removed: defined in notebook cells above\n",
    "# from .models import PipelineMode, PipelineReport, PipelineStatus  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class SimplePipelineMonitor:\n",
    "    \"\"\"\n",
    "    Simplified pipeline monitoring.\n",
    "\n",
    "    This monitor provides basic execution tracking and reporting\n",
    "    without complex metrics collection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger: PipelineLogger | None = None):\n",
    "        \"\"\"Initialize the simplified monitor.\"\"\"\n",
    "        self.logger = logger or PipelineLogger()\n",
    "        self._current_report: PipelineReport | None = None\n",
    "\n",
    "    def start_execution(\n",
    "        self,\n",
    "        pipeline_id: str,\n",
    "        mode: PipelineMode,\n",
    "        bronze_steps: Dict[str, Any],\n",
    "        silver_steps: Dict[str, Any],\n",
    "        gold_steps: Dict[str, Any],\n",
    "    ) -> PipelineReport:\n",
    "        \"\"\"Start monitoring a pipeline execution.\"\"\"\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        self._current_report = PipelineReport(\n",
    "            pipeline_id=pipeline_id,\n",
    "            execution_id=f\"exec_{pipeline_id}\",\n",
    "            status=PipelineStatus.RUNNING,\n",
    "            mode=mode,\n",
    "            start_time=start_time,\n",
    "            end_time=None,\n",
    "            duration_seconds=0.0,\n",
    "            metrics=PipelineMetrics(\n",
    "                total_steps=len(bronze_steps) + len(silver_steps) + len(gold_steps),\n",
    "                successful_steps=0,\n",
    "                failed_steps=0,\n",
    "                total_duration=0.0,\n",
    "            ),\n",
    "            errors=[],\n",
    "            warnings=[],\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"Started monitoring pipeline: {pipeline_id}\")\n",
    "        return self._current_report\n",
    "\n",
    "    def update_step_execution(\n",
    "        self,\n",
    "        step_name: str,\n",
    "        step_type: str,\n",
    "        success: bool,\n",
    "        duration: float,\n",
    "        error_message: str | None = None,\n",
    "        rows_processed: int = 0,\n",
    "        rows_written: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"Update step execution metrics.\"\"\"\n",
    "        if not self._current_report:\n",
    "            return\n",
    "\n",
    "        if success:\n",
    "            self._current_report.metrics.successful_steps += 1\n",
    "        else:\n",
    "            self._current_report.metrics.failed_steps += 1\n",
    "            if error_message:\n",
    "                self._current_report.errors.append(f\"{step_name}: {error_message}\")\n",
    "\n",
    "        self.logger.debug(\n",
    "            f\"Updated step {step_name}: success={success}, duration={duration:.2f}s\"\n",
    "        )\n",
    "\n",
    "    def finish_execution(self, success: bool) -> PipelineReport:\n",
    "        \"\"\"Finish monitoring and return final report.\"\"\"\n",
    "        if not self._current_report:\n",
    "            raise RuntimeError(\"No active execution to finish\")\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        total_duration = (end_time - self._current_report.start_time).total_seconds()\n",
    "\n",
    "        # Update final metrics\n",
    "        self._current_report.end_time = end_time\n",
    "        self._current_report.duration_seconds = total_duration\n",
    "        self._current_report.status = (\n",
    "            PipelineStatus.COMPLETED if success else PipelineStatus.FAILED\n",
    "        )\n",
    "        self._current_report.metrics.total_duration = total_duration\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Finished monitoring pipeline: {self._current_report.pipeline_id}\"\n",
    "        )\n",
    "        return self._current_report\n",
    "\n",
    "\n",
    "# Alias for backward compatibility\n",
    "PipelineMonitor = SimplePipelineMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.factory (pipeline_builder)\n",
    "#\n",
    "# Dependencies: models.base, models.exceptions, models.pipeline, pipeline_builder.models.enums, pipeline_builder.models.execution, pipeline_builder.models.steps\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# from .base import ParallelConfig, ValidationThresholds  # Removed: defined in notebook cells above\n",
    "# from .enums import ExecutionMode  # Removed: defined in notebook cells above\n",
    "# from .exceptions import PipelineConfigurationError, PipelineExecutionError  # Removed: defined in notebook cells above\n",
    "# from .execution import ExecutionContext  # Removed: defined in notebook cells above\n",
    "# from .pipeline import PipelineConfig  # Removed: defined in notebook cells above\n",
    "# from .steps import BronzeStep, GoldStep, SilverStep  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "def create_pipeline_config(\n",
    "    schema: str,\n",
    "    bronze_threshold: float = 95.0,\n",
    "    silver_threshold: float = 98.0,\n",
    "    gold_threshold: float = 99.0,\n",
    "    enable_parallel: bool = True,\n",
    "    max_workers: int = 4,\n",
    "    verbose: bool = True,\n",
    ") -> PipelineConfig:\n",
    "    \"\"\"Factory function to create pipeline configuration.\"\"\"\n",
    "    thresholds = ValidationThresholds(\n",
    "        bronze=bronze_threshold, silver=silver_threshold, gold=gold_threshold\n",
    "    )\n",
    "    parallel = ParallelConfig(enabled=enable_parallel, max_workers=max_workers)\n",
    "    return PipelineConfig(\n",
    "        schema=schema, thresholds=thresholds, parallel=parallel, verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "def create_execution_context(mode: ExecutionMode) -> ExecutionContext:\n",
    "    \"\"\"Factory function to create execution context.\"\"\"\n",
    "    return ExecutionContext(mode=mode, start_time=datetime.now(timezone.utc))\n",
    "\n",
    "\n",
    "def validate_pipeline_config(config: PipelineConfig) -> None:\n",
    "    \"\"\"Validate a pipeline configuration.\"\"\"\n",
    "    try:\n",
    "        config.validate()\n",
    "    except PipelineExecutionError as e:\n",
    "        raise PipelineConfigurationError(f\"Invalid pipeline configuration: {e}\") from e\n",
    "\n",
    "\n",
    "def validate_step_config(step: BronzeStep | SilverStep | GoldStep) -> None:\n",
    "    \"\"\"Validate a step configuration.\"\"\"\n",
    "    try:\n",
    "        step.validate()\n",
    "    except PipelineExecutionError as e:\n",
    "        raise PipelineConfigurationError(f\"Invalid step configuration: {e}\") from e\n",
    "\n",
    "\n",
    "def serialize_pipeline_config(config: PipelineConfig) -> str:\n",
    "    \"\"\"Serialize pipeline configuration to JSON.\"\"\"\n",
    "    return config.to_json()\n",
    "\n",
    "\n",
    "def deserialize_pipeline_config(json_str: str) -> PipelineConfig:\n",
    "    \"\"\"Deserialize pipeline configuration from JSON.\"\"\"\n",
    "    data = json.loads(json_str)\n",
    "    return PipelineConfig(\n",
    "        schema=data[\"schema\"],\n",
    "        thresholds=ValidationThresholds(\n",
    "            bronze=data[\"thresholds\"][\"bronze\"],\n",
    "            silver=data[\"thresholds\"][\"silver\"],\n",
    "            gold=data[\"thresholds\"][\"gold\"],\n",
    "        ),\n",
    "        parallel=ParallelConfig(\n",
    "            enabled=data[\"parallel\"][\"enabled\"],\n",
    "            max_workers=data[\"parallel\"][\"max_workers\"],\n",
    "            timeout_secs=data[\"parallel\"].get(\"timeout_secs\", 300),\n",
    "        ),\n",
    "        verbose=data.get(\"verbose\", True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.writer.models (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, pipeline_builder.models.execution\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, TypedDict\n",
    "\n",
    "# from ..compat import types  # Removed: defined in notebook cells above\n",
    "# from ..pipeline.models import PipelineReport  # Removed: defined in notebook cells above\n",
    "# from .models import ExecutionContext, ExecutionResult, StepResult  # Removed: defined in notebook cells above\n",
    "\n",
    "# Import specific types for convenience\n",
    "BooleanType = types.BooleanType\n",
    "FloatType = types.FloatType\n",
    "IntegerType = types.IntegerType\n",
    "StringType = types.StringType\n",
    "# Use the appropriate StructField based on the engine\n",
    "\n",
    "if os.environ.get(\"SPARKFORGE_ENGINE\") == \"mock\":\n",
    "    StructField = types.StructField\n",
    "else:\n",
    "    StructField = types.StructField\n",
    "StructType = types.StructType\n",
    "TimestampType = types.TimestampType\n",
    "\n",
    "# ============================================================================\n",
    "# Enums\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class WriteMode(Enum):\n",
    "    \"\"\"Write mode for log operations.\"\"\"\n",
    "\n",
    "    OVERWRITE = \"overwrite\"\n",
    "    APPEND = \"append\"\n",
    "    MERGE = \"merge\"\n",
    "    IGNORE = \"ignore\"\n",
    "\n",
    "\n",
    "class LogLevel(Enum):\n",
    "    \"\"\"Log level for writer operations.\"\"\"\n",
    "\n",
    "    DEBUG = \"DEBUG\"\n",
    "    INFO = \"INFO\"\n",
    "    WARNING = \"WARNING\"\n",
    "    ERROR = \"ERROR\"\n",
    "    CRITICAL = \"CRITICAL\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TypedDict Definitions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class LogRow(TypedDict):\n",
    "    \"\"\"\n",
    "    Enhanced log row with full type safety and framework integration.\n",
    "\n",
    "    This replaces the previous MinimalLogRow with proper integration\n",
    "    with framework models and enhanced type safety.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run-level information\n",
    "    run_id: str\n",
    "    run_mode: Literal[\"initial\", \"incremental\", \"full_refresh\", \"validation_only\"]\n",
    "    run_started_at: datetime | None\n",
    "    run_ended_at: datetime | None\n",
    "\n",
    "    # Execution context\n",
    "    execution_id: str\n",
    "    pipeline_id: str\n",
    "    schema: str\n",
    "\n",
    "    # Step-level information\n",
    "    phase: Literal[\"bronze\", \"silver\", \"gold\", \"pipeline\"]\n",
    "    step_name: str\n",
    "    step_type: str\n",
    "\n",
    "    # Timing information\n",
    "    start_time: datetime | None\n",
    "    end_time: datetime | None\n",
    "    duration_secs: float\n",
    "\n",
    "    # Table information\n",
    "    table_fqn: str | None\n",
    "    write_mode: Literal[\"overwrite\", \"append\"] | None\n",
    "\n",
    "    # Data metrics\n",
    "    input_rows: int | None\n",
    "    output_rows: int | None\n",
    "    rows_written: int | None\n",
    "    rows_processed: int\n",
    "    table_total_rows: int | None  # Total rows in table after this write\n",
    "\n",
    "    # Validation metrics\n",
    "    valid_rows: int\n",
    "    invalid_rows: int\n",
    "    validation_rate: float\n",
    "\n",
    "    # Execution status\n",
    "    success: bool\n",
    "    error_message: str | None\n",
    "\n",
    "    # Performance metrics\n",
    "    memory_usage_mb: float | None\n",
    "    cpu_usage_percent: float | None\n",
    "\n",
    "    # Metadata\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "\n",
    "class WriterMetrics(TypedDict):\n",
    "    \"\"\"Metrics for writer operations.\"\"\"\n",
    "\n",
    "    total_writes: int\n",
    "    successful_writes: int\n",
    "    failed_writes: int\n",
    "    total_duration_secs: float\n",
    "    avg_write_duration_secs: float\n",
    "    total_rows_written: int\n",
    "    memory_usage_peak_mb: float\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Models\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WriterConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the LogWriter.\n",
    "\n",
    "    Provides comprehensive configuration options for the writer module\n",
    "    including table settings, performance tuning, and feature flags.\n",
    "    \"\"\"\n",
    "\n",
    "    # Table configuration\n",
    "    table_schema: str\n",
    "    table_name: str\n",
    "    write_mode: WriteMode = WriteMode.APPEND\n",
    "\n",
    "    # Custom table naming patterns\n",
    "    table_name_pattern: str | None = None  # e.g., \"{schema}.{pipeline_id}_{timestamp}\"\n",
    "    table_suffix_pattern: str | None = None  # e.g., \"_{run_mode}_{date}\"\n",
    "\n",
    "    # Partitioning and optimization\n",
    "    partition_columns: list[str] | None = None\n",
    "    partition_count: int | None = None\n",
    "    compression: str = \"snappy\"\n",
    "\n",
    "    # Schema options\n",
    "    enable_schema_evolution: bool = True\n",
    "    schema_validation_mode: str = \"strict\"  # strict, lenient, ignore\n",
    "    auto_optimize_schema: bool = True\n",
    "\n",
    "    # Performance settings\n",
    "    batch_size: int = 1000\n",
    "    max_file_size_mb: int = 128\n",
    "    enable_optimization: bool = True\n",
    "    parallel_write_threads: int = 4\n",
    "    memory_fraction: float = 0.6\n",
    "\n",
    "    # Feature flags\n",
    "    enable_performance_monitoring: bool = True\n",
    "    enable_data_quality_checks: bool = True\n",
    "    enable_validation: bool = True\n",
    "    enable_metrics_collection: bool = True\n",
    "    enable_audit_trail: bool = True\n",
    "    enable_backup_before_write: bool = False\n",
    "\n",
    "    # Logging configuration\n",
    "    log_level: LogLevel = LogLevel.INFO\n",
    "    enable_detailed_logging: bool = False\n",
    "    log_performance_metrics: bool = True\n",
    "    log_data_quality_results: bool = True\n",
    "\n",
    "    # Error handling\n",
    "    max_retries: int = 3\n",
    "    retry_delay_secs: float = 1.0\n",
    "    fail_fast: bool = False\n",
    "    retry_exponential_backoff: bool = True\n",
    "\n",
    "    # Data quality thresholds\n",
    "    min_validation_rate: float = 95.0\n",
    "    max_invalid_rows_percent: float = 5.0\n",
    "    enable_anomaly_detection: bool = False\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the configuration.\"\"\"\n",
    "        if not self.table_schema:\n",
    "            raise ValueError(\"Table schema cannot be empty\")\n",
    "        if not self.table_name:\n",
    "            raise ValueError(\"Table name cannot be empty\")\n",
    "        if self.batch_size <= 0:\n",
    "            raise ValueError(\"Batch size must be positive\")\n",
    "        if self.max_file_size_mb <= 0:\n",
    "            raise ValueError(\"Max file size must be positive\")\n",
    "        if self.max_retries < 0:\n",
    "            raise ValueError(\"Max retries cannot be negative\")\n",
    "        if self.retry_delay_secs < 0:\n",
    "            raise ValueError(\"Retry delay cannot be negative\")\n",
    "        if self.parallel_write_threads <= 0:\n",
    "            raise ValueError(\"Parallel write threads must be positive\")\n",
    "        if not 0 < self.memory_fraction <= 1:\n",
    "            raise ValueError(\"Memory fraction must be between 0 and 1\")\n",
    "        if self.schema_validation_mode not in [\"strict\", \"lenient\", \"ignore\"]:\n",
    "            raise ValueError(\n",
    "                \"Schema validation mode must be 'strict', 'lenient', or 'ignore'\"\n",
    "            )\n",
    "        if not 0 <= self.min_validation_rate <= 100:\n",
    "            raise ValueError(\"Min validation rate must be between 0 and 100\")\n",
    "        if not 0 <= self.max_invalid_rows_percent <= 100:\n",
    "            raise ValueError(\"Max invalid rows percent must be between 0 and 100\")\n",
    "\n",
    "    def generate_table_name(\n",
    "        self,\n",
    "        pipeline_id: str | None = None,\n",
    "        run_mode: str | None = None,\n",
    "        timestamp: str | None = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate dynamic table name based on patterns.\n",
    "\n",
    "        Args:\n",
    "            pipeline_id: Pipeline identifier\n",
    "            run_mode: Run mode (initial, incremental, etc.)\n",
    "            timestamp: Timestamp for naming\n",
    "\n",
    "        Returns:\n",
    "            Generated table name\n",
    "        \"\"\"\n",
    "        table_name = self.table_name\n",
    "\n",
    "        # Apply suffix pattern if provided\n",
    "        if self.table_suffix_pattern:\n",
    "            # Use explicit None checking instead of 'or' to avoid masking None values\n",
    "            if run_mode is None:\n",
    "                raise ValueError(\n",
    "                    \"run_mode cannot be None when using table_suffix_pattern\"\n",
    "                )\n",
    "            if timestamp is None:\n",
    "                raise ValueError(\n",
    "                    \"timestamp cannot be None when using table_suffix_pattern\"\n",
    "                )\n",
    "\n",
    "            suffix_vars = {\n",
    "                \"run_mode\": run_mode,\n",
    "                \"date\": timestamp,\n",
    "                \"timestamp\": timestamp,\n",
    "            }\n",
    "            suffix = self.table_suffix_pattern.format(**suffix_vars)\n",
    "            table_name = f\"{table_name}{suffix}\"\n",
    "\n",
    "        # Apply full pattern if provided\n",
    "        if self.table_name_pattern:\n",
    "            # Use explicit None checking instead of 'or' to avoid masking None values\n",
    "            if pipeline_id is None:\n",
    "                raise ValueError(\n",
    "                    \"pipeline_id cannot be None when using table_name_pattern\"\n",
    "                )\n",
    "            if run_mode is None:\n",
    "                raise ValueError(\n",
    "                    \"run_mode cannot be None when using table_name_pattern\"\n",
    "                )\n",
    "            if timestamp is None:\n",
    "                raise ValueError(\n",
    "                    \"timestamp cannot be None when using table_name_pattern\"\n",
    "                )\n",
    "\n",
    "            pattern_vars = {\n",
    "                \"schema\": self.table_schema,\n",
    "                \"table_name\": table_name,\n",
    "                \"pipeline_id\": pipeline_id,\n",
    "                \"run_mode\": run_mode,\n",
    "                \"date\": timestamp,\n",
    "                \"timestamp\": timestamp,\n",
    "            }\n",
    "            return self.table_name_pattern.format(**pattern_vars)\n",
    "\n",
    "        return table_name\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Spark Schema Definitions\n",
    "# ============================================================================\n",
    "\n",
    "# from ..compat import types  # noqa: E402  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "def create_log_schema() -> types.StructType:\n",
    "    \"\"\"\n",
    "    Create the Spark schema for log tables.\n",
    "\n",
    "    Returns:\n",
    "        StructType: Spark schema for log tables with proper types\n",
    "    \"\"\"\n",
    "    return types.StructType(\n",
    "        [\n",
    "            # Run-level fields\n",
    "            StructField(\"run_id\", StringType(), False),\n",
    "            StructField(\"run_mode\", StringType(), False),\n",
    "            StructField(\"run_started_at\", TimestampType(), True),\n",
    "            StructField(\"run_ended_at\", TimestampType(), True),\n",
    "            # Execution context\n",
    "            StructField(\"execution_id\", StringType(), False),\n",
    "            StructField(\"pipeline_id\", StringType(), False),\n",
    "            StructField(\"schema\", StringType(), False),\n",
    "            # Step-level fields\n",
    "            StructField(\"phase\", StringType(), False),\n",
    "            StructField(\"step_name\", StringType(), False),\n",
    "            StructField(\"step_type\", StringType(), False),\n",
    "            # Timing fields\n",
    "            StructField(\"start_time\", TimestampType(), True),\n",
    "            StructField(\"end_time\", TimestampType(), True),\n",
    "            StructField(\"duration_secs\", FloatType(), False),\n",
    "            # Table fields\n",
    "            StructField(\"table_fqn\", StringType(), True),\n",
    "            StructField(\"write_mode\", StringType(), True),\n",
    "            # Data metrics\n",
    "            StructField(\"input_rows\", IntegerType(), True),\n",
    "            StructField(\"output_rows\", IntegerType(), True),\n",
    "            StructField(\"rows_written\", IntegerType(), True),\n",
    "            StructField(\"rows_processed\", IntegerType(), False),\n",
    "            StructField(\"table_total_rows\", IntegerType(), True),\n",
    "            # Validation metrics\n",
    "            StructField(\"valid_rows\", IntegerType(), False),\n",
    "            StructField(\"invalid_rows\", IntegerType(), False),\n",
    "            StructField(\"validation_rate\", FloatType(), False),\n",
    "            # Execution status\n",
    "            StructField(\"success\", BooleanType(), False),\n",
    "            StructField(\"error_message\", StringType(), True),\n",
    "            # Performance metrics\n",
    "            StructField(\"memory_usage_mb\", FloatType(), True),\n",
    "            StructField(\"cpu_usage_percent\", FloatType(), True),\n",
    "            # Metadata (stored as JSON string)\n",
    "            StructField(\"metadata\", StringType(), True),\n",
    "            # Timestamp fields for tracking\n",
    "            StructField(\"created_at\", StringType(), True),\n",
    "            StructField(\"updated_at\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Factory Functions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def create_log_row_from_step_result(\n",
    "    step_result: StepResult,\n",
    "    execution_context: ExecutionContext,\n",
    "    run_id: str,\n",
    "    run_mode: str,\n",
    "    metadata: Dict[str, Any] | None = None,\n",
    ") -> LogRow:\n",
    "    \"\"\"\n",
    "    Create a LogRow from a StepResult and ExecutionContext.\n",
    "\n",
    "    Args:\n",
    "        step_result: The step result to convert\n",
    "        execution_context: The execution context\n",
    "        run_id: Unique run identifier\n",
    "        run_mode: Mode of the run (initial, incremental, etc.)\n",
    "        metadata: Additional metadata\n",
    "\n",
    "    Returns:\n",
    "        LogRow: Log row with all fields populated\n",
    "    \"\"\"\n",
    "    return LogRow(\n",
    "        # Run-level information\n",
    "        run_id=run_id,\n",
    "        run_mode=run_mode,  # type: ignore[typeddict-item]\n",
    "        run_started_at=execution_context.started_at,\n",
    "        run_ended_at=execution_context.ended_at,\n",
    "        # Execution context\n",
    "        execution_id=execution_context.execution_id,\n",
    "        pipeline_id=execution_context.pipeline_id,\n",
    "        schema=execution_context.schema,\n",
    "        # Step-level information\n",
    "        phase=step_result.phase.value,\n",
    "        step_name=step_result.step_name,\n",
    "        step_type=(\n",
    "            step_result.step_type if step_result.step_type is not None else \"unknown\"\n",
    "        ),\n",
    "        # Timing information\n",
    "        start_time=step_result.start_time,\n",
    "        end_time=step_result.end_time,\n",
    "        duration_secs=step_result.duration_secs,\n",
    "        # Table information\n",
    "        table_fqn=step_result.table_fqn,\n",
    "        write_mode=step_result.write_mode,  # type: ignore[typeddict-item]\n",
    "        # Data metrics\n",
    "        input_rows=step_result.input_rows,\n",
    "        output_rows=step_result.rows_processed,\n",
    "        rows_written=step_result.rows_written,\n",
    "        rows_processed=step_result.rows_processed,\n",
    "        table_total_rows=None,\n",
    "        # Validation metrics\n",
    "        valid_rows=int(step_result.rows_processed * step_result.validation_rate / 100),\n",
    "        invalid_rows=int(\n",
    "            step_result.rows_processed * (100 - step_result.validation_rate) / 100\n",
    "        ),\n",
    "        validation_rate=step_result.validation_rate,\n",
    "        # Execution status\n",
    "        success=step_result.success,\n",
    "        error_message=step_result.error_message,\n",
    "        # Performance metrics\n",
    "        memory_usage_mb=None,  # TODO: Add memory metrics to StepResult\n",
    "        cpu_usage_percent=None,  # TODO: Add CPU metrics to StepResult\n",
    "        # Metadata\n",
    "        metadata=metadata or {},\n",
    "    )\n",
    "\n",
    "\n",
    "def create_log_rows_from_execution_result(\n",
    "    execution_result: ExecutionResult,\n",
    "    run_id: str,\n",
    "    run_mode: str,\n",
    "    metadata: Dict[str, Any] | None = None,\n",
    ") -> list[LogRow]:\n",
    "    \"\"\"\n",
    "    Create multiple LogRows from an ExecutionResult.\n",
    "\n",
    "    Args:\n",
    "        execution_result: The execution result to convert\n",
    "        run_id: Unique run identifier\n",
    "        run_mode: Mode of the run\n",
    "        metadata: Additional metadata\n",
    "\n",
    "    Returns:\n",
    "        List[LogRow]: List of log rows for each step\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # Process step results from the execution result\n",
    "    for step_result in execution_result.step_results:\n",
    "        row = create_log_row_from_step_result(\n",
    "            step_result=step_result,\n",
    "            execution_context=execution_result.context,\n",
    "            run_id=run_id,\n",
    "            run_mode=run_mode,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def create_log_rows_from_pipeline_report(\n",
    "    pipeline_report: PipelineReport,\n",
    "    run_id: str,\n",
    "    run_mode: str,\n",
    "    metadata: Dict[str, Any] | None = None,\n",
    ") -> list[LogRow]:\n",
    "    \"\"\"\n",
    "    Create multiple LogRows from a PipelineReport.\n",
    "\n",
    "    Args:\n",
    "        pipeline_report: The pipeline report to convert\n",
    "        run_id: Unique run identifier\n",
    "        run_mode: Mode of the run\n",
    "        metadata: Additional metadata\n",
    "\n",
    "    Returns:\n",
    "        List[LogRow]: List of log rows for each step\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    # Create a main log row for the pipeline execution\n",
    "    main_row: LogRow = {\n",
    "        \"run_id\": run_id,\n",
    "        \"run_mode\": run_mode,  # type: ignore[typeddict-item]\n",
    "        \"run_started_at\": pipeline_report.start_time,\n",
    "        \"run_ended_at\": pipeline_report.end_time,\n",
    "        \"execution_id\": pipeline_report.execution_id,\n",
    "        \"pipeline_id\": pipeline_report.pipeline_id,\n",
    "        \"schema\": \"default\",  # PipelineReport doesn't have schema\n",
    "        \"phase\": \"pipeline\",\n",
    "        \"step_name\": \"pipeline_execution\",\n",
    "        \"step_type\": \"pipeline\",\n",
    "        \"start_time\": pipeline_report.start_time,\n",
    "        \"end_time\": pipeline_report.end_time,\n",
    "        \"duration_secs\": pipeline_report.duration_seconds,\n",
    "        \"table_fqn\": None,\n",
    "        \"write_mode\": None,\n",
    "        \"input_rows\": 0,\n",
    "        \"output_rows\": 0,\n",
    "        \"rows_written\": 0,\n",
    "        \"rows_processed\": 0,\n",
    "        \"table_total_rows\": None,\n",
    "        \"valid_rows\": 0,\n",
    "        \"invalid_rows\": 0,\n",
    "        \"validation_rate\": 100.0,\n",
    "        \"success\": pipeline_report.success,\n",
    "        \"error_message\": pipeline_report.errors[0] if pipeline_report.errors else None,\n",
    "        \"memory_usage_mb\": None,\n",
    "        \"cpu_usage_percent\": None,\n",
    "        \"metadata\": metadata or {},\n",
    "    }\n",
    "    rows.append(main_row)\n",
    "\n",
    "    # Add step results from bronze, silver, and gold layers\n",
    "    all_results = {}\n",
    "    all_results.update(pipeline_report.bronze_results)\n",
    "    all_results.update(pipeline_report.silver_results)\n",
    "    all_results.update(pipeline_report.gold_results)\n",
    "\n",
    "    for step_name, _step_data in all_results.items():\n",
    "        # Create a simplified step row since we don't have full StepResult objects\n",
    "        step_row: LogRow = {\n",
    "            \"run_id\": run_id,\n",
    "            \"run_mode\": run_mode,  # type: ignore[typeddict-item]\n",
    "            \"run_started_at\": pipeline_report.start_time,\n",
    "            \"run_ended_at\": pipeline_report.end_time,\n",
    "            \"execution_id\": pipeline_report.execution_id,\n",
    "            \"pipeline_id\": pipeline_report.pipeline_id,\n",
    "            \"schema\": \"default\",\n",
    "            \"phase\": \"bronze\"\n",
    "            if step_name in pipeline_report.bronze_results\n",
    "            else \"silver\"\n",
    "            if step_name in pipeline_report.silver_results\n",
    "            else \"gold\",\n",
    "            \"step_name\": step_name,\n",
    "            \"step_type\": \"transform\",\n",
    "            \"start_time\": pipeline_report.start_time,\n",
    "            \"end_time\": pipeline_report.end_time,\n",
    "            \"duration_secs\": 0.0,  # Not available in PipelineReport\n",
    "            \"table_fqn\": None,\n",
    "            \"write_mode\": None,\n",
    "            \"input_rows\": 0,\n",
    "            \"output_rows\": 0,\n",
    "            \"rows_written\": 0,\n",
    "            \"rows_processed\": 0,\n",
    "            \"table_total_rows\": None,\n",
    "            \"valid_rows\": 0,\n",
    "            \"invalid_rows\": 0,\n",
    "            \"validation_rate\": 100.0,\n",
    "            \"success\": True,  # Assume success if in results\n",
    "            \"error_message\": None,\n",
    "            \"memory_usage_mb\": None,\n",
    "            \"cpu_usage_percent\": None,\n",
    "            \"metadata\": metadata or {},\n",
    "        }\n",
    "        rows.append(step_row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Validation Functions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def validate_log_row(row: LogRow) -> None:\n",
    "    \"\"\"\n",
    "    Validate a log row for data quality.\n",
    "\n",
    "    Args:\n",
    "        row: The log row to validate\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the log row is invalid\n",
    "    \"\"\"\n",
    "    # Validate required fields\n",
    "    if not row[\"run_id\"]:\n",
    "        raise ValueError(\"Run ID cannot be empty\")\n",
    "    if not row[\"execution_id\"]:\n",
    "        raise ValueError(\"Execution ID cannot be empty\")\n",
    "    if not row[\"pipeline_id\"]:\n",
    "        raise ValueError(\"Pipeline ID cannot be empty\")\n",
    "    if not row[\"step_name\"]:\n",
    "        raise ValueError(\"Step name cannot be empty\")\n",
    "\n",
    "    # Validate numeric fields\n",
    "    if row[\"duration_secs\"] < 0:\n",
    "        raise ValueError(\"Duration cannot be negative\")\n",
    "    if row[\"rows_processed\"] < 0:\n",
    "        raise ValueError(\"Rows processed cannot be negative\")\n",
    "    if row[\"valid_rows\"] < 0:\n",
    "        raise ValueError(\"Valid rows cannot be negative\")\n",
    "    if row[\"invalid_rows\"] < 0:\n",
    "        raise ValueError(\"Invalid rows cannot be negative\")\n",
    "    if not 0 <= row[\"validation_rate\"] <= 100:\n",
    "        raise ValueError(\"Validation rate must be between 0 and 100\")\n",
    "\n",
    "    # Validate logical consistency\n",
    "    total_rows = row[\"valid_rows\"] + row[\"invalid_rows\"]\n",
    "    if total_rows != row[\"rows_processed\"]:\n",
    "        raise ValueError(\"Valid + invalid rows must equal rows processed\")\n",
    "\n",
    "\n",
    "def validate_log_data(rows: list[LogRow]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate a list of log rows.\n",
    "\n",
    "    Args:\n",
    "        rows: List of log rows to validate\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for i, row in enumerate(rows):\n",
    "        try:\n",
    "            validate_log_row(row)\n",
    "        except ValueError as e:\n",
    "            errors.append(f\"Invalid log row at index {i}: {e}\")\n",
    "\n",
    "    return {\"is_valid\": len(errors) == 0, \"errors\": errors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.validation.data_validation (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, pipeline_builder.functions, pipeline_builder.models.execution, pipeline_builder.models.types, pipeline_builder_base.errors, pipeline_builder_base.logging\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from ..compat import Column, DataFrame  # Removed: defined in notebook cells above\n",
    "# from ..functions import FunctionsProtocol, get_default_functions  # Removed: defined in notebook cells above\n",
    "# from ..models import ColumnRules  # Removed: defined in notebook cells above\n",
    "# from .errors import ValidationError  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .models import StageStats  # Removed: defined in notebook cells above\n",
    "\n",
    "logger = PipelineLogger(\"DataValidation\")\n",
    "\n",
    "\n",
    "def _convert_rule_to_expression(\n",
    "    rule: str | list, column_name: str, functions: FunctionsProtocol | None = None\n",
    ") -> Column:\n",
    "    \"\"\"Convert a string rule to a PySpark Column expression.\"\"\"\n",
    "    if functions is None:\n",
    "        functions = get_default_functions()\n",
    "\n",
    "    # Handle list-based rules like [\"gt\", 0]\n",
    "    if isinstance(rule, list):\n",
    "        if len(rule) == 0:\n",
    "            return functions.lit(True)  # Empty rule means no validation\n",
    "        elif len(rule) == 1:\n",
    "            return _convert_rule_to_expression(rule[0], column_name, functions)\n",
    "        elif len(rule) == 2:\n",
    "            op, value = rule\n",
    "            if op == \"gt\":\n",
    "                return functions.col(column_name) > value\n",
    "            elif op == \"gte\":\n",
    "                return functions.col(column_name) >= value\n",
    "            elif op == \"lt\":\n",
    "                return functions.col(column_name) < value\n",
    "            elif op == \"lte\":\n",
    "                return functions.col(column_name) <= value\n",
    "            elif op == \"eq\":\n",
    "                return functions.col(column_name) == value\n",
    "            elif op == \"ne\":\n",
    "                return functions.col(column_name) != value\n",
    "            else:\n",
    "                # For unknown operators, assume it's a valid PySpark expression\n",
    "                return functions.expr(f\"{column_name} {op} {value}\")\n",
    "        elif len(rule) == 3:\n",
    "            op, min_val, max_val = rule\n",
    "            if op == \"between\":\n",
    "                return functions.col(column_name).between(min_val, max_val)\n",
    "            else:\n",
    "                # For unknown operators, assume it's a valid PySpark expression\n",
    "                return functions.expr(f\"{column_name} {op} {min_val} {max_val}\")\n",
    "        else:\n",
    "            # For complex rules, assume it's a valid PySpark expression\n",
    "            return functions.expr(str(rule))\n",
    "\n",
    "    # Handle string-based rules\n",
    "    if rule == \"not_null\":\n",
    "        return functions.col(column_name).isNotNull()\n",
    "    elif rule == \"positive\":\n",
    "        return functions.col(column_name) > 0\n",
    "    elif rule == \"non_negative\":\n",
    "        return functions.col(column_name) >= 0\n",
    "    elif rule == \"non_zero\":\n",
    "        return functions.col(column_name) != 0\n",
    "    else:\n",
    "        # For unknown rules, assume it's a valid PySpark expression\n",
    "        return functions.expr(rule)\n",
    "\n",
    "\n",
    "def _convert_rules_to_expressions(\n",
    "    rules: ColumnRules,\n",
    "    functions: FunctionsProtocol | None = None,\n",
    ") -> Dict[str, list[str | Column]]:\n",
    "    \"\"\"Convert string rules to PySpark Column expressions.\"\"\"\n",
    "    if functions is None:\n",
    "        functions = get_default_functions()\n",
    "\n",
    "    converted_rules: Dict[str, list[str | Column]] = {}\n",
    "    for column_name, rule_list in rules.items():\n",
    "        converted_rule_list: list[str | Column] = []\n",
    "        for rule in rule_list:\n",
    "            if isinstance(rule, (str, list)):\n",
    "                converted_rule_list.append(\n",
    "                    _convert_rule_to_expression(rule, column_name, functions)\n",
    "                )\n",
    "            else:\n",
    "                converted_rule_list.append(rule)\n",
    "        converted_rules[column_name] = converted_rule_list\n",
    "    return converted_rules\n",
    "\n",
    "\n",
    "def and_all_rules(\n",
    "    rules: ColumnRules, functions: FunctionsProtocol | None = None\n",
    ") -> Column | bool:\n",
    "    \"\"\"Combine all validation rules with AND logic.\"\"\"\n",
    "    if not rules:\n",
    "        return True\n",
    "\n",
    "    if functions is None:\n",
    "        functions = get_default_functions()\n",
    "\n",
    "    converted_rules = _convert_rules_to_expressions(rules, functions)\n",
    "    expressions = []\n",
    "    for _, exprs in converted_rules.items():\n",
    "        expressions.extend(exprs)\n",
    "\n",
    "    if not expressions:\n",
    "        return True\n",
    "\n",
    "    # Filter out non-Column expressions and convert strings to Columns\n",
    "    column_expressions = []\n",
    "    for expr in expressions:\n",
    "        # Check if it's a Column-like object (has column operations)\n",
    "        if (\n",
    "            hasattr(expr, \"__and__\")\n",
    "            and hasattr(expr, \"__invert__\")\n",
    "            and not isinstance(expr, str)\n",
    "        ):\n",
    "            column_expressions.append(expr)\n",
    "        elif isinstance(expr, Column):\n",
    "            column_expressions.append(expr)\n",
    "        elif isinstance(expr, str):\n",
    "            column_expressions.append(functions.expr(expr))\n",
    "\n",
    "    if not column_expressions:\n",
    "        return True\n",
    "\n",
    "    pred = column_expressions[0]\n",
    "    for e in column_expressions[1:]:\n",
    "        pred = pred & e\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def apply_column_rules(\n",
    "    df: DataFrame,\n",
    "    rules: ColumnRules,\n",
    "    stage: str,\n",
    "    step: str,\n",
    "    filter_columns_by_rules: bool = True,\n",
    "    functions: FunctionsProtocol | None = None,\n",
    ") -> tuple[DataFrame, DataFrame, StageStats]:\n",
    "    \"\"\"\n",
    "    Apply validation rules to a DataFrame and return valid/invalid DataFrames with statistics.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to validate\n",
    "        rules: Dictionary mapping column names to validation rules\n",
    "        stage: Pipeline stage name\n",
    "        step: Step name within the stage\n",
    "        filter_columns_by_rules: If True, output DataFrames only contain columns with rules\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (valid_df, invalid_df, stats)\n",
    "    \"\"\"\n",
    "    if rules is None:\n",
    "        raise ValidationError(\"Validation rules cannot be None\")\n",
    "\n",
    "    # Handle empty rules - return all rows as valid\n",
    "    if not rules:\n",
    "        total_rows = df.count()\n",
    "        duration = time.time() - time.time()  # 0 duration\n",
    "        stats = StageStats(\n",
    "            stage=stage,\n",
    "            step=step,\n",
    "            total_rows=total_rows,\n",
    "            valid_rows=total_rows,\n",
    "            invalid_rows=0,\n",
    "            validation_rate=100.0,\n",
    "            duration_secs=duration,\n",
    "        )\n",
    "        return (\n",
    "            df,\n",
    "            df.limit(0),\n",
    "            stats,\n",
    "        )  # Return original df as valid, empty df as invalid\n",
    "\n",
    "    # Validate that all columns referenced in rules exist in the DataFrame\n",
    "    df_columns = set(df.columns)\n",
    "    rule_columns = set(rules.keys())\n",
    "    missing_columns = rule_columns - df_columns\n",
    "\n",
    "    if missing_columns:\n",
    "        available_columns = sorted(df_columns)\n",
    "        missing_columns_list = sorted(missing_columns)\n",
    "        raise ValidationError(\n",
    "            f\"Columns referenced in validation rules do not exist in DataFrame. \"\n",
    "            f\"Missing columns: {missing_columns_list}. \"\n",
    "            f\"Available columns: {available_columns}. \"\n",
    "            f\"Stage: {stage}, Step: {step}\"\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create validation predicate\n",
    "    validation_predicate = and_all_rules(rules, functions)\n",
    "\n",
    "    # Apply validation\n",
    "    if validation_predicate is True:\n",
    "        # No validation rules, return all data as valid\n",
    "        valid_df = df\n",
    "        invalid_df = df.limit(0)  # Empty DataFrame with same schema\n",
    "        total_rows = df.count()\n",
    "        valid_rows = total_rows\n",
    "        invalid_rows = 0\n",
    "    elif isinstance(validation_predicate, Column) or (\n",
    "        hasattr(validation_predicate, \"__and__\")\n",
    "        and hasattr(validation_predicate, \"__invert__\")\n",
    "        and not isinstance(validation_predicate, bool)\n",
    "    ):\n",
    "        # Handle PySpark Column expressions\n",
    "        valid_df = df.filter(validation_predicate)\n",
    "        invalid_df = df.filter(~validation_predicate)\n",
    "        total_rows = df.count()\n",
    "        valid_rows = valid_df.count()\n",
    "        invalid_rows = invalid_df.count()\n",
    "    else:\n",
    "        # Handle boolean False case (shouldn't happen with current logic)\n",
    "        valid_df = df.limit(0)\n",
    "        invalid_df = df\n",
    "        total_rows = df.count()\n",
    "        valid_rows = 0\n",
    "        invalid_rows = total_rows\n",
    "\n",
    "    # Apply column filtering if requested\n",
    "    if filter_columns_by_rules:\n",
    "        # Only keep columns that have validation rules\n",
    "        rule_columns_list: list[str] = list(rules.keys())\n",
    "        valid_df = valid_df.select(*rule_columns_list)\n",
    "        # For invalid_df, also include the _failed_rules column if it exists\n",
    "        invalid_columns: list[str] = rule_columns_list.copy()\n",
    "        if \"_failed_rules\" in invalid_df.columns:\n",
    "            invalid_columns.append(\"_failed_rules\")\n",
    "        invalid_df = invalid_df.select(*invalid_columns)\n",
    "\n",
    "    # Calculate validation rate\n",
    "    validation_rate = (valid_rows / total_rows * 100) if total_rows > 0 else 100.0\n",
    "\n",
    "    # Create statistics\n",
    "    duration = time.time() - start_time\n",
    "    stats = StageStats(\n",
    "        stage=stage,\n",
    "        step=step,\n",
    "        total_rows=total_rows,\n",
    "        valid_rows=valid_rows,\n",
    "        invalid_rows=invalid_rows,\n",
    "        validation_rate=validation_rate,\n",
    "        duration_secs=duration,\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Validation completed for {stage}.{step}: {validation_rate:.1f}% valid\"\n",
    "    )\n",
    "\n",
    "    return valid_df, invalid_df, stats\n",
    "\n",
    "\n",
    "def validate_dataframe_schema(df: DataFrame, expected_columns: list[str]) -> bool:\n",
    "    \"\"\"Validate that DataFrame has expected columns.\"\"\"\n",
    "    actual_columns = set(df.columns)\n",
    "    expected_set = set(expected_columns)\n",
    "    missing_columns = expected_set - actual_columns\n",
    "    return len(missing_columns) == 0\n",
    "\n",
    "\n",
    "def assess_data_quality(\n",
    "    df: DataFrame,\n",
    "    rules: ColumnRules | None = None,\n",
    "    functions: FunctionsProtocol | None = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Assess data quality of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to assess\n",
    "        rules: Optional validation rules\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with quality metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_rows = df.count()\n",
    "\n",
    "        if total_rows == 0:\n",
    "            return {\n",
    "                \"total_rows\": 0,\n",
    "                \"valid_rows\": 0,\n",
    "                \"invalid_rows\": 0,\n",
    "                \"quality_rate\": 100.0,\n",
    "                \"is_empty\": True,\n",
    "            }\n",
    "\n",
    "        if rules:\n",
    "            valid_df, invalid_df, stats = apply_column_rules(\n",
    "                df, rules, \"test\", \"test\", functions=functions\n",
    "            )\n",
    "            return {\n",
    "                \"total_rows\": stats.total_rows,\n",
    "                \"valid_rows\": stats.valid_rows,\n",
    "                \"invalid_rows\": stats.invalid_rows,\n",
    "                \"quality_rate\": stats.validation_rate,\n",
    "                \"is_empty\": False,\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"total_rows\": total_rows,\n",
    "                \"valid_rows\": total_rows,\n",
    "                \"invalid_rows\": 0,\n",
    "                \"quality_rate\": 100.0,\n",
    "                \"is_empty\": False,\n",
    "            }\n",
    "    except ValidationError as e:\n",
    "        # Re-raise validation errors as they are specific and actionable\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # Log the unexpected error and re-raise with context\n",
    "        import logging\n",
    "\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.error(f\"Unexpected error in assess_data_quality: {e}\")\n",
    "        raise ValidationError(\n",
    "            f\"Data quality assessment failed: {e}\",\n",
    "            context={\"function\": \"assess_data_quality\", \"original_error\": str(e)},\n",
    "        ) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.validation.pipeline_validation (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.models.execution, pipeline_builder.models.pipeline, pipeline_builder.models.steps, pipeline_builder_base.logging\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from ..models import BronzeStep, GoldStep, SilverStep  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .models import ExecutionContext, PipelineConfig  # Removed: defined in notebook cells above\n",
    "\n",
    "# Type alias for step names\n",
    "StepName = str\n",
    "\n",
    "\n",
    "class StepValidator:\n",
    "    \"\"\"Protocol for custom step validators.\"\"\"\n",
    "\n",
    "    def validate(self, step: Any, context: ExecutionContext) -> list[str]:\n",
    "        \"\"\"Validate a step and return any validation errors.\"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of validation.\"\"\"\n",
    "\n",
    "    is_valid: bool\n",
    "    errors: list[str]\n",
    "    warnings: list[str]\n",
    "    recommendations: list[str]\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        \"\"\"Return whether validation passed.\"\"\"\n",
    "        return self.is_valid\n",
    "\n",
    "\n",
    "class UnifiedValidator:\n",
    "    \"\"\"\n",
    "    Unified validation system for both data and pipeline validation.\n",
    "\n",
    "    This class provides a single interface for all validation needs,\n",
    "    combining data validation and pipeline validation functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger: PipelineLogger | None = None):\n",
    "        \"\"\"Initialize the unified validator.\"\"\"\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger()\n",
    "        else:\n",
    "            self.logger = logger\n",
    "        self.custom_validators: list[StepValidator] = []\n",
    "\n",
    "    def add_validator(self, validator: StepValidator) -> None:\n",
    "        \"\"\"Add a custom step validator.\"\"\"\n",
    "        self.custom_validators.append(validator)\n",
    "        self.logger.info(f\"Added custom validator: {validator.__class__.__name__}\")\n",
    "\n",
    "    def validate_pipeline(\n",
    "        self,\n",
    "        config: PipelineConfig,\n",
    "        bronze_steps: Dict[StepName, BronzeStep],\n",
    "        silver_steps: Dict[StepName, SilverStep],\n",
    "        gold_steps: Dict[StepName, GoldStep],\n",
    "    ) -> ValidationResult:\n",
    "        \"\"\"Validate the entire pipeline configuration.\"\"\"\n",
    "        errors: list[str] = []\n",
    "        warnings: list[str] = []\n",
    "        recommendations: list[str] = []\n",
    "\n",
    "        # Validate configuration\n",
    "        config_errors = self._validate_config(config)\n",
    "        errors.extend(config_errors)\n",
    "\n",
    "        # Validate steps\n",
    "        bronze_errors, bronze_warnings = self._validate_bronze_steps(bronze_steps)\n",
    "        errors.extend(bronze_errors)\n",
    "        warnings.extend(bronze_warnings)\n",
    "\n",
    "        silver_errors, silver_warnings = self._validate_silver_steps(\n",
    "            silver_steps, bronze_steps\n",
    "        )\n",
    "        errors.extend(silver_errors)\n",
    "        warnings.extend(silver_warnings)\n",
    "\n",
    "        gold_errors, gold_warnings = self._validate_gold_steps(gold_steps, silver_steps)\n",
    "        errors.extend(gold_errors)\n",
    "        warnings.extend(gold_warnings)\n",
    "\n",
    "        # Validate dependencies\n",
    "        dep_errors, dep_warnings = self._validate_dependencies(\n",
    "            bronze_steps, silver_steps, gold_steps\n",
    "        )\n",
    "        errors.extend(dep_errors)\n",
    "        warnings.extend(dep_warnings)\n",
    "\n",
    "        is_valid = len(errors) == 0\n",
    "\n",
    "        # Logging is handled by the builder to avoid duplicate messages\n",
    "        return ValidationResult(\n",
    "            is_valid=is_valid,\n",
    "            errors=errors,\n",
    "            warnings=warnings,\n",
    "            recommendations=recommendations,\n",
    "        )\n",
    "\n",
    "    def validate_step(\n",
    "        self, step: Any, step_type: str, context: ExecutionContext\n",
    "    ) -> ValidationResult:\n",
    "        \"\"\"Validate a single step.\"\"\"\n",
    "        errors: list[str] = []\n",
    "        warnings: list[str] = []\n",
    "\n",
    "        # Run custom validators\n",
    "        for validator in self.custom_validators:\n",
    "            try:\n",
    "                validator_errors = validator.validate(step, context)\n",
    "                errors.extend(validator_errors)\n",
    "            except Exception as e:\n",
    "                errors.append(\n",
    "                    f\"Custom validator {validator.__class__.__name__} failed: {e}\"\n",
    "                )\n",
    "\n",
    "        return ValidationResult(\n",
    "            is_valid=len(errors) == 0,\n",
    "            errors=errors,\n",
    "            warnings=warnings,\n",
    "            recommendations=[],\n",
    "        )\n",
    "\n",
    "    def _validate_config(self, config: PipelineConfig) -> list[str]:\n",
    "        \"\"\"Validate pipeline configuration.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        if not config.schema:\n",
    "            errors.append(\"Pipeline schema is required\")\n",
    "\n",
    "        # Table prefix is optional in simplified config\n",
    "        # if not config.table_prefix:\n",
    "        #     errors.append(\"Table prefix is required\")\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def _validate_bronze_steps(\n",
    "        self, bronze_steps: Dict[StepName, BronzeStep]\n",
    "    ) -> tuple[list[str], list[str]]:\n",
    "        \"\"\"Validate bronze steps.\"\"\"\n",
    "        errors = []\n",
    "        warnings: list[str] = []\n",
    "\n",
    "        for step_name, step in bronze_steps.items():\n",
    "            # Simplified validation - just check that step has required basic attributes\n",
    "            if not step.name:\n",
    "                errors.append(f\"Bronze step {step_name} missing name\")\n",
    "\n",
    "            if not step.rules:\n",
    "                errors.append(f\"Bronze step {step_name} missing validation rules\")\n",
    "\n",
    "        return errors, warnings\n",
    "\n",
    "    def _validate_silver_steps(\n",
    "        self,\n",
    "        silver_steps: Dict[StepName, SilverStep],\n",
    "        bronze_steps: Dict[StepName, BronzeStep],\n",
    "    ) -> tuple[list[str], list[str]]:\n",
    "        \"\"\"Validate silver steps.\"\"\"\n",
    "        errors = []\n",
    "        warnings: list[str] = []\n",
    "\n",
    "        for step_name, step in silver_steps.items():\n",
    "            if not step.source_bronze:\n",
    "                errors.append(f\"Silver step {step_name} missing source_bronze\")\n",
    "\n",
    "            # Check source_bronze exists\n",
    "            if step.source_bronze not in bronze_steps:\n",
    "                errors.append(\n",
    "                    f\"Silver step {step_name} depends on non-existent bronze step {step.source_bronze}\"\n",
    "                )\n",
    "\n",
    "        return errors, warnings\n",
    "\n",
    "    def _validate_gold_steps(\n",
    "        self,\n",
    "        gold_steps: Dict[StepName, GoldStep],\n",
    "        silver_steps: Dict[StepName, SilverStep],\n",
    "    ) -> tuple[list[str], list[str]]:\n",
    "        \"\"\"Validate gold steps.\"\"\"\n",
    "        errors = []\n",
    "        warnings: list[str] = []\n",
    "\n",
    "        for step_name, step in gold_steps.items():\n",
    "            # Check source_silvers exist (if specified)\n",
    "            if step.source_silvers:\n",
    "                for silver_name in step.source_silvers:\n",
    "                    if silver_name not in silver_steps:\n",
    "                        errors.append(\n",
    "                            f\"Gold step {step_name} depends on non-existent silver step {silver_name}\"\n",
    "                        )\n",
    "\n",
    "        return errors, warnings\n",
    "\n",
    "    def _validate_dependencies(\n",
    "        self,\n",
    "        bronze_steps: Dict[StepName, BronzeStep],\n",
    "        silver_steps: Dict[StepName, SilverStep],\n",
    "        gold_steps: Dict[StepName, GoldStep],\n",
    "    ) -> tuple[list[str], list[str]]:\n",
    "        \"\"\"Validate step dependencies.\"\"\"\n",
    "        errors = []\n",
    "        warnings: list[str] = []\n",
    "\n",
    "        # Check for circular dependencies\n",
    "        all_steps = {**bronze_steps, **silver_steps, **gold_steps}\n",
    "\n",
    "        for step_name, step in all_steps.items():\n",
    "            # Check for circular dependencies in non-standard dependencies attribute\n",
    "            # This is only for custom step types that might have a dependencies field\n",
    "            if hasattr(step, \"dependencies\"):\n",
    "                dependencies = getattr(step, \"dependencies\", None)\n",
    "                if dependencies and isinstance(dependencies, (list, tuple, set)):\n",
    "                    for dep in dependencies:\n",
    "                        if hasattr(dep, \"step_name\") and dep.step_name == step_name:\n",
    "                            errors.append(\n",
    "                                f\"Step {step_name} has circular dependency on itself\"\n",
    "                            )\n",
    "\n",
    "        return errors, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.models.dependencies (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.models.base, pipeline_builder_base.errors\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from .errors import PipelineValidationError  # Removed: defined in notebook cells above\n",
    "# from .base import BaseModel  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SilverDependencyInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Dependency information for Silver steps.\n",
    "\n",
    "    Attributes:\n",
    "        step_name: Name of the silver step\n",
    "        source_bronze: Source bronze step name\n",
    "        depends_on_silvers: Set of silver step names this step depends on\n",
    "        can_run_parallel: Whether this step can run in parallel\n",
    "        execution_group: Execution group for parallel processing\n",
    "    \"\"\"\n",
    "\n",
    "    step_name: str\n",
    "    source_bronze: str\n",
    "    depends_on_silvers: set[str]\n",
    "    can_run_parallel: bool\n",
    "    execution_group: int\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate dependency information.\"\"\"\n",
    "        if not self.step_name or not isinstance(self.step_name, str):\n",
    "            raise PipelineValidationError(\"Step name must be a non-empty string\")\n",
    "        if not self.source_bronze or not isinstance(self.source_bronze, str):\n",
    "            raise PipelineValidationError(\n",
    "                \"Source bronze step name must be a non-empty string\"\n",
    "            )\n",
    "        if not isinstance(self.depends_on_silvers, set):\n",
    "            raise PipelineValidationError(\"Depends on silvers must be a set\")\n",
    "        if self.execution_group < 0:\n",
    "            raise PipelineValidationError(\"Execution group must be non-negative\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CrossLayerDependency(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a dependency between steps across different layers.\n",
    "\n",
    "    Attributes:\n",
    "        source_step: Name of the source step\n",
    "        target_step: Name of the target step\n",
    "        dependency_type: Type of dependency (data, validation, etc.)\n",
    "        is_required: Whether this dependency is required for execution\n",
    "    \"\"\"\n",
    "\n",
    "    source_step: str\n",
    "    target_step: str\n",
    "    dependency_type: str = \"data\"\n",
    "    is_required: bool = True\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate dependency information.\"\"\"\n",
    "        if not self.source_step or not isinstance(self.source_step, str):\n",
    "            raise PipelineValidationError(\"Source step must be a non-empty string\")\n",
    "        if not self.target_step or not isinstance(self.target_step, str):\n",
    "            raise PipelineValidationError(\"Target step must be a non-empty string\")\n",
    "        if self.source_step == self.target_step:\n",
    "            raise PipelineValidationError(\"Source and target steps cannot be the same\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UnifiedStepConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Unified configuration for pipeline steps.\n",
    "\n",
    "    Attributes:\n",
    "        step_name: Name of the step\n",
    "        step_type: Type of step (bronze/silver/gold)\n",
    "        dependencies: List of step dependencies\n",
    "        config: Step-specific configuration\n",
    "    \"\"\"\n",
    "\n",
    "    step_name: str\n",
    "    step_type: str\n",
    "    dependencies: list[str]\n",
    "    config: Dict[str, Any]\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate unified step configuration.\"\"\"\n",
    "        if not self.step_name or not isinstance(self.step_name, str):\n",
    "            raise PipelineValidationError(\"Step name must be a non-empty string\")\n",
    "        if self.step_type not in [\"bronze\", \"silver\", \"gold\"]:\n",
    "            raise PipelineValidationError(\"Step type must be bronze, silver, or gold\")\n",
    "        if not isinstance(self.dependencies, list):\n",
    "            raise PipelineValidationError(\"Dependencies must be a list\")\n",
    "        if not isinstance(self.config, dict):\n",
    "            raise PipelineValidationError(\"Config must be a dictionary\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UnifiedExecutionPlan(BaseModel):\n",
    "    \"\"\"\n",
    "    Unified execution plan for pipeline steps.\n",
    "\n",
    "    Attributes:\n",
    "        steps: List of unified step configurations\n",
    "        execution_order: Ordered list of step names for execution\n",
    "        parallel_groups: Groups of steps that can run in parallel\n",
    "    \"\"\"\n",
    "\n",
    "    steps: list[UnifiedStepConfig]\n",
    "    execution_order: list[str]\n",
    "    parallel_groups: list[list[str]]\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate unified execution plan.\"\"\"\n",
    "        if not isinstance(self.steps, list):\n",
    "            raise PipelineValidationError(\"Steps must be a list\")\n",
    "        if not isinstance(self.execution_order, list):\n",
    "            raise PipelineValidationError(\"Execution order must be a list\")\n",
    "        if not isinstance(self.parallel_groups, list):\n",
    "            raise PipelineValidationError(\"Parallel groups must be a list\")\n",
    "\n",
    "        # Validate that all steps in execution order exist\n",
    "        step_names = {step.step_name for step in self.steps}\n",
    "        for step_name in self.execution_order:\n",
    "            if step_name not in step_names:\n",
    "                raise PipelineValidationError(f\"Step {step_name} not found in steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.writer.monitoring (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, pipeline_builder.writer.models, pipeline_builder.writer.query_builder, pipeline_builder_base.logging, writer.exceptions\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, TypedDict\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "\n",
    "    HAS_PSUTIL = True\n",
    "except ImportError:\n",
    "    HAS_PSUTIL = False\n",
    "    psutil = None  # type: ignore[assignment, unused-ignore]\n",
    "\n",
    "# from ..compat import DataFrame, SparkSession  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .exceptions import WriterError  # Removed: defined in notebook cells above\n",
    "# from .models import WriterMetrics  # Removed: defined in notebook cells above\n",
    "# from .query_builder import QueryBuilder  # Removed: defined in notebook cells above\n",
    "\n",
    "# ============================================================================\n",
    "# TypedDict Definitions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class OperationMetrics(TypedDict):\n",
    "    \"\"\"Metrics for a single operation.\"\"\"\n",
    "\n",
    "    operation_id: str\n",
    "    success: bool\n",
    "    duration_secs: float\n",
    "    rows_written: int\n",
    "    memory_usage_mb: float\n",
    "    error_message: str | None\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "class SparkMemoryInfo(TypedDict, total=False):\n",
    "    \"\"\"Spark memory configuration.\"\"\"\n",
    "\n",
    "    executor_memory: str\n",
    "    driver_memory: str\n",
    "\n",
    "\n",
    "class MemoryUsageInfo(TypedDict):\n",
    "    \"\"\"Memory usage information structure.\"\"\"\n",
    "\n",
    "    total_mb: float\n",
    "    available_mb: float\n",
    "    used_mb: float\n",
    "    percentage: float\n",
    "    spark_memory: SparkMemoryInfo\n",
    "    psutil_available: bool\n",
    "\n",
    "\n",
    "class SuccessRateTrend(TypedDict):\n",
    "    \"\"\"Success rate trend data point.\"\"\"\n",
    "\n",
    "    date: str\n",
    "    success_rate: float\n",
    "    avg_validation_rate: float\n",
    "    avg_execution_time: float\n",
    "\n",
    "\n",
    "class PerformanceByPhase(TypedDict):\n",
    "    \"\"\"Performance metrics by phase.\"\"\"\n",
    "\n",
    "    phase: str\n",
    "    avg_execution_time: float\n",
    "    avg_validation_rate: float\n",
    "    execution_count: int\n",
    "\n",
    "\n",
    "class DataQualityTrend(TypedDict):\n",
    "    \"\"\"Data quality trend data point.\"\"\"\n",
    "\n",
    "    date: str\n",
    "    avg_validation_rate: float\n",
    "    min_validation_rate: float\n",
    "    max_validation_rate: float\n",
    "\n",
    "\n",
    "class PerformanceTrends(TypedDict):\n",
    "    \"\"\"Execution trends analysis structure.\"\"\"\n",
    "\n",
    "    success_rate_trend: list[SuccessRateTrend]\n",
    "    performance_by_phase: list[PerformanceByPhase]\n",
    "    data_quality_trend: list[DataQualityTrend]\n",
    "\n",
    "\n",
    "class PerformanceAnomaly(TypedDict):\n",
    "    \"\"\"Performance anomaly data point.\"\"\"\n",
    "\n",
    "    step: str\n",
    "    execution_time: float\n",
    "    validation_rate: float\n",
    "    success: bool\n",
    "\n",
    "\n",
    "class QualityAnomaly(TypedDict):\n",
    "    \"\"\"Quality anomaly data point.\"\"\"\n",
    "\n",
    "    step: str\n",
    "    validation_rate: float\n",
    "    valid_rows: int\n",
    "    invalid_rows: int\n",
    "\n",
    "\n",
    "class AnomalyReport(TypedDict):\n",
    "    \"\"\"Anomaly detection results structure.\"\"\"\n",
    "\n",
    "    performance_anomalies: list[PerformanceAnomaly]\n",
    "    quality_anomalies: list[QualityAnomaly]\n",
    "    anomaly_score: float\n",
    "    total_anomalies: int\n",
    "    total_executions: int\n",
    "\n",
    "\n",
    "class OverallStatistics(TypedDict):\n",
    "    \"\"\"Overall performance statistics.\"\"\"\n",
    "\n",
    "    total_executions: int\n",
    "    successful_executions: int\n",
    "    success_rate: float\n",
    "    avg_execution_time: float\n",
    "    avg_validation_rate: float\n",
    "    total_rows_written: int\n",
    "\n",
    "\n",
    "class PhaseStatistics(TypedDict):\n",
    "    \"\"\"Phase-wise performance statistics.\"\"\"\n",
    "\n",
    "    phase: str\n",
    "    execution_count: int\n",
    "    avg_execution_time: float\n",
    "    avg_validation_rate: float\n",
    "    total_rows_written: int\n",
    "\n",
    "\n",
    "class RecentPerformance(TypedDict):\n",
    "    \"\"\"Recent performance data point.\"\"\"\n",
    "\n",
    "    date: str\n",
    "    daily_executions: int\n",
    "    avg_execution_time: float\n",
    "    avg_validation_rate: float\n",
    "\n",
    "\n",
    "class PerformanceReport(TypedDict):\n",
    "    \"\"\"Comprehensive performance report structure.\"\"\"\n",
    "\n",
    "    overall_statistics: OverallStatistics\n",
    "    phase_statistics: list[PhaseStatistics]\n",
    "    recent_performance: list[RecentPerformance]\n",
    "    generated_at: str\n",
    "\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Handles performance monitoring and metrics collection.\"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession, logger: PipelineLogger | None = None):\n",
    "        \"\"\"Initialize the performance monitor.\"\"\"\n",
    "        self.spark = spark\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger(\"PerformanceMonitor\")\n",
    "        else:\n",
    "            self.logger = logger\n",
    "        self.metrics: WriterMetrics = {\n",
    "            \"total_writes\": 0,\n",
    "            \"successful_writes\": 0,\n",
    "            \"failed_writes\": 0,\n",
    "            \"total_duration_secs\": 0.0,\n",
    "            \"avg_write_duration_secs\": 0.0,\n",
    "            \"total_rows_written\": 0,\n",
    "            \"memory_usage_peak_mb\": 0.0,\n",
    "        }\n",
    "        self.operation_start_times: Dict[str, float] = {}\n",
    "\n",
    "    def start_operation(self, operation_id: str, operation_type: str) -> None:\n",
    "        \"\"\"\n",
    "        Start monitoring an operation.\n",
    "\n",
    "        Args:\n",
    "            operation_id: Unique identifier for the operation\n",
    "            operation_type: Type of operation being monitored\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.operation_start_times[operation_id] = time.time()\n",
    "            self.logger.info(\n",
    "                f\"Started monitoring {operation_type} operation: {operation_id}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Failed to start monitoring operation {operation_id}: {e}\"\n",
    "            )\n",
    "            raise WriterError(\n",
    "                f\"Failed to start monitoring operation {operation_id}: {e}\"\n",
    "            ) from e\n",
    "\n",
    "    def end_operation(\n",
    "        self,\n",
    "        operation_id: str,\n",
    "        success: bool,\n",
    "        rows_written: int = 0,\n",
    "        error_message: str | None = None,\n",
    "    ) -> OperationMetrics:\n",
    "        \"\"\"\n",
    "        End monitoring an operation and update metrics.\n",
    "\n",
    "        Args:\n",
    "            operation_id: Unique identifier for the operation\n",
    "            success: Whether the operation was successful\n",
    "            rows_written: Number of rows written\n",
    "            error_message: Error message if operation failed\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing operation metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if operation_id not in self.operation_start_times:\n",
    "                self.logger.warning(f\"Operation {operation_id} was not being monitored\")\n",
    "                # Return empty metrics matching the TypedDict\n",
    "                return {\n",
    "                    \"operation_id\": operation_id,\n",
    "                    \"success\": False,\n",
    "                    \"duration_secs\": 0.0,\n",
    "                    \"rows_written\": 0,\n",
    "                    \"memory_usage_mb\": 0.0,\n",
    "                    \"error_message\": \"Operation was not being monitored\",\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                }\n",
    "\n",
    "            # Calculate duration\n",
    "            start_time = self.operation_start_times[operation_id]\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Update metrics\n",
    "            self.metrics[\"total_writes\"] += 1\n",
    "            if success:\n",
    "                self.metrics[\"successful_writes\"] += 1\n",
    "            else:\n",
    "                self.metrics[\"failed_writes\"] += 1\n",
    "\n",
    "            self.metrics[\"total_duration_secs\"] += duration\n",
    "            self.metrics[\"total_rows_written\"] += rows_written\n",
    "\n",
    "            # Calculate average duration\n",
    "            if self.metrics[\"total_writes\"] > 0:\n",
    "                self.metrics[\"avg_write_duration_secs\"] = (\n",
    "                    self.metrics[\"total_duration_secs\"] / self.metrics[\"total_writes\"]\n",
    "                )\n",
    "\n",
    "            # Update peak memory usage\n",
    "            current_memory = self.get_memory_usage()[\"used_mb\"]\n",
    "            if current_memory > self.metrics[\"memory_usage_peak_mb\"]:\n",
    "                self.metrics[\"memory_usage_peak_mb\"] = current_memory\n",
    "\n",
    "            # Create operation metrics\n",
    "            operation_metrics = {\n",
    "                \"operation_id\": operation_id,\n",
    "                \"success\": success,\n",
    "                \"duration_secs\": duration,\n",
    "                \"rows_written\": rows_written,\n",
    "                \"memory_usage_mb\": current_memory,\n",
    "                \"error_message\": error_message,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "            # Clean up\n",
    "            del self.operation_start_times[operation_id]\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Completed monitoring {operation_id}: {duration:.2f}s, {rows_written} rows\"\n",
    "            )\n",
    "            return cast(OperationMetrics, operation_metrics)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to end monitoring operation {operation_id}: {e}\")\n",
    "            raise WriterError(\n",
    "                f\"Failed to end monitoring operation {operation_id}: {e}\"\n",
    "            ) from e\n",
    "\n",
    "    def get_metrics(self) -> WriterMetrics:\n",
    "        \"\"\"Get current performance metrics.\"\"\"\n",
    "        return self.metrics.copy()\n",
    "\n",
    "    def reset_metrics(self) -> None:\n",
    "        \"\"\"Reset performance metrics.\"\"\"\n",
    "        self.metrics = {\n",
    "            \"total_writes\": 0,\n",
    "            \"successful_writes\": 0,\n",
    "            \"failed_writes\": 0,\n",
    "            \"total_duration_secs\": 0.0,\n",
    "            \"avg_write_duration_secs\": 0.0,\n",
    "            \"total_rows_written\": 0,\n",
    "            \"memory_usage_peak_mb\": 0.0,\n",
    "        }\n",
    "        self.logger.info(\"Performance metrics reset\")\n",
    "\n",
    "    def get_memory_usage(self) -> MemoryUsageInfo:\n",
    "        \"\"\"\n",
    "        Get current memory usage information.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing memory usage details\n",
    "        \"\"\"\n",
    "        # Check if psutil is available at all\n",
    "        if not HAS_PSUTIL or psutil is None:\n",
    "            self.logger.warning(\"psutil not available, returning basic memory info\")\n",
    "            return {\n",
    "                \"total_mb\": 0.0,\n",
    "                \"available_mb\": 0.0,\n",
    "                \"used_mb\": 0.0,\n",
    "                \"percentage\": 0.0,\n",
    "                \"spark_memory\": {},\n",
    "                \"psutil_available\": False,\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Get system memory info\n",
    "            memory = psutil.virtual_memory()\n",
    "\n",
    "            # Get Spark memory info if available\n",
    "            spark_memory = {}\n",
    "            try:\n",
    "                spark_context = self.spark.sparkContext\n",
    "                spark_memory = {\n",
    "                    \"executor_memory\": spark_context.getConf().get(\n",
    "                        \"spark.executor.memory\", \"N/A\"\n",
    "                    ),\n",
    "                    \"driver_memory\": spark_context.getConf().get(\n",
    "                        \"spark.driver.memory\", \"N/A\"\n",
    "                    ),\n",
    "                }\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            memory_info = {\n",
    "                \"total_mb\": round(memory.total / (1024 * 1024), 2),\n",
    "                \"available_mb\": round(memory.available / (1024 * 1024), 2),\n",
    "                \"used_mb\": round(memory.used / (1024 * 1024), 2),\n",
    "                \"percentage\": memory.percent,\n",
    "                \"spark_memory\": spark_memory,\n",
    "                \"psutil_available\": True,\n",
    "            }\n",
    "\n",
    "            return cast(MemoryUsageInfo, memory_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get memory usage: {e}\")\n",
    "            raise WriterError(f\"Failed to get memory usage: {e}\") from e\n",
    "\n",
    "    def check_performance_thresholds(\n",
    "        self, operation_metrics: OperationMetrics\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        Check if performance thresholds are exceeded.\n",
    "\n",
    "        Args:\n",
    "            operation_metrics: Metrics for the operation\n",
    "\n",
    "        Returns:\n",
    "            List of threshold violations\n",
    "        \"\"\"\n",
    "        violations = []\n",
    "\n",
    "        try:\n",
    "            # Check duration threshold (5 minutes)\n",
    "            if operation_metrics.get(\"duration_secs\", 0) > 300:\n",
    "                violations.append(\"Operation duration exceeded 5 minutes\")\n",
    "\n",
    "            # Check memory usage threshold (8GB)\n",
    "            if operation_metrics.get(\"memory_usage_mb\", 0) > 8192:\n",
    "                violations.append(\"Memory usage exceeded 8GB\")\n",
    "\n",
    "            # Check success rate threshold (95%)\n",
    "            if self.metrics[\"total_writes\"] > 0:\n",
    "                success_rate = (\n",
    "                    self.metrics[\"successful_writes\"] / self.metrics[\"total_writes\"]\n",
    "                ) * 100\n",
    "                if success_rate < 95.0:\n",
    "                    violations.append(f\"Success rate below 95%: {success_rate:.1f}%\")\n",
    "\n",
    "            return violations\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to check performance thresholds: {e}\")\n",
    "            raise WriterError(f\"Failed to check performance thresholds: {e}\") from e\n",
    "\n",
    "\n",
    "class AnalyticsEngine:\n",
    "    \"\"\"Handles analytics and trend analysis for writer operations.\"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession, logger: PipelineLogger | None = None):\n",
    "        \"\"\"Initialize the analytics engine.\"\"\"\n",
    "        self.spark = spark\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger(\"AnalyticsEngine\")\n",
    "        else:\n",
    "            self.logger = logger\n",
    "\n",
    "    def analyze_execution_trends(self, df: DataFrame) -> PerformanceTrends:\n",
    "        \"\"\"\n",
    "        Analyze execution trends from log data.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing log data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing trend analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Analyzing execution trends\")\n",
    "\n",
    "            # Use query builder for all trend analyses\n",
    "            trends = {}\n",
    "\n",
    "            # Success rate trend using query builder\n",
    "            success_trend_df = QueryBuilder.build_daily_trends_query(df, 30)\n",
    "            success_trend = success_trend_df.collect()\n",
    "\n",
    "            trends[\"success_rate_trend\"] = [\n",
    "                {\n",
    "                    \"date\": row[\"date\"],\n",
    "                    \"success_rate\": (\n",
    "                        row[\"successful_executions\"] / row[\"daily_executions\"]\n",
    "                    )\n",
    "                    * 100,\n",
    "                    \"avg_validation_rate\": row.get(\"avg_validation_rate\", 0),\n",
    "                    \"avg_execution_time\": row[\"avg_execution_time\"],\n",
    "                }\n",
    "                for row in success_trend\n",
    "            ]\n",
    "\n",
    "            # Performance trends using query builder\n",
    "            performance_trend_df = QueryBuilder.build_phase_trends_query(df, 30)\n",
    "            performance_trend = performance_trend_df.collect()\n",
    "\n",
    "            trends[\"performance_by_phase\"] = [\n",
    "                {\n",
    "                    \"phase\": row[\"phase\"],\n",
    "                    \"avg_execution_time\": row[\"avg_execution_time\"],\n",
    "                    \"avg_validation_rate\": row[\"avg_validation_rate\"],\n",
    "                    \"execution_count\": row[\"execution_count\"],\n",
    "                }\n",
    "                for row in performance_trend\n",
    "            ]\n",
    "\n",
    "            # Data quality trends using query builder\n",
    "            quality_trend_df = QueryBuilder.build_quality_trends_query(df, 30)\n",
    "            quality_trend = quality_trend_df.collect()\n",
    "\n",
    "            trends[\"data_quality_trend\"] = [\n",
    "                {\n",
    "                    \"date\": row[\"date\"],\n",
    "                    \"avg_validation_rate\": row[\"avg_validation_rate\"],\n",
    "                    \"min_validation_rate\": row[\"min_validation_rate\"],\n",
    "                    \"max_validation_rate\": row[\"max_validation_rate\"],\n",
    "                }\n",
    "                for row in quality_trend\n",
    "            ]\n",
    "\n",
    "            self.logger.info(\"Execution trends analysis completed\")\n",
    "            return cast(PerformanceTrends, trends)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to analyze execution trends: {e}\")\n",
    "            raise WriterError(f\"Failed to analyze execution trends: {e}\") from e\n",
    "\n",
    "    def detect_anomalies(self, df: DataFrame) -> AnomalyReport:\n",
    "        \"\"\"\n",
    "        Detect anomalies in execution data.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing log data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing anomaly detection results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Detecting anomalies in execution data\")\n",
    "\n",
    "            anomalies: AnomalyReport = {\n",
    "                \"performance_anomalies\": [],\n",
    "                \"quality_anomalies\": [],\n",
    "                \"anomaly_score\": 0.0,\n",
    "                \"total_anomalies\": 0,\n",
    "                \"total_executions\": 0,\n",
    "            }\n",
    "\n",
    "            # Calculate performance thresholds using query builder\n",
    "            performance_stats = QueryBuilder.calculate_statistics(df, \"execution_time\")\n",
    "            performance_threshold = performance_stats[\"avg\"] + (\n",
    "                2 * performance_stats[\"stddev\"]\n",
    "            )\n",
    "\n",
    "            # Detect performance anomalies using query builder\n",
    "            performance_anomalies_df = QueryBuilder.build_performance_anomaly_query(\n",
    "                df, performance_threshold\n",
    "            ).select(\"step\", \"execution_time\", \"validation_rate\", \"success\")\n",
    "\n",
    "            performance_anomalies = performance_anomalies_df.collect()\n",
    "\n",
    "            anomalies[\"performance_anomalies\"] = [\n",
    "                {\n",
    "                    \"step\": row[\"step\"],\n",
    "                    \"execution_time\": row[\"execution_time\"],\n",
    "                    \"validation_rate\": row[\"validation_rate\"],\n",
    "                    \"success\": row[\"success\"],\n",
    "                }\n",
    "                for row in performance_anomalies\n",
    "            ]\n",
    "\n",
    "            # Detect data quality anomalies using query builder\n",
    "            quality_anomalies_df = (\n",
    "                QueryBuilder.build_quality_anomaly_query(df, 90.0)\n",
    "                .select(\"step\", \"validation_rate\", \"valid_rows\", \"invalid_rows\")\n",
    "                .orderBy(\"validation_rate\")\n",
    "            )\n",
    "\n",
    "            quality_anomalies = quality_anomalies_df.collect()\n",
    "\n",
    "            anomalies[\"quality_anomalies\"] = [\n",
    "                {\n",
    "                    \"step\": row[\"step\"],\n",
    "                    \"validation_rate\": row[\"validation_rate\"],\n",
    "                    \"valid_rows\": row[\"valid_rows\"],\n",
    "                    \"invalid_rows\": row[\"invalid_rows\"],\n",
    "                }\n",
    "                for row in quality_anomalies\n",
    "            ]\n",
    "\n",
    "            # Calculate anomaly score\n",
    "            total_executions = df.count()\n",
    "            anomaly_count = len(performance_anomalies) + len(quality_anomalies)\n",
    "            anomaly_score = (\n",
    "                (anomaly_count / total_executions) * 100 if total_executions > 0 else 0\n",
    "            )\n",
    "\n",
    "            anomalies[\"anomaly_score\"] = float(round(anomaly_score, 2))\n",
    "            anomalies[\"total_anomalies\"] = int(anomaly_count)\n",
    "            anomalies[\"total_executions\"] = int(total_executions)\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Anomaly detection completed: {anomaly_count} anomalies found\"\n",
    "            )\n",
    "            return anomalies\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to detect anomalies: {e}\")\n",
    "            raise WriterError(f\"Failed to detect anomalies: {e}\") from e\n",
    "\n",
    "    def generate_performance_report(self, df: DataFrame) -> PerformanceReport:\n",
    "        \"\"\"\n",
    "        Generate comprehensive performance report.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing log data\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing performance report\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Generating performance report\")\n",
    "\n",
    "            # Overall statistics using query builder\n",
    "            overall_stats_df = df.agg(**QueryBuilder.get_common_aggregations())\n",
    "            overall_stats = overall_stats_df.collect()[0]\n",
    "\n",
    "            # Phase-wise statistics using query builder\n",
    "            phase_stats_df = QueryBuilder.build_phase_trends_query(df, 30)\n",
    "            phase_stats = phase_stats_df.collect()\n",
    "\n",
    "            # Recent performance using query builder\n",
    "            recent_performance_df = QueryBuilder.build_recent_performance_query(df, 7)\n",
    "            recent_performance = recent_performance_df.collect()\n",
    "\n",
    "            report = {\n",
    "                \"overall_statistics\": {\n",
    "                    \"total_executions\": overall_stats[\"total_executions\"],\n",
    "                    \"successful_executions\": overall_stats[\"successful_executions\"],\n",
    "                    \"success_rate\": (\n",
    "                        (\n",
    "                            overall_stats[\"successful_executions\"]\n",
    "                            / overall_stats[\"total_executions\"]\n",
    "                        )\n",
    "                        * 100\n",
    "                        if overall_stats[\"total_executions\"] > 0\n",
    "                        else 0\n",
    "                    ),\n",
    "                    \"avg_execution_time\": overall_stats[\"avg_execution_time\"],\n",
    "                    \"avg_validation_rate\": overall_stats[\"avg_validation_rate\"],\n",
    "                    \"total_rows_written\": overall_stats[\"total_rows_written\"],\n",
    "                },\n",
    "                \"phase_statistics\": [\n",
    "                    {\n",
    "                        \"phase\": row[\"phase\"],\n",
    "                        \"execution_count\": row[\"execution_count\"],\n",
    "                        \"avg_execution_time\": row[\"avg_execution_time\"],\n",
    "                        \"avg_validation_rate\": row[\"avg_validation_rate\"],\n",
    "                        \"total_rows_written\": row[\"total_rows_written\"],\n",
    "                    }\n",
    "                    for row in phase_stats\n",
    "                ],\n",
    "                \"recent_performance\": [\n",
    "                    {\n",
    "                        \"date\": row[\"date\"].strftime(\"%Y-%m-%d\"),\n",
    "                        \"daily_executions\": row[\"daily_executions\"],\n",
    "                        \"avg_execution_time\": row[\"avg_execution_time\"],\n",
    "                        \"avg_validation_rate\": row[\"avg_validation_rate\"],\n",
    "                    }\n",
    "                    for row in recent_performance\n",
    "                ],\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "            self.logger.info(\"Performance report generated successfully\")\n",
    "            return cast(PerformanceReport, report)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate performance report: {e}\")\n",
    "            raise WriterError(f\"Failed to generate performance report: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.writer.operations (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, pipeline_builder.functions, pipeline_builder.models.execution, pipeline_builder.writer.models, pipeline_builder_base.logging, validation.utils, writer.exceptions\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Callable, Dict, TypedDict, Union\n",
    "\n",
    "# from ..compat import DataFrame, SparkSession  # Removed: defined in notebook cells above\n",
    "# from ..functions import FunctionsProtocol, get_default_functions  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .models import ExecutionResult, StepResult  # Removed: defined in notebook cells above\n",
    "# from ..validation import get_dataframe_info  # Removed: defined in notebook cells above\n",
    "# from .exceptions import WriterValidationError  # Removed: defined in notebook cells above\n",
    "# from .models import (  # Removed: defined in notebook cells above\n",
    "# LogRow,\n",
    "# create_log_rows_from_execution_result,\n",
    "# create_log_schema,\n",
    "# validate_log_data,\n",
    "# )\n",
    "\n",
    "# ============================================================================\n",
    "# TypedDict Definitions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DataQualityReport(TypedDict):\n",
    "    \"\"\"Data quality validation report.\"\"\"\n",
    "\n",
    "    is_valid: bool\n",
    "    total_rows: int\n",
    "    null_counts: Dict[str, int]\n",
    "    validation_issues: list[str]\n",
    "    failed_executions: int\n",
    "    data_quality_score: float\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Handles data processing and transformation operations.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        functions: FunctionsProtocol | None = None,\n",
    "        logger: PipelineLogger | None = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the data processor.\"\"\"\n",
    "        self.spark = spark\n",
    "        self.functions = functions if functions is not None else get_default_functions()\n",
    "        self.logger = logger or PipelineLogger(\"DataProcessor\")\n",
    "\n",
    "    def process_execution_result(\n",
    "        self,\n",
    "        execution_result: ExecutionResult,\n",
    "        run_id: str,\n",
    "        run_mode: str = \"initial\",\n",
    "        metadata: Union[Dict[str, Union[str, int, float, bool]], None] = None,\n",
    "        table_total_rows_provider: Callable[[str | None], int | None] | None = None,\n",
    "    ) -> list[LogRow]:\n",
    "        \"\"\"\n",
    "        Process execution result into log rows.\n",
    "\n",
    "        Args:\n",
    "            execution_result: The execution result to process\n",
    "            run_id: Unique run identifier\n",
    "            run_mode: Mode of the run\n",
    "            metadata: Additional metadata\n",
    "            table_total_rows_provider: Optional callback to supply table row counts\n",
    "\n",
    "        Returns:\n",
    "            List of processed log rows\n",
    "\n",
    "        Raises:\n",
    "            WriterValidationError: If validation fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Processing execution result for run {run_id}\")\n",
    "\n",
    "            # Create log rows from execution result\n",
    "            log_rows = create_log_rows_from_execution_result(\n",
    "                execution_result, run_id, run_mode, metadata\n",
    "            )\n",
    "\n",
    "            # Validate log data\n",
    "            validation_result = validate_log_data(log_rows)\n",
    "            if not validation_result[\"is_valid\"]:\n",
    "                raise WriterValidationError(\n",
    "                    f\"Log data validation failed: {validation_result['errors']}\",\n",
    "                    validation_errors=validation_result[\"errors\"],\n",
    "                    context={\"run_id\": run_id, \"log_rows_count\": len(log_rows)},\n",
    "                    suggestions=[\n",
    "                        \"Check data quality in source execution result\",\n",
    "                        \"Verify all required fields are present\",\n",
    "                        \"Ensure data types are correct\",\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "            # Populate table_total_rows when possible\n",
    "            if table_total_rows_provider is not None:\n",
    "                for row in log_rows:\n",
    "                    if row.get(\"table_total_rows\") is None:\n",
    "                        row[\"table_total_rows\"] = table_total_rows_provider(\n",
    "                            row.get(\"table_fqn\")\n",
    "                        )\n",
    "\n",
    "            self.logger.info(f\"Successfully processed {len(log_rows)} log rows\")\n",
    "            return log_rows\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process execution result: {e}\")\n",
    "            raise\n",
    "\n",
    "    def process_step_results(\n",
    "        self,\n",
    "        step_results: Dict[str, StepResult],\n",
    "        run_id: str,\n",
    "        run_mode: str = \"initial\",\n",
    "        metadata: Union[Dict[str, Union[str, int, float, bool]], None] = None,\n",
    "    ) -> list[LogRow]:\n",
    "        \"\"\"\n",
    "        Process step results into log rows.\n",
    "\n",
    "        Args:\n",
    "            step_results: Dictionary of step results\n",
    "            run_id: Unique run identifier\n",
    "            run_mode: Mode of the run\n",
    "            metadata: Additional metadata\n",
    "\n",
    "        Returns:\n",
    "            List of processed log rows\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\n",
    "                f\"Processing {len(step_results)} step results for run {run_id}\"\n",
    "            )\n",
    "\n",
    "            log_rows = []\n",
    "            for step_name, step_result in step_results.items():\n",
    "                # Create log row for each step\n",
    "                log_row = LogRow(\n",
    "                    run_id=run_id,\n",
    "                    run_mode=run_mode,  # type: ignore[typeddict-item]\n",
    "                    run_started_at=datetime.now(),\n",
    "                    run_ended_at=datetime.now(),\n",
    "                    execution_id=run_id,\n",
    "                    pipeline_id=run_id,\n",
    "                    schema=\"default\",\n",
    "                    phase=step_result.phase.value,\n",
    "                    step_name=step_name,\n",
    "                    step_type=step_result.phase.value,\n",
    "                    start_time=step_result.start_time,\n",
    "                    end_time=step_result.end_time,\n",
    "                    duration_secs=step_result.duration_secs,\n",
    "                    table_fqn=f\"{step_result.phase.value}_{step_name}\",\n",
    "                    write_mode=\"append\",\n",
    "                    input_rows=step_result.rows_processed,\n",
    "                    output_rows=step_result.rows_written,\n",
    "                    rows_written=step_result.rows_written,\n",
    "                    valid_rows=int(\n",
    "                        step_result.rows_processed * step_result.validation_rate / 100\n",
    "                    ),\n",
    "                    invalid_rows=int(\n",
    "                        step_result.rows_processed\n",
    "                        * (100 - step_result.validation_rate)\n",
    "                        / 100\n",
    "                    ),\n",
    "                    validation_rate=step_result.validation_rate,\n",
    "                    success=step_result.success,\n",
    "                    error_message=step_result.error_message,\n",
    "                    metadata=metadata or {},\n",
    "                    rows_processed=step_result.rows_processed,\n",
    "                    table_total_rows=None,\n",
    "                    memory_usage_mb=0.0,\n",
    "                    cpu_usage_percent=0.0,\n",
    "                )\n",
    "                log_rows.append(log_row)\n",
    "\n",
    "            self.logger.info(f\"Successfully processed {len(log_rows)} step log rows\")\n",
    "            return log_rows\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process step results: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_dataframe_from_log_rows(self, log_rows: list[LogRow]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Create DataFrame from log rows.\n",
    "\n",
    "        Args:\n",
    "            log_rows: List of log rows to convert\n",
    "\n",
    "        Returns:\n",
    "            DataFrame containing the log rows\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Creating DataFrame from {len(log_rows)} log rows\")\n",
    "\n",
    "            # Convert log rows to dictionaries\n",
    "            log_data = []\n",
    "            for row in log_rows:\n",
    "                row_dict = {\n",
    "                    \"run_id\": row[\"run_id\"],\n",
    "                    \"run_mode\": row[\"run_mode\"],\n",
    "                    \"run_started_at\": row[\"run_started_at\"],\n",
    "                    \"run_ended_at\": row[\"run_ended_at\"],\n",
    "                    \"execution_id\": row[\"execution_id\"],\n",
    "                    \"pipeline_id\": row[\"pipeline_id\"],\n",
    "                    \"schema\": row[\"schema\"],\n",
    "                    \"phase\": row[\"phase\"],\n",
    "                    \"step_name\": row[\"step_name\"],\n",
    "                    \"step_type\": row[\"step_type\"],\n",
    "                    \"start_time\": row[\"start_time\"],\n",
    "                    \"end_time\": row[\"end_time\"],\n",
    "                    \"duration_secs\": row[\"duration_secs\"],\n",
    "                    \"table_fqn\": row[\"table_fqn\"],\n",
    "                    \"write_mode\": row[\"write_mode\"],\n",
    "                    \"input_rows\": row[\"input_rows\"],\n",
    "                    \"output_rows\": row[\"output_rows\"],\n",
    "                    \"rows_written\": row[\"rows_written\"],\n",
    "                    \"rows_processed\": row[\"rows_processed\"],\n",
    "                    \"valid_rows\": row[\"valid_rows\"],\n",
    "                    \"invalid_rows\": row[\"invalid_rows\"],\n",
    "                    \"validation_rate\": row[\"validation_rate\"],\n",
    "                    \"success\": row[\"success\"],\n",
    "                    \"error_message\": row[\"error_message\"],\n",
    "                    \"memory_usage_mb\": row[\"memory_usage_mb\"],\n",
    "                    \"cpu_usage_percent\": row[\"cpu_usage_percent\"],\n",
    "                    \"metadata\": (\n",
    "                        json.dumps(row[\"metadata\"]) if row[\"metadata\"] else None\n",
    "                    ),\n",
    "                    \"created_at\": datetime.now().isoformat(),  # Include timestamp directly as string\n",
    "                }\n",
    "                log_data.append(row_dict)\n",
    "\n",
    "            # Create DataFrame with explicit schema for type safety and None value handling\n",
    "            schema = create_log_schema()\n",
    "            df = self.spark.createDataFrame(log_data, schema)  # type: ignore[attr-defined]\n",
    "\n",
    "            self.logger.info(\"Successfully created DataFrame from log rows\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create DataFrame from log rows: {e}\")\n",
    "            raise\n",
    "\n",
    "    def validate_data_quality(self, df: DataFrame) -> DataQualityReport:\n",
    "        \"\"\"\n",
    "        Validate data quality of the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing validation results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Validating data quality\")\n",
    "\n",
    "            # Get DataFrame info\n",
    "            df_info = get_dataframe_info(df)\n",
    "\n",
    "            # Check for null values in critical columns\n",
    "            critical_columns = [\"run_id\", \"phase\", \"step\", \"success\"]\n",
    "            null_counts = {}\n",
    "\n",
    "            for col_name in critical_columns:\n",
    "                if col_name in df.columns:\n",
    "                    null_count = df.filter(\n",
    "                        self.functions.col(col_name).isNull()\n",
    "                    ).count()\n",
    "                    null_counts[col_name] = null_count\n",
    "\n",
    "            # Check validation rates\n",
    "            validation_issues = []\n",
    "            if \"validation_rate\" in df.columns:\n",
    "                low_validation = df.filter(\n",
    "                    self.functions.col(\"validation_rate\") < 95.0\n",
    "                ).count()\n",
    "                if low_validation > 0:\n",
    "                    validation_issues.append(\n",
    "                        f\"{low_validation} records with validation rate < 95%\"\n",
    "                    )\n",
    "\n",
    "            # Check for failed executions\n",
    "            failed_executions = 0\n",
    "            if \"success\" in df.columns:\n",
    "                failed_executions = df.filter(~self.functions.col(\"success\")).count()\n",
    "\n",
    "            validation_result = {\n",
    "                \"is_valid\": len(validation_issues) == 0 and failed_executions == 0,\n",
    "                \"total_rows\": df_info[\"row_count\"],\n",
    "                \"null_counts\": null_counts,\n",
    "                \"validation_issues\": validation_issues,\n",
    "                \"failed_executions\": failed_executions,\n",
    "                \"data_quality_score\": self._calculate_quality_score(\n",
    "                    df_info, null_counts, validation_issues, failed_executions\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Data quality validation completed: {validation_result['is_valid']}\"\n",
    "            )\n",
    "            return cast(DataQualityReport, validation_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to validate data quality: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_quality_score(\n",
    "        self,\n",
    "        df_info: Dict[str, Union[int, str]],\n",
    "        null_counts: Dict[str, int],\n",
    "        validation_issues: list[str],\n",
    "        failed_executions: int,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate data quality score.\"\"\"\n",
    "        try:\n",
    "            total_rows = df_info[\"row_count\"]\n",
    "            if total_rows == 0:\n",
    "                return 0.0\n",
    "\n",
    "            # Ensure total_rows is an integer for division\n",
    "            if not isinstance(total_rows, int):\n",
    "                total_rows = int(total_rows) if total_rows else 0\n",
    "            if total_rows == 0:\n",
    "                return 0.0\n",
    "\n",
    "            # Calculate null penalty\n",
    "            null_penalty = sum(null_counts.values()) / total_rows\n",
    "\n",
    "            # Calculate validation penalty\n",
    "            validation_penalty = len(validation_issues) * 0.1\n",
    "\n",
    "            # Calculate failure penalty\n",
    "            failure_penalty = failed_executions / total_rows\n",
    "\n",
    "            # Calculate quality score (0-100)\n",
    "            quality_score = max(\n",
    "                0.0, 100.0 - (null_penalty + validation_penalty + failure_penalty) * 100\n",
    "            )\n",
    "\n",
    "            return float(round(quality_score, 2))\n",
    "\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def apply_data_transformations(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Apply data transformations to the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to transform\n",
    "\n",
    "        Returns:\n",
    "            Transformed DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Applying data transformations\")\n",
    "\n",
    "            # Add computed columns\n",
    "            df_transformed = df.withColumn(\n",
    "                \"processing_efficiency\",\n",
    "                self.functions.when(\n",
    "                    self.functions.col(\"input_rows\") > 0,\n",
    "                    self.functions.col(\"output_rows\")\n",
    "                    / self.functions.col(\"input_rows\")\n",
    "                    * 100,\n",
    "                ).otherwise(0),\n",
    "            ).withColumn(\n",
    "                \"data_quality_score\",\n",
    "                self.functions.when(\n",
    "                    self.functions.col(\"validation_rate\") >= 95.0, \"High\"\n",
    "                )\n",
    "                .when(self.functions.col(\"validation_rate\") >= 80.0, \"Medium\")\n",
    "                .otherwise(\"Low\"),\n",
    "            )\n",
    "\n",
    "            self.logger.info(\"Data transformations applied successfully\")\n",
    "            return df_transformed\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to apply data transformations: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder_base.reporting (pipeline_builder_base)\n",
    "#\n",
    "# Dependencies: performance\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import TypedDict\n",
    "\n",
    "# from .models import StageStats  # Removed: defined in notebook cells above\n",
    "# from .validation import safe_divide  # Removed: defined in notebook cells above\n",
    "\n",
    "# ============================================================================\n",
    "# TypedDict Definitions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class ValidationReport(TypedDict):\n",
    "    \"\"\"Validation report structure.\"\"\"\n",
    "\n",
    "    stage: str | None\n",
    "    step: str | None\n",
    "    total_rows: int\n",
    "    valid_rows: int\n",
    "    invalid_rows: int\n",
    "    validation_rate: float\n",
    "    duration_secs: float\n",
    "    start_at: datetime\n",
    "    end_at: datetime\n",
    "\n",
    "\n",
    "class TransformReport(TypedDict):\n",
    "    \"\"\"Transform operation report structure.\"\"\"\n",
    "\n",
    "    input_rows: int\n",
    "    output_rows: int\n",
    "    duration_secs: float\n",
    "    skipped: bool\n",
    "    start_at: datetime\n",
    "    end_at: datetime\n",
    "\n",
    "\n",
    "class WriteReport(TypedDict):\n",
    "    \"\"\"Write operation report structure.\"\"\"\n",
    "\n",
    "    mode: str\n",
    "    rows_written: int\n",
    "    duration_secs: float\n",
    "    table_fqn: str\n",
    "    skipped: bool\n",
    "    start_at: datetime\n",
    "    end_at: datetime\n",
    "\n",
    "\n",
    "class ExecutionSummary(TypedDict):\n",
    "    \"\"\"Execution summary nested structure.\"\"\"\n",
    "\n",
    "    total_steps: int\n",
    "    successful_steps: int\n",
    "    failed_steps: int\n",
    "    success_rate: float\n",
    "    failure_rate: float\n",
    "\n",
    "\n",
    "class PerformanceMetrics(TypedDict):\n",
    "    \"\"\"Performance metrics nested structure.\"\"\"\n",
    "\n",
    "    total_duration_secs: float\n",
    "    formatted_duration: str\n",
    "    avg_validation_rate: float\n",
    "\n",
    "\n",
    "class DataMetrics(TypedDict):\n",
    "    \"\"\"Data metrics nested structure.\"\"\"\n",
    "\n",
    "    total_rows_processed: int\n",
    "    total_rows_written: int\n",
    "    processing_efficiency: float\n",
    "\n",
    "\n",
    "class SummaryReport(TypedDict):\n",
    "    \"\"\"Complete summary report structure.\"\"\"\n",
    "\n",
    "    execution_summary: ExecutionSummary\n",
    "    performance_metrics: PerformanceMetrics\n",
    "    data_metrics: DataMetrics\n",
    "\n",
    "\n",
    "def create_validation_dict(\n",
    "    stats: StageStats | None, *, start_at: datetime, end_at: datetime\n",
    ") -> ValidationReport:\n",
    "    \"\"\"\n",
    "    Create a validation report dictionary from stage stats.\n",
    "\n",
    "    Args:\n",
    "        stats: Stage statistics\n",
    "        start_at: Start time\n",
    "        end_at: End time\n",
    "\n",
    "    Returns:\n",
    "        Validation report dictionary\n",
    "    \"\"\"\n",
    "    if stats is None:\n",
    "        return {\n",
    "            \"stage\": None,\n",
    "            \"step\": None,\n",
    "            \"total_rows\": 0,\n",
    "            \"valid_rows\": 0,\n",
    "            \"invalid_rows\": 0,\n",
    "            \"validation_rate\": 0.0,\n",
    "            \"duration_secs\": (end_at - start_at).total_seconds(),\n",
    "            \"start_at\": start_at,\n",
    "            \"end_at\": end_at,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"stage\": stats.stage,\n",
    "        \"step\": stats.step,\n",
    "        \"total_rows\": stats.total_rows,\n",
    "        \"valid_rows\": stats.valid_rows,\n",
    "        \"invalid_rows\": stats.invalid_rows,\n",
    "        \"validation_rate\": stats.validation_rate,\n",
    "        \"duration_secs\": stats.duration_secs,\n",
    "        \"start_at\": start_at,\n",
    "        \"end_at\": end_at,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_transform_dict(\n",
    "    *,\n",
    "    input_rows: int,\n",
    "    output_rows: int,\n",
    "    start_at: datetime,\n",
    "    end_at: datetime,\n",
    "    skipped: bool = False,\n",
    ") -> TransformReport:\n",
    "    \"\"\"\n",
    "    Create a transform report dictionary.\n",
    "\n",
    "    Args:\n",
    "        input_rows: Number of input rows\n",
    "        output_rows: Number of output rows\n",
    "        start_at: Start time\n",
    "        end_at: End time\n",
    "        skipped: Whether the transform was skipped\n",
    "\n",
    "    Returns:\n",
    "        Transform report dictionary\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"input_rows\": input_rows,\n",
    "        \"output_rows\": output_rows,\n",
    "        \"duration_secs\": (end_at - start_at).total_seconds(),\n",
    "        \"skipped\": skipped,\n",
    "        \"start_at\": start_at,\n",
    "        \"end_at\": end_at,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_write_dict(\n",
    "    *,\n",
    "    mode: str,\n",
    "    rows_written: int,\n",
    "    table_fqn: str,\n",
    "    start_at: datetime,\n",
    "    end_at: datetime,\n",
    "    skipped: bool = False,\n",
    ") -> WriteReport:\n",
    "    \"\"\"\n",
    "    Create a write report dictionary.\n",
    "\n",
    "    Args:\n",
    "        mode: Write mode\n",
    "        rows_written: Number of rows written\n",
    "        table_fqn: Fully qualified table name\n",
    "        start_at: Start time\n",
    "        end_at: End time\n",
    "        skipped: Whether the write was skipped\n",
    "\n",
    "    Returns:\n",
    "        Write report dictionary\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"rows_written\": rows_written,\n",
    "        \"duration_secs\": (end_at - start_at).total_seconds(),\n",
    "        \"table_fqn\": table_fqn,\n",
    "        \"skipped\": skipped,\n",
    "        \"start_at\": start_at,\n",
    "        \"end_at\": end_at,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_duration(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    Format duration in seconds to human-readable string.\n",
    "\n",
    "    Args:\n",
    "        seconds: Duration in seconds\n",
    "\n",
    "    Returns:\n",
    "        Formatted duration string\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = seconds % 60\n",
    "        return f\"{minutes}m {secs:.2f}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = seconds % 60\n",
    "        return f\"{hours}h {minutes}m {secs:.2f}s\"\n",
    "\n",
    "\n",
    "def create_summary_report(\n",
    "    *,\n",
    "    total_steps: int,\n",
    "    successful_steps: int,\n",
    "    failed_steps: int,\n",
    "    total_duration_secs: float,\n",
    "    total_rows_processed: int,\n",
    "    total_rows_written: int,\n",
    "    avg_validation_rate: float,\n",
    ") -> SummaryReport:\n",
    "    \"\"\"\n",
    "    Create a complete summary report.\n",
    "\n",
    "    Args:\n",
    "        total_steps: Total number of steps\n",
    "        successful_steps: Number of successful steps\n",
    "        failed_steps: Number of failed steps\n",
    "        total_duration_secs: Total duration in seconds\n",
    "        total_rows_processed: Total rows processed\n",
    "        total_rows_written: Total rows written\n",
    "        avg_validation_rate: Average validation rate\n",
    "\n",
    "    Returns:\n",
    "        Complete summary report\n",
    "    \"\"\"\n",
    "    success_rate = safe_divide(successful_steps, total_steps, 0.0) * 100\n",
    "    failure_rate = 100.0 - success_rate\n",
    "    processing_efficiency = (\n",
    "        safe_divide(total_rows_written, total_rows_processed, 0.0) * 100\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"execution_summary\": {\n",
    "            \"total_steps\": total_steps,\n",
    "            \"successful_steps\": successful_steps,\n",
    "            \"failed_steps\": failed_steps,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"failure_rate\": failure_rate,\n",
    "        },\n",
    "        \"performance_metrics\": {\n",
    "            \"total_duration_secs\": total_duration_secs,\n",
    "            \"formatted_duration\": format_duration(total_duration_secs),\n",
    "            \"avg_validation_rate\": avg_validation_rate,\n",
    "        },\n",
    "        \"data_metrics\": {\n",
    "            \"total_rows_processed\": total_rows_processed,\n",
    "            \"total_rows_written\": total_rows_written,\n",
    "            \"processing_efficiency\": processing_efficiency,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.performance (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, table_operations\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "from typing import Any, Callable\n",
    "\n",
    "# from .compat import DataFrame  # Removed: defined in notebook cells above\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def now_dt() -> datetime:\n",
    "    \"\"\"Get current UTC datetime.\"\"\"\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "\n",
    "def format_duration(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    Format duration in seconds to human-readable string.\n",
    "\n",
    "    Args:\n",
    "        seconds: Duration in seconds\n",
    "\n",
    "    Returns:\n",
    "        Formatted duration string\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = seconds / 60\n",
    "        return f\"{minutes:.2f}m\"\n",
    "    else:\n",
    "        hours = seconds / 3600\n",
    "        return f\"{hours:.2f}h\"\n",
    "\n",
    "\n",
    "def time_operation(operation_name: str = \"operation\") -> Callable[[Callable], Callable]:\n",
    "    \"\"\"Decorator to time operations and log performance.\"\"\"\n",
    "\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @wraps(func)\n",
    "        def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Starting {operation_name}...\")\n",
    "\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                duration = time.time() - start_time\n",
    "                logger.info(f\"Completed {operation_name} in {duration:.3f}s\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                duration = time.time() - start_time\n",
    "                logger.error(f\"Failed {operation_name} after {duration:.3f}s: {e}\")\n",
    "                raise\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def performance_monitor(\n",
    "    operation_name: str, max_duration: float | None = None\n",
    ") -> Generator[None, None, None]:\n",
    "    \"\"\"Context manager to monitor operation performance.\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting {operation_name}...\")\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "        duration = time.time() - start_time\n",
    "        logger.info(f\"Completed {operation_name} in {duration:.3f}s\")\n",
    "\n",
    "        if max_duration and duration > max_duration:\n",
    "            logger.warning(\n",
    "                f\"{operation_name} took {duration:.3f}s, exceeding threshold of {max_duration}s\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        duration = time.time() - start_time\n",
    "        logger.error(f\"Failed {operation_name} after {duration:.3f}s: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@time_operation(\"write operation\")\n",
    "def time_write_operation(\n",
    "    mode: str, df: DataFrame, fqn: str, **options: Any\n",
    ") -> tuple[int, float, datetime, datetime]:\n",
    "    \"\"\"\n",
    "    Time a write operation and return results with timing info.\n",
    "\n",
    "    Args:\n",
    "        mode: Write mode (overwrite/append)\n",
    "        df: DataFrame to write\n",
    "        fqn: Fully qualified table name\n",
    "        **options: Additional write options\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (rows_written, duration_secs, start_time, end_time)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If mode is invalid\n",
    "        TableOperationError: If write operation fails\n",
    "    \"\"\"\n",
    "    # from .table_operations import write_append_table, write_overwrite_table  # Removed: defined in notebook cells above\n",
    "\n",
    "    start = now_dt()\n",
    "    t0 = time.time()\n",
    "\n",
    "    try:\n",
    "        if mode == \"overwrite\":\n",
    "            rows = write_overwrite_table(df, fqn, **options)\n",
    "        elif mode == \"append\":\n",
    "            rows = write_append_table(df, fqn, **options)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown write mode '{mode}'. Supported modes: overwrite, append\"\n",
    "            )\n",
    "\n",
    "        t1 = time.time()\n",
    "        end = now_dt()\n",
    "        duration = round(t1 - t0, 3)\n",
    "\n",
    "        logger.info(f\"Write operation completed: {rows} rows in {duration}s to {fqn}\")\n",
    "        return rows, duration, start, end\n",
    "\n",
    "    except Exception as e:\n",
    "        t1 = time.time()\n",
    "        end = now_dt()\n",
    "        duration = round(t1 - t0, 3)\n",
    "        logger.error(f\"Write operation failed after {duration}s: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def monitor_performance(\n",
    "    operation_name: str, max_duration: float | None = None\n",
    ") -> Callable:\n",
    "    \"\"\"\n",
    "    Decorator factory for performance monitoring.\n",
    "\n",
    "    Args:\n",
    "        operation_name: Name of the operation\n",
    "        max_duration: Maximum allowed duration in seconds\n",
    "\n",
    "    Returns:\n",
    "        Decorator function\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @wraps(func)\n",
    "        def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
    "            with performance_monitor(operation_name, max_duration):\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.table_operations (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, pipeline_builder.performance, pipeline_builder_base.errors\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "\n",
    "# from .compat import AnalysisException, DataFrame, SparkSession  # Removed: defined in notebook cells above\n",
    "# from .errors import TableOperationError  # Removed: defined in notebook cells above\n",
    "# from .performance import time_operation  # Removed: defined in notebook cells above\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def fqn(schema: str, table: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a fully qualified table name.\n",
    "\n",
    "    Args:\n",
    "        schema: Database schema name\n",
    "        table: Table name\n",
    "\n",
    "    Returns:\n",
    "        Fully qualified table name\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If schema or table is empty\n",
    "    \"\"\"\n",
    "    if not schema or not table:\n",
    "        raise ValueError(\"Schema and table names cannot be empty\")\n",
    "    return f\"{schema}.{table}\"\n",
    "\n",
    "\n",
    "@time_operation(\"table write (overwrite)\")\n",
    "def write_overwrite_table(\n",
    "    df: DataFrame, fqn: str, **options: str | int | float | bool\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Write DataFrame to table in overwrite mode.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        fqn: Fully qualified table name\n",
    "        **options: Additional write options\n",
    "\n",
    "    Returns:\n",
    "        Number of rows written\n",
    "\n",
    "    Raises:\n",
    "        TableOperationError: If write operation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cache DataFrame for potential multiple operations\n",
    "        df.cache()\n",
    "        cnt: int = df.count()\n",
    "        writer = (\n",
    "            df.write.format(\"parquet\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "        )\n",
    "\n",
    "        # Apply additional options\n",
    "        for key, value in options.items():\n",
    "            writer = writer.option(key, value)\n",
    "\n",
    "        writer.saveAsTable(fqn)\n",
    "        logger.info(f\"Successfully wrote {cnt} rows to {fqn} in overwrite mode\")\n",
    "        return cnt\n",
    "\n",
    "    except Exception as e:\n",
    "        raise TableOperationError(f\"Failed to write table {fqn}: {e}\") from e\n",
    "\n",
    "\n",
    "@time_operation(\"table write (append)\")\n",
    "def write_append_table(\n",
    "    df: DataFrame, fqn: str, **options: str | int | float | bool\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Write DataFrame to table in append mode.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        fqn: Fully qualified table name\n",
    "        **options: Additional write options\n",
    "\n",
    "    Returns:\n",
    "        Number of rows written\n",
    "\n",
    "    Raises:\n",
    "        TableOperationError: If write operation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cache DataFrame for potential multiple operations\n",
    "        df.cache()\n",
    "        cnt: int = df.count()\n",
    "        writer = df.write.format(\"parquet\").mode(\"append\")\n",
    "\n",
    "        # Apply additional options\n",
    "        for key, value in options.items():\n",
    "            writer = writer.option(key, value)\n",
    "\n",
    "        writer.saveAsTable(fqn)\n",
    "        logger.info(f\"Successfully wrote {cnt} rows to {fqn} in append mode\")\n",
    "        return cnt\n",
    "\n",
    "    except Exception as e:\n",
    "        raise TableOperationError(f\"Failed to write table {fqn}: {e}\") from e\n",
    "\n",
    "\n",
    "def read_table(spark: SparkSession, fqn: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from a table.\n",
    "\n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        fqn: Fully qualified table name\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with table data\n",
    "\n",
    "    Raises:\n",
    "        TableOperationError: If read operation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.table(fqn)\n",
    "        logger.info(f\"Successfully read table {fqn}\")\n",
    "        return df\n",
    "    except AnalysisException as e:\n",
    "        raise TableOperationError(f\"Table {fqn} does not exist: {e}\") from e\n",
    "    except Exception as e:\n",
    "        raise TableOperationError(f\"Failed to read table {fqn}: {e}\") from e\n",
    "\n",
    "\n",
    "def table_exists(spark: SparkSession, fqn: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a table exists.\n",
    "\n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        fqn: Fully qualified table name\n",
    "\n",
    "    Returns:\n",
    "        True if table exists, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark.table(fqn).count()\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        logger.debug(f\"Table {fqn} does not exist (AnalysisException)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error checking if table {fqn} exists: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def drop_table(spark: SparkSession, fqn: str) -> bool:\n",
    "    \"\"\"\n",
    "    Drop a table if it exists.\n",
    "\n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        fqn: Fully qualified table name\n",
    "\n",
    "    Returns:\n",
    "        True if table was dropped, False if it didn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if table_exists(spark, fqn):\n",
    "            # Use Java SparkSession to access external catalog\n",
    "            jspark_session = spark._jsparkSession\n",
    "            external_catalog = jspark_session.sharedState().externalCatalog()\n",
    "\n",
    "            # Parse fully qualified name\n",
    "            if \".\" in fqn:\n",
    "                database_name, table_name = fqn.split(\".\", 1)\n",
    "            else:\n",
    "                database_name = \"default\"\n",
    "                table_name = fqn\n",
    "\n",
    "            # Drop the table using external catalog\n",
    "            # Parameters: db, table, ignoreIfNotExists, purge\n",
    "            external_catalog.dropTable(database_name, table_name, True, True)\n",
    "            logger.info(f\"Dropped table {fqn}\")\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to drop table {fqn}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.writer.storage (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, pipeline_builder.functions, pipeline_builder.table_operations, pipeline_builder.writer.models, pipeline_builder_base.logging, writer.exceptions\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, TypedDict, Union\n",
    "\n",
    "# from ..compat import DataFrame, SparkSession, types  # Removed: defined in notebook cells above\n",
    "\n",
    "# Handle optional Delta Lake dependency\n",
    "try:\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    HAS_DELTA = True\n",
    "except ImportError:\n",
    "    DeltaTable = None  # type: ignore[misc, assignment]\n",
    "    HAS_DELTA = False\n",
    "\n",
    "# from ..functions import FunctionsProtocol, get_default_functions  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from ..table_operations import table_exists  # Removed: defined in notebook cells above\n",
    "# from .exceptions import WriterTableError  # Removed: defined in notebook cells above\n",
    "# from .models import LogRow, WriteMode, WriterConfig, create_log_schema  # Removed: defined in notebook cells above\n",
    "\n",
    "# ============================================================================\n",
    "# TypedDict Definitions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class WriteResult(TypedDict):\n",
    "    \"\"\"Write operation result structure.\"\"\"\n",
    "\n",
    "    table_name: str\n",
    "    write_mode: str\n",
    "    rows_written: int\n",
    "    timestamp: str\n",
    "    success: bool\n",
    "\n",
    "\n",
    "class OptimizeResultSkipped(TypedDict):\n",
    "    \"\"\"Optimize operation result when skipped.\"\"\"\n",
    "\n",
    "    table_name: str\n",
    "    optimization_completed: bool  # False\n",
    "    skipped: bool  # True\n",
    "    reason: str\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "class TableInfo(TypedDict, total=False):\n",
    "    \"\"\"Table information structure.\"\"\"\n",
    "\n",
    "    table_name: str\n",
    "    row_count: int\n",
    "    details: list[dict[str, str | int | float | bool | None]]\n",
    "    history_count: int\n",
    "    last_modified: str | None\n",
    "    history: list[dict[str, str | int | float | bool | None]]\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "class OptimizeResultCompleted(TypedDict):\n",
    "    \"\"\"Optimize operation result when completed.\"\"\"\n",
    "\n",
    "    table_name: str\n",
    "    optimization_completed: bool  # True\n",
    "    timestamp: str\n",
    "    table_info: TableInfo\n",
    "\n",
    "\n",
    "# Union type for optimize result\n",
    "OptimizeResult = Union[OptimizeResultSkipped, OptimizeResultCompleted]\n",
    "\n",
    "\n",
    "class VacuumResultSkipped(TypedDict):\n",
    "    \"\"\"Vacuum operation result when skipped.\"\"\"\n",
    "\n",
    "    table_name: str\n",
    "    vacuum_completed: bool  # False\n",
    "    skipped: bool  # True\n",
    "    reason: str\n",
    "    retention_hours: int\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "class VacuumResultCompleted(TypedDict):\n",
    "    \"\"\"Vacuum operation result when completed.\"\"\"\n",
    "\n",
    "    table_name: str\n",
    "    vacuum_completed: bool  # True\n",
    "    retention_hours: int\n",
    "    timestamp: str\n",
    "\n",
    "\n",
    "# Union type for vacuum result\n",
    "VacuumResult = Union[VacuumResultSkipped, VacuumResultCompleted]\n",
    "\n",
    "\n",
    "class StorageManager:\n",
    "    \"\"\"Handles storage operations for the writer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        config: WriterConfig,\n",
    "        functions: FunctionsProtocol | None = None,\n",
    "        logger: PipelineLogger | None = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the storage manager.\"\"\"\n",
    "        self.spark = spark\n",
    "        self.config = config\n",
    "        self.functions = functions if functions is not None else get_default_functions()\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger(\"StorageManager\")\n",
    "        else:\n",
    "            self.logger = logger\n",
    "        self.table_fqn = f\"{config.table_schema}.{config.table_name}\"\n",
    "\n",
    "    def create_table_if_not_exists(self, schema: types.StructType) -> None:\n",
    "        \"\"\"\n",
    "        Create the log table if it doesn't exist.\n",
    "\n",
    "        Args:\n",
    "            schema: Spark schema for the table\n",
    "\n",
    "        Raises:\n",
    "            WriterTableError: If table creation fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Creating table if not exists: {self.table_fqn}\")\n",
    "\n",
    "            # Extract schema name from table_fqn (format: \"schema.table\")\n",
    "            schema_name = (\n",
    "                self.table_fqn.split(\".\")[0] if \".\" in self.table_fqn else None\n",
    "            )\n",
    "\n",
    "            # CRITICAL: Ensure schema exists before creating table (required in mock-spark due to DuckDB threading)\n",
    "            # This is especially important for LogWriter which creates tables in different schemas\n",
    "            if schema_name:\n",
    "                try:\n",
    "                    # Use SQL to ensure schema exists (more reliable than storage API in some contexts)\n",
    "                    self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "                except Exception as e:\n",
    "                    # If SQL fails, try storage API\n",
    "                    try:\n",
    "                        if hasattr(self.spark, \"storage\") and hasattr(\n",
    "                            self.spark.storage, \"create_schema\"\n",
    "                        ):\n",
    "                            self.spark.storage.create_schema(schema_name)\n",
    "                    except Exception:\n",
    "                        # If both fail, log warning but continue (schema might already exist)\n",
    "                        self.logger.debug(\n",
    "                            f\"Could not create schema '{schema_name}': {e}\"\n",
    "                        )\n",
    "\n",
    "            if not table_exists(self.spark, self.table_fqn):\n",
    "                # Create empty DataFrame with schema\n",
    "                empty_df = self.spark.createDataFrame([], schema)\n",
    "\n",
    "                # CRITICAL: Ensure schema exists RIGHT BEFORE saveAsTable (mock-spark DuckDB threading fix)\n",
    "                # DuckDB connections in worker threads don't see schemas created earlier\n",
    "                if schema_name:\n",
    "                    try:\n",
    "                        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "                    except Exception:\n",
    "                        pass  # Schema might already exist\n",
    "\n",
    "                # Write to Delta table\n",
    "                (\n",
    "                    empty_df.write.format(\"delta\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .option(\"overwriteSchema\", \"true\")\n",
    "                    .saveAsTable(self.table_fqn)\n",
    "                )\n",
    "\n",
    "                self.logger.info(f\"Table created successfully: {self.table_fqn}\")\n",
    "            else:\n",
    "                self.logger.info(f\"Table already exists: {self.table_fqn}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise WriterTableError(\n",
    "                f\"Failed to create table {self.table_fqn}: {e}\",\n",
    "                table_name=self.table_fqn,\n",
    "                operation=\"create_table\",\n",
    "                context={\"schema\": str(schema)},\n",
    "                suggestions=[\n",
    "                    \"Check table permissions\",\n",
    "                    \"Verify schema configuration\",\n",
    "                    \"Ensure Delta Lake is properly configured\",\n",
    "                ],\n",
    "            ) from e\n",
    "\n",
    "    def write_dataframe(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        write_mode: WriteMode = WriteMode.APPEND,\n",
    "        partition_columns: list[str] | None = None,\n",
    "    ) -> WriteResult:\n",
    "        \"\"\"\n",
    "        Write DataFrame to the log table.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to write\n",
    "            write_mode: Write mode for the operation\n",
    "            partition_columns: Columns to partition by\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing write results\n",
    "\n",
    "        Raises:\n",
    "            WriterTableError: If write operation fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\n",
    "                f\"Writing DataFrame to {self.table_fqn} with mode {write_mode.value}\"\n",
    "            )\n",
    "\n",
    "            # Prepare DataFrame for writing\n",
    "            df_prepared = self._prepare_dataframe_for_write(df)\n",
    "\n",
    "            # Configure write options\n",
    "            writer = df_prepared.write.format(\"delta\").mode(write_mode.value)\n",
    "\n",
    "            # Add partitioning if specified\n",
    "            if partition_columns:\n",
    "                writer = writer.partitionBy(*partition_columns)\n",
    "\n",
    "            # Add table-specific options\n",
    "            if write_mode == WriteMode.OVERWRITE:\n",
    "                writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "            elif write_mode == WriteMode.APPEND:\n",
    "                # Enable schema evolution for append mode to handle new columns\n",
    "                writer = writer.option(\"mergeSchema\", \"true\")\n",
    "\n",
    "            # Execute write operation\n",
    "            writer.saveAsTable(self.table_fqn)\n",
    "\n",
    "            # Get write statistics\n",
    "            row_count = df_prepared.count()\n",
    "\n",
    "            write_result = {\n",
    "                \"table_name\": self.table_fqn,\n",
    "                \"write_mode\": write_mode.value,\n",
    "                \"rows_written\": row_count,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"success\": True,\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Successfully wrote {row_count} rows to {self.table_fqn}\")\n",
    "            return cast(WriteResult, write_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Safely get row count for error context\n",
    "            try:\n",
    "                row_count = df.count() if hasattr(df, \"count\") else 0\n",
    "            except Exception:\n",
    "                row_count = 0\n",
    "\n",
    "            raise WriterTableError(\n",
    "                f\"Failed to write DataFrame to {self.table_fqn}: {e}\",\n",
    "                table_name=self.table_fqn,\n",
    "                operation=\"write_dataframe\",\n",
    "                context={\"write_mode\": write_mode.value, \"row_count\": row_count},\n",
    "                suggestions=[\n",
    "                    \"Check table permissions\",\n",
    "                    \"Verify DataFrame schema matches table schema\",\n",
    "                    \"Ensure sufficient storage space\",\n",
    "                    \"Check for schema evolution conflicts\",\n",
    "                ],\n",
    "            ) from e\n",
    "\n",
    "    def write_batch(\n",
    "        self, log_rows: list[LogRow], write_mode: WriteMode = WriteMode.APPEND\n",
    "    ) -> WriteResult:\n",
    "        \"\"\"\n",
    "        Write a batch of log rows to the table.\n",
    "\n",
    "        Args:\n",
    "            log_rows: List of log rows to write\n",
    "            write_mode: Write mode for the operation\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing write results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Writing batch of {len(log_rows)} log rows\")\n",
    "\n",
    "            # Convert log rows to DataFrame\n",
    "            df = self._create_dataframe_from_log_rows(log_rows)\n",
    "\n",
    "            # Write DataFrame\n",
    "            return self.write_dataframe(df, write_mode)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to write batch: {e}\")\n",
    "            raise\n",
    "\n",
    "    def optimize_table(self) -> OptimizeResult:\n",
    "        \"\"\"\n",
    "        Optimize the Delta table for better performance.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing optimization results\n",
    "        \"\"\"\n",
    "        if not HAS_DELTA:\n",
    "            self.logger.warning(\n",
    "                f\"Delta Lake not available, optimize operation skipped for {self.table_fqn}\"\n",
    "            )\n",
    "            return {\n",
    "                \"table_name\": self.table_fqn,\n",
    "                \"optimization_completed\": False,\n",
    "                \"skipped\": True,\n",
    "                \"reason\": \"Delta Lake not available\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            self.logger.info(f\"Optimizing table: {self.table_fqn}\")\n",
    "\n",
    "            # Run OPTIMIZE command using Delta Lake Python API\n",
    "            delta_table = DeltaTable.forName(self.spark, self.table_fqn)\n",
    "            # Note: optimize() method may not be available in all Delta Lake versions\n",
    "            if hasattr(delta_table, \"optimize\"):\n",
    "                delta_table.optimize()\n",
    "            else:\n",
    "                # Fallback: use SQL command\n",
    "                self.spark.sql(f\"OPTIMIZE {self.table_fqn}\")\n",
    "\n",
    "            # Get table statistics\n",
    "            table_info = self.get_table_info()\n",
    "\n",
    "            optimization_result = {\n",
    "                \"table_name\": self.table_fqn,\n",
    "                \"optimization_completed\": True,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"table_info\": table_info,\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Table optimization completed: {self.table_fqn}\")\n",
    "            return cast(OptimizeResult, optimization_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to optimize table {self.table_fqn}: {e}\")\n",
    "            raise WriterTableError(\n",
    "                f\"Failed to optimize table {self.table_fqn}: {e}\",\n",
    "                table_name=self.table_fqn,\n",
    "                operation=\"optimize_table\",\n",
    "                suggestions=[\n",
    "                    \"Check table permissions\",\n",
    "                    \"Verify table exists\",\n",
    "                    \"Ensure sufficient resources for optimization\",\n",
    "                ],\n",
    "            ) from e\n",
    "\n",
    "    def vacuum_table(self, retention_hours: int = 168) -> VacuumResult:\n",
    "        \"\"\"\n",
    "        Vacuum the Delta table to remove old files.\n",
    "\n",
    "        Args:\n",
    "            retention_hours: Hours of retention for old files\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing vacuum results\n",
    "        \"\"\"\n",
    "        if not HAS_DELTA:\n",
    "            self.logger.warning(\n",
    "                f\"Delta Lake not available, vacuum operation skipped for {self.table_fqn}\"\n",
    "            )\n",
    "            return {\n",
    "                \"table_name\": self.table_fqn,\n",
    "                \"vacuum_completed\": False,\n",
    "                \"skipped\": True,\n",
    "                \"reason\": \"Delta Lake not available\",\n",
    "                \"retention_hours\": retention_hours,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            self.logger.info(\n",
    "                f\"Vacuuming table: {self.table_fqn} (retention: {retention_hours}h)\"\n",
    "            )\n",
    "\n",
    "            # Run VACUUM command using Delta Lake API\n",
    "            delta_table = DeltaTable.forName(self.spark, self.table_fqn)\n",
    "            delta_table.vacuum(retentionHours=retention_hours)\n",
    "\n",
    "            vacuum_result = {\n",
    "                \"table_name\": self.table_fqn,\n",
    "                \"vacuum_completed\": True,\n",
    "                \"retention_hours\": retention_hours,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Table vacuum completed: {self.table_fqn}\")\n",
    "            return cast(VacuumResult, vacuum_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to vacuum table {self.table_fqn}: {e}\")\n",
    "            raise WriterTableError(\n",
    "                f\"Failed to vacuum table {self.table_fqn}: {e}\",\n",
    "                table_name=self.table_fqn,\n",
    "                operation=\"vacuum_table\",\n",
    "                suggestions=[\n",
    "                    \"Check table permissions\",\n",
    "                    \"Verify retention period is valid\",\n",
    "                    \"Ensure table exists\",\n",
    "                ],\n",
    "            ) from e\n",
    "\n",
    "    def get_table_info(self) -> TableInfo:\n",
    "        \"\"\"\n",
    "        Get information about the log table.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing table information\n",
    "        \"\"\"\n",
    "        if not HAS_DELTA:\n",
    "            self.logger.warning(\n",
    "                f\"Delta Lake not available, using basic table info for {self.table_fqn}\"\n",
    "            )\n",
    "            # Get basic info without Delta Lake\n",
    "            row_count = self.spark.table(self.table_fqn).count()\n",
    "            return {\n",
    "                \"table_name\": self.table_fqn,\n",
    "                \"row_count\": row_count,\n",
    "                \"details\": [],\n",
    "                \"history\": [],\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            self.logger.info(f\"Getting table info for: {self.table_fqn}\")\n",
    "\n",
    "            # Get table details using Delta Lake API\n",
    "            delta_table = DeltaTable.forName(self.spark, self.table_fqn)\n",
    "\n",
    "            # Get table details using Delta Lake Python API\n",
    "            # Note: detail() method may not be available in all Delta Lake versions\n",
    "            if hasattr(delta_table, \"detail\"):\n",
    "                table_details = delta_table.detail().collect()\n",
    "            else:\n",
    "                # Fallback: use SQL command\n",
    "                table_details = self.spark.sql(\n",
    "                    f\"DESCRIBE DETAIL {self.table_fqn}\"\n",
    "                ).collect()\n",
    "\n",
    "            # Get table history\n",
    "            table_history = delta_table.history().collect()\n",
    "\n",
    "            # Get row count\n",
    "            row_count = self.spark.table(self.table_fqn).count()\n",
    "\n",
    "            table_info = {\n",
    "                \"table_name\": self.table_fqn,\n",
    "                \"row_count\": row_count,\n",
    "                \"details\": [dict(row.asDict()) for row in table_details],\n",
    "                \"history_count\": len(table_history),\n",
    "                \"last_modified\": (\n",
    "                    table_history[0][\"timestamp\"] if table_history else None\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Table info retrieved: {row_count} rows\")\n",
    "            return cast(TableInfo, table_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get table info for {self.table_fqn}: {e}\")\n",
    "            raise WriterTableError(\n",
    "                f\"Failed to get table info for {self.table_fqn}: {e}\",\n",
    "                table_name=self.table_fqn,\n",
    "                operation=\"get_table_info\",\n",
    "            ) from e\n",
    "\n",
    "    def query_logs(\n",
    "        self,\n",
    "        limit: int | None = None,\n",
    "        filters: Union[Dict[str, Union[str, int, float, bool]], None] = None,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Query logs from the table.\n",
    "\n",
    "        Args:\n",
    "            limit: Maximum number of rows to return\n",
    "            filters: Filters to apply to the query\n",
    "\n",
    "        Returns:\n",
    "            DataFrame containing query results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Querying logs from: {self.table_fqn}\")\n",
    "\n",
    "            # Start with the base table\n",
    "            result_df = self.spark.table(self.table_fqn)\n",
    "\n",
    "            # Apply filters if provided using PySpark functions\n",
    "            if filters:\n",
    "                for column, value in filters.items():\n",
    "                    if isinstance(value, str):\n",
    "                        result_df = result_df.filter(\n",
    "                            self.functions.col(column) == self.functions.lit(value)\n",
    "                        )\n",
    "                    else:\n",
    "                        result_df = result_df.filter(\n",
    "                            self.functions.col(column) == value\n",
    "                        )\n",
    "\n",
    "            # Add ordering using PySpark functions\n",
    "            # from ..compat import desc  # Removed: defined in notebook cells above\n",
    "\n",
    "            result_df = result_df.orderBy(desc(\"created_at\"))\n",
    "\n",
    "            # Apply limit if specified\n",
    "            if limit:\n",
    "                result_df = result_df.limit(limit)\n",
    "\n",
    "            self.logger.info(f\"Query executed successfully: {result_df.count()} rows\")\n",
    "            return result_df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to query logs from {self.table_fqn}: {e}\")\n",
    "            raise WriterTableError(\n",
    "                f\"Failed to query logs: {e}\",\n",
    "                table_name=self.table_fqn,\n",
    "                operation=\"query_logs\",\n",
    "                suggestions=[\n",
    "                    \"Check table exists\",\n",
    "                    \"Verify query syntax\",\n",
    "                    \"Check column names in filters\",\n",
    "                ],\n",
    "            ) from e\n",
    "\n",
    "    def _prepare_dataframe_for_write(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Prepare DataFrame for writing to Delta table.\"\"\"\n",
    "        try:\n",
    "            # Add metadata columns if not present\n",
    "            from datetime import datetime\n",
    "\n",
    "            current_time_str = datetime.now().isoformat()\n",
    "\n",
    "            if \"created_at\" not in df.columns:\n",
    "                df = df.withColumn(\"created_at\", self.functions.lit(current_time_str))\n",
    "\n",
    "            if \"updated_at\" not in df.columns:\n",
    "                df = df.withColumn(\"updated_at\", self.functions.lit(current_time_str))\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to prepare DataFrame for write: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _create_dataframe_from_log_rows(self, log_rows: list[LogRow]) -> DataFrame:\n",
    "        \"\"\"Create DataFrame from log rows.\"\"\"\n",
    "        try:\n",
    "            # Convert log rows to dictionaries\n",
    "            from datetime import datetime\n",
    "\n",
    "            current_time_str = datetime.now().isoformat()\n",
    "\n",
    "            log_data = []\n",
    "            for row in log_rows:\n",
    "                row_dict = {\n",
    "                    \"run_id\": row[\"run_id\"],\n",
    "                    \"run_mode\": row[\"run_mode\"],\n",
    "                    \"run_started_at\": row[\"run_started_at\"],\n",
    "                    \"run_ended_at\": row[\"run_ended_at\"],\n",
    "                    \"execution_id\": row[\"execution_id\"],\n",
    "                    \"pipeline_id\": row[\"pipeline_id\"],\n",
    "                    \"schema\": row[\"schema\"],\n",
    "                    \"phase\": row[\"phase\"],\n",
    "                    \"step_name\": row[\"step_name\"],\n",
    "                    \"step_type\": row[\"step_type\"],\n",
    "                    \"start_time\": row[\"start_time\"],\n",
    "                    \"end_time\": row[\"end_time\"],\n",
    "                    \"duration_secs\": row[\"duration_secs\"],\n",
    "                    \"table_fqn\": row[\"table_fqn\"],\n",
    "                    \"write_mode\": row[\"write_mode\"],\n",
    "                    \"input_rows\": row[\"input_rows\"],\n",
    "                    \"output_rows\": row[\"output_rows\"],\n",
    "                    \"rows_written\": row[\"rows_written\"],\n",
    "                    \"rows_processed\": row[\"rows_processed\"],\n",
    "                    \"table_total_rows\": row.get(\n",
    "                        \"table_total_rows\"\n",
    "                    ),  # Include table_total_rows metric\n",
    "                    \"valid_rows\": row[\"valid_rows\"],\n",
    "                    \"invalid_rows\": row[\"invalid_rows\"],\n",
    "                    \"validation_rate\": row[\"validation_rate\"],\n",
    "                    \"success\": row[\"success\"],\n",
    "                    \"error_message\": row[\"error_message\"],\n",
    "                    \"memory_usage_mb\": row[\"memory_usage_mb\"],\n",
    "                    \"cpu_usage_percent\": row[\"cpu_usage_percent\"],\n",
    "                    \"metadata\": row[\"metadata\"],\n",
    "                    \"created_at\": current_time_str,  # Include timestamp directly as string\n",
    "                }\n",
    "                log_data.append(row_dict)\n",
    "\n",
    "            # Create DataFrame with explicit schema for type safety and None value handling\n",
    "            schema = create_log_schema()\n",
    "            df = self.spark.createDataFrame(log_data, schema)  # type: ignore[attr-defined]\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create DataFrame from log rows: {e}\")\n",
    "            raise\n",
    "\n",
    "    @property\n",
    "    def table_schema(self) -> str:\n",
    "        \"\"\"Get the table schema.\"\"\"\n",
    "        return self.config.table_schema\n",
    "\n",
    "    @property\n",
    "    def table_name(self) -> str:\n",
    "        \"\"\"Get the table name.\"\"\"\n",
    "        return self.config.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.pipeline.runner (pipeline_builder)\n",
    "#\n",
    "# Dependencies: models.pipeline, models.steps, pipeline.models, pipeline_builder.compat, pipeline_builder.execution, pipeline_builder.functions, pipeline_builder_base.logging\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from abstracts.reports.run import Report\n",
    "from abstracts.runner import Runner\n",
    "\n",
    "# from ..compat import DataFrame, SparkSession  # Removed: defined in notebook cells above\n",
    "# from ..execution import ExecutionEngine, ExecutionResult  # Removed: defined in notebook cells above\n",
    "# from ..functions import FunctionsProtocol  # Removed: defined in notebook cells above\n",
    "# from ..models import BronzeStep, GoldStep, SilverStep  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .models import (  # Removed: defined in notebook cells above\n",
    "# ExecutionMode,\n",
    "# PipelineConfig,\n",
    "# PipelineMetrics,\n",
    "# )\n",
    "# from .models import PipelineMode, PipelineReport, PipelineStatus  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class SimplePipelineRunner(Runner):\n",
    "    \"\"\"\n",
    "    Simplified pipeline runner that delegates to the execution engine.\n",
    "\n",
    "    This runner focuses on orchestration and reporting, delegating\n",
    "    actual execution to the simplified ExecutionEngine.\n",
    "\n",
    "    Implements abstracts.Runner interface while maintaining backward compatibility\n",
    "    with additional methods (run_full_refresh, run_validation_only).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        config: PipelineConfig,\n",
    "        bronze_steps: Optional[Dict[str, BronzeStep]] = None,\n",
    "        silver_steps: Optional[Dict[str, SilverStep]] = None,\n",
    "        gold_steps: Optional[Dict[str, GoldStep]] = None,\n",
    "        logger: Optional[PipelineLogger] = None,\n",
    "        functions: Optional[FunctionsProtocol] = None,\n",
    "        # Abstracts.Runner compatibility - these will be set if using abstracts interface\n",
    "        steps: Optional[list[BronzeStep | SilverStep | GoldStep]] = None,\n",
    "        engine: Optional[\n",
    "            Any\n",
    "        ] = None,  # Engine from abstracts, but we use ExecutionEngine\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the simplified pipeline runner.\n",
    "\n",
    "        Args:\n",
    "            spark: Active SparkSession instance\n",
    "            config: Pipeline configuration\n",
    "            bronze_steps: Bronze steps dictionary\n",
    "            silver_steps: Silver steps dictionary\n",
    "            gold_steps: Gold steps dictionary\n",
    "            logger: Optional logger instance\n",
    "            functions: Optional functions object for PySpark operations\n",
    "            steps: Optional list of steps (for abstracts.Runner compatibility)\n",
    "            engine: Optional engine (for abstracts.Runner compatibility, ignored)\n",
    "        \"\"\"\n",
    "        # Initialize abstracts.Runner with empty lists (we'll use our own step storage)\n",
    "        # This satisfies the abstract base class requirement\n",
    "        # Use Any for engine type to avoid type checking issues with _DummyEngine\n",
    "\n",
    "        dummy_engine: Any = _DummyEngine()\n",
    "        super().__init__(steps=[], engine=engine or dummy_engine)\n",
    "\n",
    "        self.spark = spark\n",
    "        self.config = config\n",
    "        self.bronze_steps = bronze_steps or {}\n",
    "        self.silver_steps = silver_steps or {}\n",
    "        self.gold_steps = gold_steps or {}\n",
    "        self.logger = logger or PipelineLogger()\n",
    "        self.functions = functions\n",
    "        self.execution_engine = ExecutionEngine(spark, config, self.logger, functions)\n",
    "\n",
    "        # If steps provided (from abstracts interface), convert to step dictionaries\n",
    "        if steps:\n",
    "            for step in steps:\n",
    "                if isinstance(step, BronzeStep):\n",
    "                    self.bronze_steps[step.name] = step\n",
    "                elif isinstance(step, SilverStep):\n",
    "                    self.silver_steps[step.name] = step\n",
    "                elif isinstance(step, GoldStep):\n",
    "                    self.gold_steps[step.name] = step\n",
    "\n",
    "    def run_pipeline(\n",
    "        self,\n",
    "        steps: list[BronzeStep | SilverStep | GoldStep],\n",
    "        mode: PipelineMode = PipelineMode.INITIAL,\n",
    "        bronze_sources: Dict[str, DataFrame] | None = None,\n",
    "    ) -> PipelineReport:\n",
    "        \"\"\"\n",
    "        Run a complete pipeline.\n",
    "\n",
    "        Args:\n",
    "            steps: List of pipeline steps to execute\n",
    "            mode: Pipeline execution mode\n",
    "            bronze_sources: Optional bronze source data\n",
    "\n",
    "        Returns:\n",
    "            PipelineReport with execution results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        pipeline_id = f\"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "        # Convert PipelineMode to ExecutionMode\n",
    "        execution_mode = self._convert_mode(mode)\n",
    "\n",
    "        try:\n",
    "            self.logger.info(f\"Starting pipeline execution: {pipeline_id}\")\n",
    "\n",
    "            # Prepare bronze sources if provided\n",
    "            if bronze_sources:\n",
    "                # Add bronze sources to context for execution\n",
    "                context = {}\n",
    "                for step in steps:\n",
    "                    if isinstance(step, BronzeStep) and step.name in bronze_sources:\n",
    "                        context[step.name] = bronze_sources[step.name]\n",
    "            else:\n",
    "                context = {}\n",
    "\n",
    "            # Execute pipeline using the execution engine\n",
    "            result = self.execution_engine.execute_pipeline(\n",
    "                steps, execution_mode, context=context\n",
    "            )\n",
    "\n",
    "            # Convert execution result to pipeline report\n",
    "            report = self._create_pipeline_report(\n",
    "                pipeline_id=pipeline_id,\n",
    "                mode=mode,\n",
    "                start_time=start_time,\n",
    "                execution_result=result,\n",
    "            )\n",
    "\n",
    "            self.logger.info(f\"Completed pipeline execution: {pipeline_id}\")\n",
    "            return report\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline execution failed: {e}\")\n",
    "            return self._create_error_report(\n",
    "                pipeline_id=pipeline_id, mode=mode, start_time=start_time, error=str(e)\n",
    "            )\n",
    "\n",
    "    def run_initial_load(\n",
    "        self,\n",
    "        bronze_sources: Optional[Dict[str, Source]] = None,\n",
    "        steps: Optional[\n",
    "            list\n",
    "        ] = None,  # Backward compatibility: old signature accepted steps as first arg\n",
    "    ) -> Report:  # PipelineReport satisfies Report Protocol\n",
    "        \"\"\"\n",
    "        Run initial load pipeline.\n",
    "\n",
    "        Implements abstracts.Runner.run_initial_load interface.\n",
    "        Also supports backward-compatible signature with steps parameter.\n",
    "\n",
    "        Args:\n",
    "            bronze_sources: Dictionary mapping bronze step names to Source (DataFrame), or None\n",
    "            steps: Optional list of steps (for backward compatibility with old signature)\n",
    "        \"\"\"\n",
    "        # Handle backward compatibility: if first arg is a list, treat it as steps\n",
    "        if isinstance(bronze_sources, list):\n",
    "            # Old signature: run_initial_load([steps])\n",
    "            steps = bronze_sources\n",
    "            bronze_sources = None\n",
    "\n",
    "        # Convert Source (Protocol) to DataFrame if needed\n",
    "        # Source Protocol is satisfied by DataFrame, so we accept any DataFrame-like object\n",
    "        bronze_sources_df: Optional[Dict[str, DataFrame]] = None\n",
    "        if bronze_sources:\n",
    "            bronze_sources_df = {}\n",
    "            for name, source in bronze_sources.items():\n",
    "                # Check if it's a DataFrame-like object (has DataFrame-like attributes)\n",
    "                # This works for both PySpark DataFrame and mock_spark DataFrame\n",
    "                if not (\n",
    "                    hasattr(source, \"columns\")\n",
    "                    and hasattr(source, \"count\")\n",
    "                    and hasattr(source, \"filter\")\n",
    "                ):\n",
    "                    raise TypeError(\n",
    "                        f\"bronze_sources must contain DataFrame-like objects, got {type(source)}\"\n",
    "                    )\n",
    "                bronze_sources_df[name] = source\n",
    "\n",
    "        # Use provided steps or stored steps\n",
    "        if steps is None:\n",
    "            steps = (\n",
    "                list(self.bronze_steps.values())\n",
    "                + list(self.silver_steps.values())\n",
    "                + list(self.gold_steps.values())\n",
    "            )\n",
    "\n",
    "        # PipelineReport satisfies Report Protocol structurally\n",
    "        return self.run_pipeline(steps, PipelineMode.INITIAL, bronze_sources_df)  # type: ignore[return-value]\n",
    "\n",
    "    def run_incremental(\n",
    "        self,\n",
    "        bronze_sources: Optional[Dict[str, Source]] = None,\n",
    "        steps: Optional[\n",
    "            list\n",
    "        ] = None,  # Backward compatibility: old signature accepted steps as first arg\n",
    "    ) -> Report:  # PipelineReport satisfies Report Protocol\n",
    "        \"\"\"\n",
    "        Run incremental pipeline with all stored steps.\n",
    "\n",
    "        Implements abstracts.Runner.run_incremental interface.\n",
    "        Also supports backward-compatible signature with steps parameter.\n",
    "\n",
    "        Args:\n",
    "            bronze_sources: Optional dictionary mapping bronze step names to Source (DataFrame), or None\n",
    "            steps: Optional list of steps (for backward compatibility with old signature)\n",
    "\n",
    "        Returns:\n",
    "            Report (PipelineReport) with execution results\n",
    "        \"\"\"\n",
    "        # Handle backward compatibility: if first arg is a list, treat it as steps\n",
    "        if isinstance(bronze_sources, list):\n",
    "            # Old signature: run_incremental([steps])\n",
    "            steps = bronze_sources\n",
    "            bronze_sources = None\n",
    "\n",
    "        # Convert Source (Protocol) to DataFrame if needed\n",
    "        # Source Protocol is satisfied by DataFrame, so we accept any DataFrame-like object\n",
    "        bronze_sources_df: Optional[Dict[str, DataFrame]] = None\n",
    "        if bronze_sources:\n",
    "            bronze_sources_df = {}\n",
    "            for name, source in bronze_sources.items():\n",
    "                # Check if it's a DataFrame-like object (has DataFrame-like attributes)\n",
    "                # This works for both PySpark DataFrame and mock_spark DataFrame\n",
    "                if not (\n",
    "                    hasattr(source, \"columns\")\n",
    "                    and hasattr(source, \"count\")\n",
    "                    and hasattr(source, \"filter\")\n",
    "                ):\n",
    "                    raise TypeError(\n",
    "                        f\"bronze_sources must contain DataFrame-like objects, got {type(source)}\"\n",
    "                    )\n",
    "                bronze_sources_df[name] = source\n",
    "\n",
    "        # Use provided steps or stored steps\n",
    "        if steps is None:\n",
    "            steps = (\n",
    "                list(self.bronze_steps.values())\n",
    "                + list(self.silver_steps.values())\n",
    "                + list(self.gold_steps.values())\n",
    "            )\n",
    "\n",
    "        # PipelineReport satisfies Report Protocol structurally\n",
    "        return self.run_pipeline(steps, PipelineMode.INCREMENTAL, bronze_sources_df)  # type: ignore[return-value]\n",
    "\n",
    "    def run_full_refresh(\n",
    "        self,\n",
    "        bronze_sources: Dict[str, DataFrame] | None = None,\n",
    "    ) -> PipelineReport:\n",
    "        \"\"\"\n",
    "        Run full refresh pipeline with all stored steps.\n",
    "\n",
    "        Args:\n",
    "            bronze_sources: Optional dictionary mapping bronze step names to DataFrames\n",
    "\n",
    "        Returns:\n",
    "            PipelineReport with execution results\n",
    "        \"\"\"\n",
    "        steps = (\n",
    "            list(self.bronze_steps.values())\n",
    "            + list(self.silver_steps.values())\n",
    "            + list(self.gold_steps.values())\n",
    "        )\n",
    "        return self.run_pipeline(steps, PipelineMode.FULL_REFRESH, bronze_sources)\n",
    "\n",
    "    def run_validation_only(\n",
    "        self,\n",
    "        bronze_sources: Dict[str, DataFrame] | None = None,\n",
    "    ) -> PipelineReport:\n",
    "        \"\"\"\n",
    "        Run validation-only pipeline with all stored steps.\n",
    "\n",
    "        Args:\n",
    "            bronze_sources: Optional dictionary mapping bronze step names to DataFrames\n",
    "\n",
    "        Returns:\n",
    "            PipelineReport with execution results\n",
    "        \"\"\"\n",
    "        steps = (\n",
    "            list(self.bronze_steps.values())\n",
    "            + list(self.silver_steps.values())\n",
    "            + list(self.gold_steps.values())\n",
    "        )\n",
    "        return self.run_pipeline(steps, PipelineMode.VALIDATION_ONLY, bronze_sources)\n",
    "\n",
    "    def _convert_mode(self, mode: PipelineMode) -> ExecutionMode:\n",
    "        \"\"\"Convert PipelineMode to ExecutionMode.\"\"\"\n",
    "        mode_map = {\n",
    "            PipelineMode.INITIAL: ExecutionMode.INITIAL,\n",
    "            PipelineMode.INCREMENTAL: ExecutionMode.INCREMENTAL,\n",
    "            PipelineMode.FULL_REFRESH: ExecutionMode.FULL_REFRESH,\n",
    "            PipelineMode.VALIDATION_ONLY: ExecutionMode.VALIDATION_ONLY,\n",
    "        }\n",
    "        return mode_map.get(mode, ExecutionMode.INITIAL)\n",
    "\n",
    "    def _create_pipeline_report(\n",
    "        self,\n",
    "        pipeline_id: str,\n",
    "        mode: PipelineMode,\n",
    "        start_time: datetime,\n",
    "        execution_result: ExecutionResult,\n",
    "    ) -> PipelineReport:\n",
    "        \"\"\"Create a pipeline report from execution result.\"\"\"\n",
    "        end_time = execution_result.end_time or datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "        # Count successful and failed steps\n",
    "        steps = execution_result.steps or []\n",
    "        successful_steps = [s for s in steps if s.status.value == \"completed\"]\n",
    "        failed_steps = [s for s in steps if s.status.value == \"failed\"]\n",
    "\n",
    "        # Import StepType for layer filtering\n",
    "        # from ..execution import StepType  # Removed: defined in notebook cells above\n",
    "\n",
    "        # Organize step results by layer (bronze/silver/gold)\n",
    "        bronze_results = {}\n",
    "        silver_results = {}\n",
    "        gold_results = {}\n",
    "\n",
    "        for step_result in steps:\n",
    "            step_info = {\n",
    "                \"status\": step_result.status.value,\n",
    "                \"duration\": step_result.duration,\n",
    "                \"rows_processed\": step_result.rows_processed,\n",
    "                \"output_table\": step_result.output_table,\n",
    "                \"start_time\": step_result.start_time.isoformat(),\n",
    "                \"end_time\": step_result.end_time.isoformat()\n",
    "                if step_result.end_time\n",
    "                else None,\n",
    "                \"write_mode\": step_result.write_mode,\n",
    "                \"validation_rate\": step_result.validation_rate,\n",
    "                \"rows_written\": step_result.rows_written,\n",
    "                \"input_rows\": step_result.input_rows,\n",
    "            }\n",
    "\n",
    "            # Add error if present\n",
    "            if step_result.error:\n",
    "                step_info[\"error\"] = step_result.error\n",
    "\n",
    "            # Add dataframe if available in context (for users who want to access output)\n",
    "            if hasattr(execution_result, \"context\"):\n",
    "                context = getattr(execution_result, \"context\", None)\n",
    "                if (\n",
    "                    context\n",
    "                    and isinstance(context, dict)\n",
    "                    and step_result.step_name in context\n",
    "                ):\n",
    "                    step_info[\"dataframe\"] = context[step_result.step_name]\n",
    "\n",
    "            # Categorize by step type\n",
    "            if step_result.step_type.value == \"bronze\":\n",
    "                bronze_results[step_result.step_name] = step_info\n",
    "            elif step_result.step_type.value == \"silver\":\n",
    "                silver_results[step_result.step_name] = step_info\n",
    "            elif step_result.step_type.value == \"gold\":\n",
    "                gold_results[step_result.step_name] = step_info\n",
    "\n",
    "        # Aggregate row counts from step results\n",
    "        total_rows_processed = sum(s.rows_processed or 0 for s in steps)\n",
    "        # For rows_written, only count Silver/Gold steps (those with output_table)\n",
    "        total_rows_written = sum(\n",
    "            s.rows_processed or 0 for s in steps if s.output_table is not None\n",
    "        )\n",
    "\n",
    "        # Calculate durations by layer\n",
    "        bronze_duration = sum(\n",
    "            s.duration or 0 for s in steps if s.step_type == StepType.BRONZE\n",
    "        )\n",
    "        silver_duration = sum(\n",
    "            s.duration or 0 for s in steps if s.step_type == StepType.SILVER\n",
    "        )\n",
    "        gold_duration = sum(\n",
    "            s.duration or 0 for s in steps if s.step_type == StepType.GOLD\n",
    "        )\n",
    "\n",
    "        return PipelineReport(\n",
    "            pipeline_id=pipeline_id,\n",
    "            execution_id=execution_result.execution_id,\n",
    "            status=(\n",
    "                PipelineStatus.COMPLETED\n",
    "                if execution_result.status == \"completed\"\n",
    "                else PipelineStatus.FAILED\n",
    "            ),\n",
    "            mode=mode,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            duration_seconds=duration,\n",
    "            metrics=PipelineMetrics(\n",
    "                total_steps=len(steps),\n",
    "                successful_steps=len(successful_steps),\n",
    "                failed_steps=len(failed_steps),\n",
    "                total_duration=duration,\n",
    "                bronze_duration=bronze_duration,\n",
    "                silver_duration=silver_duration,\n",
    "                gold_duration=gold_duration,\n",
    "                total_rows_processed=total_rows_processed,\n",
    "                total_rows_written=total_rows_written,\n",
    "                parallel_efficiency=execution_result.parallel_efficiency,\n",
    "            ),\n",
    "            bronze_results=bronze_results,\n",
    "            silver_results=silver_results,\n",
    "            gold_results=gold_results,\n",
    "            errors=[s.error for s in failed_steps if s.error],\n",
    "            warnings=[],\n",
    "            execution_groups_count=execution_result.execution_groups_count,\n",
    "            max_group_size=execution_result.max_group_size,\n",
    "        )\n",
    "\n",
    "    def _create_error_report(\n",
    "        self, pipeline_id: str, mode: PipelineMode, start_time: datetime, error: str\n",
    "    ) -> PipelineReport:\n",
    "        \"\"\"Create an error pipeline report.\"\"\"\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "        return PipelineReport(\n",
    "            pipeline_id=pipeline_id,\n",
    "            execution_id=f\"error_{pipeline_id}\",\n",
    "            status=PipelineStatus.FAILED,\n",
    "            mode=mode,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            duration_seconds=duration,\n",
    "            metrics=PipelineMetrics(\n",
    "                total_steps=0,\n",
    "                successful_steps=0,\n",
    "                failed_steps=0,\n",
    "                total_duration=duration,\n",
    "            ),\n",
    "            errors=[error],\n",
    "            warnings=[],\n",
    "        )\n",
    "\n",
    "\n",
    "class _DummyEngine:\n",
    "    \"\"\"Dummy engine for Runner.__init__ compatibility.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Alias for backward compatibility\n",
    "PipelineRunner = SimplePipelineRunner\n",
    "\n",
    "# Explicitly clear abstract methods since they are implemented\n",
    "# Python's ABC mechanism sometimes doesn't recognize implementations with positional-only args\n",
    "if hasattr(SimplePipelineRunner, \"__abstractmethods__\"):\n",
    "    SimplePipelineRunner.__abstractmethods__ = frozenset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.execution (pipeline_builder)\n",
    "#\n",
    "# Dependencies: compat, dependencies, models.pipeline, models.steps, pipeline_builder.functions, pipeline_builder_base.errors, pipeline_builder_base.logging, table_operations, validation.data_validation\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from .compat import DataFrame, F, SparkSession, is_mock_spark  # Removed: defined in notebook cells above\n",
    "# from .functions import FunctionsProtocol  # Removed: defined in notebook cells above\n",
    "# from .models import BronzeStep, GoldStep, SilverStep  # Removed: defined in notebook cells above\n",
    "# from .table_operations import fqn  # Removed: defined in notebook cells above\n",
    "# from .validation import apply_column_rules  # Removed: defined in notebook cells above\n",
    "# from .dependencies import DependencyAnalyzer  # Removed: defined in notebook cells above\n",
    "# from .errors import ExecutionError  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .models import (  # Removed: defined in notebook cells above\n",
    "# ExecutionContext,\n",
    "# ExecutionMode,\n",
    "# PipelineConfig,\n",
    "# PipelineMetrics,\n",
    "# StepResult,\n",
    "# )\n",
    "\n",
    "\n",
    "class StepStatus(Enum):\n",
    "    \"\"\"Step execution status.\"\"\"\n",
    "\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    SKIPPED = \"skipped\"\n",
    "\n",
    "\n",
    "class StepType(Enum):\n",
    "    \"\"\"Types of pipeline steps.\"\"\"\n",
    "\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepExecutionResult:\n",
    "    \"\"\"Result of step execution.\"\"\"\n",
    "\n",
    "    step_name: str\n",
    "    step_type: StepType\n",
    "    status: StepStatus\n",
    "    start_time: datetime\n",
    "    end_time: datetime | None = None\n",
    "    duration: float | None = None\n",
    "    error: str | None = None\n",
    "    rows_processed: int | None = None\n",
    "    output_table: str | None = None\n",
    "    write_mode: str | None = None\n",
    "    validation_rate: float = 100.0\n",
    "    rows_written: int | None = None\n",
    "    input_rows: int | None = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.end_time and self.start_time:\n",
    "            self.duration = (self.end_time - self.start_time).total_seconds()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExecutionResult:\n",
    "    \"\"\"Result of pipeline execution.\"\"\"\n",
    "\n",
    "    execution_id: str\n",
    "    mode: ExecutionMode\n",
    "    start_time: datetime\n",
    "    end_time: datetime | None = None\n",
    "    duration: float | None = None\n",
    "    status: str = \"running\"\n",
    "    steps: list[StepExecutionResult] | None = None\n",
    "    error: str | None = None\n",
    "    parallel_efficiency: float = 0.0\n",
    "    execution_groups_count: int = 0\n",
    "    max_group_size: int = 0\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.steps is None:\n",
    "            self.steps = []\n",
    "        if self.end_time and self.start_time:\n",
    "            self.duration = (self.end_time - self.start_time).total_seconds()\n",
    "\n",
    "\n",
    "class ExecutionEngine:\n",
    "    \"\"\"\n",
    "    Simplified execution engine for the framework pipelines.\n",
    "\n",
    "    This engine handles both individual step execution and full pipeline execution\n",
    "    with a clean, unified interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        config: PipelineConfig,\n",
    "        logger: PipelineLogger | None = None,\n",
    "        functions: FunctionsProtocol | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the execution engine.\n",
    "\n",
    "        Args:\n",
    "            spark: Active SparkSession instance\n",
    "            config: Pipeline configuration\n",
    "            logger: Optional logger instance\n",
    "            functions: Optional functions object for PySpark operations\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.config = config\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger()\n",
    "        else:\n",
    "            self.logger = logger\n",
    "\n",
    "        # Store functions for validation\n",
    "        if functions is None:\n",
    "            # from .functions import get_default_functions  # Removed: defined in notebook cells above\n",
    "\n",
    "            self.functions = get_default_functions()\n",
    "        else:\n",
    "            self.functions = functions\n",
    "\n",
    "    def _ensure_schema_exists(self, schema: str) -> None:\n",
    "        \"\"\"\n",
    "        Ensure a schema exists, creating it if necessary.\n",
    "\n",
    "        Args:\n",
    "            schema: Schema name to create\n",
    "\n",
    "        Raises:\n",
    "            ExecutionError: If schema creation fails\n",
    "        \"\"\"\n",
    "        # Check if schema already exists\n",
    "        try:\n",
    "            databases = [db.name for db in self.spark.catalog.listDatabases()]\n",
    "            if schema in databases:\n",
    "                return  # Schema already exists, nothing to do\n",
    "        except Exception:\n",
    "            pass  # If we can't check, try to create anyway\n",
    "\n",
    "        try:\n",
    "            # Try using mock-spark storage API if available (for mock-spark compatibility)\n",
    "            if hasattr(self.spark, \"storage\") and hasattr(\n",
    "                self.spark.storage, \"create_schema\"\n",
    "            ):\n",
    "                try:\n",
    "                    self.spark.storage.create_schema(schema)\n",
    "                    # Verify it was created\n",
    "                    databases = [db.name for db in self.spark.catalog.listDatabases()]\n",
    "                    if schema in databases:\n",
    "                        return  # Success\n",
    "                    else:\n",
    "                        raise ExecutionError(\n",
    "                            f\"Schema '{schema}' creation via storage API failed - schema not in catalog. \"\n",
    "                            f\"Available databases: {databases}\"\n",
    "                        )\n",
    "                except Exception as storage_error:\n",
    "                    # If storage API fails, fall through to SQL approach\n",
    "                    self.logger.debug(\n",
    "                        f\"Storage API schema creation failed: {storage_error}, trying SQL\"\n",
    "                    )\n",
    "\n",
    "            # Fall back to SQL for real Spark or if storage API not available\n",
    "            self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "            # Verify it was created\n",
    "            databases = [db.name for db in self.spark.catalog.listDatabases()]\n",
    "            if schema not in databases:\n",
    "                raise ExecutionError(\n",
    "                    f\"Schema '{schema}' creation via SQL failed - schema not in catalog. \"\n",
    "                    f\"Available databases: {databases}\"\n",
    "                )\n",
    "        except ExecutionError:\n",
    "            raise  # Re-raise ExecutionError\n",
    "        except Exception as e:\n",
    "            # Wrap other exceptions\n",
    "            raise ExecutionError(f\"Failed to create schema '{schema}': {str(e)}\") from e\n",
    "\n",
    "    def _ensure_materialized_for_validation(\n",
    "        self, df: DataFrame, rules: Dict[str, Any]\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Force DataFrame materialization before validation to avoid CTE optimization issues.\n",
    "\n",
    "        Mock-spark's CTE optimization can fail when validation rules reference columns\n",
    "        created by transforms (via withColumn). By materializing the DataFrame first,\n",
    "        we ensure all columns are available in the validation context.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to potentially materialize\n",
    "            rules: Validation rules dictionary\n",
    "\n",
    "        Returns:\n",
    "            Materialized DataFrame (or original if materialization not needed/available)\n",
    "        \"\"\"\n",
    "        # Check if rules reference columns that might be new (not in original input)\n",
    "        # For now, we'll materialize if rules exist and we're in mock-spark mode\n",
    "        # This is a conservative approach to avoid CTE issues\n",
    "        try:\n",
    "            # Check if we're using mock-spark\n",
    "            # from .compat import is_mock_spark  # Removed: defined in notebook cells above\n",
    "\n",
    "            if is_mock_spark() and rules:\n",
    "                # Force full materialization by collecting and recreating DataFrame\n",
    "                # This bypasses CTE optimization entirely\n",
    "                try:\n",
    "                    # Get schema first\n",
    "                    schema = df.schema\n",
    "\n",
    "                    # Collect data to force full materialization\n",
    "                    # This bypasses CTE optimization in mock-spark\n",
    "                    collected_data = df.collect()\n",
    "\n",
    "                    # Convert Row objects to dictionaries to preserve column names\n",
    "                    # This fixes a bug in mock-spark's Polars backend where Row objects\n",
    "                    # lose column names during materialization after filter operations\n",
    "                    if collected_data and hasattr(collected_data[0], \"asDict\"):\n",
    "                        # Convert Row objects to dictionaries\n",
    "                        dict_data = [row.asDict() for row in collected_data]\n",
    "                    elif collected_data:\n",
    "                        # Fallback: try to convert to dict if possible\n",
    "                        try:\n",
    "                            dict_data = [dict(row) for row in collected_data]\n",
    "                        except (TypeError, ValueError):\n",
    "                            # If conversion fails, use original data\n",
    "                            dict_data = collected_data\n",
    "                    else:\n",
    "                        dict_data = collected_data\n",
    "\n",
    "                    # Recreate DataFrame from dictionary data\n",
    "                    # This ensures all columns are fully materialized with correct names\n",
    "                    df = self.spark.createDataFrame(dict_data, schema)\n",
    "                except Exception as e:\n",
    "                    # If materialization fails, try alternative: just cache and count\n",
    "                    try:\n",
    "                        if hasattr(df, \"cache\"):\n",
    "                            df = df.cache()\n",
    "                        _ = df.count()  # Force evaluation\n",
    "                    except Exception:\n",
    "                        # If all materialization attempts fail, return original\n",
    "                        # Validation will still be attempted\n",
    "                        self.logger.debug(f\"Could not materialize DataFrame: {e}\")\n",
    "                        pass\n",
    "        except Exception:\n",
    "            # If we can't determine mock-spark status or materialization fails,\n",
    "            # return original DataFrame\n",
    "            pass\n",
    "\n",
    "        return df\n",
    "\n",
    "    def execute_step(\n",
    "        self,\n",
    "        step: BronzeStep | SilverStep | GoldStep,\n",
    "        context: Dict[str, DataFrame],\n",
    "        mode: ExecutionMode = ExecutionMode.INITIAL,\n",
    "    ) -> StepExecutionResult:\n",
    "        \"\"\"\n",
    "        Execute a single pipeline step.\n",
    "\n",
    "        Args:\n",
    "            step: The step to execute\n",
    "            context: Execution context with available DataFrames\n",
    "            mode: Execution mode\n",
    "\n",
    "        Returns:\n",
    "            StepExecutionResult with execution details\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        # Determine step type based on class\n",
    "        if isinstance(step, BronzeStep):\n",
    "            step_type = StepType.BRONZE\n",
    "        elif isinstance(step, SilverStep):\n",
    "            step_type = StepType.SILVER\n",
    "        elif isinstance(step, GoldStep):\n",
    "            step_type = StepType.GOLD\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown step type: {type(step)}\")\n",
    "\n",
    "        result = StepExecutionResult(\n",
    "            step_name=step.name,\n",
    "            step_type=step_type,\n",
    "            status=StepStatus.RUNNING,\n",
    "            start_time=start_time,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Use logger's step_start method for consistent formatting with emoji and uppercase\n",
    "            self.logger.step_start(step_type.value, step.name)\n",
    "\n",
    "            # Execute the step based on type\n",
    "            if isinstance(step, BronzeStep):\n",
    "                output_df = self._execute_bronze_step(step, context)\n",
    "            elif isinstance(step, SilverStep):\n",
    "                output_df = self._execute_silver_step(step, context, mode)\n",
    "            elif isinstance(step, GoldStep):\n",
    "                output_df = self._execute_gold_step(step, context)\n",
    "\n",
    "            # Apply validation if not in validation-only mode\n",
    "            validation_rate = 100.0\n",
    "            invalid_rows = 0\n",
    "            if mode != ExecutionMode.VALIDATION_ONLY:\n",
    "                # All step types (Bronze, Silver, Gold) have rules attribute\n",
    "                if step.rules:\n",
    "                    # CRITICAL: Force materialization before validation to avoid CTE optimization issues\n",
    "                    # When transforms create new columns with withColumn(), mock-spark's CTE optimization\n",
    "                    # can fail because those columns aren't visible in CTE context during validation.\n",
    "                    # Materializing ensures all columns are available.\n",
    "                    output_df = self._ensure_materialized_for_validation(\n",
    "                        output_df, step.rules\n",
    "                    )\n",
    "                    output_df, _, validation_stats = apply_column_rules(\n",
    "                        output_df,\n",
    "                        step.rules,\n",
    "                        \"pipeline\",\n",
    "                        step.name,\n",
    "                        functions=self.functions,\n",
    "                    )\n",
    "                    # Capture validation stats for logging (handle different return types for test mocking)\n",
    "                    if validation_stats is not None:\n",
    "                        validation_rate = getattr(\n",
    "                            validation_stats, \"validation_rate\", 100.0\n",
    "                        )\n",
    "                        invalid_rows = getattr(validation_stats, \"invalid_rows\", 0)\n",
    "\n",
    "            # Write output if not in validation-only mode\n",
    "            # Note: Bronze steps only validate data, they don't write to tables\n",
    "            if mode != ExecutionMode.VALIDATION_ONLY and not isinstance(\n",
    "                step, BronzeStep\n",
    "            ):\n",
    "                # Use table_name attribute for SilverStep and GoldStep\n",
    "                table_name = getattr(step, \"table_name\", step.name)\n",
    "                schema = getattr(step, \"schema\", None)\n",
    "\n",
    "                # Validate schema is provided\n",
    "                if schema is None:\n",
    "                    raise ExecutionError(\n",
    "                        f\"Step '{step.name}' requires a schema to be specified. \"\n",
    "                        f\"Silver and Gold steps must have a valid schema for table operations. \"\n",
    "                        f\"Please provide a schema when creating the step.\"\n",
    "                    )\n",
    "\n",
    "                output_table = fqn(schema, table_name)\n",
    "\n",
    "                # CRITICAL FIX for mock-spark threading issue:\n",
    "                # DuckDB connections in worker threads don't see schemas created in other threads.\n",
    "                # This issue persists in mock-spark 3.1.0 and earlier versions.\n",
    "                # We MUST create schema in THIS thread's connection right before saveAsTable.\n",
    "                # Try multiple methods to ensure schema is visible to this DuckDB connection.\n",
    "                try:\n",
    "                    # Method 1: Try SQL (most reliable for DuckDB in mock-spark)\n",
    "                    self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "                    # Method 2: Also try storage API if available (redundancy for mock-spark)\n",
    "                    if hasattr(self.spark, \"storage\") and hasattr(\n",
    "                        self.spark.storage, \"create_schema\"\n",
    "                    ):\n",
    "                        try:\n",
    "                            self.spark.storage.create_schema(schema)\n",
    "                        except Exception:\n",
    "                            pass  # SQL might be enough, continue\n",
    "                    # Method 3: Try catalog API as well\n",
    "                    try:\n",
    "                        self.spark.catalog.createDatabase(schema, ignoreIfExists=True)\n",
    "                    except Exception:\n",
    "                        pass  # SQL might be enough, continue\n",
    "                except Exception as e:\n",
    "                    # If all methods fail, raise error - schema creation is critical\n",
    "                    raise ExecutionError(\n",
    "                        f\"Failed to create schema '{schema}' before table creation: {e}\"\n",
    "                    ) from e\n",
    "\n",
    "                # Determine write mode\n",
    "                # - Gold steps always use overwrite to prevent duplicate aggregates\n",
    "                # - Silver steps append during incremental runs to preserve history\n",
    "                # - All other modes overwrite\n",
    "                if isinstance(step, GoldStep):\n",
    "                    write_mode_str = \"overwrite\"\n",
    "                elif mode == ExecutionMode.INCREMENTAL:\n",
    "                    write_mode_str = \"append\"\n",
    "                else:  # INITIAL or FULL_REFRESH\n",
    "                    write_mode_str = \"overwrite\"\n",
    "\n",
    "                output_df.write.mode(write_mode_str).saveAsTable(output_table)\n",
    "                result.output_table = output_table\n",
    "                result.rows_processed = output_df.count()\n",
    "\n",
    "                # Set write mode in result for tracking\n",
    "                result.write_mode = write_mode_str\n",
    "            elif isinstance(step, BronzeStep):\n",
    "                # Bronze steps only validate data, don't write to tables\n",
    "                result.rows_processed = output_df.count()\n",
    "                result.write_mode = None\n",
    "            else:  # VALIDATION_ONLY mode\n",
    "                # Validation-only mode doesn't write to tables\n",
    "                result.rows_processed = output_df.count()\n",
    "                result.write_mode = None\n",
    "\n",
    "            result.status = StepStatus.COMPLETED\n",
    "            result.end_time = datetime.now()\n",
    "            result.duration = (result.end_time - result.start_time).total_seconds()\n",
    "\n",
    "            # Populate result fields\n",
    "            rows_processed = result.rows_processed or 0\n",
    "            # For Silver/Gold steps, rows_written equals rows_processed (since we write the output)\n",
    "            # For Bronze steps, rows_written is None (they don't write to tables)\n",
    "            rows_written = rows_processed if not isinstance(step, BronzeStep) else None\n",
    "\n",
    "            result.rows_written = rows_written\n",
    "            result.input_rows = rows_processed\n",
    "            result.validation_rate = (\n",
    "                validation_rate if validation_rate is not None else 100.0\n",
    "            )\n",
    "\n",
    "            # Use logger's step_complete method for consistent formatting with emoji and uppercase\n",
    "            self.logger.step_complete(\n",
    "                step_type.value,\n",
    "                step.name,\n",
    "                result.duration,\n",
    "                rows_processed=rows_processed,\n",
    "                rows_written=rows_written,\n",
    "                invalid_rows=invalid_rows,\n",
    "                validation_rate=validation_rate,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            result.status = StepStatus.FAILED\n",
    "            result.error = str(e)\n",
    "            result.end_time = datetime.now()\n",
    "            result.duration = (result.end_time - result.start_time).total_seconds()\n",
    "\n",
    "            # Log step failure\n",
    "            self.logger.error(\n",
    "                f\"❌ Failed {step_type.value.upper()} step: {step.name} ({result.duration:.2f}s) - {str(e)}\"\n",
    "            )\n",
    "            raise ExecutionError(f\"Step execution failed: {e}\") from e\n",
    "\n",
    "        return result\n",
    "\n",
    "    def execute_pipeline(\n",
    "        self,\n",
    "        steps: list[BronzeStep | SilverStep | GoldStep],\n",
    "        mode: ExecutionMode = ExecutionMode.INITIAL,\n",
    "        max_workers: int = 4,\n",
    "        context: Dict[str, DataFrame] | None = None,\n",
    "    ) -> ExecutionResult:\n",
    "        \"\"\"\n",
    "        Execute a complete pipeline with smart dependency-aware parallel execution.\n",
    "\n",
    "        This method automatically analyzes step dependencies and executes independent\n",
    "        steps in parallel across all layers (Bronze, Silver, Gold). Steps within each\n",
    "        execution group run concurrently using ThreadPoolExecutor, while groups are\n",
    "        executed sequentially to respect dependencies.\n",
    "\n",
    "        Parallelism is controlled by the PipelineConfig.parallel settings:\n",
    "        - If parallel.enabled is True, uses parallel.max_workers (default: 4)\n",
    "        - If parallel.enabled is False, executes sequentially (max_workers=1)\n",
    "        - The max_workers parameter is ignored; config settings take precedence\n",
    "\n",
    "        Args:\n",
    "            steps: List of steps to execute\n",
    "            mode: Execution mode (INITIAL, INCREMENTAL, FULL_REFRESH, VALIDATION_ONLY)\n",
    "            max_workers: Deprecated - use PipelineConfig.parallel.max_workers instead\n",
    "            context: Optional initial execution context with DataFrames\n",
    "\n",
    "        Returns:\n",
    "            ExecutionResult with execution details and parallel execution metrics\n",
    "\n",
    "        Example:\n",
    "            >>> # Default config enables parallel execution with 4 workers\n",
    "            >>> config = PipelineConfig.create_default(schema=\"my_schema\")\n",
    "            >>> engine = ExecutionEngine(spark, config)\n",
    "            >>> result = engine.execute_pipeline(steps=[bronze, silver1, silver2, gold])\n",
    "            >>> print(f\"Parallel efficiency: {result.parallel_efficiency:.2f}\")\n",
    "            >>> print(f\"Execution groups: {result.execution_groups_count}\")\n",
    "        \"\"\"\n",
    "        execution_id = str(uuid.uuid4())\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        result = ExecutionResult(\n",
    "            execution_id=execution_id,\n",
    "            mode=mode,\n",
    "            start_time=start_time,\n",
    "            status=\"running\",\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Logging is handled by the runner to avoid duplicate messages\n",
    "            # Ensure all required schemas exist before parallel execution (required in mock-spark due to DuckDB threading)\n",
    "            # This MUST happen in the main thread before any worker threads start\n",
    "            # Collect unique schemas from all steps\n",
    "            required_schemas = set()\n",
    "            for step in steps:\n",
    "                if hasattr(step, \"schema\") and step.schema:\n",
    "                    schema_value = step.schema\n",
    "                    # Handle both string schemas and Mock objects (for tests)\n",
    "                    if isinstance(schema_value, str):\n",
    "                        required_schemas.add(schema_value)\n",
    "            # Create all required schemas upfront - always try to create, don't rely on catalog checks\n",
    "            # This is critical for mock-spark where DuckDB connections in worker threads\n",
    "            # don't see schemas created via catalog API, so we must create them in main thread first\n",
    "            for schema in required_schemas:\n",
    "                try:\n",
    "                    # Always try to create schema - CREATE SCHEMA IF NOT EXISTS is idempotent\n",
    "                    self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "                    # Also use _ensure_schema_exists as backup (tries multiple methods)\n",
    "                    self._ensure_schema_exists(schema)\n",
    "                except Exception as e:\n",
    "                    # Log but don't fail - schema might already exist or creation might work later\n",
    "                    self.logger.debug(f\"Schema '{schema}' pre-creation attempt: {e}\")\n",
    "\n",
    "            # Validate context parameter\n",
    "            if context is None:\n",
    "                context = {}\n",
    "            elif not isinstance(context, dict):\n",
    "                raise TypeError(f\"context must be a dictionary, got {type(context)}\")\n",
    "\n",
    "            # Group steps by type for dependency analysis\n",
    "            bronze_steps = [s for s in steps if isinstance(s, BronzeStep)]\n",
    "            silver_steps = [s for s in steps if isinstance(s, SilverStep)]\n",
    "            gold_steps = [s for s in steps if isinstance(s, GoldStep)]\n",
    "\n",
    "            # Build dependency graph and get execution groups\n",
    "            analyzer = DependencyAnalyzer()\n",
    "            analysis = analyzer.analyze_dependencies(\n",
    "                bronze_steps={s.name: s for s in bronze_steps},\n",
    "                silver_steps={s.name: s for s in silver_steps},\n",
    "                gold_steps={s.name: s for s in gold_steps},\n",
    "            )\n",
    "\n",
    "            execution_groups = analysis.execution_groups\n",
    "            result.execution_groups_count = len(execution_groups)\n",
    "            result.max_group_size = (\n",
    "                max(len(group) for group in execution_groups) if execution_groups else 0\n",
    "            )\n",
    "\n",
    "            # Log dependency analysis results\n",
    "            self.logger.info(\n",
    "                f\"Dependency analysis complete: {len(execution_groups)} execution groups, \"\n",
    "                f\"max group size: {result.max_group_size}\"\n",
    "            )\n",
    "\n",
    "            # Determine worker count from config\n",
    "            # After PipelineConfig.__post_init__, parallel is always ParallelConfig\n",
    "            # But handle mocked configs gracefully\n",
    "            # from .models import ParallelConfig  # Removed: defined in notebook cells above\n",
    "\n",
    "            if isinstance(self.config.parallel, ParallelConfig):\n",
    "                if self.config.parallel.enabled:\n",
    "                    workers = self.config.parallel.max_workers\n",
    "                    self.logger.info(\n",
    "                        f\"Parallel execution enabled with {workers} workers\"\n",
    "                    )\n",
    "                else:\n",
    "                    workers = 1\n",
    "                    self.logger.info(\"Sequential execution mode\")\n",
    "            elif hasattr(self.config.parallel, \"enabled\"):\n",
    "                # Handle Mock or other types with enabled attribute\n",
    "                enabled = getattr(self.config.parallel, \"enabled\", True)\n",
    "                if enabled:\n",
    "                    workers = getattr(self.config.parallel, \"max_workers\", 4)\n",
    "                else:\n",
    "                    workers = 1\n",
    "            else:\n",
    "                # Fallback for tests with mock configs\n",
    "                workers = 1\n",
    "                self.logger.info(\"Sequential execution mode (default)\")\n",
    "\n",
    "            # Thread-safe context management\n",
    "            context_lock = threading.Lock()\n",
    "\n",
    "            # Create a mapping of step names to step objects\n",
    "            step_map = {s.name: s for s in steps}\n",
    "\n",
    "            # Track timing for parallel efficiency calculation\n",
    "            group_timings = []\n",
    "\n",
    "            # Execute each group in parallel\n",
    "            for group_idx, group in enumerate(execution_groups):\n",
    "                group_start = datetime.now()\n",
    "                self.logger.info(\n",
    "                    f\"Executing group {group_idx + 1}/{len(execution_groups)}: \"\n",
    "                    f\"{len(group)} steps - {', '.join(group)}\"\n",
    "                )\n",
    "\n",
    "                if workers > 1:\n",
    "                    # Parallel execution\n",
    "                    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "                        futures = {}\n",
    "                        for step_name in group:\n",
    "                            if step_name not in step_map:\n",
    "                                self.logger.warning(\n",
    "                                    f\"Step {step_name} in execution group but not found in step list\"\n",
    "                                )\n",
    "                                continue\n",
    "\n",
    "                            step = step_map[step_name]\n",
    "                            future = executor.submit(\n",
    "                                self._execute_step_safe,\n",
    "                                step,\n",
    "                                context,\n",
    "                                mode,\n",
    "                                context_lock,\n",
    "                            )\n",
    "                            futures[future] = step_name\n",
    "\n",
    "                        # Wait for all steps in group to complete\n",
    "                        for future in as_completed(futures):\n",
    "                            step_name = futures[future]\n",
    "                            try:\n",
    "                                step_result = future.result()\n",
    "                                if result.steps is not None:\n",
    "                                    result.steps.append(step_result)\n",
    "\n",
    "                                if step_result.status == StepStatus.FAILED:\n",
    "                                    self.logger.error(\n",
    "                                        f\"Step {step_name} failed: {step_result.error}\"\n",
    "                                    )\n",
    "                            except Exception as e:\n",
    "                                self.logger.error(\n",
    "                                    f\"Exception executing step {step_name}: {e}\"\n",
    "                                )\n",
    "                                # Determine correct step type\n",
    "                                step_obj = step_map.get(step_name)\n",
    "                                if step_obj is not None and isinstance(\n",
    "                                    step_obj, BronzeStep\n",
    "                                ):\n",
    "                                    step_type_enum = StepType.BRONZE\n",
    "                                elif step_obj is not None and isinstance(\n",
    "                                    step_obj, SilverStep\n",
    "                                ):\n",
    "                                    step_type_enum = StepType.SILVER\n",
    "                                elif step_obj is not None and isinstance(\n",
    "                                    step_obj, GoldStep\n",
    "                                ):\n",
    "                                    step_type_enum = StepType.GOLD\n",
    "                                else:\n",
    "                                    step_type_enum = StepType.BRONZE  # fallback\n",
    "\n",
    "                                # Create failed step result\n",
    "                                step_result = StepExecutionResult(\n",
    "                                    step_name=step_name,\n",
    "                                    step_type=step_type_enum,\n",
    "                                    status=StepStatus.FAILED,\n",
    "                                    error=str(e),\n",
    "                                    start_time=datetime.now(),\n",
    "                                    end_time=datetime.now(),\n",
    "                                    duration=0.0,\n",
    "                                )\n",
    "                                if result.steps is not None:\n",
    "                                    result.steps.append(step_result)\n",
    "                else:\n",
    "                    # Sequential execution (workers == 1)\n",
    "                    for step_name in group:\n",
    "                        if step_name not in step_map:\n",
    "                            self.logger.warning(\n",
    "                                f\"Step {step_name} in execution group but not found in step list\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        step = step_map[step_name]\n",
    "                        try:\n",
    "                            step_result = self._execute_step_safe(\n",
    "                                step, context, mode, context_lock\n",
    "                            )\n",
    "                            if result.steps is not None:\n",
    "                                result.steps.append(step_result)\n",
    "\n",
    "                            if step_result.status == StepStatus.FAILED:\n",
    "                                self.logger.error(\n",
    "                                    f\"Step {step_name} failed: {step_result.error}\"\n",
    "                                )\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(\n",
    "                                f\"Exception executing step {step_name}: {e}\"\n",
    "                            )\n",
    "                            # Determine correct step type\n",
    "                            step_obj = step_map.get(step_name)\n",
    "                            if step_obj is not None and isinstance(\n",
    "                                step_obj, BronzeStep\n",
    "                            ):\n",
    "                                step_type_enum = StepType.BRONZE\n",
    "                            elif step_obj is not None and isinstance(\n",
    "                                step_obj, SilverStep\n",
    "                            ):\n",
    "                                step_type_enum = StepType.SILVER\n",
    "                            elif step_obj is not None and isinstance(\n",
    "                                step_obj, GoldStep\n",
    "                            ):\n",
    "                                step_type_enum = StepType.GOLD\n",
    "                            else:\n",
    "                                step_type_enum = StepType.BRONZE  # fallback\n",
    "\n",
    "                            step_result = StepExecutionResult(\n",
    "                                step_name=step_name,\n",
    "                                step_type=step_type_enum,\n",
    "                                status=StepStatus.FAILED,\n",
    "                                error=str(e),\n",
    "                                start_time=datetime.now(),\n",
    "                                end_time=datetime.now(),\n",
    "                                duration=0.0,\n",
    "                            )\n",
    "                            if result.steps is not None:\n",
    "                                result.steps.append(step_result)\n",
    "\n",
    "                group_end = datetime.now()\n",
    "                group_duration = (group_end - group_start).total_seconds()\n",
    "                group_timings.append((len(group), group_duration))\n",
    "                self.logger.info(\n",
    "                    f\"Group {group_idx + 1} completed in {group_duration:.2f}s\"\n",
    "                )\n",
    "\n",
    "            # Calculate parallel efficiency\n",
    "            if result.steps:\n",
    "                total_step_time = sum(\n",
    "                    s.duration for s in result.steps if s.duration is not None\n",
    "                )\n",
    "                total_wall_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "                if total_wall_time > 0 and workers > 1:\n",
    "                    # Efficiency = (total sequential time / total parallel time) / workers\n",
    "                    # This gives a ratio of how well we utilized parallelism\n",
    "                    ideal_parallel_time = total_step_time / workers\n",
    "                    result.parallel_efficiency = min(\n",
    "                        (ideal_parallel_time / total_wall_time) * 100, 100.0\n",
    "                    )\n",
    "                else:\n",
    "                    result.parallel_efficiency = (\n",
    "                        100.0  # Sequential execution is 100% efficient\n",
    "                    )\n",
    "\n",
    "            # Determine overall pipeline status based on step results\n",
    "            if result.steps is None:\n",
    "                result.steps = []\n",
    "            step_results: list[StepExecutionResult] = result.steps\n",
    "            failed_steps = [s for s in step_results if s.status == StepStatus.FAILED]\n",
    "\n",
    "            if failed_steps:\n",
    "                result.status = \"failed\"\n",
    "                self.logger.error(\n",
    "                    f\"Pipeline execution failed: {len(failed_steps)} steps failed\"\n",
    "                )\n",
    "            else:\n",
    "                result.status = \"completed\"\n",
    "                self.logger.info(\n",
    "                    f\"Completed pipeline execution: {execution_id} - \"\n",
    "                    f\"Parallel efficiency: {result.parallel_efficiency:.1f}%\"\n",
    "                )\n",
    "\n",
    "            result.end_time = datetime.now()\n",
    "\n",
    "        except Exception as e:\n",
    "            result.status = \"failed\"\n",
    "            result.error = str(e)\n",
    "            result.end_time = datetime.now()\n",
    "            self.logger.error(f\"Pipeline execution failed: {e}\")\n",
    "            raise ExecutionError(f\"Pipeline execution failed: {e}\") from e\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _execute_step_safe(\n",
    "        self,\n",
    "        step: BronzeStep | SilverStep | GoldStep,\n",
    "        context: Dict[str, DataFrame],\n",
    "        mode: ExecutionMode,\n",
    "        context_lock: threading.Lock,\n",
    "    ) -> StepExecutionResult:\n",
    "        \"\"\"\n",
    "        Execute a step with thread-safe context access.\n",
    "\n",
    "        This method wraps execute_step() to provide thread-safe access to the\n",
    "        shared execution context when running steps in parallel.\n",
    "\n",
    "        Args:\n",
    "            step: The step to execute\n",
    "            context: Shared execution context with available DataFrames\n",
    "            mode: Execution mode\n",
    "            context_lock: Threading lock for thread-safe context access\n",
    "\n",
    "        Returns:\n",
    "            StepExecutionResult with execution details\n",
    "        \"\"\"\n",
    "        # CRITICAL: Ensure schema exists in THIS worker thread before execution\n",
    "        # mock-spark has DuckDB threading issues where schemas created in one thread\n",
    "        # are not visible to DuckDB connections in other threads. We serialize schema creation\n",
    "        # with a lock, but the real fix is in execute_step() where we CREATE SCHEMA right before saveAsTable.\n",
    "        # This is just a safety check.\n",
    "        if hasattr(step, \"schema\") and step.schema:\n",
    "            with context_lock:\n",
    "                # Try to ensure schema exists (serialized to avoid race conditions)\n",
    "                try:\n",
    "                    # Use SQL to ensure schema exists (more reliable than storage API in threads)\n",
    "                    self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {step.schema}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(\n",
    "                        f\"Schema '{step.schema}' creation in worker thread (non-critical): {e}\"\n",
    "                    )\n",
    "\n",
    "        # Read from context with lock to get a snapshot\n",
    "        with context_lock:\n",
    "            local_context = dict(context)\n",
    "\n",
    "        # Execute step (this can happen in parallel without lock)\n",
    "        result = self.execute_step(step, local_context, mode)\n",
    "\n",
    "        # Write to context with lock (for Silver/Gold steps that write tables)\n",
    "        if result.status == StepStatus.COMPLETED and not isinstance(step, BronzeStep):\n",
    "            with context_lock:\n",
    "                # Get table name and schema from step\n",
    "                table_name = getattr(step, \"table_name\", step.name)\n",
    "                schema = getattr(step, \"schema\", None)\n",
    "\n",
    "                if schema is not None:\n",
    "                    # Add the step's output table to context for downstream steps\n",
    "                    context[step.name] = self.spark.table(fqn(schema, table_name))\n",
    "                else:\n",
    "                    self.logger.warning(\n",
    "                        f\"Step '{step.name}' completed but has no schema. \"\n",
    "                        f\"Cannot add to context for downstream steps.\"\n",
    "                    )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _execute_bronze_step(\n",
    "        self, step: BronzeStep, context: Dict[str, DataFrame]\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Execute a bronze step.\"\"\"\n",
    "        # Bronze steps require data to be provided in context\n",
    "        # This is the expected behavior - bronze steps validate existing data\n",
    "        if step.name not in context:\n",
    "            raise ExecutionError(\n",
    "                f\"Bronze step '{step.name}' requires data to be provided in context. \"\n",
    "                f\"Bronze steps are for validating existing data, not creating it. \"\n",
    "                f\"Please provide data using bronze_sources parameter or context dictionary. \"\n",
    "                f\"Available context keys: {list(context.keys())}\"\n",
    "            )\n",
    "\n",
    "        df = context[step.name]\n",
    "\n",
    "        # Validate that the DataFrame is not empty (optional check)\n",
    "        if df.count() == 0:\n",
    "            self.logger.warning(\n",
    "                f\"Bronze step '{step.name}' received empty DataFrame. \"\n",
    "                f\"This may indicate missing or invalid data source.\"\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _execute_silver_step(\n",
    "        self,\n",
    "        step: SilverStep,\n",
    "        context: Dict[str, DataFrame],\n",
    "        mode: ExecutionMode,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Execute a silver step.\"\"\"\n",
    "\n",
    "        # Get source bronze data\n",
    "        if step.source_bronze not in context:\n",
    "            raise ExecutionError(\n",
    "                f\"Source bronze step {step.source_bronze} not found in context\"\n",
    "            )\n",
    "\n",
    "        bronze_df = context[step.source_bronze]\n",
    "\n",
    "        if mode == ExecutionMode.INCREMENTAL:\n",
    "            bronze_df = self._filter_incremental_bronze_input(step, bronze_df)\n",
    "\n",
    "        # Apply transform with source bronze data and empty silvers dict\n",
    "        return step.transform(self.spark, bronze_df, {})\n",
    "\n",
    "    def _filter_incremental_bronze_input(\n",
    "        self, step: SilverStep, bronze_df: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Filter bronze input rows that were already processed in previous incremental runs.\n",
    "\n",
    "        Uses the source bronze step's incremental column and the silver step's watermark\n",
    "        column to eliminate rows whose incremental value is less than or equal to the\n",
    "        last processed watermark.\n",
    "        \"\"\"\n",
    "\n",
    "        incremental_col = getattr(step, \"source_incremental_col\", None)\n",
    "        watermark_col = getattr(step, \"watermark_col\", None)\n",
    "        schema = getattr(step, \"schema\", None)\n",
    "        table_name = getattr(step, \"table_name\", step.name)\n",
    "\n",
    "        if not incremental_col or not watermark_col or schema is None:\n",
    "            return bronze_df\n",
    "\n",
    "        if incremental_col not in getattr(bronze_df, \"columns\", []):\n",
    "            self.logger.debug(\n",
    "                f\"Silver step {step.name}: incremental column '{incremental_col}' \"\n",
    "                f\"not present in bronze DataFrame; skipping incremental filter\"\n",
    "            )\n",
    "            return bronze_df\n",
    "\n",
    "        output_table = fqn(schema, table_name)\n",
    "\n",
    "        try:\n",
    "            existing_table = self.spark.table(output_table)\n",
    "        except Exception as exc:\n",
    "            self.logger.debug(\n",
    "                f\"Silver step {step.name}: unable to read existing table {output_table} \"\n",
    "                f\"for incremental filter: {exc}\"\n",
    "            )\n",
    "            return bronze_df\n",
    "\n",
    "        if watermark_col not in getattr(existing_table, \"columns\", []):\n",
    "            self.logger.debug(\n",
    "                f\"Silver step {step.name}: watermark column '{watermark_col}' \"\n",
    "                f\"not present in existing table {output_table}; skipping incremental filter\"\n",
    "            )\n",
    "            return bronze_df\n",
    "\n",
    "        try:\n",
    "            watermark_rows = existing_table.select(watermark_col).collect()\n",
    "        except Exception as exc:\n",
    "            self.logger.warning(\n",
    "                f\"Silver step {step.name}: failed to collect watermark values \"\n",
    "                f\"from {output_table}: {exc}\"\n",
    "            )\n",
    "            return bronze_df\n",
    "\n",
    "        if not watermark_rows:\n",
    "            return bronze_df\n",
    "\n",
    "        cutoff_value = None\n",
    "        for row in watermark_rows:\n",
    "            value = None\n",
    "            if hasattr(row, \"__getitem__\"):\n",
    "                try:\n",
    "                    value = row[watermark_col]\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        value = row[0]\n",
    "                    except Exception:\n",
    "                        value = None\n",
    "            if value is None and hasattr(row, \"asDict\"):\n",
    "                value = row.asDict().get(watermark_col)\n",
    "            if value is None:\n",
    "                continue\n",
    "            cutoff_value = value if cutoff_value is None else max(cutoff_value, value)\n",
    "\n",
    "        if cutoff_value is None:\n",
    "            return bronze_df\n",
    "\n",
    "        try:\n",
    "            filtered_df = bronze_df.filter(F.col(incremental_col) > F.lit(cutoff_value))\n",
    "        except Exception as exc:\n",
    "            if self._using_mock_spark():\n",
    "                mock_df = self._filter_bronze_rows_mock(\n",
    "                    bronze_df, incremental_col, cutoff_value\n",
    "                )\n",
    "                if mock_df is not None:\n",
    "                    self.logger.debug(\n",
    "                        f\"Silver step {step.name}: applied mock fallback filter \"\n",
    "                        f\"for {incremental_col} > {cutoff_value}\"\n",
    "                    )\n",
    "                    filtered_df = mock_df\n",
    "                else:\n",
    "                    self.logger.warning(\n",
    "                        f\"Silver step {step.name}: failed to filter bronze rows using \"\n",
    "                        f\"{incremental_col} > {cutoff_value}: {exc!r}\"\n",
    "                    )\n",
    "                    return bronze_df\n",
    "            else:\n",
    "                self.logger.warning(\n",
    "                    f\"Silver step {step.name}: failed to filter bronze rows using \"\n",
    "                    f\"{incremental_col} > {cutoff_value}: {exc!r}\"\n",
    "                )\n",
    "                return bronze_df\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Silver step {step.name}: filtering bronze rows where \"\n",
    "            f\"{incremental_col} <= {cutoff_value}\"\n",
    "        )\n",
    "        return filtered_df\n",
    "\n",
    "    def _using_mock_spark(self) -> bool:\n",
    "        \"\"\"Determine if current spark session is backed by mock-spark.\"\"\"\n",
    "\n",
    "        try:\n",
    "            spark_module = type(self.spark).__module__\n",
    "        except Exception:\n",
    "            spark_module = \"\"\n",
    "        return is_mock_spark() or \"mock_spark\" in spark_module\n",
    "\n",
    "    def _filter_bronze_rows_mock(\n",
    "        self, bronze_df: DataFrame, incremental_col: str, cutoff_value: object\n",
    "    ) -> DataFrame | None:\n",
    "        \"\"\"\n",
    "        Mock-spark fallback: collect rows and filter in-memory when column operations fail.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            rows = bronze_df.collect()\n",
    "            schema = bronze_df.schema\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "        filtered_rows = []\n",
    "        for row in rows:\n",
    "            value = self._extract_row_value(row, incremental_col)\n",
    "            if value is None:\n",
    "                continue\n",
    "            try:\n",
    "                if value > cutoff_value:\n",
    "                    filtered_rows.append(row)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not filtered_rows:\n",
    "            try:\n",
    "                return bronze_df.limit(0)\n",
    "            except Exception:\n",
    "                pass\n",
    "            return self.spark.createDataFrame([], schema)\n",
    "\n",
    "        try:\n",
    "            column_order: list[str] = []\n",
    "            if hasattr(schema, \"__iter__\"):\n",
    "                column_order = [getattr(field, \"name\", field) for field in schema]\n",
    "            if not column_order and hasattr(schema, \"fieldNames\"):\n",
    "                column_order = list(schema.fieldNames())\n",
    "            if not column_order and hasattr(schema, \"names\"):\n",
    "                column_order = list(schema.names)\n",
    "            if not column_order:\n",
    "                column_order = list(getattr(bronze_df, \"columns\", []))\n",
    "            structured_rows = []\n",
    "            for row in filtered_rows:\n",
    "                if hasattr(row, \"asDict\"):\n",
    "                    row_dict = row.asDict()\n",
    "                    structured_rows.append(\n",
    "                        tuple(row_dict.get(col) for col in column_order)\n",
    "                    )\n",
    "                else:\n",
    "                    structured_rows.append(tuple(row))\n",
    "            return self.spark.createDataFrame(\n",
    "                structured_rows, schema, verifySchema=False\n",
    "            )\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_row_value(row: Any, column: str) -> object | None:\n",
    "        \"\"\"Safely extract a column value from a Row-like object.\"\"\"\n",
    "        if hasattr(row, \"__getitem__\"):\n",
    "            try:\n",
    "                return row[column]\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return row[0]\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if hasattr(row, \"asDict\"):\n",
    "            try:\n",
    "                return row.asDict().get(column)\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    def _execute_gold_step(\n",
    "        self, step: GoldStep, context: Dict[str, DataFrame]\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Execute a gold step.\"\"\"\n",
    "\n",
    "        # Build silvers dict from source_silvers\n",
    "        silvers = {}\n",
    "        if step.source_silvers is not None:\n",
    "            for silver_name in step.source_silvers:\n",
    "                if silver_name not in context:\n",
    "                    raise ExecutionError(\n",
    "                        f\"Source silver {silver_name} not found in context\"\n",
    "                    )\n",
    "                silvers[silver_name] = context[silver_name]\n",
    "\n",
    "        return step.transform(self.spark, silvers)\n",
    "\n",
    "\n",
    "# Backward compatibility aliases\n",
    "UnifiedExecutionEngine = ExecutionEngine\n",
    "UnifiedStepExecutionResult = StepExecutionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.pipeline.builder (pipeline_builder)\n",
    "#\n",
    "# Dependencies: models.base, models.pipeline, models.steps, pipeline_builder.compat, pipeline_builder.functions, pipeline_builder.pipeline.runner, pipeline_builder.types, pipeline_builder_base.errors, pipeline_builder_base.logging, validation.data_validation, validation.pipeline_validation\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "from abstracts.builder import PipelineBuilder as AbstractsPipelineBuilder\n",
    "\n",
    "# from ..compat import DataFrame, SparkSession  # Removed: defined in notebook cells above\n",
    "# from ..engine import SparkEngine  # Removed: defined in notebook cells above\n",
    "# from ..functions import FunctionsProtocol, get_default_functions  # Removed: defined in notebook cells above\n",
    "# from ..models import (  # Removed: defined in notebook cells above\n",
    "# BronzeStep,\n",
    "# GoldStep,\n",
    "# SilverStep,\n",
    "# )\n",
    "# from .errors import (  # Removed: defined in notebook cells above\n",
    "# ConfigurationError as PipelineConfigurationError,\n",
    "# ExecutionError as StepError,\n",
    "# )\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .models import (  # Removed: defined in notebook cells above\n",
    "# ParallelConfig,\n",
    "# PipelineConfig,\n",
    "# ValidationThresholds,\n",
    "# )\n",
    "# from ..types import (  # Removed: defined in notebook cells above\n",
    "# ColumnRules,\n",
    "# GoldTransformFunction,\n",
    "# SilverTransformFunction,\n",
    "# StepName,\n",
    "# TableName,\n",
    "# )\n",
    "# from ..validation import UnifiedValidator as UnifiedValidator  # Removed: defined in notebook cells above\n",
    "# from ..validation import _convert_rules_to_expressions  # Removed: defined in notebook cells above\n",
    "# from .runner import PipelineRunner  # Removed: defined in notebook cells above\n",
    "\n",
    "\n",
    "class PipelineBuilder:\n",
    "    \"\"\"\n",
    "    Production-ready builder for creating data pipelines with Bronze → Silver → Gold architecture.\n",
    "\n",
    "    The PipelineBuilder provides a fluent API for constructing robust data pipelines with\n",
    "    comprehensive validation, automatic dependency management, and enterprise-grade features.\n",
    "\n",
    "    Key Features:\n",
    "    - **Fluent API**: Chain methods for intuitive pipeline construction\n",
    "    - **Robust Validation**: Early error detection with clear validation messages\n",
    "    - **Auto-inference**: Automatic dependency detection and validation\n",
    "    - **String Rules**: Convert human-readable rules to PySpark expressions\n",
    "    - **Multi-schema Support**: Cross-schema data flows for enterprise environments\n",
    "    - **Comprehensive Error Handling**: Detailed error messages with suggestions\n",
    "\n",
    "    Validation Requirements:\n",
    "        All pipeline steps must have validation rules. Invalid configurations are rejected\n",
    "        during construction with clear error messages.\n",
    "\n",
    "    Example:\n",
    "        from the framework import PipelineBuilder\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        # Initialize builder\n",
    "        builder = PipelineBuilder(spark=spark, schema=\"analytics\")\n",
    "\n",
    "        # Bronze: Raw data validation (required)\n",
    "        builder.with_bronze_rules(\n",
    "            name=\"events\",\n",
    "            rules={\"user_id\": [\"not_null\"], \"timestamp\": [\"not_null\"]},  # String rules\n",
    "            incremental_col=\"timestamp\"\n",
    "        )\n",
    "\n",
    "        # Silver: Data transformation (required)\n",
    "        builder.add_silver_transform(\n",
    "            name=\"clean_events\",\n",
    "            source_bronze=\"events\",\n",
    "            transform=lambda spark, df, silvers: df.filter(F.col(\"value\") > 0),\n",
    "            rules={\"value\": [\"gt\", 0]},  # String rules\n",
    "            table_name=\"clean_events\"\n",
    "        )\n",
    "\n",
    "        # Gold: Business analytics (required)\n",
    "        builder.add_gold_transform(\n",
    "            name=\"daily_metrics\",\n",
    "            transform=lambda spark, silvers: silvers[\"clean_events\"].groupBy(\"date\").agg(F.count(\"*\").alias(\"count\")),\n",
    "            rules={\"count\": [\"gt\", 0]},  # String rules\n",
    "            table_name=\"daily_metrics\",\n",
    "            source_silvers=[\"clean_events\"]\n",
    "        )\n",
    "\n",
    "        # Build and execute pipeline\n",
    "        pipeline = builder.to_pipeline()\n",
    "        result = pipeline.run_initial_load(bronze_sources={\"events\": source_df})\n",
    "\n",
    "    String Rules Support:\n",
    "        You can use human-readable string rules that are automatically converted to PySpark expressions:\n",
    "\n",
    "        - \"not_null\" → F.col(\"column\").isNotNull()\n",
    "        - \"gt\", value → F.col(\"column\") > value\n",
    "        - \"lt\", value → F.col(\"column\") < value\n",
    "        - \"eq\", value → F.col(\"column\") == value\n",
    "        - \"in\", [values] → F.col(\"column\").isin(values)\n",
    "        - \"between\", min, max → F.col(\"column\").between(min, max)\n",
    "\n",
    "    Args:\n",
    "        spark: Active SparkSession instance\n",
    "        schema: Target schema name for pipeline tables\n",
    "        quality_thresholds: Validation thresholds for each layer (default: Bronze=90%, Silver=95%, Gold=98%)\n",
    "        parallel_config: Parallel execution configuration\n",
    "        logger: Optional logger instance\n",
    "\n",
    "    Raises:\n",
    "        ValidationError: If validation rules are invalid or missing\n",
    "        ConfigurationError: If configuration parameters are invalid\n",
    "        StepError: If step dependencies cannot be resolved\n",
    "\n",
    "    Example:\n",
    "        >>> from the framework import PipelineBuilder\n",
    "        >>> from pyspark.sql import SparkSession, functions as F\n",
    "        >>>\n",
    "        >>> spark = SparkSession.builder.appName(\"My Pipeline\").getOrCreate()\n",
    "        >>> builder = PipelineBuilder(spark=spark, schema=\"my_schema\")\n",
    "        >>>\n",
    "        >>> # Bronze layer - raw data validation\n",
    "        >>> builder.with_bronze_rules(\n",
    "        ...     name=\"events\",\n",
    "        ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "        ...     incremental_col=\"timestamp\"\n",
    "        ... )\n",
    "        >>>\n",
    "        >>> # Silver layer - data transformation\n",
    "        >>> builder.add_silver_transform(\n",
    "        ...     name=\"clean_events\",\n",
    "        ...     source_bronze=\"events\",\n",
    "        ...     transform=lambda spark, df, silvers: df.filter(F.col(\"status\") == \"active\"),\n",
    "        ...     rules={\"status\": [F.col(\"status\").isNotNull()]},\n",
    "        ...     table_name=\"clean_events\",\n",
    "        ...     watermark_col=\"timestamp\"\n",
    "        ... )\n",
    "        >>>\n",
    "        >>> # Gold layer - business analytics\n",
    "        >>> builder.add_gold_transform(\n",
    "        ...     name=\"user_analytics\",\n",
    "        ...     transform=lambda spark, silvers: silvers[\"clean_events\"].groupBy(\"user_id\").count(),\n",
    "        ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "        ...     table_name=\"user_analytics\",\n",
    "        ...     source_silvers=[\"clean_events\"]\n",
    "        ... )\n",
    "        >>>\n",
    "        >>> # Build and execute pipeline\n",
    "        >>> pipeline = builder.to_pipeline()\n",
    "        >>> result = pipeline.initial_load(bronze_sources={\"events\": source_df})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        spark: SparkSession,\n",
    "        schema: str,\n",
    "        min_bronze_rate: float = 95.0,\n",
    "        min_silver_rate: float = 98.0,\n",
    "        min_gold_rate: float = 99.0,\n",
    "        verbose: bool = True,\n",
    "        functions: FunctionsProtocol | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a new PipelineBuilder instance.\n",
    "\n",
    "        Args:\n",
    "            spark: Active SparkSession instance for data processing\n",
    "            schema: Database schema name where tables will be created\n",
    "            min_bronze_rate: Minimum data quality rate for Bronze layer (0-100)\n",
    "            min_silver_rate: Minimum data quality rate for Silver layer (0-100)\n",
    "            min_gold_rate: Minimum data quality rate for Gold layer (0-100)\n",
    "            verbose: Enable verbose logging output\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If quality rates are not between 0 and 100\n",
    "            RuntimeError: If Spark session is not active\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        if not spark:\n",
    "            raise PipelineConfigurationError(\n",
    "                \"Spark session is required\",\n",
    "                suggestions=[\n",
    "                    \"Ensure SparkSession is properly initialized\",\n",
    "                    \"Check Spark configuration\",\n",
    "                ],\n",
    "            )\n",
    "        if not schema:\n",
    "            raise PipelineConfigurationError(\n",
    "                \"Schema name cannot be empty\",\n",
    "                suggestions=[\n",
    "                    \"Provide a valid schema name\",\n",
    "                    \"Check database configuration\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Store configuration\n",
    "        thresholds = ValidationThresholds(\n",
    "            bronze=min_bronze_rate, silver=min_silver_rate, gold=min_gold_rate\n",
    "        )\n",
    "        # Use default parallel config (enabled with 4 workers)\n",
    "        parallel_config = ParallelConfig.create_default()\n",
    "        self.config = PipelineConfig(\n",
    "            schema=schema,\n",
    "            thresholds=thresholds,\n",
    "            parallel=parallel_config,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # Initialize components\n",
    "        self.spark = spark\n",
    "        self.logger = PipelineLogger(verbose=verbose)\n",
    "        self.validator = UnifiedValidator(self.logger)\n",
    "        self.functions = functions if functions is not None else get_default_functions()\n",
    "\n",
    "        # Expose schema for backward compatibility\n",
    "        self.schema = schema\n",
    "        self.pipeline_id = (\n",
    "            f\"pipeline_{schema}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        )\n",
    "\n",
    "        # Expose validators for backward compatibility\n",
    "        self.validators = self.validator.custom_validators\n",
    "\n",
    "        # Pipeline definition\n",
    "        self.bronze_steps: Dict[str, BronzeStep] = {}\n",
    "        self.silver_steps: Dict[str, SilverStep] = {}\n",
    "        self.gold_steps: Dict[str, GoldStep] = {}\n",
    "\n",
    "        # Create SparkEngine for abstracts layer\n",
    "        self.spark_engine = SparkEngine(\n",
    "            spark=self.spark,\n",
    "            config=self.config,\n",
    "            logger=self.logger,\n",
    "            functions=self.functions,\n",
    "        )\n",
    "\n",
    "        # Create abstracts.PipelineBuilder with SparkEngine injection\n",
    "        # We'll use PipelineRunner as the runner class\n",
    "        self._abstracts_builder = AbstractsPipelineBuilder(\n",
    "            runner_cls=PipelineRunner,\n",
    "            engine=self.spark_engine,\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"🔧 PipelineBuilder initialized (schema: {schema})\")\n",
    "\n",
    "    def with_bronze_rules(\n",
    "        self,\n",
    "        *,\n",
    "        name: StepName,\n",
    "        rules: ColumnRules,\n",
    "        incremental_col: str | None = None,\n",
    "        description: str | None = None,\n",
    "        schema: str | None = None,\n",
    "    ) -> PipelineBuilder:\n",
    "        \"\"\"\n",
    "        Add Bronze layer validation rules for raw data ingestion.\n",
    "\n",
    "        Bronze steps represent the first layer of the Medallion Architecture,\n",
    "        handling raw data ingestion and initial validation. All Bronze steps\n",
    "        must have non-empty validation rules.\n",
    "\n",
    "        Args:\n",
    "            name: Unique identifier for this Bronze step\n",
    "            rules: Dictionary mapping column names to validation rule lists.\n",
    "                   Supports both PySpark Column expressions and string rules:\n",
    "                   - PySpark: {\"user_id\": [F.col(\"user_id\").isNotNull()]}\n",
    "                   - String: {\"user_id\": [\"not_null\"], \"age\": [\"gt\", 0]}\n",
    "            incremental_col: Column name for incremental processing (e.g., \"timestamp\", \"updated_at\").\n",
    "                            If provided, enables incremental processing with append mode.\n",
    "            description: Optional description of this Bronze step\n",
    "            schema: Optional schema name for reading bronze data. If not provided, uses the builder's default schema.\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "\n",
    "        Raises:\n",
    "            ValidationError: If rules are empty or invalid\n",
    "            ConfigurationError: If step name conflicts or configuration is invalid\n",
    "\n",
    "        Example:\n",
    "            >>> # Using PySpark Column expressions\n",
    "            >>> builder.with_bronze_rules(\n",
    "            ...     name=\"events\",\n",
    "            ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "            ...     incremental_col=\"timestamp\"\n",
    "            ... )\n",
    "\n",
    "            >>> # Using string rules (automatically converted)\n",
    "            >>> builder.with_bronze_rules(\n",
    "            ...     name=\"users\",\n",
    "            ...     rules={\"user_id\": [\"not_null\"], \"age\": [\"gt\", 0], \"status\": [\"in\", [\"active\", \"inactive\"]]},\n",
    "            ...     incremental_col=\"updated_at\"\n",
    "            ... )\n",
    "\n",
    "        String Rules Support:\n",
    "            - \"not_null\" → F.col(\"column\").isNotNull()\n",
    "            - \"gt\", value → F.col(\"column\") > value\n",
    "            - \"lt\", value → F.col(\"column\") < value\n",
    "            - \"eq\", value → F.col(\"column\") == value\n",
    "            - \"in\", [values] → F.col(\"column\").isin(values)\n",
    "            - \"between\", min, max → F.col(\"column\").between(min, max)\n",
    "            ...     name=\"user_events\",\n",
    "            ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "            ...     incremental_col=\"timestamp\",\n",
    "            ...     schema=\"raw_data\"  # Read from different schema\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            raise StepError(\n",
    "                \"Bronze step name cannot be empty\",\n",
    "                context={\"step_name\": name or \"unknown\", \"step_type\": \"bronze\"},\n",
    "                suggestions=[\n",
    "                    \"Provide a valid step name\",\n",
    "                    \"Check step naming conventions\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        if name in self.bronze_steps:\n",
    "            raise StepError(\n",
    "                f\"Bronze step '{name}' already exists\",\n",
    "                context={\"step_name\": name, \"step_type\": \"bronze\"},\n",
    "                suggestions=[\n",
    "                    \"Use a different step name\",\n",
    "                    \"Remove the existing step first\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Validate schema if provided\n",
    "        if schema is not None:\n",
    "            self._validate_schema(schema)\n",
    "\n",
    "        # Convert string rules to PySpark Column objects\n",
    "        converted_rules = _convert_rules_to_expressions(rules, self.functions)\n",
    "\n",
    "        # Create bronze step\n",
    "        bronze_step = BronzeStep(\n",
    "            name=name,\n",
    "            rules=converted_rules,\n",
    "            incremental_col=incremental_col,\n",
    "            schema=schema,\n",
    "        )\n",
    "\n",
    "        self.bronze_steps[name] = bronze_step\n",
    "        self.logger.info(f\"✅ Added Bronze step: {name}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def with_silver_rules(\n",
    "        self,\n",
    "        *,\n",
    "        name: StepName,\n",
    "        table_name: TableName,\n",
    "        rules: ColumnRules,\n",
    "        watermark_col: str | None = None,\n",
    "        description: str | None = None,\n",
    "        schema: str | None = None,\n",
    "    ) -> PipelineBuilder:\n",
    "        \"\"\"\n",
    "        Add existing Silver layer table for validation and monitoring.\n",
    "\n",
    "        This method is used when you have an existing Silver table that you want to\n",
    "        include in the pipeline for validation and monitoring purposes, but don't\n",
    "        need to transform the data.\n",
    "\n",
    "        Args:\n",
    "            name: Unique identifier for this Silver step\n",
    "            table_name: Existing Delta table name\n",
    "            rules: Dictionary mapping column names to validation rule lists\n",
    "            watermark_col: Column name for watermarking (optional)\n",
    "            description: Optional description of this Silver step\n",
    "            schema: Optional schema name for reading silver data. If not provided, uses the builder's default schema.\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "\n",
    "        Example:\n",
    "            >>> builder.with_silver_rules(\n",
    "            ...     name=\"existing_clean_events\",\n",
    "            ...     table_name=\"clean_events\",\n",
    "            ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "            ...     watermark_col=\"updated_at\",\n",
    "            ...     schema=\"staging\"  # Read from different schema\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            raise StepError(\n",
    "                \"Silver step name cannot be empty\",\n",
    "                context={\"step_name\": name or \"unknown\", \"step_type\": \"silver\"},\n",
    "                suggestions=[\n",
    "                    \"Provide a valid step name\",\n",
    "                    \"Check step naming conventions\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        if name in self.silver_steps:\n",
    "            raise StepError(\n",
    "                f\"Silver step '{name}' already exists\",\n",
    "                context={\"step_name\": name, \"step_type\": \"silver\"},\n",
    "                suggestions=[\n",
    "                    \"Use a different step name\",\n",
    "                    \"Remove the existing step first\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Validate schema if provided\n",
    "        if schema is not None:\n",
    "            self._validate_schema(schema)\n",
    "\n",
    "        # Create SilverStep for existing table\n",
    "        # Create a dummy transform function for existing tables\n",
    "        def dummy_transform_func(\n",
    "            spark: SparkSession,\n",
    "            bronze_df: DataFrame,\n",
    "            prior_silvers: Dict[str, DataFrame],\n",
    "        ) -> DataFrame:\n",
    "            return bronze_df\n",
    "\n",
    "        # Type the function properly\n",
    "        dummy_transform: SilverTransformFunction = dummy_transform_func\n",
    "\n",
    "        # Convert string rules to PySpark Column objects\n",
    "        converted_rules = _convert_rules_to_expressions(rules, self.functions)\n",
    "\n",
    "        silver_step = SilverStep(\n",
    "            name=name,\n",
    "            source_bronze=\"\",  # No source for existing tables\n",
    "            transform=dummy_transform,\n",
    "            rules=converted_rules,\n",
    "            table_name=table_name,\n",
    "            watermark_col=watermark_col,\n",
    "            existing=True,\n",
    "            schema=schema,\n",
    "            source_incremental_col=None,\n",
    "        )\n",
    "\n",
    "        self.silver_steps[name] = silver_step\n",
    "        self.logger.info(f\"✅ Added existing Silver step: {name}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def add_validator(self, validator: Any) -> PipelineBuilder:\n",
    "        \"\"\"\n",
    "        Add a custom step validator to the pipeline.\n",
    "\n",
    "        Custom validators allow you to add additional validation logic\n",
    "        beyond the built-in validation rules.\n",
    "\n",
    "        Args:\n",
    "            validator: Custom validator implementing StepValidator protocol\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "\n",
    "        Example:\n",
    "            >>> class CustomValidator(StepValidator):\n",
    "            ...     def validate(self, step, context):\n",
    "            ...         if step.name == \"special_step\":\n",
    "            ...             return [\"Special validation failed\"]\n",
    "            ...         return []\n",
    "            >>>\n",
    "            >>> builder.add_validator(CustomValidator())\n",
    "        \"\"\"\n",
    "        self.validator.add_validator(validator)\n",
    "        return self\n",
    "\n",
    "    def add_silver_transform(\n",
    "        self,\n",
    "        *,\n",
    "        name: StepName,\n",
    "        source_bronze: StepName | None = None,\n",
    "        transform: SilverTransformFunction,\n",
    "        rules: ColumnRules,\n",
    "        table_name: TableName,\n",
    "        watermark_col: str | None = None,\n",
    "        description: str | None = None,\n",
    "        depends_on: list[StepName] | None = None,\n",
    "        schema: str | None = None,\n",
    "    ) -> PipelineBuilder:\n",
    "        \"\"\"\n",
    "        Add Silver layer transformation step for data cleaning and enrichment.\n",
    "\n",
    "        Silver steps represent the second layer of the Medallion Architecture,\n",
    "        transforming raw Bronze data into clean, business-ready datasets. All Silver steps\n",
    "        must have non-empty validation rules and a valid transform function.\n",
    "\n",
    "        Args:\n",
    "            name: Unique identifier for this Silver step\n",
    "            source_bronze: Name of the Bronze step this Silver step depends on.\n",
    "                          If not provided, will automatically infer from the most recent\n",
    "                          with_bronze_rules() call. If no bronze steps exist, will raise an error.\n",
    "            transform: Transformation function with signature:\n",
    "                     (spark: SparkSession, bronze_df: DataFrame, prior_silvers: Dict[str, DataFrame]) -> DataFrame\n",
    "                     Must be callable and cannot be None.\n",
    "            rules: Dictionary mapping column names to validation rule lists.\n",
    "                   Supports both PySpark Column expressions and string rules:\n",
    "                   - PySpark: {\"user_id\": [F.col(\"user_id\").isNotNull()]}\n",
    "                   - String: {\"user_id\": [\"not_null\"], \"age\": [\"gt\", 0]}\n",
    "            table_name: Target Delta table name where results will be stored\n",
    "            watermark_col: Column name for watermarking (e.g., \"timestamp\", \"updated_at\").\n",
    "                          If provided, enables incremental processing with append mode.\n",
    "            description: Optional description of this Silver step\n",
    "            depends_on: List of other Silver step names that must complete before this step.\n",
    "            schema: Optional schema name for writing silver data. If not provided, uses the builder's default schema.\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "\n",
    "        Raises:\n",
    "            ValidationError: If rules are empty, transform is None, or configuration is invalid\n",
    "            ConfigurationError: If step name conflicts or dependencies cannot be resolved\n",
    "\n",
    "        Example:\n",
    "            >>> def clean_user_events(spark, bronze_df, prior_silvers):\n",
    "            ...     return (bronze_df\n",
    "            ...         .filter(F.col(\"user_id\").isNotNull())\n",
    "            ...         .withColumn(\"event_date\", F.date_trunc(\"day\", \"timestamp\"))\n",
    "            ...     )\n",
    "            >>>\n",
    "            >>> # Using PySpark Column expressions\n",
    "            >>> builder.add_silver_transform(\n",
    "            ...     name=\"clean_events\",\n",
    "            ...     source_bronze=\"user_events\",\n",
    "            ...     transform=clean_user_events,\n",
    "            ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "            ...     table_name=\"clean_events\"\n",
    "            ... )\n",
    "\n",
    "            >>> # Using string rules (automatically converted)\n",
    "            >>> builder.add_silver_transform(\n",
    "            ...     name=\"enriched_events\",\n",
    "            ...     source_bronze=\"user_events\",\n",
    "            ...     transform=lambda spark, df, silvers: df.withColumn(\"processed_at\", F.current_timestamp()),\n",
    "            ...     rules={\"user_id\": [\"not_null\"], \"processed_at\": [\"not_null\"]},\n",
    "            ...     table_name=\"enriched_events\",\n",
    "            ...     watermark_col=\"processed_at\"\n",
    "            ... )\n",
    "\n",
    "        String Rules Support:\n",
    "            - \"not_null\" → F.col(\"column\").isNotNull()\n",
    "            - \"gt\", value → F.col(\"column\") > value\n",
    "            - \"lt\", value → F.col(\"column\") < value\n",
    "            - \"eq\", value → F.col(\"column\") == value\n",
    "            - \"in\", [values] → F.col(\"column\").isin(values)\n",
    "            - \"between\", min, max → F.col(\"column\").between(min, max)\n",
    "            ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "            ...     table_name=\"clean_user_events\",\n",
    "            ...     watermark_col=\"timestamp\"\n",
    "            ... )\n",
    "            >>>\n",
    "            >>> # Auto-infer source_bronze from most recent with_bronze_rules()\n",
    "            >>> builder.add_silver_transform(\n",
    "            ...     name=\"enriched_events\",\n",
    "            ...     transform=enrich_user_events,\n",
    "            ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "            ...     table_name=\"enriched_user_events\",\n",
    "            ...     schema=\"processing\"  # Write to different schema\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            raise StepError(\n",
    "                \"Silver step name cannot be empty\",\n",
    "                context={\"step_name\": name or \"unknown\", \"step_type\": \"silver\"},\n",
    "                suggestions=[\n",
    "                    \"Provide a valid step name\",\n",
    "                    \"Check step naming conventions\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        if name in self.silver_steps:\n",
    "            raise StepError(\n",
    "                f\"Silver step '{name}' already exists\",\n",
    "                context={\"step_name\": name, \"step_type\": \"silver\"},\n",
    "                suggestions=[\n",
    "                    \"Use a different step name\",\n",
    "                    \"Remove the existing step first\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Auto-infer source_bronze if not provided\n",
    "        if source_bronze is None:\n",
    "            if not self.bronze_steps:\n",
    "                raise StepError(\n",
    "                    \"No bronze steps available for auto-inference\",\n",
    "                    context={\"step_name\": name, \"step_type\": \"silver\"},\n",
    "                    suggestions=[\n",
    "                        \"Add a bronze step first using with_bronze_rules()\",\n",
    "                        \"Explicitly specify source_bronze parameter\",\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "            # Use the most recently added bronze step\n",
    "            source_bronze = list(self.bronze_steps.keys())[-1]\n",
    "            self.logger.info(f\"🔍 Auto-inferred source_bronze: {source_bronze}\")\n",
    "\n",
    "        # Validate that the source_bronze exists\n",
    "        if source_bronze not in self.bronze_steps:\n",
    "            raise StepError(\n",
    "                f\"Bronze step '{source_bronze}' not found\",\n",
    "                context={\"step_name\": name, \"step_type\": \"silver\"},\n",
    "                suggestions=[\n",
    "                    f\"Available bronze steps: {list(self.bronze_steps.keys())}\",\n",
    "                    \"Add the bronze step first using with_bronze_rules()\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Note: Dependency validation is deferred to validate_pipeline()\n",
    "        # This allows for more flexible pipeline construction\n",
    "\n",
    "        # Use builder's schema if not provided\n",
    "        if schema is None:\n",
    "            schema = self.config.schema\n",
    "        else:\n",
    "            self._validate_schema(schema)\n",
    "\n",
    "        # Convert string rules to PySpark Column objects\n",
    "        converted_rules = _convert_rules_to_expressions(rules, self.functions)\n",
    "\n",
    "        # Capture the incremental column from the source bronze step (if any)\n",
    "        source_incremental_col = self.bronze_steps[source_bronze].incremental_col\n",
    "\n",
    "        # Create silver step\n",
    "        silver_step = SilverStep(\n",
    "            name=name,\n",
    "            source_bronze=source_bronze,\n",
    "            transform=transform,\n",
    "            rules=converted_rules,\n",
    "            table_name=table_name,\n",
    "            watermark_col=watermark_col,\n",
    "            schema=schema,\n",
    "            source_incremental_col=source_incremental_col,\n",
    "        )\n",
    "\n",
    "        self.silver_steps[name] = silver_step\n",
    "        self.logger.info(f\"✅ Added Silver step: {name} (source: {source_bronze})\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def add_gold_transform(\n",
    "        self,\n",
    "        *,\n",
    "        name: StepName,\n",
    "        transform: GoldTransformFunction,\n",
    "        rules: ColumnRules,\n",
    "        table_name: TableName,\n",
    "        source_silvers: list[StepName] | None = None,\n",
    "        description: str | None = None,\n",
    "        schema: str | None = None,\n",
    "    ) -> PipelineBuilder:\n",
    "        \"\"\"\n",
    "        Add Gold layer transformation step for business analytics and aggregations.\n",
    "\n",
    "        Gold steps represent the third layer of the Medallion Architecture,\n",
    "        creating business-ready datasets for analytics and reporting. All Gold steps\n",
    "        must have non-empty validation rules and a valid transform function.\n",
    "\n",
    "        Args:\n",
    "            name: Unique identifier for this Gold step\n",
    "            transform: Transformation function with signature:\n",
    "                     (spark: SparkSession, silvers: Dict[str, DataFrame]) -> DataFrame\n",
    "                     Must be callable and cannot be None.\n",
    "            rules: Dictionary mapping column names to validation rule lists.\n",
    "                   Supports both PySpark Column expressions and string rules:\n",
    "                   - PySpark: {\"user_id\": [F.col(\"user_id\").isNotNull()]}\n",
    "                   - String: {\"user_id\": [\"not_null\"], \"count\": [\"gt\", 0]}\n",
    "            table_name: Target Delta table name where results will be stored\n",
    "            source_silvers: List of Silver step names this Gold step depends on.\n",
    "                           If not provided, will automatically use all available Silver steps.\n",
    "                           If no Silver steps exist, will raise an error.\n",
    "            description: Optional description of this Gold step\n",
    "            schema: Optional schema name for writing gold data. If not provided, uses the builder's default schema.\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "\n",
    "        Raises:\n",
    "            ValidationError: If rules are empty, transform is None, or configuration is invalid\n",
    "            ConfigurationError: If step name conflicts or dependencies cannot be resolved\n",
    "\n",
    "        Example:\n",
    "            >>> def user_daily_metrics(spark, silvers):\n",
    "            ...     events_df = silvers[\"clean_events\"]\n",
    "            ...     return (events_df\n",
    "            ...         .groupBy(\"user_id\", \"event_date\")\n",
    "            ...         .agg(F.count(\"*\").alias(\"event_count\"))\n",
    "            ...     )\n",
    "            >>>\n",
    "            >>> # Using PySpark Column expressions\n",
    "            >>> builder.add_gold_transform(\n",
    "            ...     name=\"user_metrics\",\n",
    "            ...     transform=user_daily_metrics,\n",
    "            ...     rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\n",
    "            ...     table_name=\"user_daily_metrics\",\n",
    "            ...     source_silvers=[\"clean_events\"]\n",
    "            ... )\n",
    "\n",
    "            >>> # Using string rules (automatically converted)\n",
    "            >>> builder.add_gold_transform(\n",
    "            ...     name=\"daily_analytics\",\n",
    "            ...     transform=lambda spark, silvers: silvers[\"clean_events\"].groupBy(\"date\").agg(F.count(\"*\").alias(\"count\")),\n",
    "            ...     rules={\"date\": [\"not_null\"], \"count\": [\"gt\", 0]},\n",
    "            ...     table_name=\"daily_analytics\",\n",
    "            ...     source_silvers=[\"clean_events\"]\n",
    "            ... )\n",
    "\n",
    "        String Rules Support:\n",
    "            - \"not_null\" → F.col(\"column\").isNotNull()\n",
    "            - \"gt\", value → F.col(\"column\") > value\n",
    "            - \"lt\", value → F.col(\"column\") < value\n",
    "            - \"eq\", value → F.col(\"column\") == value\n",
    "            - \"in\", [values] → F.col(\"column\").isin(values)\n",
    "            - \"between\", min, max → F.col(\"column\").between(min, max)\n",
    "            >>> # Auto-infer source_silvers from all available Silver steps\n",
    "            >>> builder.add_gold_transform(\n",
    "            ...     name=\"daily_analytics\",\n",
    "            ...     transform=daily_analytics,\n",
    "            ...     rules={\"event_date\": [F.col(\"event_date\").isNotNull()]},\n",
    "            ...     table_name=\"daily_analytics\",\n",
    "            ...     schema=\"analytics\"  # Write to different schema\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        if not name:\n",
    "            raise StepError(\n",
    "                \"Gold step name cannot be empty\",\n",
    "                context={\"step_name\": name or \"unknown\", \"step_type\": \"gold\"},\n",
    "                suggestions=[\n",
    "                    \"Provide a valid step name\",\n",
    "                    \"Check step naming conventions\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        if name in self.gold_steps:\n",
    "            raise StepError(\n",
    "                f\"Gold step '{name}' already exists\",\n",
    "                context={\"step_name\": name, \"step_type\": \"gold\"},\n",
    "                suggestions=[\n",
    "                    \"Use a different step name\",\n",
    "                    \"Remove the existing step first\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Auto-infer source_silvers if not provided\n",
    "        if source_silvers is None:\n",
    "            if not self.silver_steps:\n",
    "                raise StepError(\n",
    "                    \"No silver steps available for auto-inference\",\n",
    "                    context={\"step_name\": name, \"step_type\": \"gold\"},\n",
    "                    suggestions=[\n",
    "                        \"Add a silver step first using add_silver_transform()\",\n",
    "                        \"Explicitly specify source_silvers parameter\",\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "            # Use all available silver steps\n",
    "            source_silvers = list(self.silver_steps.keys())\n",
    "            self.logger.info(f\"🔍 Auto-inferred source_silvers: {source_silvers}\")\n",
    "\n",
    "        # Validate that all source_silvers exist\n",
    "        invalid_silvers = [s for s in source_silvers if s not in self.silver_steps]\n",
    "        if invalid_silvers:\n",
    "            raise StepError(\n",
    "                f\"Silver steps not found: {invalid_silvers}\",\n",
    "                context={\"step_name\": name, \"step_type\": \"gold\"},\n",
    "                suggestions=[\n",
    "                    f\"Available silver steps: {list(self.silver_steps.keys())}\",\n",
    "                    \"Add the missing silver steps first using add_silver_transform()\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Note: Dependency validation is deferred to validate_pipeline()\n",
    "        # This allows for more flexible pipeline construction\n",
    "\n",
    "        # Use builder's schema if not provided\n",
    "        if schema is None:\n",
    "            schema = self.config.schema\n",
    "        else:\n",
    "            self._validate_schema(schema)\n",
    "\n",
    "        # Convert string rules to PySpark Column objects\n",
    "        converted_rules = _convert_rules_to_expressions(rules, self.functions)\n",
    "\n",
    "        # Create gold step\n",
    "        gold_step = GoldStep(\n",
    "            name=name,\n",
    "            transform=transform,\n",
    "            rules=converted_rules,\n",
    "            table_name=table_name,\n",
    "            source_silvers=source_silvers,\n",
    "            schema=schema,\n",
    "        )\n",
    "\n",
    "        self.gold_steps[name] = gold_step\n",
    "        self.logger.info(f\"✅ Added Gold step: {name} (sources: {source_silvers})\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def validate_pipeline(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Validate the entire pipeline configuration.\n",
    "\n",
    "        Returns:\n",
    "            List of validation errors (empty if valid)\n",
    "        \"\"\"\n",
    "        validation_result = self.validator.validate_pipeline(\n",
    "            self.config, self.bronze_steps, self.silver_steps, self.gold_steps\n",
    "        )\n",
    "\n",
    "        if validation_result.errors:\n",
    "            self.logger.error(\n",
    "                f\"Pipeline validation failed with {len(validation_result.errors)} errors\"\n",
    "            )\n",
    "            for error in validation_result.errors:\n",
    "                self.logger.error(f\"  - {error}\")\n",
    "        else:\n",
    "            self.logger.info(\"✅ Pipeline validation passed\")\n",
    "\n",
    "        return validation_result.errors\n",
    "\n",
    "    # ============================================================================\n",
    "    # PRESET CONFIGURATIONS AND HELPER METHODS\n",
    "    # ============================================================================\n",
    "\n",
    "    @classmethod\n",
    "    def for_development(\n",
    "        cls,\n",
    "        spark: SparkSession,\n",
    "        schema: str,\n",
    "        functions: FunctionsProtocol | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> PipelineBuilder:\n",
    "        \"\"\"\n",
    "        Create a PipelineBuilder optimized for development with relaxed validation.\n",
    "\n",
    "        Args:\n",
    "            spark: Active SparkSession instance\n",
    "            schema: Database schema name\n",
    "            **kwargs: Additional configuration parameters\n",
    "\n",
    "        Returns:\n",
    "            PipelineBuilder instance with development-optimized settings\n",
    "\n",
    "        Example:\n",
    "            >>> builder = PipelineBuilder.for_development(\n",
    "            ...     spark=spark,\n",
    "            ...     schema=\"dev_schema\"\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            spark=spark,\n",
    "            schema=schema,\n",
    "            min_bronze_rate=80.0,  # Relaxed validation\n",
    "            min_silver_rate=85.0,\n",
    "            min_gold_rate=90.0,\n",
    "            verbose=True,\n",
    "            functions=functions,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def for_production(\n",
    "        cls,\n",
    "        spark: SparkSession,\n",
    "        schema: str,\n",
    "        functions: FunctionsProtocol | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> PipelineBuilder:\n",
    "        \"\"\"\n",
    "        Create a PipelineBuilder optimized for production with strict validation.\n",
    "\n",
    "        Args:\n",
    "            spark: Active SparkSession instance\n",
    "            schema: Database schema name\n",
    "            **kwargs: Additional configuration parameters\n",
    "\n",
    "        Returns:\n",
    "            PipelineBuilder instance with production-optimized settings\n",
    "\n",
    "        Example:\n",
    "            >>> builder = PipelineBuilder.for_production(\n",
    "            ...     spark=spark,\n",
    "            ...     schema=\"prod_schema\"\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            spark=spark,\n",
    "            schema=schema,\n",
    "            min_bronze_rate=95.0,  # Strict validation\n",
    "            min_silver_rate=98.0,\n",
    "            min_gold_rate=99.0,\n",
    "            verbose=False,\n",
    "            functions=functions,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def for_testing(\n",
    "        cls,\n",
    "        spark: SparkSession,\n",
    "        schema: str,\n",
    "        functions: FunctionsProtocol | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> PipelineBuilder:\n",
    "        \"\"\"\n",
    "        Create a PipelineBuilder optimized for testing with minimal validation.\n",
    "\n",
    "        Args:\n",
    "            spark: Active SparkSession instance\n",
    "            schema: Database schema name\n",
    "            **kwargs: Additional configuration parameters\n",
    "\n",
    "        Returns:\n",
    "            PipelineBuilder instance with testing-optimized settings\n",
    "\n",
    "        Example:\n",
    "            >>> builder = PipelineBuilder.for_testing(\n",
    "            ...     spark=spark,\n",
    "            ...     schema=\"my_schema\"\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        return cls(\n",
    "            spark=spark,\n",
    "            schema=schema,\n",
    "            min_bronze_rate=70.0,  # Very relaxed validation\n",
    "            min_silver_rate=75.0,\n",
    "            min_gold_rate=80.0,\n",
    "            verbose=True,\n",
    "            functions=functions,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # ============================================================================\n",
    "    # VALIDATION HELPER METHODS\n",
    "    # ============================================================================\n",
    "\n",
    "    @staticmethod\n",
    "    def not_null_rules(\n",
    "        columns: list[str], functions: FunctionsProtocol | None = None\n",
    "    ) -> ColumnRules:\n",
    "        \"\"\"\n",
    "        Create validation rules for non-null constraints on multiple columns.\n",
    "\n",
    "        Args:\n",
    "            columns: List of column names to validate for non-null\n",
    "            functions: Optional functions object for column operations\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of validation rules\n",
    "\n",
    "        Example:\n",
    "            >>> rules = PipelineBuilder.not_null_rules([\"user_id\", \"timestamp\", \"value\"])\n",
    "            >>> # Equivalent to:\n",
    "            >>> # {\n",
    "            >>> #     \"user_id\": [F.col(\"user_id\").isNotNull()],\n",
    "            >>> #     \"timestamp\": [F.col(\"timestamp\").isNotNull()],\n",
    "            >>> #     \"value\": [F.col(\"value\").isNotNull()]\n",
    "            >>> # }\n",
    "        \"\"\"\n",
    "        if functions is None:\n",
    "            functions = get_default_functions()\n",
    "        return {col: [functions.col(col).isNotNull()] for col in columns}\n",
    "\n",
    "    @staticmethod\n",
    "    def positive_number_rules(\n",
    "        columns: list[str], functions: FunctionsProtocol | None = None\n",
    "    ) -> ColumnRules:\n",
    "        \"\"\"\n",
    "        Create validation rules for positive number constraints on multiple columns.\n",
    "\n",
    "        Args:\n",
    "            columns: List of column names to validate for positive numbers\n",
    "            functions: Optional functions object for column operations\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of validation rules\n",
    "\n",
    "        Example:\n",
    "            >>> rules = PipelineBuilder.positive_number_rules([\"value\", \"count\"])\n",
    "            >>> # Equivalent to:\n",
    "            >>> # {\n",
    "            >>> #     \"value\": [F.col(\"value\").isNotNull(), F.col(\"value\") > 0],\n",
    "            >>> #     \"count\": [F.col(\"count\").isNotNull(), F.col(\"count\") > 0]\n",
    "            >>> # }\n",
    "        \"\"\"\n",
    "        if functions is None:\n",
    "            functions = get_default_functions()\n",
    "        return {\n",
    "            col: [functions.col(col).isNotNull(), functions.col(col) > 0]  # type: ignore[list-item]\n",
    "            for col in columns\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def string_not_empty_rules(\n",
    "        columns: list[str], functions: FunctionsProtocol | None = None\n",
    "    ) -> ColumnRules:\n",
    "        \"\"\"\n",
    "        Create validation rules for non-empty string constraints on multiple columns.\n",
    "\n",
    "        Args:\n",
    "            columns: List of column names to validate for non-empty strings\n",
    "            functions: Optional functions object for column operations\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of validation rules\n",
    "\n",
    "        Example:\n",
    "            >>> rules = PipelineBuilder.string_not_empty_rules([\"name\", \"category\"])\n",
    "            >>> # Equivalent to:\n",
    "            >>> # {\n",
    "            >>> #     \"name\": [F.col(\"name\").isNotNull(), F.length(F.col(\"name\")) > 0],\n",
    "            >>> #     \"category\": [F.col(\"category\").isNotNull(), F.length(F.col(\"category\")) > 0]\n",
    "            >>> # }\n",
    "        \"\"\"\n",
    "        if functions is None:\n",
    "            functions = get_default_functions()\n",
    "        return {\n",
    "            col: [\n",
    "                functions.col(col).isNotNull(),\n",
    "                functions.length(functions.col(col)) > 0,  # type: ignore[list-item]\n",
    "            ]\n",
    "            for col in columns\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def timestamp_rules(\n",
    "        columns: list[str], functions: FunctionsProtocol | None = None\n",
    "    ) -> ColumnRules:\n",
    "        \"\"\"\n",
    "        Create validation rules for timestamp constraints on multiple columns.\n",
    "\n",
    "        Args:\n",
    "            columns: List of column names to validate as timestamps\n",
    "            functions: Optional functions object for column operations\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of validation rules\n",
    "\n",
    "        Example:\n",
    "            >>> rules = PipelineBuilder.timestamp_rules([\"created_at\", \"updated_at\"])\n",
    "            >>> # Equivalent to:\n",
    "            >>> # {\n",
    "            >>> #     \"created_at\": [F.col(\"created_at\").isNotNull(), F.col(\"created_at\").isNotNull()],\n",
    "            >>> #     \"updated_at\": [F.col(\"updated_at\").isNotNull(), F.col(\"updated_at\").isNotNull()]\n",
    "            >>> # }\n",
    "        \"\"\"\n",
    "        if functions is None:\n",
    "            functions = get_default_functions()\n",
    "        return {\n",
    "            col: [functions.col(col).isNotNull(), functions.col(col).isNotNull()]\n",
    "            for col in columns\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_timestamp_columns(df_schema: Any) -> list[str]:\n",
    "        \"\"\"\n",
    "        Detect timestamp columns from a DataFrame schema.\n",
    "\n",
    "        Args:\n",
    "            df_schema: DataFrame schema or list of column names with types\n",
    "\n",
    "        Returns:\n",
    "            List of column names that appear to be timestamps\n",
    "\n",
    "        Example:\n",
    "            >>> timestamp_cols = PipelineBuilder.detect_timestamp_columns(df.schema)\n",
    "            >>> # Returns columns like [\"timestamp\", \"created_at\", \"updated_at\"]\n",
    "        \"\"\"\n",
    "        timestamp_keywords = [\n",
    "            \"timestamp\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\",\n",
    "            \"event_time\",\n",
    "            \"process_time\",\n",
    "            \"ingestion_time\",\n",
    "            \"load_time\",\n",
    "            \"modified_at\",\n",
    "            \"date_time\",\n",
    "            \"ts\",\n",
    "        ]\n",
    "\n",
    "        if hasattr(df_schema, \"fields\"):\n",
    "            # DataFrame schema\n",
    "            columns = [field.name.lower() for field in df_schema.fields]\n",
    "        else:\n",
    "            # List of column names\n",
    "            columns = [col.lower() for col in df_schema]\n",
    "\n",
    "        # Find columns that match timestamp patterns\n",
    "        timestamp_cols = []\n",
    "        for col in columns:\n",
    "            if any(keyword in col for keyword in timestamp_keywords):\n",
    "                timestamp_cols.append(col)\n",
    "\n",
    "        return timestamp_cols\n",
    "\n",
    "    def _validate_schema(self, schema: str) -> None:\n",
    "        \"\"\"\n",
    "        Validate that a schema exists and is accessible.\n",
    "\n",
    "        Args:\n",
    "            schema: Schema name to validate\n",
    "\n",
    "        Raises:\n",
    "            StepError: If schema doesn't exist or is not accessible\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if schema exists using catalog API\n",
    "            databases = [db.name for db in self.spark.catalog.listDatabases()]\n",
    "            if schema not in databases:\n",
    "                raise StepError(\n",
    "                    f\"Schema '{schema}' does not exist\",\n",
    "                    context={\n",
    "                        \"step_name\": \"schema_validation\",\n",
    "                        \"step_type\": \"validation\",\n",
    "                    },\n",
    "                    suggestions=[\n",
    "                        f\"Create the schema first: CREATE SCHEMA IF NOT EXISTS {schema}\",\n",
    "                        \"Check schema permissions\",\n",
    "                        \"Verify schema name spelling\",\n",
    "                    ],\n",
    "                )\n",
    "            self.logger.debug(f\"✅ Schema '{schema}' is accessible\")\n",
    "        except StepError:\n",
    "            # Re-raise StepError as-is\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            raise StepError(\n",
    "                f\"Schema '{schema}' is not accessible: {str(e)}\",\n",
    "                context={\"step_name\": \"schema_validation\", \"step_type\": \"validation\"},\n",
    "                suggestions=[\n",
    "                    f\"Create the schema first: CREATE SCHEMA IF NOT EXISTS {schema}\",\n",
    "                    \"Check schema permissions\",\n",
    "                    \"Verify schema name spelling\",\n",
    "                ],\n",
    "            ) from e\n",
    "\n",
    "    def _create_schema_if_not_exists(self, schema: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a schema if it doesn't exist.\n",
    "\n",
    "        Args:\n",
    "            schema: Schema name to create\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use SQL to create schema\n",
    "            self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "            self.logger.info(f\"✅ Schema '{schema}' created or already exists\")\n",
    "        except Exception as e:\n",
    "            raise StepError(\n",
    "                f\"Failed to create schema '{schema}': {str(e)}\",\n",
    "                context={\"step_name\": \"schema_creation\", \"step_type\": \"validation\"},\n",
    "                suggestions=[\n",
    "                    \"Check schema permissions\",\n",
    "                    \"Verify schema name is valid\",\n",
    "                    \"Check for naming conflicts\",\n",
    "                ],\n",
    "            ) from e\n",
    "\n",
    "    def _get_effective_schema(self, step_schema: str | None) -> str:\n",
    "        \"\"\"\n",
    "        Get the effective schema for a step, falling back to the builder's default schema.\n",
    "\n",
    "        Args:\n",
    "            step_schema: Schema specified for the step\n",
    "\n",
    "        Returns:\n",
    "            The effective schema name\n",
    "        \"\"\"\n",
    "        return step_schema if step_schema is not None else self.schema\n",
    "\n",
    "    def to_pipeline(self) -> PipelineRunner:\n",
    "        \"\"\"\n",
    "        Build and return a PipelineRunner for executing this pipeline.\n",
    "\n",
    "        Returns:\n",
    "            PipelineRunner instance ready for execution (implements abstracts.Runner)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If pipeline validation fails\n",
    "        \"\"\"\n",
    "        # Validate pipeline before building\n",
    "        validation_errors = self.validate_pipeline()\n",
    "        if validation_errors:\n",
    "            raise ValueError(\n",
    "                f\"Pipeline validation failed with {len(validation_errors)} errors: {', '.join(validation_errors)}\"\n",
    "            )\n",
    "\n",
    "        # Build steps list for abstracts.PipelineBuilder validation\n",
    "        all_steps = (\n",
    "            list(self.bronze_steps.values())\n",
    "            + list(self.silver_steps.values())\n",
    "            + list(self.gold_steps.values())\n",
    "        )\n",
    "\n",
    "        # Use abstracts.PipelineBuilder to validate steps\n",
    "        # This ensures step validation follows the abstracts interface\n",
    "        # Type cast needed because BronzeStep/SilverStep/GoldStep satisfy Step Protocol\n",
    "        try:\n",
    "            from abstracts.step import Step as AbstractsStep\n",
    "\n",
    "            steps_for_validation: list[AbstractsStep] = all_steps  # type: ignore[assignment]\n",
    "            self._abstracts_builder.validate_steps(steps_for_validation)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Step validation failed: {e}\") from e\n",
    "\n",
    "        # Create PipelineRunner with proper configuration\n",
    "        # PipelineRunner implements abstracts.Runner, so this satisfies the interface\n",
    "        # Note: steps and engine are optional parameters for abstracts compatibility\n",
    "        # but we pass them to ensure the runner is properly initialized\n",
    "        runner = PipelineRunner(\n",
    "            spark=self.spark,\n",
    "            config=self.config,\n",
    "            bronze_steps=self.bronze_steps,\n",
    "            silver_steps=self.silver_steps,\n",
    "            gold_steps=self.gold_steps,\n",
    "            logger=self.logger,\n",
    "            functions=self.functions,\n",
    "            steps=all_steps\n",
    "            if all_steps\n",
    "            else None,  # Pass steps for abstracts.Runner compatibility\n",
    "            engine=self.spark_engine,  # Pass engine for abstracts.Runner compatibility\n",
    "        )\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"🚀 Pipeline built successfully with {len(self.bronze_steps)} bronze, {len(self.silver_steps)} silver, {len(self.gold_steps)} gold steps\"\n",
    "        )\n",
    "\n",
    "        return runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Module: pipeline_builder.writer.core (pipeline_builder)\n",
    "#\n",
    "# Dependencies: pipeline_builder.compat, pipeline_builder.functions, pipeline_builder.models.execution, pipeline_builder.writer.models, pipeline_builder.writer.monitoring, pipeline_builder_base.logging, validation.utils, writer.analytics, writer.exceptions, writer.operations, writer.storage\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "# from ..compat import SparkSession  # Removed: defined in notebook cells above\n",
    "# from ..functions import FunctionsProtocol, get_default_functions  # Removed: defined in notebook cells above\n",
    "# from .logging import PipelineLogger  # Removed: defined in notebook cells above\n",
    "# from .models import ExecutionResult, StepResult  # Removed: defined in notebook cells above\n",
    "# from ..pipeline.models import PipelineReport  # Removed: defined in notebook cells above\n",
    "# from ..table_operations import table_exists  # Removed: defined in notebook cells above\n",
    "# from .analytics import (  # Removed: defined in notebook cells above\n",
    "# DataQualityAnalyzer,\n",
    "# ExecutionTrends,\n",
    "# QualityAnomalies,\n",
    "# QualityTrends,\n",
    "# TrendAnalyzer,\n",
    "# )\n",
    "# from .exceptions import WriterConfigurationError, WriterError  # Removed: defined in notebook cells above\n",
    "# from .models import LogRow, WriteMode, WriterConfig, WriterMetrics, create_log_schema  # Removed: defined in notebook cells above\n",
    "# from .monitoring import (  # Removed: defined in notebook cells above\n",
    "# AnalyticsEngine,\n",
    "# AnomalyReport,\n",
    "# MemoryUsageInfo,\n",
    "# PerformanceMonitor,\n",
    "# PerformanceReport,\n",
    "# )\n",
    "# from .operations import DataProcessor, DataQualityReport  # Removed: defined in notebook cells above\n",
    "# from .storage import (  # Removed: defined in notebook cells above\n",
    "# OptimizeResult,\n",
    "# StorageManager,\n",
    "# TableInfo,\n",
    "# VacuumResult,\n",
    "# WriteResult,\n",
    "# )\n",
    "\n",
    "\n",
    "def time_write_operation(\n",
    "    operation_func: Any, *args: Any, **kwargs: Any\n",
    ") -> tuple[int, float, Any, Any]:\n",
    "    \"\"\"\n",
    "    Time a write operation and return metrics.\n",
    "\n",
    "    Args:\n",
    "        operation_func: Function to time\n",
    "        *args: Arguments for the function\n",
    "        **kwargs: Keyword arguments for the function\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (rows_written, duration_secs, start_time, end_time)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    start_ts = time.time()\n",
    "\n",
    "    try:\n",
    "        result = operation_func(*args, **kwargs)\n",
    "        rows_written = result.get(\"rows_written\", 0) if isinstance(result, dict) else 0\n",
    "    except Exception:\n",
    "        rows_written = 0\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    duration_secs = time.time() - start_ts\n",
    "\n",
    "    return rows_written, duration_secs, start_time, end_time\n",
    "\n",
    "\n",
    "def validate_log_data(log_rows: list[LogRow]) -> None:\n",
    "    \"\"\"\n",
    "    Validate log data for quality and consistency.\n",
    "\n",
    "    Args:\n",
    "        log_rows: List of log rows to validate\n",
    "\n",
    "    Raises:\n",
    "        WriterValidationError: If validation fails\n",
    "    \"\"\"\n",
    "    if not log_rows:\n",
    "        return\n",
    "\n",
    "    # Basic validation - check required fields\n",
    "    required_fields = {\"run_id\", \"phase\", \"step_name\"}\n",
    "    for i, row in enumerate(log_rows):\n",
    "        missing_fields = required_fields - set(row.keys())\n",
    "        if missing_fields:\n",
    "            # from .exceptions import WriterValidationError  # Removed: defined in notebook cells above\n",
    "\n",
    "            raise WriterValidationError(\n",
    "                f\"Log row {i} missing required fields: {missing_fields}\",\n",
    "                validation_errors=[f\"Missing fields: {missing_fields}\"],\n",
    "                context={\"row_index\": i, \"row\": row},\n",
    "            )\n",
    "\n",
    "\n",
    "def create_log_rows_from_execution_result(\n",
    "    execution_result: ExecutionResult,\n",
    "    run_id: str,\n",
    "    run_mode: str = \"initial\",\n",
    "    metadata: Dict[str, Any] | None = None,\n",
    ") -> list[LogRow]:\n",
    "    \"\"\"\n",
    "    Create log rows from an execution result.\n",
    "\n",
    "    Args:\n",
    "        execution_result: The execution result\n",
    "        run_id: Run identifier\n",
    "        run_mode: Mode of the run\n",
    "        metadata: Additional metadata\n",
    "\n",
    "    Returns:\n",
    "        List of log rows\n",
    "    \"\"\"\n",
    "\n",
    "    log_rows = []\n",
    "\n",
    "    # Create a main log row for the execution\n",
    "    main_row: LogRow = {\n",
    "        \"run_id\": run_id,\n",
    "        \"run_mode\": run_mode,  # type: ignore[typeddict-item]\n",
    "        \"run_started_at\": getattr(execution_result, \"start_time\", None),\n",
    "        \"run_ended_at\": getattr(execution_result, \"end_time\", None),\n",
    "        \"execution_id\": getattr(execution_result, \"execution_id\", run_id),\n",
    "        \"pipeline_id\": getattr(execution_result, \"pipeline_id\", \"unknown\"),\n",
    "        \"schema\": getattr(execution_result, \"schema\", \"default\"),\n",
    "        \"phase\": \"bronze\",\n",
    "        \"step_name\": \"pipeline_execution\",\n",
    "        \"step_type\": \"pipeline\",\n",
    "        \"start_time\": getattr(execution_result, \"start_time\", None),\n",
    "        \"end_time\": getattr(execution_result, \"end_time\", None),\n",
    "        \"duration_secs\": getattr(execution_result, \"duration\", 0.0) or 0.0,\n",
    "        \"table_fqn\": None,\n",
    "        \"write_mode\": None,\n",
    "        \"input_rows\": 0,\n",
    "        \"output_rows\": 0,\n",
    "        \"rows_written\": 0,\n",
    "        \"rows_processed\": 0,\n",
    "        \"table_total_rows\": None,\n",
    "        \"valid_rows\": 0,\n",
    "        \"invalid_rows\": 0,\n",
    "        \"validation_rate\": 100.0,\n",
    "        \"success\": getattr(execution_result, \"status\", \"unknown\") == \"completed\",\n",
    "        \"error_message\": getattr(execution_result, \"error\", None),\n",
    "        \"memory_usage_mb\": 0.0,\n",
    "        \"cpu_usage_percent\": 0.0,\n",
    "        \"metadata\": {},\n",
    "    }\n",
    "\n",
    "    log_rows.append(main_row)\n",
    "\n",
    "    # Add step results if available\n",
    "    if hasattr(execution_result, \"steps\"):\n",
    "        steps = getattr(execution_result, \"steps\", None)\n",
    "        if steps and isinstance(steps, (list, tuple)):\n",
    "            for step in steps:\n",
    "                step_row: LogRow = {\n",
    "                    \"run_id\": run_id,\n",
    "                    \"run_mode\": run_mode,  # type: ignore[typeddict-item]\n",
    "                    \"run_started_at\": getattr(execution_result, \"start_time\", None),\n",
    "                    \"run_ended_at\": getattr(execution_result, \"end_time\", None),\n",
    "                    \"execution_id\": getattr(execution_result, \"execution_id\", run_id),\n",
    "                    \"pipeline_id\": getattr(execution_result, \"pipeline_id\", \"unknown\"),\n",
    "                    \"schema\": getattr(execution_result, \"schema\", \"default\"),\n",
    "                    \"phase\": getattr(step, \"step_type\", \"bronze\").lower(),  # type: ignore[typeddict-item]\n",
    "                    \"step_name\": getattr(step, \"step_name\", \"unknown\"),\n",
    "                    \"step_type\": getattr(step, \"step_type\", \"unknown\"),\n",
    "                    \"start_time\": getattr(step, \"start_time\", None),\n",
    "                    \"end_time\": getattr(step, \"end_time\", None),\n",
    "                    \"duration_secs\": getattr(step, \"duration\", 0.0),\n",
    "                    \"table_fqn\": getattr(step, \"output_table\", None),\n",
    "                    \"write_mode\": getattr(step, \"write_mode\", None),\n",
    "                    \"input_rows\": getattr(step, \"input_rows\", 0),\n",
    "                    \"output_rows\": getattr(step, \"rows_processed\", 0),\n",
    "                    \"rows_written\": getattr(step, \"rows_written\", 0),\n",
    "                    \"rows_processed\": getattr(step, \"rows_processed\", 0),\n",
    "                    \"table_total_rows\": None,\n",
    "                    \"valid_rows\": 0,\n",
    "                    \"invalid_rows\": 0,\n",
    "                    \"validation_rate\": 100.0,\n",
    "                    \"success\": getattr(step, \"status\", \"unknown\") == \"completed\",\n",
    "                    \"error_message\": getattr(step, \"error\", None),\n",
    "                    \"memory_usage_mb\": 0.0,\n",
    "                    \"cpu_usage_percent\": 0.0,\n",
    "                    \"metadata\": {},\n",
    "                }\n",
    "                log_rows.append(step_row)\n",
    "\n",
    "    return log_rows\n",
    "\n",
    "\n",
    "class LogWriter:\n",
    "    \"\"\"\n",
    "    Refactored LogWriter with modular architecture.\n",
    "\n",
    "    This class orchestrates the various writer components to provide\n",
    "    comprehensive logging functionality for pipeline execution results.\n",
    "\n",
    "    Components:\n",
    "    - DataProcessor: Handles data processing and transformations\n",
    "    - StorageManager: Manages Delta Lake storage operations\n",
    "    - PerformanceMonitor: Tracks performance metrics\n",
    "    - AnalyticsEngine: Provides analytics and trend analysis\n",
    "    - DataQualityAnalyzer: Analyzes data quality metrics\n",
    "    - TrendAnalyzer: Analyzes execution trends\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        schema: str | None = None,\n",
    "        table_name: str | None = None,\n",
    "        config: WriterConfig | None = None,\n",
    "        functions: FunctionsProtocol | None = None,\n",
    "        logger: PipelineLogger | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the LogWriter with modular components.\n",
    "\n",
    "        Args:\n",
    "            spark: Spark session\n",
    "            schema: Database schema name (simplified API)\n",
    "            table_name: Table name (simplified API)\n",
    "            config: Writer configuration (deprecated, use schema and table_name instead)\n",
    "            functions: Functions protocol (optional, uses default if not provided)\n",
    "            logger: Pipeline logger (optional)\n",
    "\n",
    "        Raises:\n",
    "            WriterConfigurationError: If configuration is invalid\n",
    "\n",
    "        Example (new simplified API):\n",
    "            >>> writer = LogWriter(spark, schema=\"analytics\", table_name=\"pipeline_logs\")\n",
    "\n",
    "        Example (old API, deprecated):\n",
    "            >>> config = WriterConfig(table_schema=\"analytics\", table_name=\"pipeline_logs\")\n",
    "            >>> writer = LogWriter(spark, config=config)\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "\n",
    "        # Handle both old and new API\n",
    "        if config is not None:\n",
    "            # Old API: config provided\n",
    "            import warnings\n",
    "\n",
    "            warnings.warn(\n",
    "                \"Passing WriterConfig is deprecated. Use LogWriter(spark, schema='...', table_name='...') instead.\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            self.config = config\n",
    "        elif schema is not None and table_name is not None:\n",
    "            # New API: schema and table_name provided\n",
    "            self.config = WriterConfig(\n",
    "                table_schema=schema, table_name=table_name, write_mode=WriteMode.APPEND\n",
    "            )\n",
    "        else:\n",
    "            raise WriterConfigurationError(\n",
    "                \"Must provide either (schema and table_name) or config parameter\",\n",
    "                config_errors=[\"Missing required parameters\"],\n",
    "                suggestions=[\n",
    "                    \"Use: LogWriter(spark, schema='my_schema', table_name='my_table')\",\n",
    "                    \"Or: LogWriter(spark, config=WriterConfig(...))\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        self.functions = functions if functions is not None else get_default_functions()\n",
    "        if logger is None:\n",
    "            self.logger = PipelineLogger(\"LogWriter\")\n",
    "        else:\n",
    "            self.logger = logger\n",
    "\n",
    "        # Validate configuration\n",
    "        try:\n",
    "            self.config.validate()\n",
    "        except ValueError as e:\n",
    "            raise WriterConfigurationError(\n",
    "                f\"Invalid writer configuration: {e}\",\n",
    "                config_errors=[str(e)],\n",
    "                context={\"config\": self.config.__dict__},\n",
    "                suggestions=[\n",
    "                    \"Check configuration values\",\n",
    "                    \"Ensure all required fields are provided\",\n",
    "                    \"Verify numeric values are positive\",\n",
    "                ],\n",
    "            ) from e\n",
    "\n",
    "        # Initialize components\n",
    "        self._initialize_components()\n",
    "\n",
    "        # Initialize metrics\n",
    "        self.metrics: WriterMetrics = {\n",
    "            \"total_writes\": 0,\n",
    "            \"successful_writes\": 0,\n",
    "            \"failed_writes\": 0,\n",
    "            \"total_duration_secs\": 0.0,\n",
    "            \"avg_write_duration_secs\": 0.0,\n",
    "            \"total_rows_written\": 0,\n",
    "            \"memory_usage_peak_mb\": 0.0,\n",
    "        }\n",
    "\n",
    "        # Initialize schema\n",
    "        self.schema = create_log_schema()\n",
    "\n",
    "        # Set table FQN for compatibility\n",
    "        self.table_fqn = f\"{self.config.table_schema}.{self.config.table_name}\"\n",
    "\n",
    "        # Cache table row counts to avoid repeated counts within a single write operation\n",
    "        self._table_total_rows_cache: dict[str, int | None] = {}\n",
    "\n",
    "        self.logger.info(f\"LogWriter initialized for table: {self.table_fqn}\")\n",
    "\n",
    "    def _initialize_components(self) -> None:\n",
    "        \"\"\"Initialize all writer components.\"\"\"\n",
    "        # Data processing component\n",
    "        self.data_processor = DataProcessor(self.spark, self.functions, self.logger)\n",
    "\n",
    "        # Storage management component\n",
    "        self.storage_manager = StorageManager(\n",
    "            self.spark, self.config, self.functions, self.logger\n",
    "        )\n",
    "\n",
    "        # Performance monitoring component\n",
    "        self.performance_monitor = PerformanceMonitor(self.spark, self.logger)\n",
    "\n",
    "        # Analytics components\n",
    "        self.analytics_engine = AnalyticsEngine(self.spark, self.logger)\n",
    "        self.quality_analyzer = DataQualityAnalyzer(self.spark, self.logger)\n",
    "        self.trend_analyzer = TrendAnalyzer(self.spark, self.logger)\n",
    "\n",
    "    def write_execution_result(\n",
    "        self,\n",
    "        execution_result: ExecutionResult,\n",
    "        run_id: str | None = None,\n",
    "        run_mode: str = \"initial\",\n",
    "        metadata: Dict[str, Any] | None = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Write execution result to log table.\n",
    "\n",
    "        Args:\n",
    "            execution_result: The execution result to write\n",
    "            run_id: Unique run identifier (generated if not provided)\n",
    "            run_mode: Mode of the run (initial, incremental, etc.)\n",
    "            metadata: Additional metadata\n",
    "\n",
    "        Returns:\n",
    "            Dict containing write results and metrics\n",
    "\n",
    "        Raises:\n",
    "            WriterValidationError: If validation fails\n",
    "            WriterTableError: If table operations fail\n",
    "            WriterPerformanceError: If performance thresholds exceeded\n",
    "        \"\"\"\n",
    "        operation_id = str(uuid.uuid4())\n",
    "        if run_id is None:\n",
    "            run_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            # Reset per-operation cache\n",
    "            self._reset_table_total_rows_cache()\n",
    "\n",
    "            # Start performance monitoring\n",
    "            self.performance_monitor.start_operation(\n",
    "                operation_id, \"write_execution_result\"\n",
    "            )\n",
    "\n",
    "            # Log operation start\n",
    "            self.logger.info(f\"Writing execution result for run {run_id}\")\n",
    "\n",
    "            # Process execution result\n",
    "            log_rows = self.data_processor.process_execution_result(\n",
    "                execution_result,\n",
    "                run_id,\n",
    "                run_mode,\n",
    "                metadata,\n",
    "                table_total_rows_provider=self._get_table_total_rows,\n",
    "            )\n",
    "\n",
    "            # Create table if not exists\n",
    "            self.storage_manager.create_table_if_not_exists(self.schema)\n",
    "\n",
    "            # Write to storage\n",
    "            write_result = self.storage_manager.write_batch(\n",
    "                log_rows, self.config.write_mode\n",
    "            )\n",
    "\n",
    "            # Update metrics\n",
    "            self._update_metrics(write_result, True)\n",
    "\n",
    "            # End performance monitoring\n",
    "            operation_metrics = self.performance_monitor.end_operation(\n",
    "                operation_id, True, write_result.get(\"rows_written\", 0)\n",
    "            )\n",
    "\n",
    "            # Check performance thresholds\n",
    "            threshold_violations = (\n",
    "                self.performance_monitor.check_performance_thresholds(operation_metrics)\n",
    "            )\n",
    "            if threshold_violations:\n",
    "                self.logger.warning(\n",
    "                    f\"Performance threshold violations: {threshold_violations}\"\n",
    "                )\n",
    "\n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"run_id\": run_id,\n",
    "                \"operation_id\": operation_id,\n",
    "                \"rows_written\": write_result.get(\"rows_written\", 0),\n",
    "                \"write_result\": write_result,\n",
    "                \"operation_metrics\": operation_metrics,\n",
    "                \"threshold_violations\": threshold_violations,\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Successfully wrote execution result for run {run_id}\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            # End performance monitoring with failure\n",
    "            self.performance_monitor.end_operation(operation_id, False, 0, str(e))\n",
    "            # Create empty WriteResult for error case\n",
    "            empty_result: WriteResult = {\n",
    "                \"table_name\": self.storage_manager.table_fqn,\n",
    "                \"write_mode\": self.config.write_mode.value,\n",
    "                \"rows_written\": 0,\n",
    "                \"timestamp\": \"\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "            self._update_metrics(empty_result, False)\n",
    "\n",
    "            self.logger.error(f\"Failed to write execution result for run {run_id}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def write_step_results(\n",
    "        self,\n",
    "        step_results: Dict[str, StepResult],\n",
    "        run_id: str | None = None,\n",
    "        run_mode: str = \"initial\",\n",
    "        metadata: Dict[str, Any] | None = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Write step results to log table.\n",
    "\n",
    "        Args:\n",
    "            step_results: Dictionary of step results\n",
    "            run_id: Unique run identifier (generated if not provided)\n",
    "            run_mode: Mode of the run\n",
    "            metadata: Additional metadata\n",
    "\n",
    "        Returns:\n",
    "            Dict containing write results and metrics\n",
    "        \"\"\"\n",
    "        operation_id = str(uuid.uuid4())\n",
    "        if run_id is None:\n",
    "            run_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            # Start performance monitoring\n",
    "            self.performance_monitor.start_operation(operation_id, \"write_step_results\")\n",
    "\n",
    "            # Log operation start\n",
    "            self.logger.info(\n",
    "                f\"Writing {len(step_results)} step results for run {run_id}\"\n",
    "            )\n",
    "\n",
    "            # Process step results\n",
    "            log_rows = self.data_processor.process_step_results(\n",
    "                step_results, run_id, run_mode, metadata\n",
    "            )\n",
    "\n",
    "            # Create table if not exists\n",
    "            self.storage_manager.create_table_if_not_exists(self.schema)\n",
    "\n",
    "            # Write to storage\n",
    "            write_result = self.storage_manager.write_batch(\n",
    "                log_rows, self.config.write_mode\n",
    "            )\n",
    "\n",
    "            # Update metrics\n",
    "            self._update_metrics(write_result, True)\n",
    "\n",
    "            # End performance monitoring\n",
    "            operation_metrics = self.performance_monitor.end_operation(\n",
    "                operation_id, True, write_result.get(\"rows_written\", 0)\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"run_id\": run_id,\n",
    "                \"operation_id\": operation_id,\n",
    "                \"rows_written\": write_result.get(\"rows_written\", 0),\n",
    "                \"write_result\": write_result,\n",
    "                \"operation_metrics\": operation_metrics,\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Successfully wrote step results for run {run_id}\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            # End performance monitoring with failure\n",
    "            self.performance_monitor.end_operation(operation_id, False, 0, str(e))\n",
    "            # Create empty WriteResult for error case\n",
    "            empty_result: WriteResult = {\n",
    "                \"table_name\": self.storage_manager.table_fqn,\n",
    "                \"write_mode\": self.config.write_mode.value,\n",
    "                \"rows_written\": 0,\n",
    "                \"timestamp\": \"\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "            self._update_metrics(empty_result, False)\n",
    "\n",
    "            self.logger.error(f\"Failed to write step results for run {run_id}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def write_log_rows(\n",
    "        self,\n",
    "        log_rows: list[LogRow],\n",
    "        run_id: str | None = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Write log rows directly to the table.\n",
    "\n",
    "        Args:\n",
    "            log_rows: List of log rows to write\n",
    "            run_id: Unique run identifier (generated if not provided)\n",
    "\n",
    "        Returns:\n",
    "            Dict containing write results and metrics\n",
    "        \"\"\"\n",
    "        operation_id = str(uuid.uuid4())\n",
    "        if run_id is None:\n",
    "            run_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            # Start performance monitoring\n",
    "            self.performance_monitor.start_operation(operation_id, \"write_log_rows\")\n",
    "\n",
    "            # Log operation start\n",
    "            self.logger.info(f\"Writing {len(log_rows)} log rows for run {run_id}\")\n",
    "\n",
    "            # Create table if not exists\n",
    "            self.storage_manager.create_table_if_not_exists(self.schema)\n",
    "\n",
    "            # Write to storage\n",
    "            write_result = self.storage_manager.write_batch(\n",
    "                log_rows, self.config.write_mode\n",
    "            )\n",
    "\n",
    "            # Update metrics\n",
    "            self._update_metrics(write_result, True)\n",
    "\n",
    "            # End performance monitoring\n",
    "            operation_metrics = self.performance_monitor.end_operation(\n",
    "                operation_id, True, write_result.get(\"rows_written\", 0)\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"run_id\": run_id,\n",
    "                \"operation_id\": operation_id,\n",
    "                \"rows_written\": write_result.get(\"rows_written\", 0),\n",
    "                \"write_result\": write_result,\n",
    "                \"operation_metrics\": operation_metrics,\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"Successfully wrote log rows for run {run_id}\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            # End performance monitoring with failure\n",
    "            self.performance_monitor.end_operation(operation_id, False, 0, str(e))\n",
    "            # Create empty WriteResult for error case\n",
    "            empty_result: WriteResult = {\n",
    "                \"table_name\": self.storage_manager.table_fqn,\n",
    "                \"write_mode\": self.config.write_mode.value,\n",
    "                \"rows_written\": 0,\n",
    "                \"timestamp\": \"\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "            self._update_metrics(empty_result, False)\n",
    "\n",
    "            self.logger.error(f\"Failed to write log rows for run {run_id}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def write_execution_result_batch(\n",
    "        self,\n",
    "        execution_results: list[ExecutionResult],\n",
    "        run_ids: list[str] | None = None,\n",
    "        run_mode: str = \"initial\",\n",
    "        metadata: Dict[str, Any] | None = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Write multiple execution results in batch.\n",
    "\n",
    "        Args:\n",
    "            execution_results: List of execution results to write\n",
    "            run_ids: List of run identifiers (generated if not provided)\n",
    "            run_mode: Mode of the runs\n",
    "            metadata: Additional metadata\n",
    "\n",
    "        Returns:\n",
    "            Dict containing batch write results and metrics\n",
    "        \"\"\"\n",
    "        operation_id = str(uuid.uuid4())\n",
    "        if run_ids is None:\n",
    "            run_ids = [str(uuid.uuid4()) for _ in execution_results]\n",
    "\n",
    "        try:\n",
    "            # Start performance monitoring\n",
    "            self.performance_monitor.start_operation(\n",
    "                operation_id, \"write_execution_result_batch\"\n",
    "            )\n",
    "\n",
    "            # Log operation start\n",
    "            self.logger.info(\n",
    "                f\"Writing batch of {len(execution_results)} execution results\"\n",
    "            )\n",
    "\n",
    "            # Process all execution results\n",
    "            all_log_rows = []\n",
    "            self._reset_table_total_rows_cache()\n",
    "            for i, execution_result in enumerate(execution_results):\n",
    "                run_id = run_ids[i] if i < len(run_ids) else str(uuid.uuid4())\n",
    "                log_rows = self.data_processor.process_execution_result(\n",
    "                    execution_result,\n",
    "                    run_id,\n",
    "                    run_mode,\n",
    "                    metadata,\n",
    "                    table_total_rows_provider=self._get_table_total_rows,\n",
    "                )\n",
    "                all_log_rows.extend(log_rows)\n",
    "\n",
    "            # Create table if not exists\n",
    "            self.storage_manager.create_table_if_not_exists(self.schema)\n",
    "\n",
    "            # Write to storage\n",
    "            write_result = self.storage_manager.write_batch(\n",
    "                all_log_rows, self.config.write_mode\n",
    "            )\n",
    "\n",
    "            # Update metrics\n",
    "            self._update_metrics(write_result, True)\n",
    "\n",
    "            # End performance monitoring\n",
    "            operation_metrics = self.performance_monitor.end_operation(\n",
    "                operation_id, True, write_result.get(\"rows_written\", 0)\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"operation_id\": operation_id,\n",
    "                \"execution_results_count\": len(execution_results),\n",
    "                \"total_rows_written\": write_result.get(\"rows_written\", 0),\n",
    "                \"write_result\": write_result,\n",
    "                \"operation_metrics\": operation_metrics,\n",
    "            }\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Successfully wrote batch of {len(execution_results)} execution results\"\n",
    "            )\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            # End performance monitoring with failure\n",
    "            self.performance_monitor.end_operation(operation_id, False, 0, str(e))\n",
    "            # Create empty WriteResult for error case\n",
    "            empty_result: WriteResult = {\n",
    "                \"table_name\": self.storage_manager.table_fqn,\n",
    "                \"write_mode\": self.config.write_mode.value,\n",
    "                \"rows_written\": 0,\n",
    "                \"timestamp\": \"\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "            self._update_metrics(empty_result, False)\n",
    "\n",
    "            self.logger.error(f\"Failed to write execution result batch: {e}\")\n",
    "            raise\n",
    "\n",
    "    def show_logs(self, limit: int | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Display logs from the table.\n",
    "\n",
    "        Args:\n",
    "            limit: Maximum number of rows to display\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\n",
    "                f\"Displaying logs from {self.config.table_schema}.{self.config.table_name}\"\n",
    "            )\n",
    "\n",
    "            # Query logs using spark.table for compatibility\n",
    "            df = self.spark.table(\n",
    "                f\"{self.config.table_schema}.{self.config.table_name}\"\n",
    "            )\n",
    "\n",
    "            # Show DataFrame\n",
    "            if limit is not None:\n",
    "                df.show(limit)\n",
    "            else:\n",
    "                df.show()\n",
    "\n",
    "            self.logger.info(\"Logs displayed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to display logs: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_table_info(self) -> TableInfo:\n",
    "        \"\"\"\n",
    "        Get information about the log table.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing table information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.storage_manager.get_table_info()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get table info: {e}\")\n",
    "            raise WriterError(f\"Failed to get table info: {e}\") from e\n",
    "\n",
    "    def _reset_table_total_rows_cache(self) -> None:\n",
    "        \"\"\"Clear cached table counts so subsequent operations refresh totals.\"\"\"\n",
    "        self._table_total_rows_cache.clear()\n",
    "\n",
    "    def _get_table_total_rows(self, table_fqn: str | None) -> int | None:\n",
    "        \"\"\"\n",
    "        Determine the total number of rows for a given table.\n",
    "\n",
    "        Args:\n",
    "            table_fqn: Fully qualified table name.\n",
    "\n",
    "        Returns:\n",
    "            Row count if available, otherwise None.\n",
    "        \"\"\"\n",
    "        if not table_fqn:\n",
    "            return None\n",
    "\n",
    "        if table_fqn in self._table_total_rows_cache:\n",
    "            return self._table_total_rows_cache[table_fqn]\n",
    "\n",
    "        try:\n",
    "            table_accessor = getattr(self.spark, \"table\", None)\n",
    "            if not callable(table_accessor):\n",
    "                self.logger.debug(\n",
    "                    \"table_total_rows: spark session does not expose table(); skipping count\"\n",
    "                )\n",
    "                self._table_total_rows_cache[table_fqn] = None\n",
    "                return None\n",
    "\n",
    "            if not table_exists(self.spark, table_fqn):\n",
    "                self.logger.debug(\n",
    "                    f\"table_total_rows: table {table_fqn} does not exist; leaving value as None\"\n",
    "                )\n",
    "                self._table_total_rows_cache[table_fqn] = None\n",
    "                return None\n",
    "\n",
    "            table_df = table_accessor(table_fqn)\n",
    "            count_method = getattr(table_df, \"count\", None)\n",
    "            if not callable(count_method):\n",
    "                self.logger.debug(\n",
    "                    f\"table_total_rows: object for {table_fqn} lacks count(); skipping\"\n",
    "                )\n",
    "                self._table_total_rows_cache[table_fqn] = None\n",
    "                return None\n",
    "\n",
    "            raw_count = count_method()\n",
    "            if isinstance(raw_count, (int, float)):\n",
    "                row_count = int(raw_count)\n",
    "            else:\n",
    "                row_count = None\n",
    "\n",
    "            self._table_total_rows_cache[table_fqn] = row_count\n",
    "            return row_count\n",
    "        except Exception as exc:  # pragma: no cover - defensive logging path\n",
    "            self.logger.warning(\n",
    "                f\"table_total_rows: unable to compute row count for {table_fqn}: {exc}\"\n",
    "            )\n",
    "            self._table_total_rows_cache[table_fqn] = None\n",
    "            return None\n",
    "\n",
    "    def optimize_table(self) -> OptimizeResult:\n",
    "        \"\"\"\n",
    "        Optimize the Delta table for better performance.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing optimization results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Optimizing Delta table\")\n",
    "            return self.storage_manager.optimize_table()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to optimize table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def vacuum_table(self, retention_hours: int = 168) -> VacuumResult:\n",
    "        \"\"\"\n",
    "        Vacuum the Delta table to remove old files.\n",
    "\n",
    "        Args:\n",
    "            retention_hours: Hours of retention for old files\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing vacuum results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Vacuuming Delta table (retention: {retention_hours}h)\")\n",
    "            return self.storage_manager.vacuum_table(retention_hours)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to vacuum table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_quality_trends(self, days: int = 30) -> QualityTrends:\n",
    "        \"\"\"\n",
    "        Analyze data quality trends.\n",
    "\n",
    "        Args:\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing quality trend analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Analyzing quality trends for last {days} days\")\n",
    "\n",
    "            # Query recent logs\n",
    "            df = self.storage_manager.query_logs()\n",
    "\n",
    "            # Analyze quality trends\n",
    "            return self.quality_analyzer.analyze_quality_trends(df, days)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to analyze quality trends: {e}\")\n",
    "            raise WriterError(f\"Failed to analyze quality trends: {e}\") from e\n",
    "\n",
    "    def analyze_execution_trends(self, days: int = 30) -> ExecutionTrends:\n",
    "        \"\"\"\n",
    "        Analyze execution trends.\n",
    "\n",
    "        Args:\n",
    "            days: Number of days to analyze\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing execution trend analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Analyzing execution trends for last {days} days\")\n",
    "\n",
    "            # Query recent logs\n",
    "            df = self.storage_manager.query_logs()\n",
    "\n",
    "            # Analyze execution trends\n",
    "            return self.trend_analyzer.analyze_execution_trends(df, days)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to analyze execution trends: {e}\")\n",
    "            raise WriterError(f\"Failed to analyze execution trends: {e}\") from e\n",
    "\n",
    "    def detect_quality_anomalies(self) -> QualityAnomalies:\n",
    "        \"\"\"\n",
    "        Detect data quality anomalies.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing anomaly detection results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Detecting quality anomalies\")\n",
    "\n",
    "            # Query logs\n",
    "            df = self.storage_manager.query_logs()\n",
    "\n",
    "            # Detect anomalies\n",
    "            return self.quality_analyzer.detect_quality_anomalies(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to detect quality anomalies: {e}\")\n",
    "            raise WriterError(f\"Failed to detect quality anomalies: {e}\") from e\n",
    "\n",
    "    def generate_performance_report(self) -> PerformanceReport:\n",
    "        \"\"\"\n",
    "        Generate comprehensive performance report.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing performance report\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Generating performance report\")\n",
    "\n",
    "            # Query logs\n",
    "            df = self.storage_manager.query_logs()\n",
    "\n",
    "            # Generate report\n",
    "            return self.analytics_engine.generate_performance_report(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate performance report: {e}\")\n",
    "            raise WriterError(f\"Failed to generate performance report: {e}\") from e\n",
    "\n",
    "    def get_metrics(self) -> WriterMetrics:\n",
    "        \"\"\"Get current writer metrics.\"\"\"\n",
    "        return self.performance_monitor.get_metrics()\n",
    "\n",
    "    def reset_metrics(self) -> None:\n",
    "        \"\"\"Reset writer metrics.\"\"\"\n",
    "        # Reset LogWriter metrics\n",
    "        self.metrics = {\n",
    "            \"total_writes\": 0,\n",
    "            \"successful_writes\": 0,\n",
    "            \"failed_writes\": 0,\n",
    "            \"total_duration_secs\": 0.0,\n",
    "            \"avg_write_duration_secs\": 0.0,\n",
    "            \"total_rows_written\": 0,\n",
    "            \"memory_usage_peak_mb\": 0.0,\n",
    "        }\n",
    "        # Reset performance monitor metrics\n",
    "        self.performance_monitor.reset_metrics()\n",
    "\n",
    "    def get_memory_usage(self) -> MemoryUsageInfo:\n",
    "        \"\"\"Get current memory usage information.\"\"\"\n",
    "        return self.performance_monitor.get_memory_usage()\n",
    "\n",
    "    def _update_metrics(self, write_result: WriteResult, success: bool) -> None:\n",
    "        \"\"\"Update writer metrics.\"\"\"\n",
    "        try:\n",
    "            self.metrics[\"total_writes\"] += 1\n",
    "            if success:\n",
    "                self.metrics[\"successful_writes\"] += 1\n",
    "            else:\n",
    "                self.metrics[\"failed_writes\"] += 1\n",
    "\n",
    "            if \"rows_written\" in write_result:\n",
    "                self.metrics[\"total_rows_written\"] += write_result[\"rows_written\"]\n",
    "\n",
    "            # Update performance monitor metrics\n",
    "            self.performance_monitor.metrics.update(self.metrics)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to update metrics: {e}\")\n",
    "\n",
    "    # Backward compatibility methods for tests\n",
    "    def _write_log_rows(\n",
    "        self,\n",
    "        log_rows: list[LogRow],\n",
    "        run_id: str,\n",
    "        metadata: Dict[str, Any] | None = None,\n",
    "    ) -> WriteResult:\n",
    "        \"\"\"Write log rows directly (for backward compatibility with tests).\"\"\"\n",
    "        return self.storage_manager.write_batch(log_rows, self.config.write_mode)\n",
    "\n",
    "    def _write_log_rows_batch(\n",
    "        self, log_rows: list[LogRow], run_id: str, batch_size: int = 100\n",
    "    ) -> WriteResult:\n",
    "        \"\"\"Write log rows in batches (for backward compatibility with tests).\"\"\"\n",
    "        results = []\n",
    "        for i in range(0, len(log_rows), batch_size):\n",
    "            batch = log_rows[i : i + batch_size]\n",
    "            result = self._write_log_rows(batch, run_id)\n",
    "            results.append(result)\n",
    "\n",
    "        total_rows = sum(r.get(\"rows_written\", 0) for r in results)\n",
    "        from datetime import datetime\n",
    "\n",
    "        return {\n",
    "            \"table_name\": self.storage_manager.table_fqn,\n",
    "            \"write_mode\": self.config.write_mode.value,\n",
    "            \"rows_written\": total_rows,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"success\": True,\n",
    "        }\n",
    "\n",
    "    def _create_dataframe_from_log_rows(self, log_rows: list[LogRow]) -> Any:\n",
    "        \"\"\"Create DataFrame from log rows (for backward compatibility with tests).\"\"\"\n",
    "        # Convert TypedDict to regular dicts for createDataFrame\n",
    "        dict_rows = [dict(row) for row in log_rows]\n",
    "        return self.spark.createDataFrame(dict_rows, schema=self.schema)  # type: ignore[attr-defined]\n",
    "\n",
    "    def detect_anomalies(self, log_rows: list[LogRow]) -> AnomalyReport:\n",
    "        \"\"\"Detect anomalies in log data (for backward compatibility with tests).\"\"\"\n",
    "        if not self.config.enable_anomaly_detection:\n",
    "            return {\n",
    "                \"performance_anomalies\": [],\n",
    "                \"quality_anomalies\": [],\n",
    "                \"anomaly_score\": 0.0,\n",
    "                \"total_anomalies\": 0,\n",
    "                \"total_executions\": 0,\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Basic anomaly detection logic\n",
    "            if not log_rows:\n",
    "                return {\n",
    "                    \"performance_anomalies\": [],\n",
    "                    \"quality_anomalies\": [],\n",
    "                    \"anomaly_score\": 0.0,\n",
    "                    \"total_anomalies\": 0,\n",
    "                    \"total_executions\": len(log_rows),\n",
    "                }\n",
    "\n",
    "            # Check for duration anomalies (very simple logic)\n",
    "            durations = [\n",
    "                row.get(\"duration_secs\", 0)\n",
    "                for row in log_rows\n",
    "                if \"duration_secs\" in row\n",
    "            ]\n",
    "            if not durations:\n",
    "                return {\n",
    "                    \"performance_anomalies\": [],\n",
    "                    \"quality_anomalies\": [],\n",
    "                    \"anomaly_score\": 0.0,\n",
    "                    \"total_anomalies\": 0,\n",
    "                    \"total_executions\": len(log_rows),\n",
    "                }\n",
    "\n",
    "            avg_duration = sum(durations) / len(durations)\n",
    "            threshold = avg_duration * 2  # 2x average is anomalous\n",
    "\n",
    "            # from .monitoring import PerformanceAnomaly  # Removed: defined in notebook cells above\n",
    "\n",
    "            performance_anomalies = []\n",
    "            for row in log_rows:\n",
    "                duration = row.get(\"duration_secs\", 0)\n",
    "                if duration > threshold:\n",
    "                    anomaly: PerformanceAnomaly = {\n",
    "                        \"step\": row.get(\"step_name\", \"unknown\"),\n",
    "                        \"execution_time\": float(duration),\n",
    "                        \"validation_rate\": float(row.get(\"validation_rate\", 0.0)),\n",
    "                        \"success\": bool(row.get(\"success\", False)),\n",
    "                    }\n",
    "                    performance_anomalies.append(anomaly)\n",
    "\n",
    "            total_anomalies = len(performance_anomalies)\n",
    "            total_executions = len(log_rows)\n",
    "            anomaly_score = (\n",
    "                (total_anomalies / total_executions * 100)\n",
    "                if total_executions > 0\n",
    "                else 0.0\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"performance_anomalies\": performance_anomalies,\n",
    "                \"quality_anomalies\": [],\n",
    "                \"anomaly_score\": round(anomaly_score, 2),\n",
    "                \"total_anomalies\": total_anomalies,\n",
    "                \"total_executions\": total_executions,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Anomaly detection failed: {e}\")\n",
    "            return {\n",
    "                \"performance_anomalies\": [],\n",
    "                \"quality_anomalies\": [],\n",
    "                \"anomaly_score\": 0.0,\n",
    "                \"total_anomalies\": 0,\n",
    "                \"total_executions\": len(log_rows) if log_rows else 0,\n",
    "            }\n",
    "\n",
    "    # Additional methods expected by tests\n",
    "    def validate_log_data_quality(self, log_rows: list[LogRow]) -> DataQualityReport:\n",
    "        \"\"\"Validate log data quality (for backward compatibility with tests).\"\"\"\n",
    "        try:\n",
    "            # from ..validation.utils import get_dataframe_info  # Removed: defined in notebook cells above\n",
    "\n",
    "            if not log_rows:\n",
    "                return {\n",
    "                    \"is_valid\": True,\n",
    "                    \"total_rows\": 0,\n",
    "                    \"null_counts\": {},\n",
    "                    \"validation_issues\": [],\n",
    "                    \"failed_executions\": 0,\n",
    "                    \"data_quality_score\": 100.0,\n",
    "                }\n",
    "\n",
    "            # Create DataFrame for validation\n",
    "            df = self._create_dataframe_from_log_rows(log_rows)\n",
    "\n",
    "            # Get basic info\n",
    "            df_info = get_dataframe_info(df)\n",
    "\n",
    "            # Count failed executions\n",
    "            failed_executions = sum(\n",
    "                1 for row in log_rows if not row.get(\"success\", True)\n",
    "            )\n",
    "\n",
    "            # Calculate quality score\n",
    "            total_rows = df_info.get(\"row_count\", len(log_rows))\n",
    "            validation_rate = 100.0  # Simplified\n",
    "            data_quality_score = (\n",
    "                validation_rate\n",
    "                if failed_executions == 0\n",
    "                else max(0, validation_rate - (failed_executions / total_rows * 100))\n",
    "            )\n",
    "\n",
    "            # Check for null values in critical columns\n",
    "            null_counts: Dict[str, int] = {}\n",
    "\n",
    "            # Determine validation issues\n",
    "            validation_issues = []\n",
    "            if failed_executions > 0:\n",
    "                validation_issues.append(f\"{failed_executions} failed executions\")\n",
    "\n",
    "            return {\n",
    "                \"is_valid\": failed_executions == 0 and len(validation_issues) == 0,\n",
    "                \"total_rows\": total_rows,\n",
    "                \"null_counts\": null_counts,\n",
    "                \"validation_issues\": validation_issues,\n",
    "                \"failed_executions\": failed_executions,\n",
    "                \"data_quality_score\": round(data_quality_score, 2),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"is_valid\": False,\n",
    "                \"total_rows\": len(log_rows) if log_rows else 0,\n",
    "                \"null_counts\": {},\n",
    "                \"validation_issues\": [str(e)],\n",
    "                \"failed_executions\": 0,\n",
    "                \"data_quality_score\": 0.0,\n",
    "            }\n",
    "\n",
    "    # ========================================================================\n",
    "    # New simplified API methods for working with PipelineReport\n",
    "    # ========================================================================\n",
    "\n",
    "    def _convert_report_to_log_rows(\n",
    "        self, report: PipelineReport, run_id: str | None = None\n",
    "    ) -> list[LogRow]:\n",
    "        \"\"\"\n",
    "        Convert a PipelineReport to log rows for storage.\n",
    "\n",
    "        This method extracts data from a PipelineReport and creates one log row\n",
    "        per pipeline step (bronze, silver, gold) with step-specific metrics.\n",
    "\n",
    "        Args:\n",
    "            report: PipelineReport to convert\n",
    "            run_id: Optional run ID (generated if not provided)\n",
    "\n",
    "        Returns:\n",
    "            List of LogRow dictionaries ready for storage (one per step)\n",
    "        \"\"\"\n",
    "\n",
    "        if run_id is None:\n",
    "            run_id = str(uuid.uuid4())\n",
    "\n",
    "        log_rows: list[LogRow] = []\n",
    "\n",
    "        # Helper function to parse datetime strings\n",
    "        def parse_datetime(dt_str: str | None) -> datetime | None:\n",
    "            if dt_str is None:\n",
    "                return None\n",
    "            try:\n",
    "                return datetime.fromisoformat(dt_str)\n",
    "            except (ValueError, AttributeError):\n",
    "                return None\n",
    "\n",
    "        # Process bronze steps\n",
    "        for step_name, step_info in report.bronze_results.items():\n",
    "            # Calculate valid/invalid rows from validation rate\n",
    "            rows_processed = int(step_info.get(\"rows_processed\") or 0)\n",
    "            validation_rate_val = step_info.get(\"validation_rate\")\n",
    "            validation_rate = float(\n",
    "                validation_rate_val if validation_rate_val is not None else 100.0\n",
    "            )\n",
    "            valid_rows = int(rows_processed * validation_rate / 100.0)\n",
    "            invalid_rows = rows_processed - valid_rows\n",
    "\n",
    "            table_fqn = step_info.get(\"output_table\")\n",
    "            table_total_rows = step_info.get(\"table_total_rows\")\n",
    "            if table_total_rows is None:\n",
    "                table_total_rows = self._get_table_total_rows(table_fqn)\n",
    "\n",
    "            bronze_log_row: LogRow = {\n",
    "                # Run-level information\n",
    "                \"run_id\": run_id,\n",
    "                \"run_mode\": report.mode.value,\n",
    "                \"run_started_at\": report.start_time,\n",
    "                \"run_ended_at\": report.end_time,\n",
    "                # Execution context\n",
    "                \"execution_id\": report.execution_id,\n",
    "                \"pipeline_id\": report.pipeline_id,\n",
    "                \"schema\": self.config.table_schema,\n",
    "                # Step-level information\n",
    "                \"phase\": \"bronze\",\n",
    "                \"step_name\": step_name,\n",
    "                \"step_type\": \"bronze\",\n",
    "                # Timing information\n",
    "                \"start_time\": parse_datetime(step_info.get(\"start_time\")),\n",
    "                \"end_time\": parse_datetime(step_info.get(\"end_time\")),\n",
    "                \"duration_secs\": float(step_info.get(\"duration\", 0.0)),\n",
    "                # Table information\n",
    "                \"table_fqn\": step_info.get(\"output_table\"),\n",
    "                \"write_mode\": step_info.get(\"write_mode\"),\n",
    "                # Data metrics\n",
    "                \"rows_processed\": rows_processed,\n",
    "                \"rows_written\": int(step_info.get(\"rows_written\") or rows_processed),\n",
    "                \"input_rows\": int(step_info.get(\"input_rows\") or rows_processed),\n",
    "                \"output_rows\": int(step_info.get(\"rows_written\") or rows_processed),\n",
    "                \"table_total_rows\": table_total_rows,\n",
    "                # Validation metrics\n",
    "                \"valid_rows\": valid_rows,\n",
    "                \"invalid_rows\": invalid_rows,\n",
    "                \"validation_rate\": validation_rate,\n",
    "                # Execution status\n",
    "                \"success\": step_info.get(\"status\") == \"completed\",\n",
    "                \"error_message\": step_info.get(\"error\"),\n",
    "                # Performance metrics\n",
    "                \"memory_usage_mb\": None,\n",
    "                \"cpu_usage_percent\": None,\n",
    "                # Metadata\n",
    "                \"metadata\": {},\n",
    "            }\n",
    "            log_rows.append(bronze_log_row)\n",
    "\n",
    "        # Process silver steps\n",
    "        for step_name, step_info in report.silver_results.items():\n",
    "            # Calculate valid/invalid rows from validation rate\n",
    "            rows_processed = int(step_info.get(\"rows_processed\") or 0)\n",
    "            validation_rate_val = step_info.get(\"validation_rate\")\n",
    "            validation_rate = float(\n",
    "                validation_rate_val if validation_rate_val is not None else 100.0\n",
    "            )\n",
    "            valid_rows = int(rows_processed * validation_rate / 100.0)\n",
    "            invalid_rows = rows_processed - valid_rows\n",
    "\n",
    "            table_fqn = step_info.get(\"output_table\")\n",
    "            table_total_rows = step_info.get(\"table_total_rows\")\n",
    "            if table_total_rows is None:\n",
    "                table_total_rows = self._get_table_total_rows(table_fqn)\n",
    "\n",
    "            silver_log_row: LogRow = {\n",
    "                # Run-level information\n",
    "                \"run_id\": run_id,\n",
    "                \"run_mode\": report.mode.value,\n",
    "                \"run_started_at\": report.start_time,\n",
    "                \"run_ended_at\": report.end_time,\n",
    "                # Execution context\n",
    "                \"execution_id\": report.execution_id,\n",
    "                \"pipeline_id\": report.pipeline_id,\n",
    "                \"schema\": self.config.table_schema,\n",
    "                # Step-level information\n",
    "                \"phase\": \"silver\",\n",
    "                \"step_name\": step_name,\n",
    "                \"step_type\": \"silver\",\n",
    "                # Timing information\n",
    "                \"start_time\": parse_datetime(step_info.get(\"start_time\")),\n",
    "                \"end_time\": parse_datetime(step_info.get(\"end_time\")),\n",
    "                \"duration_secs\": float(step_info.get(\"duration\", 0.0)),\n",
    "                # Table information\n",
    "                \"table_fqn\": step_info.get(\"output_table\"),\n",
    "                \"write_mode\": step_info.get(\"write_mode\"),\n",
    "                # Data metrics\n",
    "                \"rows_processed\": rows_processed,\n",
    "                \"rows_written\": int(step_info.get(\"rows_written\") or rows_processed),\n",
    "                \"input_rows\": int(step_info.get(\"input_rows\") or rows_processed),\n",
    "                \"output_rows\": int(step_info.get(\"rows_written\") or rows_processed),\n",
    "                \"table_total_rows\": table_total_rows,\n",
    "                # Validation metrics\n",
    "                \"valid_rows\": valid_rows,\n",
    "                \"invalid_rows\": invalid_rows,\n",
    "                \"validation_rate\": validation_rate,\n",
    "                # Execution status\n",
    "                \"success\": step_info.get(\"status\") == \"completed\",\n",
    "                \"error_message\": step_info.get(\"error\"),\n",
    "                # Performance metrics\n",
    "                \"memory_usage_mb\": None,\n",
    "                \"cpu_usage_percent\": None,\n",
    "                # Metadata\n",
    "                \"metadata\": {},\n",
    "            }\n",
    "            log_rows.append(silver_log_row)\n",
    "\n",
    "        # Process gold steps\n",
    "        for step_name, step_info in report.gold_results.items():\n",
    "            # Calculate valid/invalid rows from validation rate\n",
    "            rows_processed = int(step_info.get(\"rows_processed\") or 0)\n",
    "            validation_rate_val = step_info.get(\"validation_rate\")\n",
    "            validation_rate = float(\n",
    "                validation_rate_val if validation_rate_val is not None else 100.0\n",
    "            )\n",
    "            valid_rows = int(rows_processed * validation_rate / 100.0)\n",
    "            invalid_rows = rows_processed - valid_rows\n",
    "\n",
    "            table_fqn = step_info.get(\"output_table\")\n",
    "            table_total_rows = step_info.get(\"table_total_rows\")\n",
    "            if table_total_rows is None:\n",
    "                table_total_rows = self._get_table_total_rows(table_fqn)\n",
    "\n",
    "            gold_log_row: LogRow = {\n",
    "                # Run-level information\n",
    "                \"run_id\": run_id,\n",
    "                \"run_mode\": report.mode.value,\n",
    "                \"run_started_at\": report.start_time,\n",
    "                \"run_ended_at\": report.end_time,\n",
    "                # Execution context\n",
    "                \"execution_id\": report.execution_id,\n",
    "                \"pipeline_id\": report.pipeline_id,\n",
    "                \"schema\": self.config.table_schema,\n",
    "                # Step-level information\n",
    "                \"phase\": \"gold\",\n",
    "                \"step_name\": step_name,\n",
    "                \"step_type\": \"gold\",\n",
    "                # Timing information\n",
    "                \"start_time\": parse_datetime(step_info.get(\"start_time\")),\n",
    "                \"end_time\": parse_datetime(step_info.get(\"end_time\")),\n",
    "                \"duration_secs\": float(step_info.get(\"duration\", 0.0)),\n",
    "                # Table information\n",
    "                \"table_fqn\": step_info.get(\"output_table\"),\n",
    "                \"write_mode\": step_info.get(\"write_mode\"),\n",
    "                # Data metrics\n",
    "                \"rows_processed\": rows_processed,\n",
    "                \"rows_written\": int(step_info.get(\"rows_written\") or rows_processed),\n",
    "                \"input_rows\": int(step_info.get(\"input_rows\") or rows_processed),\n",
    "                \"output_rows\": int(step_info.get(\"rows_written\") or rows_processed),\n",
    "                \"table_total_rows\": table_total_rows,\n",
    "                # Validation metrics\n",
    "                \"valid_rows\": valid_rows,\n",
    "                \"invalid_rows\": invalid_rows,\n",
    "                \"validation_rate\": validation_rate,\n",
    "                # Execution status\n",
    "                \"success\": step_info.get(\"status\") == \"completed\",\n",
    "                \"error_message\": step_info.get(\"error\"),\n",
    "                # Performance metrics\n",
    "                \"memory_usage_mb\": None,\n",
    "                \"cpu_usage_percent\": None,\n",
    "                # Metadata\n",
    "                \"metadata\": {},\n",
    "            }\n",
    "            log_rows.append(gold_log_row)\n",
    "\n",
    "        return log_rows\n",
    "\n",
    "    def create_table(\n",
    "        self, report: PipelineReport, run_id: str | None = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create or overwrite the log table with data from a PipelineReport.\n",
    "\n",
    "        This method creates the log table if it doesn't exist, and writes\n",
    "        the report data using OVERWRITE mode (replacing any existing data).\n",
    "\n",
    "        Args:\n",
    "            report: PipelineReport to write\n",
    "            run_id: Optional run ID (generated if not provided)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with write results including:\n",
    "                - success: Whether the operation succeeded\n",
    "                - run_id: The run identifier used\n",
    "                - rows_written: Number of rows written\n",
    "                - table_fqn: Fully qualified table name\n",
    "\n",
    "        Example:\n",
    "            >>> writer = LogWriter(spark, schema=\"analytics\", table_name=\"logs\")\n",
    "            >>> result = writer.create_table(pipeline_report)\n",
    "            >>> print(f\"Created table with {result['rows_written']} rows\")\n",
    "        \"\"\"\n",
    "        operation_id = str(uuid.uuid4())\n",
    "        if run_id is None:\n",
    "            run_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            # Reset per-operation cache\n",
    "            self._reset_table_total_rows_cache()\n",
    "\n",
    "            # Start performance monitoring\n",
    "            self.performance_monitor.start_operation(operation_id, \"create_table\")\n",
    "\n",
    "            # Log operation start\n",
    "            self.logger.info(f\"📊 Creating log table {self.table_fqn} for run {run_id}\")\n",
    "\n",
    "            # Convert report to log rows\n",
    "            log_rows = self._convert_report_to_log_rows(report, run_id)\n",
    "\n",
    "            # Create table if not exists\n",
    "            self.storage_manager.create_table_if_not_exists(self.schema)\n",
    "\n",
    "            # Write to storage with OVERWRITE mode\n",
    "            write_result = self.storage_manager.write_batch(\n",
    "                log_rows, WriteMode.OVERWRITE\n",
    "            )\n",
    "\n",
    "            # Update metrics\n",
    "            self._update_metrics(write_result, True)\n",
    "\n",
    "            # End performance monitoring\n",
    "            operation_metrics = self.performance_monitor.end_operation(\n",
    "                operation_id, True, write_result.get(\"rows_written\", 0)\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"run_id\": run_id,\n",
    "                \"operation_id\": operation_id,\n",
    "                \"rows_written\": write_result.get(\"rows_written\", 0),\n",
    "                \"table_fqn\": self.table_fqn,\n",
    "                \"write_result\": write_result,\n",
    "                \"operation_metrics\": operation_metrics,\n",
    "            }\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"✅ Successfully created log table {self.table_fqn} with \"\n",
    "                f\"{result['rows_written']} row(s) for run {run_id}\"\n",
    "            )\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            # End performance monitoring with failure\n",
    "            self.performance_monitor.end_operation(operation_id, False, 0, str(e))\n",
    "            # Create empty WriteResult for error case\n",
    "            empty_result: WriteResult = {\n",
    "                \"table_name\": self.storage_manager.table_fqn,\n",
    "                \"write_mode\": self.config.write_mode.value,\n",
    "                \"rows_written\": 0,\n",
    "                \"timestamp\": \"\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "            self._update_metrics(empty_result, False)\n",
    "\n",
    "            self.logger.error(f\"❌ Failed to create log table for run {run_id}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def append(\n",
    "        self, report: PipelineReport, run_id: str | None = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Append data from a PipelineReport to the log table.\n",
    "\n",
    "        This method appends the report data to an existing log table. If the\n",
    "        table doesn't exist, it will be created first.\n",
    "\n",
    "        Args:\n",
    "            report: PipelineReport to append\n",
    "            run_id: Optional run ID (generated if not provided)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with write results including:\n",
    "                - success: Whether the operation succeeded\n",
    "                - run_id: The run identifier used\n",
    "                - rows_written: Number of rows written\n",
    "                - table_fqn: Fully qualified table name\n",
    "\n",
    "        Example:\n",
    "            >>> writer = LogWriter(spark, schema=\"analytics\", table_name=\"logs\")\n",
    "            >>> result = writer.append(pipeline_report)\n",
    "            >>> print(f\"Appended {result['rows_written']} rows to {result['table_fqn']}\")\n",
    "        \"\"\"\n",
    "        operation_id = str(uuid.uuid4())\n",
    "        if run_id is None:\n",
    "            run_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            # Reset per-operation cache\n",
    "            self._reset_table_total_rows_cache()\n",
    "\n",
    "            # Start performance monitoring\n",
    "            self.performance_monitor.start_operation(operation_id, \"append\")\n",
    "\n",
    "            # Log operation start\n",
    "            self.logger.info(\n",
    "                f\"📊 Appending to log table {self.table_fqn} for run {run_id}\"\n",
    "            )\n",
    "\n",
    "            # Convert report to log rows\n",
    "            log_rows = self._convert_report_to_log_rows(report, run_id)\n",
    "\n",
    "            # Create table if not exists (for first append)\n",
    "            self.storage_manager.create_table_if_not_exists(self.schema)\n",
    "\n",
    "            # Write to storage with APPEND mode\n",
    "            write_result = self.storage_manager.write_batch(log_rows, WriteMode.APPEND)\n",
    "\n",
    "            # Update metrics\n",
    "            self._update_metrics(write_result, True)\n",
    "\n",
    "            # End performance monitoring\n",
    "            operation_metrics = self.performance_monitor.end_operation(\n",
    "                operation_id, True, write_result.get(\"rows_written\", 0)\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"run_id\": run_id,\n",
    "                \"operation_id\": operation_id,\n",
    "                \"rows_written\": write_result.get(\"rows_written\", 0),\n",
    "                \"table_fqn\": self.table_fqn,\n",
    "                \"write_result\": write_result,\n",
    "                \"operation_metrics\": operation_metrics,\n",
    "            }\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"✅ Successfully appended {result['rows_written']} row(s) to \"\n",
    "                f\"{self.table_fqn} for run {run_id}\"\n",
    "            )\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            # End performance monitoring with failure\n",
    "            self.performance_monitor.end_operation(operation_id, False, 0, str(e))\n",
    "            # Create empty WriteResult for error case\n",
    "            empty_result: WriteResult = {\n",
    "                \"table_name\": self.storage_manager.table_fqn,\n",
    "                \"write_mode\": self.config.write_mode.value,\n",
    "                \"rows_written\": 0,\n",
    "                \"timestamp\": \"\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "            self._update_metrics(empty_result, False)\n",
    "\n",
    "            self.logger.error(f\"❌ Failed to append to log table for run {run_id}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example\n",
    "#\n",
    "# Here's how to initialize PipelineBuilder and LogWriter:\n",
    "\n",
    "# Example: Initialize PipelineBuilder and LogWriter\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PipelineBuilder Example\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Initialize PipelineBuilder\n",
    "builder = PipelineBuilder(spark=spark, schema=\"analytics\")\n",
    "print(\"✅ PipelineBuilder initialized\")\n",
    "\n",
    "# Initialize LogWriter (simplified API)\n",
    "log_writer = LogWriter(spark, schema=\"analytics\", table_name=\"pipeline_logs\")\n",
    "print(\"✅ LogWriter initialized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}