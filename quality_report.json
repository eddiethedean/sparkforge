{
  "black": {
    "success": false,
    "output": "--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/conftest_integration.py\t2025-09-20 14:00:38.758512+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/conftest_integration.py\t2025-09-20 16:53:34.636973+00:00\n@@ -2,11 +2,10 @@\n Integration test configuration and fixtures.\n \n This module provides fixtures and configuration for integration tests,\n which use real Spark sessions but mock external systems.\n \"\"\"\n-\n \n import pytest\n from pyspark.sql import SparkSession\n from pyspark.sql import functions as F\n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/sparkforge/logging.py\t2025-09-18 21:58:36.169990+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/sparkforge/logging.py\t2025-09-20 16:53:34.650657+00:00\n@@ -127,11 +127,13 @@\n \n     def step_failed(\n         self, stage: str, step: str, error: str, duration: float = 0\n     ) -> None:\n         \"\"\"Log step failure.\"\"\"\n-        self.error(f\"\u274c Failed {stage.upper()} step: {step} ({duration:.2f}s) - {error}\")\n+        self.error(\n+            f\"\u274c Failed {stage.upper()} step: {step} ({duration:.2f}s) - {error}\"\n+        )\n \n     def validation_passed(\n         self, stage: str, step: str, rate: float, threshold: float\n     ) -> None:\n         \"\"\"Log validation success.\"\"\"\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/__init__.py\t2025-09-20 16:10:36.186609+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/__init__.py\t2025-09-20 16:53:34.657194+00:00\n@@ -14,11 +14,11 @@\n     run_model_creation_performance_tests,\n     run_serialization_performance_tests,\n )\n \n __all__ = [\n-    'PerformanceMonitor',\n-    'PerformanceResult',\n-    'run_validation_performance_tests',\n-    'run_model_creation_performance_tests',\n-    'run_serialization_performance_tests',\n+    \"PerformanceMonitor\",\n+    \"PerformanceResult\",\n+    \"run_validation_performance_tests\",\n+    \"run_model_creation_performance_tests\",\n+    \"run_serialization_performance_tests\",\n ]\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/conftest.py\t2025-09-20 16:10:36.186810+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/conftest.py\t2025-09-20 16:53:34.706221+00:00\n@@ -21,18 +21,18 @@\n @pytest.fixture(scope=\"session\", autouse=True)\n def setup_performance_testing():\n     \"\"\"Setup performance testing environment.\"\"\"\n     # Ensure we have a clean performance monitor\n     performance_monitor.results.clear()\n-    \n+\n     # Set up environment variables for performance testing\n     os.environ[\"SPARKFORGE_PERFORMANCE_TESTING\"] = \"true\"\n-    \n+\n     yield\n-    \n+\n     # Cleanup after all performance tests\n-    if hasattr(performance_monitor, 'results'):\n+    if hasattr(performance_monitor, \"results\"):\n         performance_monitor.results.clear()\n \n \n @pytest.fixture(scope=\"function\")\n def performance_monitor_clean():\n@@ -93,74 +93,83 @@\n \n \n # Performance test markers\n def pytest_configure(config):\n     \"\"\"Configure performance test markers.\"\"\"\n-    config.addinivalue_line(\n-        \"markers\", \"performance: mark test as a performance test\"\n-    )\n-    config.addinivalue_line(\n-        \"markers\", \"slow: mark test as slow running\"\n-    )\n-    config.addinivalue_line(\n-        \"markers\", \"memory: mark test that measures memory usage\"\n-    )\n+    config.addinivalue_line(\"markers\", \"performance: mark test as a performance test\")\n+    config.addinivalue_line(\"markers\", \"slow: mark test as slow running\")\n+    config.addinivalue_line(\"markers\", \"memory: mark test that measures memory usage\")\n     config.addinivalue_line(\n         \"markers\", \"regression: mark test that checks for performance regression\"\n     )\n-    config.addinivalue_line(\n-        \"markers\", \"benchmark: mark test as a benchmark test\"\n-    )\n+    config.addinivalue_line(\"markers\", \"benchmark: mark test as a benchmark test\")\n \n \n # Performance test collection\n def pytest_collection_modifyitems(config, items):\n     \"\"\"Modify test collection for performance tests.\"\"\"\n     for item in items:\n         # Add performance marker to all tests in performance directory\n         if \"performance\" in str(item.fspath):\n             item.add_marker(pytest.mark.performance)\n-        \n+\n         # Add slow marker to tests that are expected to be slow\n-        if any(keyword in item.name.lower() for keyword in [\"memory\", \"large\", \"stress\"]):\n+        if any(\n+            keyword in item.name.lower() for keyword in [\"memory\", \"large\", \"stress\"]\n+        ):\n             item.add_marker(pytest.mark.slow)\n-        \n+\n         # Add memory marker to tests that measure memory\n         if \"memory\" in item.name.lower():\n             item.add_marker(pytest.mark.memory)\n-        \n+\n         # Add regression marker to tests that check regressions\n         if \"regression\" in item.name.lower():\n             item.add_marker(pytest.mark.regression)\n \n \n # Performance test reporting\n def pytest_terminal_summary(terminalreporter, exitstatus, config):\n     \"\"\"Add performance test summary to terminal output.\"\"\"\n     if not config.getoption(\"--quiet\", False):\n         # Only show performance summary if we ran performance tests\n-        if any(\"performance\" in str(item.fspath) for item in terminalreporter.config.getoption(\"--collect-only\", False) or []):\n+        if any(\n+            \"performance\" in str(item.fspath)\n+            for item in terminalreporter.config.getoption(\"--collect-only\", False) or []\n+        ):\n             terminalreporter.write_sep(\"=\", \"Performance Test Summary\")\n-            \n+\n             summary = performance_monitor.get_performance_summary()\n             if summary.get(\"total_tests\", 0) > 0:\n-                terminalreporter.write(f\"Total performance tests: {summary['total_tests']}\\n\")\n-                terminalreporter.write(f\"Successful tests: {summary['successful_tests']}\\n\")\n+                terminalreporter.write(\n+                    f\"Total performance tests: {summary['total_tests']}\\n\"\n+                )\n+                terminalreporter.write(\n+                    f\"Successful tests: {summary['successful_tests']}\\n\"\n+                )\n                 terminalreporter.write(f\"Failed tests: {summary['failed_tests']}\\n\")\n-                terminalreporter.write(f\"Functions tested: {summary['functions_tested']}\\n\")\n-                terminalreporter.write(f\"Total execution time: {summary['total_execution_time']:.2f}s\\n\")\n-                \n+                terminalreporter.write(\n+                    f\"Functions tested: {summary['functions_tested']}\\n\"\n+                )\n+                terminalreporter.write(\n+                    f\"Total execution time: {summary['total_execution_time']:.2f}s\\n\"\n+                )\n+\n                 # Check for regressions\n                 regressions = []\n                 for result in performance_monitor.results:\n                     if result.success:\n-                        regression = performance_monitor.check_regression(result.function_name)\n+                        regression = performance_monitor.check_regression(\n+                            result.function_name\n+                        )\n                         if regression[\"status\"] == \"regression_detected\":\n                             regressions.append(result.function_name)\n-                \n+\n                 if regressions:\n-                    terminalreporter.write(f\"\u26a0\ufe0f  Performance regressions detected in: {', '.join(regressions)}\\n\")\n+                    terminalreporter.write(\n+                        f\"\u26a0\ufe0f  Performance regressions detected in: {', '.join(regressions)}\\n\"\n+                    )\n                 else:\n                     terminalreporter.write(\"\u2705 No performance regressions detected\\n\")\n             else:\n                 terminalreporter.write(\"No performance test results available\\n\")\n \n@@ -186,11 +195,13 @@\n     if \"performance\" in str(item.fspath):\n         if call.when == \"call\":\n             # Check if test exceeded time limits\n             for result in performance_monitor.results:\n                 if result.execution_time > 10.0:  # 10 second timeout\n-                    pytest.fail(f\"Performance test exceeded time limit: {result.execution_time:.2f}s\")\n+                    pytest.fail(\n+                        f\"Performance test exceeded time limit: {result.execution_time:.2f}s\"\n+                    )\n \n \n # Environment setup for performance tests\n @pytest.fixture(scope=\"session\")\n def performance_test_environment():\n@@ -199,18 +210,18 @@\n         \"SPARKFORGE_PERFORMANCE_TESTING\": \"true\",\n         \"SPARKFORGE_PERFORMANCE_BASELINE_FILE\": \"performance_baseline.json\",\n         \"SPARKFORGE_PERFORMANCE_REPORT_FILE\": \"performance_report.json\",\n         \"SPARKFORGE_PERFORMANCE_TOLERANCE\": \"0.2\",\n     }\n-    \n+\n     original_env = {}\n     for key, value in env_vars.items():\n         original_env[key] = os.environ.get(key)\n         os.environ[key] = value\n-    \n+\n     yield env_vars\n-    \n+\n     # Restore original environment\n     for key, original_value in original_env.items():\n         if original_value is None:\n             os.environ.pop(key, None)\n         else:\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_pipeline_execution.py\t2025-09-20 14:00:38.760096+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_pipeline_execution.py\t2025-09-20 16:53:34.730586+00:00\n@@ -2,11 +2,10 @@\n Tests for pipeline execution functionality.\n \n This module tests the actual pipeline execution flow with real data,\n including bronze, silver, and gold step execution in sequence.\n \"\"\"\n-\n \n from pyspark.sql import functions as F\n \n from sparkforge.execution import ExecutionEngine\n from sparkforge.models import (\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine_new.py\t2025-09-20 14:00:38.759516+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine_new.py\t2025-09-20 16:53:34.737398+00:00\n@@ -2,11 +2,10 @@\n Tests for the simplified execution engine.\n \n This module tests the step-by-step execution functionality of the simplified\n SparkForge execution system.\n \"\"\"\n-\n \n import pytest\n from pyspark.sql import DataFrame\n from pyspark.sql import functions as F\n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitor.py\t2025-09-20 16:10:36.187182+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitor.py\t2025-09-20 16:53:34.752082+00:00\n@@ -19,225 +19,248 @@\n \n \n @dataclass\n class PerformanceResult:\n     \"\"\"Container for performance measurement results.\"\"\"\n-    \n+\n     function_name: str\n     execution_time: float\n     memory_usage_mb: float\n     peak_memory_mb: float\n     iterations: int\n     timestamp: float\n     success: bool\n     error_message: Optional[str] = None\n-    \n+\n     @property\n     def throughput(self) -> float:\n         \"\"\"Calculate throughput (iterations per second).\"\"\"\n         if self.execution_time > 0:\n             return self.iterations / self.execution_time\n         return 0.0\n-    \n+\n     @property\n     def avg_time_per_iteration(self) -> float:\n         \"\"\"Calculate average time per iteration in milliseconds.\"\"\"\n         if self.iterations > 0:\n             return (self.execution_time * 1000) / self.iterations\n         return 0.0\n \n \n class PerformanceMonitor:\n     \"\"\"Performance monitoring and regression detection.\"\"\"\n-    \n+\n     def __init__(self, baseline_file: str = \"performance_baseline.json\"):\n         \"\"\"Initialize performance monitor with baseline storage.\"\"\"\n         self.baseline_file = baseline_file\n         self.baselines: Dict[str, PerformanceResult] = {}\n         self.results: List[PerformanceResult] = []\n         self.load_baselines()\n-    \n+\n     def load_baselines(self) -> None:\n         \"\"\"Load performance baselines from file.\"\"\"\n         import json\n-        \n+\n         if os.path.exists(self.baseline_file):\n             try:\n-                with open(self.baseline_file, 'r') as f:\n+                with open(self.baseline_file, \"r\") as f:\n                     baseline_data = json.load(f)\n                     for name, data in baseline_data.items():\n                         self.baselines[name] = PerformanceResult(**data)\n             except (json.JSONDecodeError, KeyError, TypeError):\n                 # If baseline file is corrupted, start fresh\n                 self.baselines = {}\n-    \n+\n     def save_baselines(self) -> None:\n         \"\"\"Save current baselines to file.\"\"\"\n         import json\n-        \n+\n         baseline_data = {}\n         for name, result in self.baselines.items():\n             baseline_data[name] = {\n-                'function_name': result.function_name,\n-                'execution_time': result.execution_time,\n-                'memory_usage_mb': result.memory_usage_mb,\n-                'peak_memory_mb': result.peak_memory_mb,\n-                'iterations': result.iterations,\n-                'timestamp': result.timestamp,\n-                'success': result.success,\n-                'error_message': result.error_message\n+                \"function_name\": result.function_name,\n+                \"execution_time\": result.execution_time,\n+                \"memory_usage_mb\": result.memory_usage_mb,\n+                \"peak_memory_mb\": result.peak_memory_mb,\n+                \"iterations\": result.iterations,\n+                \"timestamp\": result.timestamp,\n+                \"success\": result.success,\n+                \"error_message\": result.error_message,\n             }\n-        \n-        with open(self.baseline_file, 'w') as f:\n+\n+        with open(self.baseline_file, \"w\") as f:\n             json.dump(baseline_data, f, indent=2)\n-    \n+\n     @contextmanager\n     def measure_performance(self, function_name: str, iterations: int = 1):\n         \"\"\"Context manager for measuring function performance.\"\"\"\n         # Start memory tracing\n         tracemalloc.start()\n         process = psutil.Process()\n         initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n-        \n+\n         start_time = time.time()\n         start_timestamp = time.time()\n-        \n+\n         result = PerformanceResult(\n             function_name=function_name,\n             execution_time=0.0,\n             memory_usage_mb=0.0,\n             peak_memory_mb=0.0,\n             iterations=iterations,\n             timestamp=start_timestamp,\n-            success=False\n-        )\n-        \n+            success=False,\n+        )\n+\n         try:\n             yield result\n-            \n+\n             # Measure final metrics\n             end_time = time.time()\n             current, peak = tracemalloc.get_traced_memory()\n             final_memory = process.memory_info().rss / 1024 / 1024  # MB\n-            \n+\n             result.execution_time = end_time - start_time\n             result.memory_usage_mb = (current / 1024 / 1024) - initial_memory\n             result.peak_memory_mb = (peak / 1024 / 1024) - initial_memory\n             result.success = True\n-            \n+\n         except Exception as e:\n             result.error_message = str(e)\n             result.success = False\n-            \n+\n         finally:\n             tracemalloc.stop()\n             self.results.append(result)\n-    \n+\n     def run_performance_test(\n         self,\n         function: Callable,\n         function_name: str,\n         iterations: int = 1,\n         args: Tuple = (),\n-        kwargs: Dict[str, Any] = None\n+        kwargs: Dict[str, Any] = None,\n     ) -> PerformanceResult:\n         \"\"\"Run a performance test on a function.\"\"\"\n         if kwargs is None:\n             kwargs = {}\n-        \n+\n         with self.measure_performance(function_name, iterations) as result:\n             for _ in range(iterations):\n                 function(*args, **kwargs)\n-        \n+\n         return result\n-    \n-    def check_regression(self, function_name: str, tolerance: float = 0.2) -> Dict[str, Any]:\n+\n+    def check_regression(\n+        self, function_name: str, tolerance: float = 0.2\n+    ) -> Dict[str, Any]:\n         \"\"\"Check for performance regression against baseline.\"\"\"\n         # Find the most recent result for this function\n         recent_results = [r for r in self.results if r.function_name == function_name]\n         if not recent_results:\n             return {\"status\": \"no_data\", \"message\": \"No recent results found\"}\n-        \n+\n         latest_result = recent_results[-1]\n-        \n+\n         if function_name not in self.baselines:\n             return {\n                 \"status\": \"no_baseline\",\n                 \"message\": \"No baseline found for this function\",\n-                \"current\": latest_result\n+                \"current\": latest_result,\n             }\n-        \n+\n         baseline = self.baselines[function_name]\n-        \n+\n         # Check for regressions\n-        time_regression = latest_result.execution_time > baseline.execution_time * (1 + tolerance)\n-        memory_regression = latest_result.memory_usage_mb > baseline.memory_usage_mb * (1 + tolerance)\n-        \n+        time_regression = latest_result.execution_time > baseline.execution_time * (\n+            1 + tolerance\n+        )\n+        memory_regression = latest_result.memory_usage_mb > baseline.memory_usage_mb * (\n+            1 + tolerance\n+        )\n+\n         regression_info = {\n             \"status\": \"ok\",\n             \"function_name\": function_name,\n             \"baseline\": baseline,\n             \"current\": latest_result,\n             \"time_regression\": time_regression,\n             \"memory_regression\": memory_regression,\n-            \"time_change_percent\": ((latest_result.execution_time - baseline.execution_time) / baseline.execution_time) * 100,\n-            \"memory_change_percent\": ((latest_result.memory_usage_mb - baseline.memory_usage_mb) / baseline.memory_usage_mb) * 100\n+            \"time_change_percent\": (\n+                (latest_result.execution_time - baseline.execution_time)\n+                / baseline.execution_time\n+            )\n+            * 100,\n+            \"memory_change_percent\": (\n+                (latest_result.memory_usage_mb - baseline.memory_usage_mb)\n+                / baseline.memory_usage_mb\n+            )\n+            * 100,\n         }\n-        \n+\n         if time_regression or memory_regression:\n             regression_info[\"status\"] = \"regression_detected\"\n-        \n+\n         return regression_info\n-    \n+\n     def update_baseline(self, function_name: str) -> None:\n         \"\"\"Update baseline for a function with the most recent result.\"\"\"\n-        recent_results = [r for r in self.results if r.function_name == function_name and r.success]\n+        recent_results = [\n+            r for r in self.results if r.function_name == function_name and r.success\n+        ]\n         if recent_results:\n             self.baselines[function_name] = recent_results[-1]\n             self.save_baselines()\n-    \n+\n     def get_performance_summary(self) -> Dict[str, Any]:\n         \"\"\"Get summary of all performance results.\"\"\"\n         if not self.results:\n             return {\"message\": \"No performance data available\"}\n-        \n+\n         successful_results = [r for r in self.results if r.success]\n-        \n+\n         summary = {\n             \"total_tests\": len(self.results),\n             \"successful_tests\": len(successful_results),\n             \"failed_tests\": len(self.results) - len(successful_results),\n             \"functions_tested\": len(set(r.function_name for r in self.results)),\n             \"total_execution_time\": sum(r.execution_time for r in successful_results),\n-            \"avg_execution_time\": sum(r.execution_time for r in successful_results) / len(successful_results) if successful_results else 0,\n+            \"avg_execution_time\": (\n+                sum(r.execution_time for r in successful_results)\n+                / len(successful_results)\n+                if successful_results\n+                else 0\n+            ),\n             \"total_memory_used\": sum(r.memory_usage_mb for r in successful_results),\n-            \"results\": [r.__dict__ for r in self.results]\n+            \"results\": [r.__dict__ for r in self.results],\n         }\n-        \n+\n         return summary\n-    \n+\n     def benchmark_function(\n         self,\n         function: Callable,\n         function_name: str,\n         iterations: int = 100,\n         args: Tuple = (),\n         kwargs: Dict[str, Any] = None,\n-        warmup_iterations: int = 10\n+        warmup_iterations: int = 10,\n     ) -> PerformanceResult:\n         \"\"\"Benchmark a function with warmup and multiple iterations.\"\"\"\n         if kwargs is None:\n             kwargs = {}\n-        \n+\n         # Warmup runs\n         for _ in range(warmup_iterations):\n             function(*args, **kwargs)\n-        \n+\n         # Force garbage collection\n         gc.collect()\n-        \n+\n         # Actual benchmark\n-        return self.run_performance_test(function, function_name, iterations, args, kwargs)\n+        return self.run_performance_test(\n+            function, function_name, iterations, args, kwargs\n+        )\n \n \n # Global performance monitor instance\n performance_monitor = PerformanceMonitor()\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_pipeline_runner.py\t2025-09-20 14:00:38.760704+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_pipeline_runner.py\t2025-09-20 16:53:34.767987+00:00\n@@ -223,13 +223,11 @@\n         runner = SimplePipelineRunner(mock_spark, mock_config)\n         runner.execution_engine = mock_execution_engine\n \n         bronze_sources = {\"test_bronze\": Mock(spec=DataFrame)}\n \n-        runner.run_pipeline(\n-            [sample_bronze_step], PipelineMode.INITIAL, bronze_sources\n-        )\n+        runner.run_pipeline([sample_bronze_step], PipelineMode.INITIAL, bronze_sources)\n \n         # Verify execution engine was called\n         mock_execution_engine.execute_pipeline.assert_called_once()\n \n     def test_run_pipeline_without_bronze_sources(\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/caching_strategies.py\t2025-09-20 16:45:07.583390+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/caching_strategies.py\t2025-09-20 16:53:34.797627+00:00\n@@ -23,10 +23,11 @@\n \n \n @dataclass\n class CacheEntry:\n     \"\"\"Cache entry data structure.\"\"\"\n+\n     key: str\n     value: Any\n     created_at: datetime\n     last_accessed: datetime\n     access_count: int\n@@ -35,10 +36,11 @@\n \n \n @dataclass\n class CacheStats:\n     \"\"\"Cache statistics data structure.\"\"\"\n+\n     total_entries: int\n     total_size_bytes: int\n     hit_count: int\n     miss_count: int\n     eviction_count: int\n@@ -46,295 +48,300 @@\n     memory_usage_mb: float\n \n \n class MemoryCache:\n     \"\"\"In-memory cache with TTL and LRU eviction.\"\"\"\n-    \n+\n     def __init__(self, max_size_mb: int = 100, default_ttl: Optional[int] = None):\n         self.max_size_mb = max_size_mb\n         self.default_ttl = default_ttl\n         self.cache: Dict[str, CacheEntry] = {}\n         self.access_order: List[str] = []\n         self.lock = threading.RLock()\n-        \n+\n         # Statistics\n         self.hit_count = 0\n         self.miss_count = 0\n         self.eviction_count = 0\n-    \n+\n     def get(self, key: str) -> Optional[Any]:\n         \"\"\"Get value from cache.\"\"\"\n         with self.lock:\n             if key not in self.cache:\n                 self.miss_count += 1\n                 return None\n-            \n+\n             entry = self.cache[key]\n-            \n+\n             # Check TTL\n             if entry.ttl_seconds and self._is_expired(entry):\n                 del self.cache[key]\n                 self.access_order.remove(key)\n                 self.miss_count += 1\n                 return None\n-            \n+\n             # Update access info\n             entry.last_accessed = datetime.now()\n             entry.access_count += 1\n-            \n+\n             # Move to end of access order (most recently used)\n             self.access_order.remove(key)\n             self.access_order.append(key)\n-            \n+\n             self.hit_count += 1\n             return entry.value\n-    \n+\n     def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:\n         \"\"\"Set value in cache.\"\"\"\n         with self.lock:\n             # Calculate size\n             size_bytes = self._estimate_size(value)\n-            \n+\n             # Remove existing entry if it exists\n             if key in self.cache:\n                 self._remove_entry(key)\n-            \n+\n             # Create new entry\n             ttl = ttl_seconds or self.default_ttl\n             entry = CacheEntry(\n                 key=key,\n                 value=value,\n                 created_at=datetime.now(),\n                 last_accessed=datetime.now(),\n                 access_count=1,\n                 size_bytes=size_bytes,\n-                ttl_seconds=ttl\n+                ttl_seconds=ttl,\n             )\n-            \n+\n             # Add to cache\n             self.cache[key] = entry\n             self.access_order.append(key)\n-            \n+\n             # Check if we need to evict\n             self._evict_if_needed()\n-    \n+\n     def delete(self, key: str) -> bool:\n         \"\"\"Delete entry from cache.\"\"\"\n         with self.lock:\n             if key in self.cache:\n                 self._remove_entry(key)\n                 return True\n             return False\n-    \n+\n     def clear(self) -> None:\n         \"\"\"Clear all cache entries.\"\"\"\n         with self.lock:\n             self.cache.clear()\n             self.access_order.clear()\n-    \n+\n     def _remove_entry(self, key: str) -> None:\n         \"\"\"Remove entry from cache.\"\"\"\n         if key in self.cache:\n             del self.cache[key]\n             if key in self.access_order:\n                 self.access_order.remove(key)\n-    \n+\n     def _is_expired(self, entry: CacheEntry) -> bool:\n         \"\"\"Check if entry is expired.\"\"\"\n         if not entry.ttl_seconds:\n             return False\n-        \n+\n         elapsed = (datetime.now() - entry.created_at).total_seconds()\n         return elapsed > entry.ttl_seconds\n-    \n+\n     def _estimate_size(self, obj: Any) -> int:\n         \"\"\"Estimate size of object in bytes.\"\"\"\n         try:\n             return len(pickle.dumps(obj))\n         except Exception:\n             # Fallback estimation\n-            return len(str(obj).encode('utf-8'))\n-    \n+            return len(str(obj).encode(\"utf-8\"))\n+\n     def _evict_if_needed(self) -> None:\n         \"\"\"Evict entries if cache is too large.\"\"\"\n-        current_size_mb = sum(entry.size_bytes for entry in self.cache.values()) / 1024 / 1024\n-        \n+        current_size_mb = (\n+            sum(entry.size_bytes for entry in self.cache.values()) / 1024 / 1024\n+        )\n+\n         while current_size_mb > self.max_size_mb and self.access_order:\n             # Remove least recently used entry\n             oldest_key = self.access_order[0]\n             self._remove_entry(oldest_key)\n             self.eviction_count += 1\n-            \n+\n             # Recalculate size\n-            current_size_mb = sum(entry.size_bytes for entry in self.cache.values()) / 1024 / 1024\n-    \n+            current_size_mb = (\n+                sum(entry.size_bytes for entry in self.cache.values()) / 1024 / 1024\n+            )\n+\n     def get_stats(self) -> CacheStats:\n         \"\"\"Get cache statistics.\"\"\"\n         with self.lock:\n             total_size_bytes = sum(entry.size_bytes for entry in self.cache.values())\n             total_requests = self.hit_count + self.miss_count\n-            hit_rate = (self.hit_count / total_requests * 100) if total_requests > 0 else 0\n-            \n+            hit_rate = (\n+                (self.hit_count / total_requests * 100) if total_requests > 0 else 0\n+            )\n+\n             return CacheStats(\n                 total_entries=len(self.cache),\n                 total_size_bytes=total_size_bytes,\n                 hit_count=self.hit_count,\n                 miss_count=self.miss_count,\n                 eviction_count=self.eviction_count,\n                 hit_rate=hit_rate,\n-                memory_usage_mb=total_size_bytes / 1024 / 1024\n+                memory_usage_mb=total_size_bytes / 1024 / 1024,\n             )\n-    \n+\n     def cleanup_expired(self) -> int:\n         \"\"\"Remove expired entries and return count.\"\"\"\n         with self.lock:\n             expired_keys = [\n-                key for key, entry in self.cache.items()\n-                if self._is_expired(entry)\n+                key for key, entry in self.cache.items() if self._is_expired(entry)\n             ]\n-            \n+\n             for key in expired_keys:\n                 self._remove_entry(key)\n-            \n+\n             return len(expired_keys)\n \n \n class PersistentCache:\n     \"\"\"File-based persistent cache.\"\"\"\n-    \n+\n     def __init__(self, cache_dir: Union[str, Path], max_file_size_mb: int = 50):\n         self.cache_dir = Path(cache_dir)\n         self.cache_dir.mkdir(parents=True, exist_ok=True)\n         self.max_file_size_mb = max_file_size_mb\n         self.lock = threading.RLock()\n-    \n+\n     def get(self, key: str) -> Optional[Any]:\n         \"\"\"Get value from persistent cache.\"\"\"\n         cache_file = self._get_cache_file(key)\n-        \n+\n         if not cache_file.exists():\n             return None\n-        \n+\n         try:\n-            with open(cache_file, 'rb') as f:\n+            with open(cache_file, \"rb\") as f:\n                 entry = pickle.load(f)\n-            \n+\n             # Check if expired\n             if entry.ttl_seconds and self._is_expired(entry):\n                 cache_file.unlink()\n                 return None\n-            \n+\n             return entry.value\n-            \n+\n         except Exception:\n             # Remove corrupted cache file\n             if cache_file.exists():\n                 cache_file.unlink()\n             return None\n-    \n+\n     def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:\n         \"\"\"Set value in persistent cache.\"\"\"\n         cache_file = self._get_cache_file(key)\n-        \n+\n         try:\n             entry = CacheEntry(\n                 key=key,\n                 value=value,\n                 created_at=datetime.now(),\n                 last_accessed=datetime.now(),\n                 access_count=1,\n                 size_bytes=self._estimate_size(value),\n-                ttl_seconds=ttl_seconds\n+                ttl_seconds=ttl_seconds,\n             )\n-            \n+\n             # Check file size limit\n             estimated_size = self._estimate_size(entry) / 1024 / 1024\n             if estimated_size > self.max_file_size_mb:\n                 return  # Skip caching large objects\n-            \n-            with open(cache_file, 'wb') as f:\n+\n+            with open(cache_file, \"wb\") as f:\n                 pickle.dump(entry, f)\n-                \n+\n         except Exception:\n             # Skip caching if there's an error\n             pass\n-    \n+\n     def delete(self, key: str) -> bool:\n         \"\"\"Delete entry from persistent cache.\"\"\"\n         cache_file = self._get_cache_file(key)\n-        \n+\n         if cache_file.exists():\n             cache_file.unlink()\n             return True\n         return False\n-    \n+\n     def clear(self) -> None:\n         \"\"\"Clear all cache entries.\"\"\"\n         for cache_file in self.cache_dir.glob(\"*.cache\"):\n             cache_file.unlink()\n-    \n+\n     def _get_cache_file(self, key: str) -> Path:\n         \"\"\"Get cache file path for key.\"\"\"\n         # Create hash of key to avoid filesystem issues\n         key_hash = hashlib.md5(key.encode()).hexdigest()\n         return self.cache_dir / f\"{key_hash}.cache\"\n-    \n+\n     def _is_expired(self, entry: CacheEntry) -> bool:\n         \"\"\"Check if entry is expired.\"\"\"\n         if not entry.ttl_seconds:\n             return False\n-        \n+\n         elapsed = (datetime.now() - entry.created_at).total_seconds()\n         return elapsed > entry.ttl_seconds\n-    \n+\n     def _estimate_size(self, obj: Any) -> int:\n         \"\"\"Estimate size of object in bytes.\"\"\"\n         try:\n             return len(pickle.dumps(obj))\n         except Exception:\n-            return len(str(obj).encode('utf-8'))\n+            return len(str(obj).encode(\"utf-8\"))\n \n \n class HybridCache:\n     \"\"\"Hybrid cache combining memory and persistent storage.\"\"\"\n-    \n+\n     def __init__(self, memory_cache: MemoryCache, persistent_cache: PersistentCache):\n         self.memory_cache = memory_cache\n         self.persistent_cache = persistent_cache\n         self.lock = threading.RLock()\n-    \n+\n     def get(self, key: str) -> Optional[Any]:\n         \"\"\"Get value from hybrid cache (memory first, then persistent).\"\"\"\n         # Try memory cache first\n         value = self.memory_cache.get(key)\n         if value is not None:\n             return value\n-        \n+\n         # Try persistent cache\n         value = self.persistent_cache.get(key)\n         if value is not None:\n             # Promote to memory cache\n             self.memory_cache.set(key, value)\n             return value\n-        \n+\n         return None\n-    \n+\n     def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:\n         \"\"\"Set value in both caches.\"\"\"\n         # Set in memory cache\n         self.memory_cache.set(key, value, ttl_seconds)\n-        \n+\n         # Set in persistent cache (with longer TTL if specified)\n         persistent_ttl = ttl_seconds * 2 if ttl_seconds else None\n         self.persistent_cache.set(key, value, persistent_ttl)\n-    \n+\n     def delete(self, key: str) -> bool:\n         \"\"\"Delete from both caches.\"\"\"\n         memory_deleted = self.memory_cache.delete(key)\n         persistent_deleted = self.persistent_cache.delete(key)\n         return memory_deleted or persistent_deleted\n-    \n+\n     def clear(self) -> None:\n         \"\"\"Clear both caches.\"\"\"\n         self.memory_cache.clear()\n         self.persistent_cache.clear()\n \n@@ -343,129 +350,141 @@\n _memory_cache = MemoryCache(max_size_mb=100, default_ttl=3600)  # 1 hour default\n _persistent_cache = PersistentCache(cache_dir=\".cache/sparkforge\", max_file_size_mb=50)\n _hybrid_cache = HybridCache(_memory_cache, _persistent_cache)\n \n \n-def cache_result(ttl_seconds: Optional[int] = None, cache_type: str = \"memory\") -> Callable:\n+def cache_result(\n+    ttl_seconds: Optional[int] = None, cache_type: str = \"memory\"\n+) -> Callable:\n     \"\"\"Decorator to cache function results.\"\"\"\n+\n     def decorator(func: Callable) -> Callable:\n         @wraps(func)\n         def wrapper(*args, **kwargs):\n             # Create cache key\n             cache_key = _create_cache_key(func.__name__, args, kwargs)\n-            \n+\n             # Get appropriate cache\n             cache = _get_cache(cache_type)\n-            \n+\n             # Try to get from cache\n             result = cache.get(cache_key)\n             if result is not None:\n                 return result\n-            \n+\n             # Execute function and cache result\n             result = func(*args, **kwargs)\n             cache.set(cache_key, result, ttl_seconds)\n-            \n+\n             return result\n-        \n+\n         return wrapper\n+\n     return decorator\n \n \n def cache_dataframe(ttl_seconds: int = 3600, cache_type: str = \"hybrid\") -> Callable:\n     \"\"\"Decorator specifically for caching DataFrame operations.\"\"\"\n+\n     def decorator(func: Callable) -> Callable:\n         @wraps(func)\n         def wrapper(*args, **kwargs):\n             # Create cache key including DataFrame schema\n             cache_key = _create_dataframe_cache_key(func.__name__, args, kwargs)\n-            \n+\n             # Get appropriate cache\n             cache = _get_cache(cache_type)\n-            \n+\n             # Try to get from cache\n             result = cache.get(cache_key)\n             if result is not None:\n                 return result\n-            \n+\n             # Execute function and cache result\n             result = func(*args, **kwargs)\n-            \n+\n             # Cache result (DataFrames are cached by reference in Spark)\n             cache.set(cache_key, result, ttl_seconds)\n-            \n+\n             return result\n-        \n+\n         return wrapper\n+\n     return decorator\n \n \n def cache_validation_result(ttl_seconds: int = 1800) -> Callable:\n     \"\"\"Decorator for caching validation results.\"\"\"\n+\n     def decorator(func: Callable) -> Callable:\n         @wraps(func)\n         def wrapper(*args, **kwargs):\n             cache_key = _create_cache_key(f\"validation_{func.__name__}\", args, kwargs)\n-            \n+\n             result = _memory_cache.get(cache_key)\n             if result is not None:\n                 return result\n-            \n+\n             result = func(*args, **kwargs)\n             _memory_cache.set(cache_key, result, ttl_seconds)\n-            \n+\n             return result\n-        \n+\n         return wrapper\n+\n     return decorator\n \n \n def cache_pipeline_step(ttl_seconds: int = 7200) -> Callable:\n     \"\"\"Decorator for caching pipeline step results.\"\"\"\n+\n     def decorator(func: Callable) -> Callable:\n         @wraps(func)\n         def wrapper(*args, **kwargs):\n             cache_key = _create_cache_key(f\"pipeline_{func.__name__}\", args, kwargs)\n-            \n+\n             result = _hybrid_cache.get(cache_key)\n             if result is not None:\n                 return result\n-            \n+\n             result = func(*args, **kwargs)\n             _hybrid_cache.set(cache_key, result, ttl_seconds)\n-            \n+\n             return result\n-        \n+\n         return wrapper\n+\n     return decorator\n \n \n def _create_cache_key(func_name: str, args: tuple, kwargs: dict) -> str:\n     \"\"\"Create cache key from function name and arguments.\"\"\"\n     # Create hash of arguments\n     args_str = str(args) + str(sorted(kwargs.items()))\n     args_hash = hashlib.md5(args_str.encode()).hexdigest()\n-    \n+\n     return f\"{func_name}:{args_hash}\"\n \n \n def _create_dataframe_cache_key(func_name: str, args: tuple, kwargs: dict) -> str:\n     \"\"\"Create cache key for DataFrame operations including schema info.\"\"\"\n     key_parts = [func_name]\n-    \n+\n     for arg in args:\n-        if hasattr(arg, 'schema') and hasattr(arg, 'count'):\n+        if hasattr(arg, \"schema\") and hasattr(arg, \"count\"):\n             # DataFrame - include schema and row count\n             schema_info = str(arg.schema)\n             row_count = arg.count()\n-            key_parts.append(f\"df:{hashlib.md5(schema_info.encode()).hexdigest()}:{row_count}\")\n+            key_parts.append(\n+                f\"df:{hashlib.md5(schema_info.encode()).hexdigest()}:{row_count}\"\n+            )\n         else:\n             key_parts.append(str(arg))\n-    \n+\n     for key, value in sorted(kwargs.items()):\n         key_parts.append(f\"{key}:{value}\")\n-    \n+\n     key_str = \":\".join(key_parts)\n     return hashlib.md5(key_str.encode()).hexdigest()\n \n \n def _get_cache(cache_type: str):\n@@ -480,50 +499,50 @@\n         raise ValueError(f\"Unknown cache type: {cache_type}\")\n \n \n class CacheManager:\n     \"\"\"Centralized cache management.\"\"\"\n-    \n+\n     def __init__(self):\n         self.caches = {\n             \"memory\": _memory_cache,\n             \"persistent\": _persistent_cache,\n-            \"hybrid\": _hybrid_cache\n+            \"hybrid\": _hybrid_cache,\n         }\n-    \n+\n     def get_stats(self) -> Dict[str, CacheStats]:\n         \"\"\"Get statistics for all caches.\"\"\"\n         stats = {}\n         for name, cache in self.caches.items():\n-            if hasattr(cache, 'get_stats'):\n+            if hasattr(cache, \"get_stats\"):\n                 stats[name] = cache.get_stats()\n-            elif hasattr(cache, 'memory_cache'):\n+            elif hasattr(cache, \"memory_cache\"):\n                 stats[name] = cache.memory_cache.get_stats()\n         return stats\n-    \n+\n     def clear_all(self) -> None:\n         \"\"\"Clear all caches.\"\"\"\n         for cache in self.caches.values():\n             cache.clear()\n-    \n+\n     def cleanup_expired(self) -> Dict[str, int]:\n         \"\"\"Clean up expired entries in all caches.\"\"\"\n         results = {}\n         for name, cache in self.caches.items():\n-            if hasattr(cache, 'cleanup_expired'):\n+            if hasattr(cache, \"cleanup_expired\"):\n                 results[name] = cache.cleanup_expired()\n-            elif hasattr(cache, 'memory_cache'):\n+            elif hasattr(cache, \"memory_cache\"):\n                 results[name] = cache.memory_cache.cleanup_expired()\n         return results\n-    \n+\n     def optimize_memory(self) -> None:\n         \"\"\"Optimize memory usage by cleaning up and evicting.\"\"\"\n         # Clean up expired entries\n         self.cleanup_expired()\n-        \n+\n         # Force eviction in memory cache\n-        if hasattr(_memory_cache, '_evict_if_needed'):\n+        if hasattr(_memory_cache, \"_evict_if_needed\"):\n             _memory_cache._evict_if_needed()\n \n \n # Global cache manager\n cache_manager = CacheManager()\n@@ -555,47 +574,47 @@\n         return df\n \n \n if __name__ == \"__main__\":\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(description=\"SparkForge Caching Strategies\")\n     parser.add_argument(\"--test\", action=\"store_true\", help=\"Run cache tests\")\n     parser.add_argument(\"--stats\", action=\"store_true\", help=\"Show cache statistics\")\n     parser.add_argument(\"--clear\", action=\"store_true\", help=\"Clear all caches\")\n-    \n+\n     args = parser.parse_args()\n-    \n+\n     if args.test:\n         # Run cache tests\n         print(\"Running cache tests...\")\n-        \n+\n         # Test memory cache\n         start_time = time.time()\n         result1 = cached_calculation([1, 2, 3, 4, 5])\n         first_call_time = time.time() - start_time\n-        \n+\n         start_time = time.time()\n         result2 = cached_calculation([1, 2, 3, 4, 5])  # Should be cached\n         second_call_time = time.time() - start_time\n-        \n+\n         print(f\"First call: {first_call_time:.3f}s, result: {result1}\")\n         print(f\"Second call: {second_call_time:.3f}s, result: {result2}\")\n         print(f\"Cache hit speedup: {first_call_time / second_call_time:.1f}x\")\n-    \n+\n     if args.stats:\n         # Show cache statistics\n         stats = cache_manager.get_stats()\n         for cache_name, cache_stats in stats.items():\n             print(f\"\\n{cache_name.upper()} Cache Statistics:\")\n             print(f\"  Entries: {cache_stats.total_entries}\")\n             print(f\"  Hit Rate: {cache_stats.hit_rate:.1f}%\")\n             print(f\"  Memory Usage: {cache_stats.memory_usage_mb:.2f} MB\")\n             print(f\"  Evictions: {cache_stats.eviction_count}\")\n-    \n+\n     if args.clear:\n         # Clear all caches\n         cache_manager.clear_all()\n         print(\"All caches cleared\")\n-    \n+\n     if not any([args.test, args.stats, args.clear]):\n         print(\"Use --test, --stats, or --clear to run cache operations\")\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_validation_integration.py\t2025-09-20 14:00:38.761343+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_validation_integration.py\t2025-09-20 16:53:34.806449+00:00\n@@ -144,13 +144,17 @@\n         assert result is True\n \n     def test_no_valid_expressions_returns_true(self):\n         \"\"\"Test that when no valid expressions are generated, returns True.\"\"\"\n         # This tests the case where _convert_rules_to_expressions returns empty dict\n-        with patch('sparkforge.validation._convert_rules_to_expressions') as mock_convert:\n+        with patch(\n+            \"sparkforge.validation._convert_rules_to_expressions\"\n+        ) as mock_convert:\n             mock_convert.return_value = {}  # No expressions generated\n-            rules = {\"test\": [\"invalid_rule\"]}  # Invalid rule that generates no expressions\n+            rules = {\n+                \"test\": [\"invalid_rule\"]\n+            }  # Invalid rule that generates no expressions\n             result = and_all_rules(rules)\n             assert result is True\n \n \n class TestValidateDataframeSchema:\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/memory_optimization.py\t2025-09-20 16:45:07.583652+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/memory_optimization.py\t2025-09-20 16:53:34.811626+00:00\n@@ -25,20 +25,22 @@\n \n \n @dataclass\n class MemorySnapshot:\n     \"\"\"Memory snapshot data structure.\"\"\"\n+\n     timestamp: datetime\n     current_memory_mb: float\n     peak_memory_mb: float\n     allocated_blocks: int\n     top_allocations: List[Dict[str, Any]]\n \n \n @dataclass\n class MemoryLeak:\n     \"\"\"Memory leak detection result.\"\"\"\n+\n     leak_id: str\n     object_type: str\n     count: int\n     size_bytes: int\n     growth_rate: float\n@@ -47,439 +49,465 @@\n \n \n @dataclass\n class MemoryOptimizationReport:\n     \"\"\"Memory optimization report.\"\"\"\n+\n     total_memory_mb: float\n     memory_efficiency_score: float\n     detected_leaks: List[MemoryLeak]\n     optimization_opportunities: List[str]\n     recommendations: List[str]\n     gc_stats: Dict[str, Any]\n \n \n class MemoryProfiler:\n     \"\"\"Memory profiling and analysis.\"\"\"\n-    \n+\n     def __init__(self, enable_tracemalloc: bool = True):\n         self.enable_tracemalloc = enable_tracemalloc\n         self.snapshots: List[MemorySnapshot] = []\n         self.object_counts: Dict[str, int] = defaultdict(int)\n         self.object_sizes: Dict[str, int] = defaultdict(int)\n         self.leak_detector = MemoryLeakDetector()\n-        \n+\n         if self.enable_tracemalloc:\n             tracemalloc.start()\n-        \n+\n         self.logger = logging.getLogger(\"memory_profiler\")\n-    \n+\n     def take_snapshot(self) -> MemorySnapshot:\n         \"\"\"Take a memory snapshot.\"\"\"\n         try:\n             # Get current memory usage\n             process = psutil.Process()\n             current_memory = process.memory_info().rss / 1024 / 1024  # MB\n-            \n+\n             # Get peak memory if tracemalloc is enabled\n             if self.enable_tracemalloc:\n                 current, peak = tracemalloc.get_traced_memory()\n                 peak_memory = peak / 1024 / 1024\n                 snapshot = tracemalloc.take_snapshot()\n-                top_stats = snapshot.statistics('lineno')\n-                \n+                top_stats = snapshot.statistics(\"lineno\")\n+\n                 top_allocations = []\n                 for stat in top_stats[:10]:\n-                    top_allocations.append({\n-                        \"filename\": stat.traceback.format()[0],\n-                        \"size_mb\": stat.size / 1024 / 1024,\n-                        \"count\": stat.count\n-                    })\n+                    top_allocations.append(\n+                        {\n+                            \"filename\": stat.traceback.format()[0],\n+                            \"size_mb\": stat.size / 1024 / 1024,\n+                            \"count\": stat.count,\n+                        }\n+                    )\n             else:\n                 peak_memory = current_memory\n                 top_allocations = []\n-            \n+\n             # Count objects\n             object_counts = {}\n             for obj in gc.get_objects():\n                 obj_type = type(obj).__name__\n                 object_counts[obj_type] = object_counts.get(obj_type, 0) + 1\n                 self.object_counts[obj_type] += 1\n                 self.object_sizes[obj_type] += sys.getsizeof(obj)\n-            \n+\n             snapshot = MemorySnapshot(\n                 timestamp=datetime.now(),\n                 current_memory_mb=current_memory,\n                 peak_memory_mb=peak_memory,\n                 allocated_blocks=len(gc.get_objects()),\n-                top_allocations=top_allocations\n+                top_allocations=top_allocations,\n             )\n-            \n+\n             self.snapshots.append(snapshot)\n             self.leak_detector.analyze_snapshot(snapshot)\n-            \n+\n             return snapshot\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error taking memory snapshot: {e}\")\n             return MemorySnapshot(\n                 timestamp=datetime.now(),\n                 current_memory_mb=0,\n                 peak_memory_mb=0,\n                 allocated_blocks=0,\n-                top_allocations=[]\n+                top_allocations=[],\n             )\n-    \n+\n     def get_memory_stats(self) -> Dict[str, Any]:\n         \"\"\"Get comprehensive memory statistics.\"\"\"\n         if not self.snapshots:\n             return {}\n-        \n+\n         latest_snapshot = self.snapshots[-1]\n-        \n+\n         # Calculate memory trends\n         if len(self.snapshots) > 1:\n             memory_trend = []\n             for snapshot in self.snapshots[-10:]:  # Last 10 snapshots\n-                memory_trend.append({\n-                    \"timestamp\": snapshot.timestamp.isoformat(),\n-                    \"memory_mb\": snapshot.current_memory_mb\n-                })\n+                memory_trend.append(\n+                    {\n+                        \"timestamp\": snapshot.timestamp.isoformat(),\n+                        \"memory_mb\": snapshot.current_memory_mb,\n+                    }\n+                )\n         else:\n             memory_trend = []\n-        \n+\n         return {\n             \"current_memory_mb\": latest_snapshot.current_memory_mb,\n             \"peak_memory_mb\": latest_snapshot.peak_memory_mb,\n             \"allocated_blocks\": latest_snapshot.allocated_blocks,\n             \"top_allocations\": latest_snapshot.top_allocations,\n             \"object_counts\": dict(self.object_counts),\n-            \"object_sizes\": {k: v / 1024 / 1024 for k, v in self.object_sizes.items()},  # Convert to MB\n+            \"object_sizes\": {\n+                k: v / 1024 / 1024 for k, v in self.object_sizes.items()\n+            },  # Convert to MB\n             \"memory_trend\": memory_trend,\n-            \"gc_stats\": self._get_gc_stats()\n+            \"gc_stats\": self._get_gc_stats(),\n         }\n-    \n+\n     def _get_gc_stats(self) -> Dict[str, Any]:\n         \"\"\"Get garbage collection statistics.\"\"\"\n         try:\n             gc_stats = gc.get_stats()\n             return {\n                 \"generation_0\": gc_stats[0] if len(gc_stats) > 0 else {},\n                 \"generation_1\": gc_stats[1] if len(gc_stats) > 1 else {},\n                 \"generation_2\": gc_stats[2] if len(gc_stats) > 2 else {},\n                 \"total_collections\": sum(stat[\"collections\"] for stat in gc_stats),\n-                \"total_collected\": sum(stat[\"collected\"] for stat in gc_stats)\n+                \"total_collected\": sum(stat[\"collected\"] for stat in gc_stats),\n             }\n         except Exception as e:\n             self.logger.error(f\"Error getting GC stats: {e}\")\n             return {}\n \n \n class MemoryLeakDetector:\n     \"\"\"Memory leak detection and analysis.\"\"\"\n-    \n+\n     def __init__(self):\n         self.object_history: Dict[str, List[int]] = defaultdict(list)\n         self.detected_leaks: List[MemoryLeak] = []\n         self.analysis_interval = 60  # seconds\n         self.last_analysis = datetime.now()\n-    \n+\n     def analyze_snapshot(self, snapshot: MemorySnapshot) -> None:\n         \"\"\"Analyze memory snapshot for potential leaks.\"\"\"\n         current_time = datetime.now()\n-        \n+\n         # Only analyze if enough time has passed\n         if (current_time - self.last_analysis).total_seconds() < self.analysis_interval:\n             return\n-        \n+\n         self.last_analysis = current_time\n-        \n+\n         # Track object counts over time\n         for allocation in snapshot.top_allocations:\n             obj_type = allocation[\"filename\"]\n             count = allocation[\"count\"]\n-            \n+\n             self.object_history[obj_type].append(count)\n-            \n+\n             # Keep only last 10 measurements\n             if len(self.object_history[obj_type]) > 10:\n                 self.object_history[obj_type] = self.object_history[obj_type][-10:]\n-            \n+\n             # Detect potential leaks\n             if len(self.object_history[obj_type]) >= 5:\n                 self._detect_leak(obj_type, allocation, current_time)\n-    \n-    def _detect_leak(self, obj_type: str, allocation: Dict[str, Any], timestamp: datetime) -> None:\n+\n+    def _detect_leak(\n+        self, obj_type: str, allocation: Dict[str, Any], timestamp: datetime\n+    ) -> None:\n         \"\"\"Detect potential memory leak for specific object type.\"\"\"\n         history = self.object_history[obj_type]\n-        \n+\n         # Calculate growth rate\n         if len(history) >= 3:\n-            recent_growth = (history[-1] - history[-3]) / 2  # Average growth over last 2 intervals\n-            \n+            recent_growth = (\n+                history[-1] - history[-3]\n+            ) / 2  # Average growth over last 2 intervals\n+\n             # Consider it a leak if growth rate is positive and significant\n             if recent_growth > 10:  # More than 10 objects per interval\n                 leak = MemoryLeak(\n                     leak_id=f\"leak_{obj_type}_{int(timestamp.timestamp())}\",\n                     object_type=obj_type,\n                     count=history[-1],\n                     size_bytes=allocation[\"size_mb\"] * 1024 * 1024,\n                     growth_rate=recent_growth,\n                     first_detected=timestamp,\n-                    severity=\"high\" if recent_growth > 50 else \"medium\"\n+                    severity=\"high\" if recent_growth > 50 else \"medium\",\n                 )\n-                \n+\n                 # Only add if not already detected\n-                if not any(l.object_type == obj_type and not l.resolved for l in self.detected_leaks):\n+                if not any(\n+                    l.object_type == obj_type and not l.resolved\n+                    for l in self.detected_leaks\n+                ):\n                     self.detected_leaks.append(leak)\n-    \n+\n     def get_detected_leaks(self) -> List[MemoryLeak]:\n         \"\"\"Get list of detected memory leaks.\"\"\"\n-        return [leak for leak in self.detected_leaks if not getattr(leak, 'resolved', False)]\n+        return [\n+            leak for leak in self.detected_leaks if not getattr(leak, \"resolved\", False)\n+        ]\n \n \n class MemoryOptimizer:\n     \"\"\"Memory optimization strategies and utilities.\"\"\"\n-    \n+\n     def __init__(self):\n         self.optimization_strategies = {\n             \"gc_optimization\": self._optimize_garbage_collection,\n             \"object_pooling\": self._implement_object_pooling,\n             \"lazy_evaluation\": self._implement_lazy_evaluation,\n             \"memory_mapping\": self._implement_memory_mapping,\n-            \"weak_references\": self._use_weak_references\n+            \"weak_references\": self._use_weak_references,\n         }\n         self.logger = logging.getLogger(\"memory_optimizer\")\n-    \n+\n     def optimize_memory_usage(self, target_memory_mb: float) -> Dict[str, Any]:\n         \"\"\"Optimize memory usage to target level.\"\"\"\n         results = {}\n-        \n+\n         # Get current memory usage\n         process = psutil.Process()\n         current_memory = process.memory_info().rss / 1024 / 1024\n-        \n+\n         if current_memory <= target_memory_mb:\n             return {\"status\": \"already_optimized\", \"current_memory_mb\": current_memory}\n-        \n+\n         # Apply optimization strategies\n         for strategy_name, strategy_func in self.optimization_strategies.items():\n             try:\n                 result = strategy_func()\n                 results[strategy_name] = result\n-                \n+\n                 # Check if we've reached target\n                 new_memory = process.memory_info().rss / 1024 / 1024\n                 if new_memory <= target_memory_mb:\n                     break\n-                    \n+\n             except Exception as e:\n-                self.logger.error(f\"Error applying optimization strategy {strategy_name}: {e}\")\n+                self.logger.error(\n+                    f\"Error applying optimization strategy {strategy_name}: {e}\"\n+                )\n                 results[strategy_name] = {\"error\": str(e)}\n-        \n+\n         final_memory = process.memory_info().rss / 1024 / 1024\n         results[\"final_memory_mb\"] = final_memory\n         results[\"memory_reduced_mb\"] = current_memory - final_memory\n-        \n+\n         return results\n-    \n+\n     def _optimize_garbage_collection(self) -> Dict[str, Any]:\n         \"\"\"Optimize garbage collection settings.\"\"\"\n         # Get current GC settings\n         old_thresholds = gc.get_threshold()\n-        \n+\n         # Optimize thresholds for better memory management\n         gc.set_threshold(700, 10, 10)  # More aggressive collection\n-        \n+\n         # Force garbage collection\n         collected = gc.collect()\n-        \n+\n         return {\n             \"old_thresholds\": old_thresholds,\n             \"new_thresholds\": gc.get_threshold(),\n-            \"objects_collected\": collected\n+            \"objects_collected\": collected,\n         }\n-    \n+\n     def _implement_object_pooling(self) -> Dict[str, Any]:\n         \"\"\"Implement object pooling for frequently created objects.\"\"\"\n         # This is a placeholder - actual implementation would depend on specific use cases\n         return {\n             \"strategy\": \"object_pooling\",\n             \"status\": \"placeholder_implementation\",\n-            \"note\": \"Object pooling should be implemented based on specific application needs\"\n+            \"note\": \"Object pooling should be implemented based on specific application needs\",\n         }\n-    \n+\n     def _implement_lazy_evaluation(self) -> Dict[str, Any]:\n         \"\"\"Implement lazy evaluation for memory-intensive operations.\"\"\"\n         # This is a placeholder - actual implementation would depend on specific use cases\n         return {\n             \"strategy\": \"lazy_evaluation\",\n             \"status\": \"placeholder_implementation\",\n-            \"note\": \"Lazy evaluation should be implemented for specific data processing operations\"\n+            \"note\": \"Lazy evaluation should be implemented for specific data processing operations\",\n         }\n-    \n+\n     def _implement_memory_mapping(self) -> Dict[str, Any]:\n         \"\"\"Implement memory mapping for large data structures.\"\"\"\n         # This is a placeholder - actual implementation would depend on specific use cases\n         return {\n             \"strategy\": \"memory_mapping\",\n             \"status\": \"placeholder_implementation\",\n-            \"note\": \"Memory mapping should be implemented for large file-based data structures\"\n+            \"note\": \"Memory mapping should be implemented for large file-based data structures\",\n         }\n-    \n+\n     def _use_weak_references(self) -> Dict[str, Any]:\n         \"\"\"Use weak references to prevent memory leaks.\"\"\"\n         # This is a placeholder - actual implementation would depend on specific use cases\n         return {\n             \"strategy\": \"weak_references\",\n             \"status\": \"placeholder_implementation\",\n-            \"note\": \"Weak references should be used for caching and observer patterns\"\n+            \"note\": \"Weak references should be used for caching and observer patterns\",\n         }\n-    \n+\n     def analyze_memory_efficiency(self) -> float:\n         \"\"\"Analyze memory efficiency and return score (0-100).\"\"\"\n         try:\n             process = psutil.Process()\n             memory_info = process.memory_info()\n-            \n+\n             # Calculate efficiency based on various factors\n             memory_usage_mb = memory_info.rss / 1024 / 1024\n-            \n+\n             # Base score\n             efficiency_score = 100.0\n-            \n+\n             # Penalize high memory usage\n             if memory_usage_mb > 1000:  # > 1GB\n                 efficiency_score -= min(50, (memory_usage_mb - 1000) / 100 * 10)\n-            \n+\n             # Check for memory fragmentation\n-            if hasattr(memory_info, 'vms') and memory_info.vms > memory_info.rss * 2:\n+            if hasattr(memory_info, \"vms\") and memory_info.vms > memory_info.rss * 2:\n                 efficiency_score -= 20  # High virtual memory usage\n-            \n+\n             # Check GC efficiency\n             gc_stats = gc.get_stats()\n             total_collections = sum(stat[\"collections\"] for stat in gc_stats)\n             if total_collections > 1000:  # Too many collections\n                 efficiency_score -= 15\n-            \n+\n             return max(0, min(100, efficiency_score))\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error analyzing memory efficiency: {e}\")\n             return 0.0\n \n \n class MemoryEfficientDataStructures:\n     \"\"\"Memory-efficient data structures and utilities.\"\"\"\n-    \n+\n     @staticmethod\n     def create_weak_cache(max_size: int = 1000) -> weakref.WeakValueDictionary:\n         \"\"\"Create a memory-efficient cache using weak references.\"\"\"\n         return weakref.WeakValueDictionary()\n-    \n+\n     @staticmethod\n     def create_object_pool(pool_size: int = 100, factory_func: Callable = None):\n         \"\"\"Create an object pool to reuse expensive objects.\"\"\"\n         pool = []\n-        \n+\n         def get_object():\n             if pool:\n                 return pool.pop()\n             elif factory_func:\n                 return factory_func()\n             else:\n                 return None\n-        \n+\n         def return_object(obj):\n             if len(pool) < pool_size:\n                 pool.append(obj)\n-        \n+\n         return get_object, return_object\n-    \n+\n     @staticmethod\n     def create_memory_efficient_list(max_size: int = 10000):\n         \"\"\"Create a memory-efficient list that automatically manages size.\"\"\"\n+\n         class MemoryEfficientList:\n             def __init__(self, max_size):\n                 self.max_size = max_size\n                 self.items = []\n-            \n+\n             def append(self, item):\n                 self.items.append(item)\n                 if len(self.items) > self.max_size:\n-                    self.items = self.items[-self.max_size:]  # Keep only recent items\n-            \n+                    self.items = self.items[-self.max_size :]  # Keep only recent items\n+\n             def __getitem__(self, index):\n                 return self.items[index]\n-            \n+\n             def __len__(self):\n                 return len(self.items)\n-            \n+\n             def __iter__(self):\n                 return iter(self.items)\n-        \n+\n         return MemoryEfficientList(max_size)\n-    \n+\n     @staticmethod\n     def create_lazy_dict(factory_func: Callable):\n         \"\"\"Create a lazy dictionary that creates values on demand.\"\"\"\n+\n         class LazyDict(dict):\n             def __init__(self, factory_func):\n                 super().__init__()\n                 self.factory_func = factory_func\n-            \n+\n             def __missing__(self, key):\n                 value = self.factory_func(key)\n                 self[key] = value\n                 return value\n-        \n+\n         return LazyDict(factory_func)\n \n \n def memory_monitor(interval_seconds: int = 30) -> Callable:\n     \"\"\"Decorator to monitor memory usage of functions.\"\"\"\n+\n     def decorator(func: Callable) -> Callable:\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs):\n             # Get initial memory\n             process = psutil.Process()\n             initial_memory = process.memory_info().rss / 1024 / 1024\n-            \n+\n             # Start memory monitoring in background\n             monitor_thread = threading.Thread(\n                 target=_monitor_memory_usage,\n                 args=(interval_seconds, func.__name__),\n-                daemon=True\n+                daemon=True,\n             )\n             monitor_thread.start()\n-            \n+\n             try:\n                 result = func(*args, **kwargs)\n                 return result\n             finally:\n                 # Get final memory\n                 final_memory = process.memory_info().rss / 1024 / 1024\n                 memory_delta = final_memory - initial_memory\n-                \n+\n                 if memory_delta > 100:  # More than 100MB increase\n-                    logging.warning(f\"Function {func.__name__} used {memory_delta:.1f}MB of memory\")\n-        \n+                    logging.warning(\n+                        f\"Function {func.__name__} used {memory_delta:.1f}MB of memory\"\n+                    )\n+\n         return wrapper\n+\n     return decorator\n \n \n def _monitor_memory_usage(interval_seconds: int, function_name: str) -> None:\n     \"\"\"Background memory monitoring function.\"\"\"\n     process = psutil.Process()\n     initial_memory = process.memory_info().rss / 1024 / 1024\n-    \n+\n     while True:\n         time.sleep(interval_seconds)\n         current_memory = process.memory_info().rss / 1024 / 1024\n         memory_delta = current_memory - initial_memory\n-        \n+\n         if memory_delta > 200:  # More than 200MB increase\n-            logging.warning(f\"Memory usage in {function_name} increased by {memory_delta:.1f}MB\")\n+            logging.warning(\n+                f\"Memory usage in {function_name} increased by {memory_delta:.1f}MB\"\n+            )\n \n \n def optimize_spark_memory() -> Dict[str, Any]:\n     \"\"\"Optimize Spark memory configuration.\"\"\"\n     # This would contain Spark-specific memory optimization\n@@ -488,77 +516,85 @@\n             \"spark.sql.adaptive.enabled\": \"true\",\n             \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n             \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n             \"spark.sql.adaptive.localShuffleReader.enabled\": \"true\",\n             \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n-            \"spark.sql.execution.arrow.pyspark.enabled\": \"true\"\n+            \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n         },\n         \"memory_settings\": {\n             \"spark.driver.memory\": \"4g\",\n             \"spark.executor.memory\": \"8g\",\n             \"spark.driver.maxResultSize\": \"2g\",\n             \"spark.executor.memoryFraction\": \"0.8\",\n-            \"spark.storage.memoryFraction\": \"0.6\"\n-        }\n+            \"spark.storage.memoryFraction\": \"0.6\",\n+        },\n     }\n \n \n # CLI interface\n if __name__ == \"__main__\":\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(description=\"SparkForge Memory Optimization\")\n     parser.add_argument(\"--profile\", action=\"store_true\", help=\"Run memory profiling\")\n-    parser.add_argument(\"--optimize\", action=\"store_true\", help=\"Run memory optimization\")\n-    parser.add_argument(\"--target-memory\", type=float, default=500, help=\"Target memory in MB\")\n-    parser.add_argument(\"--duration\", type=int, default=60, help=\"Profiling duration in seconds\")\n-    \n+    parser.add_argument(\n+        \"--optimize\", action=\"store_true\", help=\"Run memory optimization\"\n+    )\n+    parser.add_argument(\n+        \"--target-memory\", type=float, default=500, help=\"Target memory in MB\"\n+    )\n+    parser.add_argument(\n+        \"--duration\", type=int, default=60, help=\"Profiling duration in seconds\"\n+    )\n+\n     args = parser.parse_args()\n-    \n+\n     if args.profile:\n         profiler = MemoryProfiler()\n         print(\"Starting memory profiling...\")\n-        \n+\n         # Take initial snapshot\n         initial_snapshot = profiler.take_snapshot()\n         print(f\"Initial memory: {initial_snapshot.current_memory_mb:.1f}MB\")\n-        \n+\n         # Monitor for specified duration\n         try:\n             time.sleep(args.duration)\n         except KeyboardInterrupt:\n             print(\"\\nProfiling stopped by user\")\n-        \n+\n         # Take final snapshot\n         final_snapshot = profiler.take_snapshot()\n         print(f\"Final memory: {final_snapshot.current_memory_mb:.1f}MB\")\n-        \n+\n         # Show stats\n         stats = profiler.get_memory_stats()\n         print(f\"\\nMemory Statistics:\")\n         print(f\"Peak memory: {stats.get('peak_memory_mb', 0):.1f}MB\")\n         print(f\"Allocated blocks: {stats.get('allocated_blocks', 0)}\")\n-        \n+\n         # Show detected leaks\n         leaks = profiler.leak_detector.get_detected_leaks()\n         if leaks:\n             print(f\"\\nDetected Memory Leaks: {len(leaks)}\")\n             for leak in leaks:\n-                print(f\"  - {leak.object_type}: {leak.count} objects, growth rate: {leak.growth_rate:.1f}\")\n+                print(\n+                    f\"  - {leak.object_type}: {leak.count} objects, growth rate: {leak.growth_rate:.1f}\"\n+                )\n         else:\n             print(\"\\nNo memory leaks detected\")\n-    \n+\n     if args.optimize:\n         optimizer = MemoryOptimizer()\n         print(f\"Optimizing memory to target: {args.target_memory}MB\")\n-        \n+\n         results = optimizer.optimize_memory_usage(args.target_memory)\n         print(f\"\\nOptimization Results:\")\n         print(f\"Final memory: {results.get('final_memory_mb', 0):.1f}MB\")\n         print(f\"Memory reduced: {results.get('memory_reduced_mb', 0):.1f}MB\")\n-        \n+\n         # Show efficiency score\n         efficiency_score = optimizer.analyze_memory_efficiency()\n         print(f\"Memory efficiency score: {efficiency_score:.1f}/100\")\n-    \n+\n     if not args.profile and not args.optimize:\n         print(\"Use --profile or --optimize to run memory operations\")\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine.py\t2025-09-20 14:00:38.759172+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine.py\t2025-09-20 16:53:34.817654+00:00\n@@ -451,54 +451,53 @@\n             )\n \n     def test_execute_bronze_step_not_in_context(self, mock_spark, mock_config):\n         \"\"\"Test bronze step execution when step name is not in context.\"\"\"\n         engine = ExecutionEngine(mock_spark, mock_config)\n-        \n+\n         # Create a bronze step\n         bronze_step = BronzeStep(\n-            name=\"test_bronze\",\n-            rules={\"id\": [\"not_null\"]},\n-            schema=\"test_schema\"\n-        )\n-        \n+            name=\"test_bronze\", rules={\"id\": [\"not_null\"]}, schema=\"test_schema\"\n+        )\n+\n         # Mock the spark.read.format().load() to raise an exception\n-        mock_spark.read.format.return_value.load.side_effect = Exception(\"File not found\")\n-        \n+        mock_spark.read.format.return_value.load.side_effect = Exception(\n+            \"File not found\"\n+        )\n+\n         # Execute bronze step without the step name in context\n         # This should trigger the fallback to createDataFrame\n         result_df = engine._execute_bronze_step(bronze_step, {})\n-        \n+\n         # Verify that createDataFrame was called as fallback\n         mock_spark.createDataFrame.assert_called_once()\n         assert result_df is not None\n \n     def test_execute_bronze_step_fallback_logging(self, mock_spark, mock_config):\n         \"\"\"Test that fallback logging occurs when read fails.\"\"\"\n         engine = ExecutionEngine(mock_spark, mock_config)\n-        \n+\n         # Create a bronze step\n         bronze_step = BronzeStep(\n-            name=\"test_bronze\",\n-            rules={\"id\": [\"not_null\"]},\n-            schema=\"test_schema\"\n-        )\n-        \n+            name=\"test_bronze\", rules={\"id\": [\"not_null\"]}, schema=\"test_schema\"\n+        )\n+\n         # Mock the spark.read.format().load() to raise an exception\n-        mock_spark.read.format.return_value.load.side_effect = Exception(\"File not found\")\n-        \n+        mock_spark.read.format.return_value.load.side_effect = Exception(\n+            \"File not found\"\n+        )\n+\n         # Mock the logger to capture warning messages\n-        with patch.object(engine.logger, 'warning') as mock_warning:\n+        with patch.object(engine.logger, \"warning\") as mock_warning:\n             # Execute bronze step without the step name in context\n             result_df = engine._execute_bronze_step(bronze_step, {})\n-            \n+\n             # Verify that warning was logged about the read failure\n             mock_warning.assert_called_once()\n             warning_call = mock_warning.call_args[0][0]\n             assert \"Failed to read data for bronze step 'test_bronze'\" in warning_call\n             assert \"Creating empty DataFrame as fallback\" in warning_call\n-\n \n     def test_execute_step_silver_without_dependencies(self, mock_spark, mock_config):\n         \"\"\"Test silver step creation without valid dependencies should fail.\"\"\"\n         # A SilverStep without a valid source_bronze is logically invalid\n         # and should be rejected during construction\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/__init__.py\t2025-09-20 16:34:51.329306+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/__init__.py\t2025-09-20 16:53:34.822658+00:00\n@@ -10,10 +10,6 @@\n \n from .security_tests import SecurityTestSuite\n from .vulnerability_scanner import VulnerabilityScanner\n from .compliance_checker import ComplianceChecker\n \n-__all__ = [\n-    'SecurityTestSuite',\n-    'VulnerabilityScanner', \n-    'ComplianceChecker'\n-]\n+__all__ = [\"SecurityTestSuite\", \"VulnerabilityScanner\", \"ComplianceChecker\"]\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/test_performance_integration.py\t2025-09-20 16:50:36.913455+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/test_performance_integration.py\t2025-09-20 16:53:34.837503+00:00\n@@ -15,425 +15,481 @@\n from pathlib import Path\n from unittest.mock import Mock, patch\n from datetime import datetime, timedelta\n \n # Import performance components\n-from tests.performance.performance_profiler import PerformanceProfiler, profile_function, quick_profile\n+from tests.performance.performance_profiler import (\n+    PerformanceProfiler,\n+    profile_function,\n+    quick_profile,\n+)\n from tests.performance.caching_strategies import (\n-    MemoryCache, PersistentCache, HybridCache, cache_result, cache_dataframe,\n-    cache_manager\n+    MemoryCache,\n+    PersistentCache,\n+    HybridCache,\n+    cache_result,\n+    cache_dataframe,\n+    cache_manager,\n )\n from tests.performance.performance_monitoring import (\n-    PerformanceMonitor, get_performance_monitor, record_metric,\n-    start_performance_monitoring, stop_performance_monitoring\n+    PerformanceMonitor,\n+    get_performance_monitor,\n+    record_metric,\n+    start_performance_monitoring,\n+    stop_performance_monitoring,\n )\n from tests.performance.memory_optimization import (\n-    MemoryProfiler, MemoryOptimizer, memory_monitor, optimize_spark_memory\n+    MemoryProfiler,\n+    MemoryOptimizer,\n+    memory_monitor,\n+    optimize_spark_memory,\n )\n from tests.performance.performance_benchmarking import (\n-    PerformanceBenchmark, benchmark, compare_benchmarks, load_test\n+    PerformanceBenchmark,\n+    benchmark,\n+    compare_benchmarks,\n+    load_test,\n )\n \n \n class TestPerformanceIntegration:\n     \"\"\"Integration tests for performance components.\"\"\"\n-    \n+\n     @pytest.fixture\n     def temp_dir(self):\n         \"\"\"Create temporary directory for testing.\"\"\"\n         with tempfile.TemporaryDirectory() as temp_dir:\n             yield Path(temp_dir)\n-    \n+\n     @pytest.fixture\n     def performance_profiler(self):\n         \"\"\"Create performance profiler instance.\"\"\"\n-        return PerformanceProfiler(enable_memory_tracking=True, enable_cpu_tracking=True)\n-    \n+        return PerformanceProfiler(\n+            enable_memory_tracking=True, enable_cpu_tracking=True\n+        )\n+\n     @pytest.fixture\n     def performance_monitor(self):\n         \"\"\"Create performance monitor instance.\"\"\"\n         config = {\n             \"monitoring_interval\": 1,\n             \"enable_alerts\": True,\n-            \"enable_resource_monitoring\": True\n+            \"enable_resource_monitoring\": True,\n         }\n         return PerformanceMonitor(config)\n-    \n+\n     @pytest.fixture\n     def memory_profiler(self):\n         \"\"\"Create memory profiler instance.\"\"\"\n         return MemoryProfiler(enable_tracemalloc=True)\n-    \n+\n     @pytest.fixture\n     def performance_benchmark(self):\n         \"\"\"Create performance benchmark instance.\"\"\"\n         return PerformanceBenchmark()\n-    \n+\n     def test_performance_profiler_integration(self, performance_profiler):\n         \"\"\"Test performance profiler integration.\"\"\"\n+\n         # Test function profiling\n         @performance_profiler.profile_function\n         def test_function(x):\n             time.sleep(0.01)  # Simulate work\n             return x * 2\n-        \n+\n         # Profile function\n         result, report = performance_profiler.profile_pipeline(test_function, 5)\n-        \n+\n         assert result == 10\n         assert report.total_execution_time > 0\n         assert len(report.function_metrics) > 0\n         assert len(report.recommendations) > 0\n-    \n+\n     def test_caching_strategies_integration(self, temp_dir):\n         \"\"\"Test caching strategies integration.\"\"\"\n         # Test memory cache\n         memory_cache = MemoryCache(max_size_mb=10, default_ttl=60)\n-        \n+\n         # Test basic caching\n         memory_cache.set(\"test_key\", \"test_value\")\n         value = memory_cache.get(\"test_key\")\n         assert value == \"test_value\"\n-        \n+\n         # Test cache statistics\n         stats = memory_cache.get_stats()\n         assert stats.total_entries == 1\n         assert stats.hit_count == 1\n-        \n+\n         # Test persistent cache\n         persistent_cache = PersistentCache(temp_dir, max_file_size_mb=5)\n         persistent_cache.set(\"persistent_key\", \"persistent_value\")\n         persistent_value = persistent_cache.get(\"persistent_key\")\n         assert persistent_value == \"persistent_value\"\n-        \n+\n         # Test hybrid cache\n         hybrid_cache = HybridCache(memory_cache, persistent_cache)\n         hybrid_cache.set(\"hybrid_key\", \"hybrid_value\")\n         hybrid_value = hybrid_cache.get(\"hybrid_key\")\n         assert hybrid_value == \"hybrid_value\"\n-    \n+\n     def test_performance_monitoring_integration(self, performance_monitor):\n         \"\"\"Test performance monitoring integration.\"\"\"\n         # Start monitoring\n         performance_monitor.start_monitoring()\n-        \n+\n         # Record some metrics\n         performance_monitor.record_metric(\"test_metric\", 100.0, \"ms\")\n         performance_monitor.record_metric(\"throughput\", 50.0, \"rps\")\n-        \n+\n         # Wait for monitoring to collect data\n         time.sleep(2)\n-        \n+\n         # Get dashboard data\n         dashboard_data = performance_monitor.get_dashboard_data()\n         assert \"metrics\" in dashboard_data\n         assert \"resources\" in dashboard_data\n         assert \"alerts\" in dashboard_data\n-        \n+\n         # Stop monitoring\n         performance_monitor.stop_monitoring()\n         assert not performance_monitor.monitoring_active\n-    \n+\n     def test_memory_optimization_integration(self, memory_profiler):\n         \"\"\"Test memory optimization integration.\"\"\"\n         # Take initial snapshot\n         initial_snapshot = memory_profiler.take_snapshot()\n         assert initial_snapshot.current_memory_mb >= 0\n-        \n+\n         # Create some objects to test memory tracking\n         test_objects = [f\"test_object_{i}\" for i in range(1000)]\n-        \n+\n         # Take another snapshot\n         final_snapshot = memory_profiler.take_snapshot()\n         assert final_snapshot.current_memory_mb >= initial_snapshot.current_memory_mb\n-        \n+\n         # Get memory stats\n         stats = memory_profiler.get_memory_stats()\n         assert \"current_memory_mb\" in stats\n         assert \"object_counts\" in stats\n-        \n+\n         # Test memory optimizer\n         optimizer = MemoryOptimizer()\n         efficiency_score = optimizer.analyze_memory_efficiency()\n         assert 0 <= efficiency_score <= 100\n-    \n+\n     def test_performance_benchmarking_integration(self, performance_benchmark):\n         \"\"\"Test performance benchmarking integration.\"\"\"\n+\n         # Test function to benchmark\n         def benchmark_function(x):\n             time.sleep(0.001)  # Simulate work\n             return x * x\n-        \n+\n         # Benchmark function\n-        stats = performance_benchmark.benchmark_function(benchmark_function, 5, iterations=50)\n-        \n+        stats = performance_benchmark.benchmark_function(\n+            benchmark_function, 5, iterations=50\n+        )\n+\n         assert stats.function_name.endswith(\"benchmark_function\")\n         assert stats.mean_time > 0\n         assert stats.total_iterations == 50\n         assert stats.success_rate == 100.0\n-        \n+\n         # Set baseline\n         performance_benchmark.set_baseline(stats)\n-        \n+\n         # Test regression detection\n         regressions = performance_benchmark.detect_performance_regressions()\n         assert isinstance(regressions, list)\n-    \n+\n     def test_load_testing_integration(self, performance_benchmark):\n         \"\"\"Test load testing integration.\"\"\"\n+\n         def load_test_function(x):\n             time.sleep(0.001)  # Simulate work\n             return x + 1\n-        \n+\n         # Run load test\n-        load_result = performance_benchmark.load_test(\n-            load_test_function, 5, 50, 10\n-        )\n-        \n+        load_result = performance_benchmark.load_test(load_test_function, 5, 50, 10)\n+\n         assert load_result.concurrent_users == 5\n         assert load_result.total_requests == 50\n         assert load_result.successful_requests >= 0\n         assert load_result.avg_response_time >= 0\n         assert load_result.throughput_rps >= 0\n-    \n+\n     def test_caching_decorators_integration(self):\n         \"\"\"Test caching decorators integration.\"\"\"\n         call_count = 0\n-        \n+\n         @cache_result(ttl_seconds=60)\n         def cached_function(x):\n             nonlocal call_count\n             call_count += 1\n             time.sleep(0.01)  # Simulate work\n             return x * 2\n-        \n+\n         # First call\n         result1 = cached_function(5)\n         assert result1 == 10\n         assert call_count == 1\n-        \n+\n         # Second call (should be cached)\n         result2 = cached_function(5)\n         assert result2 == 10\n         assert call_count == 1  # Should not increment\n-        \n+\n         # Different argument (should not be cached)\n         result3 = cached_function(6)\n         assert result3 == 12\n         assert call_count == 2\n-    \n+\n     def test_memory_monitoring_decorator_integration(self):\n         \"\"\"Test memory monitoring decorator integration.\"\"\"\n         from tests.performance.memory_optimization import memory_monitor\n-        \n+\n         @memory_monitor(interval_seconds=1)\n         def memory_intensive_function():\n             # Create some objects to use memory\n             objects = [f\"object_{i}\" for i in range(10000)]\n             time.sleep(0.1)\n             return len(objects)\n-        \n+\n         # Execute function\n         result = memory_intensive_function()\n         assert result == 10000\n-    \n+\n     def test_performance_profiler_decorator_integration(self):\n         \"\"\"Test performance profiler decorator integration.\"\"\"\n+\n         @profile_function\n         def profiled_function(x):\n             time.sleep(0.01)\n             return x * 3\n-        \n+\n         # Execute function\n         result = profiled_function(4)\n         assert result == 12\n-    \n-    def test_comprehensive_performance_workflow(self, performance_profiler, \n-                                             performance_monitor, memory_profiler,\n-                                             performance_benchmark):\n+\n+    def test_comprehensive_performance_workflow(\n+        self,\n+        performance_profiler,\n+        performance_monitor,\n+        memory_profiler,\n+        performance_benchmark,\n+    ):\n         \"\"\"Test comprehensive performance workflow integration.\"\"\"\n         # Step 1: Start monitoring\n         performance_monitor.start_monitoring()\n-        \n+\n         # Step 2: Take memory snapshot\n         memory_snapshot = memory_profiler.take_snapshot()\n-        \n+\n         # Step 3: Profile a function\n         @performance_profiler.profile_function\n         def workflow_function(data):\n             time.sleep(0.01)\n             return [x * 2 for x in data]\n-        \n+\n         # Step 4: Benchmark the function\n         stats = performance_benchmark.benchmark_function(\n             workflow_function, [1, 2, 3, 4, 5], iterations=20\n         )\n-        \n+\n         # Step 5: Record performance metrics\n-        performance_monitor.record_metric(\"workflow_execution_time\", stats.mean_time, \"ms\")\n-        performance_monitor.record_metric(\"workflow_memory_usage\", \n-                                        memory_profiler.get_memory_stats().get(\"current_memory_mb\", 0), \"mb\")\n-        \n+        performance_monitor.record_metric(\n+            \"workflow_execution_time\", stats.mean_time, \"ms\"\n+        )\n+        performance_monitor.record_metric(\n+            \"workflow_memory_usage\",\n+            memory_profiler.get_memory_stats().get(\"current_memory_mb\", 0),\n+            \"mb\",\n+        )\n+\n         # Step 6: Wait for monitoring\n         time.sleep(2)\n-        \n+\n         # Step 7: Get comprehensive results\n         dashboard_data = performance_monitor.get_dashboard_data()\n         memory_stats = memory_profiler.get_memory_stats()\n         profiler_report = performance_profiler.generate_report()\n-        \n+\n         # Step 8: Verify integration\n         assert dashboard_data[\"monitoring_active\"]\n         assert \"metrics\" in dashboard_data\n         assert memory_stats[\"current_memory_mb\"] >= 0\n         assert profiler_report.total_execution_time > 0\n-        \n+\n         # Step 9: Stop monitoring\n         performance_monitor.stop_monitoring()\n-    \n+\n     def test_performance_optimization_workflow(self, temp_dir):\n         \"\"\"Test performance optimization workflow.\"\"\"\n         # Create cache manager\n         cache_manager.clear_all()\n-        \n+\n         # Test optimization workflow\n         optimizer = MemoryOptimizer()\n-        \n+\n         # Test Spark memory optimization\n         spark_config = optimize_spark_memory()\n         assert \"spark_config\" in spark_config\n         assert \"memory_settings\" in spark_config\n-        \n+\n         # Test memory efficiency analysis\n         efficiency_score = optimizer.analyze_memory_efficiency()\n         assert 0 <= efficiency_score <= 100\n-    \n-    def test_performance_reporting_integration(self, performance_profiler,\n-                                            performance_monitor, memory_profiler,\n-                                            performance_benchmark, temp_dir):\n+\n+    def test_performance_reporting_integration(\n+        self,\n+        performance_profiler,\n+        performance_monitor,\n+        memory_profiler,\n+        performance_benchmark,\n+        temp_dir,\n+    ):\n         \"\"\"Test performance reporting integration.\"\"\"\n+\n         # Generate some performance data\n         @performance_profiler.profile_function\n         def report_function(x):\n             time.sleep(0.01)\n-            return x ** 2\n-        \n+            return x**2\n+\n         # Profile function\n-        result, profiler_report = performance_profiler.profile_pipeline(report_function, 5)\n-        \n+        result, profiler_report = performance_profiler.profile_pipeline(\n+            report_function, 5\n+        )\n+\n         # Benchmark function\n-        benchmark_stats = performance_benchmark.benchmark_function(report_function, 5, iterations=10)\n-        \n+        benchmark_stats = performance_benchmark.benchmark_function(\n+            report_function, 5, iterations=10\n+        )\n+\n         # Start monitoring and record metrics\n         performance_monitor.start_monitoring()\n         performance_monitor.record_metric(\"report_metric\", 100.0, \"ms\")\n         time.sleep(1)\n-        \n+\n         # Take memory snapshot\n         memory_snapshot = memory_profiler.take_snapshot()\n-        \n+\n         # Generate reports\n-        profiler_report_file = performance_profiler.export_report(temp_dir / \"profiler_report.json\")\n-        benchmark_report_file = performance_benchmark.export_benchmark_report(temp_dir / \"benchmark_report.json\")\n+        profiler_report_file = performance_profiler.export_report(\n+            temp_dir / \"profiler_report.json\"\n+        )\n+        benchmark_report_file = performance_benchmark.export_benchmark_report(\n+            temp_dir / \"benchmark_report.json\"\n+        )\n         performance_report = performance_monitor.generate_performance_report()\n-        performance_report_file = performance_monitor.export_report(performance_report, temp_dir / \"performance_report.json\")\n-        \n+        performance_report_file = performance_monitor.export_report(\n+            performance_report, temp_dir / \"performance_report.json\"\n+        )\n+\n         # Verify reports were created\n         assert profiler_report_file.exists()\n         assert benchmark_report_file.exists()\n         assert performance_report_file.exists()\n-        \n+\n         # Stop monitoring\n         performance_monitor.stop_monitoring()\n-    \n-    def test_error_handling_integration(self, performance_profiler, performance_monitor):\n+\n+    def test_error_handling_integration(\n+        self, performance_profiler, performance_monitor\n+    ):\n         \"\"\"Test error handling in performance components.\"\"\"\n+\n         # Test profiler with failing function\n         @performance_profiler.profile_function\n         def failing_function():\n             raise ValueError(\"Test error\")\n-        \n+\n         # Should handle error gracefully\n         try:\n             failing_function()\n         except ValueError:\n             pass  # Expected\n-        \n+\n         # Check that error was recorded\n         error_results = [r for r in performance_profiler.metrics if not r.success]\n         assert len(error_results) > 0\n-        \n+\n         # Test monitor with invalid metrics\n         performance_monitor.start_monitoring()\n-        \n+\n         # Record invalid metric\n         performance_monitor.record_metric(\"\", -1.0, \"\")  # Invalid name and value\n-        \n+\n         # Should handle gracefully\n         dashboard_data = performance_monitor.get_dashboard_data()\n         assert \"metrics\" in dashboard_data\n-        \n+\n         performance_monitor.stop_monitoring()\n-    \n-    def test_concurrent_performance_operations(self, performance_profiler, performance_monitor):\n+\n+    def test_concurrent_performance_operations(\n+        self, performance_profiler, performance_monitor\n+    ):\n         \"\"\"Test concurrent performance operations.\"\"\"\n         import threading\n-        \n+\n         results = []\n         errors = []\n-        \n+\n         def concurrent_operation(operation_id):\n             try:\n+\n                 @performance_profiler.profile_function\n                 def concurrent_function(x):\n                     time.sleep(0.01)\n                     return x + operation_id\n-                \n+\n                 result = concurrent_function(10)\n                 results.append(result)\n-                \n+\n                 # Record metric\n-                performance_monitor.record_metric(f\"concurrent_metric_{operation_id}\", result, \"count\")\n-                \n+                performance_monitor.record_metric(\n+                    f\"concurrent_metric_{operation_id}\", result, \"count\"\n+                )\n+\n             except Exception as e:\n                 errors.append(str(e))\n-        \n+\n         # Start monitoring\n         performance_monitor.start_monitoring()\n-        \n+\n         # Run concurrent operations\n         threads = []\n         for i in range(5):\n             thread = threading.Thread(target=concurrent_operation, args=(i,))\n             threads.append(thread)\n             thread.start()\n-        \n+\n         # Wait for completion\n         for thread in threads:\n             thread.join()\n-        \n+\n         # Verify results\n         assert len(results) == 5\n         assert len(errors) == 0\n-        \n+\n         # Check monitoring data\n         time.sleep(1)\n         dashboard_data = performance_monitor.get_dashboard_data()\n         assert dashboard_data[\"monitoring_active\"]\n-        \n+\n         performance_monitor.stop_monitoring()\n \n \n # Pytest markers for performance tests\n @pytest.mark.performance\n class TestPerformanceMarkers:\n     \"\"\"Test performance-specific pytest markers.\"\"\"\n-    \n+\n     def test_performance_marker_works(self):\n         \"\"\"Test that performance marker works.\"\"\"\n         assert True\n-    \n+\n     @pytest.mark.slow\n     def test_slow_performance_test(self):\n         \"\"\"Test slow performance test marker.\"\"\"\n         time.sleep(0.1)  # Simulate slow test\n         assert True\n@@ -444,33 +500,37 @@\n     \"\"\"Test performance integration with CI/CD pipeline.\"\"\"\n     # Create performance components\n     profiler = PerformanceProfiler()\n     monitor = PerformanceMonitor({\"monitoring_interval\": 1})\n     benchmark = PerformanceBenchmark()\n-    \n+\n     # Test function\n     def cicd_test_function(data):\n         return sum(data)\n-    \n+\n     # Profile function\n-    result, profiler_report = profiler.profile_pipeline(cicd_test_function, [1, 2, 3, 4, 5])\n+    result, profiler_report = profiler.profile_pipeline(\n+        cicd_test_function, [1, 2, 3, 4, 5]\n+    )\n     assert result == 15\n-    \n+\n     # Benchmark function\n-    benchmark_stats = benchmark.benchmark_function(cicd_test_function, [1, 2, 3, 4, 5], iterations=10)\n+    benchmark_stats = benchmark.benchmark_function(\n+        cicd_test_function, [1, 2, 3, 4, 5], iterations=10\n+    )\n     assert benchmark_stats.mean_time >= 0\n     assert benchmark_stats.success_rate == 100.0\n-    \n+\n     # Start monitoring\n     monitor.start_monitoring()\n     monitor.record_metric(\"cicd_metric\", benchmark_stats.mean_time, \"ms\")\n     time.sleep(1)\n-    \n+\n     # Get results\n     dashboard_data = monitor.get_dashboard_data()\n     assert dashboard_data[\"monitoring_active\"]\n-    \n+\n     monitor.stop_monitoring()\n \n \n if __name__ == \"__main__\":\n     # Run integration tests\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py\t2025-09-20 16:10:36.187491+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py\t2025-09-20 16:53:34.844412+00:00\n@@ -27,409 +27,400 @@\n from .performance_monitor import performance_monitor\n \n \n class TestValidationPerformance:\n     \"\"\"Performance tests for validation functions.\"\"\"\n-    \n+\n     @pytest.mark.performance\n     @pytest.mark.benchmark\n-    def test_safe_divide_performance(self, performance_monitor_clean, benchmark_iterations, performance_tolerance) -> None:\n+    def test_safe_divide_performance(\n+        self, performance_monitor_clean, benchmark_iterations, performance_tolerance\n+    ) -> None:\n         \"\"\"Test performance of safe_divide function.\"\"\"\n         iterations = 10000\n-        \n+\n         result = performance_monitor.benchmark_function(\n             safe_divide,\n             \"safe_divide\",\n             iterations=iterations,\n             args=(100.0, 5.0),\n-            warmup_iterations=100\n-        )\n-        \n+            warmup_iterations=100,\n+        )\n+\n         # Performance assertions\n         assert result.success\n         assert result.execution_time < 1.0  # Should complete within 1 second\n         assert result.avg_time_per_iteration < 0.1  # Each call should be < 0.1ms\n         assert result.throughput > 1000  # Should handle > 1000 calls/second\n-        \n+\n         # Check for regression\n         regression = performance_monitor_clean.check_regression(\"safe_divide\")\n         if regression[\"status\"] == \"regression_detected\":\n             pytest.fail(f\"Performance regression detected: {regression}\")\n-    \n+\n     @pytest.mark.performance\n     def test_safe_divide_zero_denominator_performance(self) -> None:\n         \"\"\"Test performance of safe_divide with zero denominator.\"\"\"\n         iterations = 5000\n-        \n+\n         result = performance_monitor.benchmark_function(\n             safe_divide,\n             \"safe_divide_zero\",\n             iterations=iterations,\n             args=(100.0, 0.0),\n-            warmup_iterations=50\n-        )\n-        \n+            warmup_iterations=50,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 0.5\n         assert result.avg_time_per_iteration < 0.1\n-    \n+\n     def test_validate_dataframe_schema_performance(self) -> None:\n         \"\"\"Test performance of validate_dataframe_schema function.\"\"\"\n         iterations = 1000\n-        \n+\n         # Create mock DataFrame with many columns\n         mock_df = Mock()\n         mock_df.columns = [f\"col{i}\" for i in range(100)]\n-        \n+\n         expected_columns = [f\"col{i}\" for i in range(50)]\n-        \n+\n         def test_function():\n             return validate_dataframe_schema(mock_df, expected_columns)\n-        \n+\n         result = performance_monitor.benchmark_function(\n             test_function,\n             \"validate_dataframe_schema\",\n             iterations=iterations,\n-            warmup_iterations=10\n-        )\n-        \n+            warmup_iterations=10,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 0.5\n         assert result.avg_time_per_iteration < 1.0\n-    \n+\n     def test_assess_data_quality_performance(self) -> None:\n         \"\"\"Test performance of assess_data_quality function.\"\"\"\n         iterations = 100\n-        \n+\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.count.return_value = 1000\n-        \n-        rules = {\n-            f\"col{i}\": [f\"col{i} > 0\", f\"col{i} IS NOT NULL\"]\n-            for i in range(10)\n-        }\n-        \n+\n+        rules = {f\"col{i}\": [f\"col{i} > 0\", f\"col{i} IS NOT NULL\"] for i in range(10)}\n+\n         def test_function():\n             return assess_data_quality(mock_df, rules)\n-        \n+\n         result = performance_monitor.benchmark_function(\n             test_function,\n             \"assess_data_quality\",\n             iterations=iterations,\n-            warmup_iterations=5\n-        )\n-        \n+            warmup_iterations=5,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 2.0\n         assert result.avg_time_per_iteration < 20.0\n-    \n+\n     def test_get_dataframe_info_performance(self) -> None:\n         \"\"\"Test performance of get_dataframe_info function.\"\"\"\n         iterations = 500\n-        \n+\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.count.return_value = 10000\n         mock_df.columns = [f\"col{i}\" for i in range(50)]\n-        \n+\n         # Create mock schema\n         mock_schema = Mock()\n         mock_schema.__str__ = Mock(return_value=\"struct<col1:string,col2:string>\")\n         mock_df.schema = mock_schema\n-        \n+\n         def test_function():\n             return get_dataframe_info(mock_df)\n-        \n+\n         result = performance_monitor.benchmark_function(\n             test_function,\n             \"get_dataframe_info\",\n             iterations=iterations,\n-            warmup_iterations=5\n-        )\n-        \n+            warmup_iterations=5,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 1.0\n         assert result.avg_time_per_iteration < 2.0\n \n \n class TestModelCreationPerformance:\n     \"\"\"Performance tests for model creation operations.\"\"\"\n-    \n+\n     @pytest.mark.performance\n     @pytest.mark.benchmark\n     def test_validation_thresholds_creation_performance(self) -> None:\n         \"\"\"Test performance of ValidationThresholds creation.\"\"\"\n         iterations = 10000\n-        \n+\n         def create_thresholds():\n             return ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n-        \n+\n         result = performance_monitor.benchmark_function(\n             create_thresholds,\n             \"validation_thresholds_creation\",\n             iterations=iterations,\n-            warmup_iterations=100\n-        )\n-        \n+            warmup_iterations=100,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 1.0\n         assert result.avg_time_per_iteration < 0.1\n         assert result.throughput > 5000\n-    \n+\n     def test_parallel_config_creation_performance(self) -> None:\n         \"\"\"Test performance of ParallelConfig creation.\"\"\"\n         iterations = 10000\n-        \n+\n         def create_parallel_config():\n             return ParallelConfig(enabled=True, max_workers=4)\n-        \n+\n         result = performance_monitor.benchmark_function(\n             create_parallel_config,\n             \"parallel_config_creation\",\n             iterations=iterations,\n-            warmup_iterations=100\n-        )\n-        \n+            warmup_iterations=100,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 1.0\n         assert result.avg_time_per_iteration < 0.1\n-    \n+\n     def test_pipeline_config_creation_performance(self) -> None:\n         \"\"\"Test performance of PipelineConfig creation.\"\"\"\n         iterations = 1000\n-        \n+\n         def create_pipeline_config():\n             return PipelineConfig.create_default(\"test_schema\")\n-        \n+\n         result = performance_monitor.benchmark_function(\n             create_pipeline_config,\n             \"pipeline_config_creation\",\n             iterations=iterations,\n-            warmup_iterations=10\n-        )\n-        \n+            warmup_iterations=10,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 0.5\n         assert result.avg_time_per_iteration < 0.5\n-    \n+\n     def test_bronze_step_creation_performance(self) -> None:\n         \"\"\"Test performance of BronzeStep creation.\"\"\"\n         iterations = 5000\n-        \n-        rules = {\n-            f\"col{i}\": [f\"col{i} > 0\", f\"col{i} IS NOT NULL\"]\n-            for i in range(5)\n-        }\n-        \n+\n+        rules = {f\"col{i}\": [f\"col{i} > 0\", f\"col{i} IS NOT NULL\"] for i in range(5)}\n+\n         def create_bronze_step():\n             return BronzeStep(\n-                name=\"test_step\",\n-                rules=rules,\n-                incremental_col=\"updated_at\"\n+                name=\"test_step\", rules=rules, incremental_col=\"updated_at\"\n             )\n-        \n+\n         result = performance_monitor.benchmark_function(\n             create_bronze_step,\n             \"bronze_step_creation\",\n             iterations=iterations,\n-            warmup_iterations=50\n-        )\n-        \n+            warmup_iterations=50,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 1.0\n         assert result.avg_time_per_iteration < 0.2\n-    \n+\n     def test_silver_step_creation_performance(self) -> None:\n         \"\"\"Test performance of SilverStep creation.\"\"\"\n         iterations = 2000\n-        \n+\n         def mock_transform(spark, bronze_df, prior_silvers):\n             return bronze_df\n-        \n-        rules = {\n-            f\"col{i}\": [f\"col{i} > 0\"]\n-            for i in range(3)\n-        }\n-        \n+\n+        rules = {f\"col{i}\": [f\"col{i} > 0\"] for i in range(3)}\n+\n         def create_silver_step():\n             return SilverStep(\n                 name=\"test_step\",\n                 source_bronze=\"bronze1\",\n                 transform=mock_transform,\n                 rules=rules,\n-                table_name=\"silver_table\"\n+                table_name=\"silver_table\",\n             )\n-        \n+\n         result = performance_monitor.benchmark_function(\n             create_silver_step,\n             \"silver_step_creation\",\n             iterations=iterations,\n-            warmup_iterations=20\n-        )\n-        \n+            warmup_iterations=20,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 1.0\n         assert result.avg_time_per_iteration < 0.5\n-    \n+\n     def test_gold_step_creation_performance(self) -> None:\n         \"\"\"Test performance of GoldStep creation.\"\"\"\n         iterations = 2000\n-        \n+\n         def mock_transform(spark, silver_dfs):\n             return silver_dfs\n-        \n+\n         rules = {\"col1\": [\"col1 > 0\"]}\n         source_silvers = [\"silver1\", \"silver2\"]\n-        \n+\n         def create_gold_step():\n             return GoldStep(\n                 name=\"test_step\",\n                 table_name=\"gold_table\",\n                 transform=mock_transform,\n                 rules=rules,\n-                source_silvers=source_silvers\n+                source_silvers=source_silvers,\n             )\n-        \n+\n         result = performance_monitor.benchmark_function(\n             create_gold_step,\n             \"gold_step_creation\",\n             iterations=iterations,\n-            warmup_iterations=20\n-        )\n-        \n+            warmup_iterations=20,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 1.0\n         assert result.avg_time_per_iteration < 0.5\n \n \n class TestSerializationPerformance:\n     \"\"\"Performance tests for serialization operations.\"\"\"\n-    \n+\n     def test_model_to_dict_performance(self) -> None:\n         \"\"\"Test performance of model.to_dict() serialization.\"\"\"\n         iterations = 1000\n-        \n+\n         # Create a complex model\n         config = PipelineConfig.create_default(\"test_schema\")\n-        \n+\n         def serialize_model():\n             return config.to_dict()\n-        \n+\n         result = performance_monitor.benchmark_function(\n             serialize_model,\n             \"model_to_dict\",\n             iterations=iterations,\n-            warmup_iterations=10\n-        )\n-        \n+            warmup_iterations=10,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 0.5\n         assert result.avg_time_per_iteration < 0.5\n-    \n+\n     def test_model_to_json_performance(self) -> None:\n         \"\"\"Test performance of model.to_json() serialization.\"\"\"\n         iterations = 1000\n-        \n+\n         # Create a complex model\n         config = PipelineConfig.create_default(\"test_schema\")\n-        \n+\n         def serialize_model():\n             return config.to_json()\n-        \n+\n         result = performance_monitor.benchmark_function(\n             serialize_model,\n             \"model_to_json\",\n             iterations=iterations,\n-            warmup_iterations=10\n-        )\n-        \n+            warmup_iterations=10,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 1.0\n         assert result.avg_time_per_iteration < 1.0\n-    \n+\n     def test_model_validation_performance(self) -> None:\n         \"\"\"Test performance of model validation.\"\"\"\n         iterations = 500\n-        \n+\n         # Create a model\n         config = PipelineConfig.create_default(\"test_schema\")\n-        \n+\n         def validate_model():\n             config.validate()\n-        \n+\n         result = performance_monitor.benchmark_function(\n             validate_model,\n             \"model_validation\",\n             iterations=iterations,\n-            warmup_iterations=5\n-        )\n-        \n+            warmup_iterations=5,\n+        )\n+\n         assert result.success\n         assert result.execution_time < 0.5\n         assert result.avg_time_per_iteration < 1.0\n \n \n class TestMemoryUsagePerformance:\n     \"\"\"Performance tests focusing on memory usage.\"\"\"\n-    \n+\n     @pytest.mark.performance\n     @pytest.mark.memory\n     def test_model_creation_memory_usage(self, memory_limit_mb) -> None:\n         \"\"\"Test memory usage of model creation.\"\"\"\n         iterations = 1000\n-        \n+\n         def create_models():\n             models = []\n             for i in range(10):\n                 models.append(PipelineConfig.create_default(f\"schema_{i}\"))\n             return models\n-        \n+\n         result = performance_monitor.benchmark_function(\n             create_models,\n             \"model_creation_memory\",\n             iterations=iterations,\n-            warmup_iterations=5\n-        )\n-        \n+            warmup_iterations=5,\n+        )\n+\n         assert result.success\n         assert result.memory_usage_mb < memory_limit_mb  # Should use < limit\n-        assert result.peak_memory_mb < memory_limit_mb * 2   # Peak should be < 2x limit\n-    \n+        assert result.peak_memory_mb < memory_limit_mb * 2  # Peak should be < 2x limit\n+\n     def test_serialization_memory_usage(self) -> None:\n         \"\"\"Test memory usage of serialization operations.\"\"\"\n         iterations = 500\n-        \n+\n         # Create a complex model\n         config = PipelineConfig.create_default(\"test_schema\")\n-        \n+\n         def serialize_and_deserialize():\n             data = config.to_dict()\n             json_str = config.to_json()\n             return data, json_str\n-        \n+\n         result = performance_monitor.benchmark_function(\n             serialize_and_deserialize,\n             \"serialization_memory\",\n             iterations=iterations,\n-            warmup_iterations=5\n-        )\n-        \n-        assert result.success\n-        assert result.memory_usage_mb < 50   # Should use < 50MB\n-        assert result.peak_memory_mb < 100   # Peak should be < 100MB\n+            warmup_iterations=5,\n+        )\n+\n+        assert result.success\n+        assert result.memory_usage_mb < 50  # Should use < 50MB\n+        assert result.peak_memory_mb < 100  # Peak should be < 100MB\n \n \n def test_performance_summary() -> None:\n     \"\"\"Test that generates a performance summary.\"\"\"\n     summary = performance_monitor.get_performance_summary()\n-    \n+\n     assert \"total_tests\" in summary\n     assert \"successful_tests\" in summary\n     assert \"functions_tested\" in summary\n     assert summary[\"total_tests\"] > 0\n-    \n+\n     # Print summary for manual review\n     print(f\"\\nPerformance Test Summary:\")\n     print(f\"Total tests: {summary['total_tests']}\")\n     print(f\"Successful tests: {summary['successful_tests']}\")\n     print(f\"Functions tested: {summary['functions_tested']}\")\n@@ -440,11 +431,11 @@\n def test_update_baselines() -> None:\n     \"\"\"Test function to update performance baselines.\"\"\"\n     # Update baselines for all tested functions\n     test_functions = [\n         \"safe_divide\",\n-        \"safe_divide_zero\", \n+        \"safe_divide_zero\",\n         \"validate_dataframe_schema\",\n         \"assess_data_quality\",\n         \"get_dataframe_info\",\n         \"validation_thresholds_creation\",\n         \"parallel_config_creation\",\n@@ -454,16 +445,16 @@\n         \"gold_step_creation\",\n         \"model_to_dict\",\n         \"model_to_json\",\n         \"model_validation\",\n         \"model_creation_memory\",\n-        \"serialization_memory\"\n+        \"serialization_memory\",\n     ]\n-    \n+\n     for func_name in test_functions:\n         performance_monitor.update_baseline(func_name)\n-    \n+\n     print(\"Performance baselines updated successfully\")\n \n \n # Convenience functions for external use\n def run_validation_performance_tests():\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_benchmarking.py\t2025-09-20 16:45:07.583932+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_benchmarking.py\t2025-09-20 16:53:34.874127+00:00\n@@ -24,10 +24,11 @@\n \n \n @dataclass\n class BenchmarkResult:\n     \"\"\"Benchmark result data structure.\"\"\"\n+\n     function_name: str\n     execution_time: float\n     memory_usage: float\n     cpu_usage: float\n     iterations: int\n@@ -37,10 +38,11 @@\n \n \n @dataclass\n class BenchmarkStats:\n     \"\"\"Benchmark statistics data structure.\"\"\"\n+\n     function_name: str\n     min_time: float\n     max_time: float\n     mean_time: float\n     median_time: float\n@@ -54,10 +56,11 @@\n \n \n @dataclass\n class PerformanceRegression:\n     \"\"\"Performance regression detection result.\"\"\"\n+\n     function_name: str\n     baseline_time: float\n     current_time: float\n     regression_percent: float\n     severity: str\n@@ -66,10 +69,11 @@\n \n \n @dataclass\n class LoadTestResult:\n     \"\"\"Load test result data structure.\"\"\"\n+\n     test_name: str\n     concurrent_users: int\n     total_requests: int\n     successful_requests: int\n     failed_requests: int\n@@ -81,154 +85,181 @@\n     duration: float\n \n \n class PerformanceBenchmark:\n     \"\"\"Comprehensive performance benchmarking system.\"\"\"\n-    \n+\n     def __init__(self, config: Optional[Dict[str, Any]] = None):\n         self.config = config or self._default_config()\n         self.benchmark_results: List[BenchmarkResult] = []\n         self.baseline_results: Dict[str, BenchmarkStats] = {}\n         self.regression_threshold = 0.15  # 15% regression threshold\n         self.logger = logging.getLogger(\"performance_benchmark\")\n-    \n+\n     def _default_config(self) -> Dict[str, Any]:\n         \"\"\"Get default configuration.\"\"\"\n         return {\n             \"default_iterations\": 100,\n             \"warmup_iterations\": 10,\n             \"timeout_seconds\": 300,\n             \"memory_tracking\": True,\n             \"cpu_tracking\": True,\n             \"statistical_analysis\": True,\n-            \"regression_detection\": True\n+            \"regression_detection\": True,\n         }\n-    \n-    def benchmark_function(self, func: Callable, *args, iterations: Optional[int] = None,\n-                          warmup_iterations: Optional[int] = None, **kwargs) -> BenchmarkStats:\n+\n+    def benchmark_function(\n+        self,\n+        func: Callable,\n+        *args,\n+        iterations: Optional[int] = None,\n+        warmup_iterations: Optional[int] = None,\n+        **kwargs,\n+    ) -> BenchmarkStats:\n         \"\"\"Benchmark a function with statistical analysis.\"\"\"\n         iterations = iterations or self.config.get(\"default_iterations\", 100)\n-        warmup_iterations = warmup_iterations or self.config.get(\"warmup_iterations\", 10)\n-        \n+        warmup_iterations = warmup_iterations or self.config.get(\n+            \"warmup_iterations\", 10\n+        )\n+\n         function_name = f\"{func.__module__}.{func.__name__}\"\n         results = []\n         memory_usage = []\n         cpu_usage = []\n-        \n+\n         # Warmup iterations\n-        self.logger.info(f\"Warming up {function_name} with {warmup_iterations} iterations\")\n+        self.logger.info(\n+            f\"Warming up {function_name} with {warmup_iterations} iterations\"\n+        )\n         for _ in range(warmup_iterations):\n             try:\n                 func(*args, **kwargs)\n             except Exception as e:\n                 self.logger.warning(f\"Warmup iteration failed: {e}\")\n-        \n+\n         # Force garbage collection before benchmarking\n         gc.collect()\n-        \n+\n         # Benchmark iterations\n         self.logger.info(f\"Benchmarking {function_name} with {iterations} iterations\")\n         for i in range(iterations):\n             try:\n                 # Measure execution time\n                 start_time = time.perf_counter()\n                 start_memory = self._get_memory_usage()\n                 start_cpu = self._get_cpu_usage()\n-                \n+\n                 result = func(*args, **kwargs)\n-                \n+\n                 end_time = time.perf_counter()\n                 end_memory = self._get_memory_usage()\n                 end_cpu = self._get_cpu_usage()\n-                \n-                execution_time = (end_time - start_time) * 1000  # Convert to milliseconds\n+\n+                execution_time = (\n+                    end_time - start_time\n+                ) * 1000  # Convert to milliseconds\n                 memory_delta = end_memory - start_memory\n                 cpu_delta = end_cpu - start_cpu\n-                \n+\n                 # Store result\n                 benchmark_result = BenchmarkResult(\n                     function_name=function_name,\n                     execution_time=execution_time,\n                     memory_usage=memory_delta,\n                     cpu_usage=cpu_delta,\n                     iterations=1,\n                     timestamp=datetime.now(),\n-                    success=True\n-                )\n-                \n+                    success=True,\n+                )\n+\n                 self.benchmark_results.append(benchmark_result)\n                 results.append(execution_time)\n                 memory_usage.append(memory_delta)\n                 cpu_usage.append(cpu_delta)\n-                \n+\n             except Exception as e:\n                 error_result = BenchmarkResult(\n                     function_name=function_name,\n                     execution_time=0,\n                     memory_usage=0,\n                     cpu_usage=0,\n                     iterations=1,\n                     timestamp=datetime.now(),\n                     success=False,\n-                    error_message=str(e)\n-                )\n-                \n+                    error_message=str(e),\n+                )\n+\n                 self.benchmark_results.append(error_result)\n                 self.logger.error(f\"Benchmark iteration {i} failed: {e}\")\n-        \n+\n         # Calculate statistics\n         stats = self._calculate_benchmark_stats(\n             function_name, results, memory_usage, cpu_usage\n         )\n-        \n+\n         return stats\n-    \n-    def _calculate_benchmark_stats(self, function_name: str, execution_times: List[float],\n-                                 memory_usage: List[float], cpu_usage: List[float]) -> BenchmarkStats:\n+\n+    def _calculate_benchmark_stats(\n+        self,\n+        function_name: str,\n+        execution_times: List[float],\n+        memory_usage: List[float],\n+        cpu_usage: List[float],\n+    ) -> BenchmarkStats:\n         \"\"\"Calculate comprehensive benchmark statistics.\"\"\"\n         if not execution_times:\n             return BenchmarkStats(\n                 function_name=function_name,\n-                min_time=0, max_time=0, mean_time=0, median_time=0, std_dev=0,\n-                p95_time=0, p99_time=0, total_iterations=0, success_rate=0,\n-                memory_stats={}, cpu_stats={}\n+                min_time=0,\n+                max_time=0,\n+                mean_time=0,\n+                median_time=0,\n+                std_dev=0,\n+                p95_time=0,\n+                p99_time=0,\n+                total_iterations=0,\n+                success_rate=0,\n+                memory_stats={},\n+                cpu_stats={},\n             )\n-        \n+\n         # Execution time statistics\n         min_time = min(execution_times)\n         max_time = max(execution_times)\n         mean_time = statistics.mean(execution_times)\n         median_time = statistics.median(execution_times)\n         std_dev = statistics.stdev(execution_times) if len(execution_times) > 1 else 0\n         p95_time = self._percentile(execution_times, 95)\n         p99_time = self._percentile(execution_times, 99)\n-        \n+\n         # Memory statistics\n         memory_stats = {}\n         if memory_usage:\n             memory_stats = {\n                 \"min_mb\": min(memory_usage),\n                 \"max_mb\": max(memory_usage),\n                 \"mean_mb\": statistics.mean(memory_usage),\n-                \"median_mb\": statistics.median(memory_usage)\n+                \"median_mb\": statistics.median(memory_usage),\n             }\n-        \n+\n         # CPU statistics\n         cpu_stats = {}\n         if cpu_usage:\n             cpu_stats = {\n                 \"min_percent\": min(cpu_usage),\n                 \"max_percent\": max(cpu_usage),\n                 \"mean_percent\": statistics.mean(cpu_usage),\n-                \"median_percent\": statistics.median(cpu_usage)\n+                \"median_percent\": statistics.median(cpu_usage),\n             }\n-        \n+\n         # Success rate\n         total_results = len(self.benchmark_results)\n         successful_results = len([r for r in self.benchmark_results if r.success])\n-        success_rate = (successful_results / total_results * 100) if total_results > 0 else 0\n-        \n+        success_rate = (\n+            (successful_results / total_results * 100) if total_results > 0 else 0\n+        )\n+\n         return BenchmarkStats(\n             function_name=function_name,\n             min_time=min_time,\n             max_time=max_time,\n             mean_time=mean_time,\n@@ -237,68 +268,79 @@\n             p95_time=p95_time,\n             p99_time=p99_time,\n             total_iterations=len(execution_times),\n             success_rate=success_rate,\n             memory_stats=memory_stats,\n-            cpu_stats=cpu_stats\n-        )\n-    \n+            cpu_stats=cpu_stats,\n+        )\n+\n     def _percentile(self, values: List[float], percentile: int) -> float:\n         \"\"\"Calculate percentile of values.\"\"\"\n         if not values:\n             return 0.0\n-        \n+\n         sorted_values = sorted(values)\n         index = (percentile / 100.0) * (len(sorted_values) - 1)\n-        \n+\n         if index.is_integer():\n             return sorted_values[int(index)]\n         else:\n             lower = sorted_values[int(index)]\n             upper = sorted_values[int(index) + 1]\n             return lower + (upper - lower) * (index - int(index))\n-    \n+\n     def _get_memory_usage(self) -> float:\n         \"\"\"Get current memory usage in MB.\"\"\"\n         try:\n             process = psutil.Process()\n             return process.memory_info().rss / 1024 / 1024\n         except Exception:\n             return 0.0\n-    \n+\n     def _get_cpu_usage(self) -> float:\n         \"\"\"Get current CPU usage percentage.\"\"\"\n         try:\n             return psutil.cpu_percent()\n         except Exception:\n             return 0.0\n-    \n-    def compare_benchmarks(self, baseline_stats: BenchmarkStats, \n-                          current_stats: BenchmarkStats) -> Dict[str, Any]:\n+\n+    def compare_benchmarks(\n+        self, baseline_stats: BenchmarkStats, current_stats: BenchmarkStats\n+    ) -> Dict[str, Any]:\n         \"\"\"Compare two benchmark results.\"\"\"\n-        time_regression = ((current_stats.mean_time - baseline_stats.mean_time) / \n-                          baseline_stats.mean_time * 100) if baseline_stats.mean_time > 0 else 0\n-        \n+        time_regression = (\n+            (\n+                (current_stats.mean_time - baseline_stats.mean_time)\n+                / baseline_stats.mean_time\n+                * 100\n+            )\n+            if baseline_stats.mean_time > 0\n+            else 0\n+        )\n+\n         memory_regression = 0\n         if baseline_stats.memory_stats and current_stats.memory_stats:\n             baseline_memory = baseline_stats.memory_stats.get(\"mean_mb\", 0)\n             current_memory = current_stats.memory_stats.get(\"mean_mb\", 0)\n-            memory_regression = ((current_memory - baseline_memory) / \n-                               baseline_memory * 100) if baseline_memory > 0 else 0\n-        \n+            memory_regression = (\n+                ((current_memory - baseline_memory) / baseline_memory * 100)\n+                if baseline_memory > 0\n+                else 0\n+            )\n+\n         return {\n             \"function_name\": current_stats.function_name,\n             \"time_regression_percent\": time_regression,\n             \"memory_regression_percent\": memory_regression,\n             \"baseline_time\": baseline_stats.mean_time,\n             \"current_time\": current_stats.mean_time,\n             \"baseline_memory\": baseline_stats.memory_stats.get(\"mean_mb\", 0),\n             \"current_memory\": current_stats.memory_stats.get(\"mean_mb\", 0),\n             \"improvement\": time_regression < 0,\n-            \"regression_severity\": self._calculate_regression_severity(time_regression)\n+            \"regression_severity\": self._calculate_regression_severity(time_regression),\n         }\n-    \n+\n     def _calculate_regression_severity(self, regression_percent: float) -> str:\n         \"\"\"Calculate regression severity based on percentage.\"\"\"\n         if regression_percent > 50:\n             return \"critical\"\n         elif regression_percent > 25:\n@@ -307,101 +349,122 @@\n             return \"medium\"\n         elif regression_percent > 5:\n             return \"low\"\n         else:\n             return \"negligible\"\n-    \n+\n     def detect_performance_regressions(self) -> List[PerformanceRegression]:\n         \"\"\"Detect performance regressions compared to baseline.\"\"\"\n         regressions = []\n-        \n+\n         # Group results by function name\n         function_results = {}\n         for result in self.benchmark_results:\n             if result.function_name not in function_results:\n                 function_results[result.function_name] = []\n             function_results[result.function_name].append(result)\n-        \n+\n         # Check each function for regressions\n         for function_name, results in function_results.items():\n             if function_name in self.baseline_results:\n                 baseline_stats = self.baseline_results[function_name]\n                 current_stats = self._calculate_benchmark_stats(\n                     function_name,\n                     [r.execution_time for r in results if r.success],\n                     [r.memory_usage for r in results if r.success],\n-                    [r.cpu_usage for r in results if r.success]\n-                )\n-                \n+                    [r.cpu_usage for r in results if r.success],\n+                )\n+\n                 # Check for regression\n-                time_regression = ((current_stats.mean_time - baseline_stats.mean_time) / \n-                                 baseline_stats.mean_time * 100) if baseline_stats.mean_time > 0 else 0\n-                \n+                time_regression = (\n+                    (\n+                        (current_stats.mean_time - baseline_stats.mean_time)\n+                        / baseline_stats.mean_time\n+                        * 100\n+                    )\n+                    if baseline_stats.mean_time > 0\n+                    else 0\n+                )\n+\n                 if time_regression > (self.regression_threshold * 100):\n                     regression = PerformanceRegression(\n                         function_name=function_name,\n                         baseline_time=baseline_stats.mean_time,\n                         current_time=current_stats.mean_time,\n                         regression_percent=time_regression,\n                         severity=self._calculate_regression_severity(time_regression),\n                         confidence=min(95, 70 + abs(time_regression) * 0.5),\n-                        timestamp=datetime.now()\n+                        timestamp=datetime.now(),\n                     )\n                     regressions.append(regression)\n-        \n+\n         return regressions\n-    \n+\n     def set_baseline(self, stats: BenchmarkStats) -> None:\n         \"\"\"Set baseline benchmark statistics.\"\"\"\n         self.baseline_results[stats.function_name] = stats\n-        self.logger.info(f\"Set baseline for {stats.function_name}: {stats.mean_time:.2f}ms\")\n-    \n-    def load_test(self, func: Callable, concurrent_users: int, total_requests: int,\n-                  *args, **kwargs) -> LoadTestResult:\n+        self.logger.info(\n+            f\"Set baseline for {stats.function_name}: {stats.mean_time:.2f}ms\"\n+        )\n+\n+    def load_test(\n+        self,\n+        func: Callable,\n+        concurrent_users: int,\n+        total_requests: int,\n+        *args,\n+        **kwargs,\n+    ) -> LoadTestResult:\n         \"\"\"Perform load testing with concurrent users.\"\"\"\n         test_name = f\"{func.__module__}.{func.__name__}\"\n-        self.logger.info(f\"Starting load test: {test_name} with {concurrent_users} concurrent users\")\n-        \n+        self.logger.info(\n+            f\"Starting load test: {test_name} with {concurrent_users} concurrent users\"\n+        )\n+\n         results = []\n         errors = []\n         start_time = time.time()\n-        \n+\n         # Create thread pool for concurrent execution\n-        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n+        with concurrent.futures.ThreadPoolExecutor(\n+            max_workers=concurrent_users\n+        ) as executor:\n             # Submit all requests\n             futures = []\n             for i in range(total_requests):\n                 future = executor.submit(self._execute_request, func, *args, **kwargs)\n                 futures.append(future)\n-            \n+\n             # Collect results\n             for future in concurrent.futures.as_completed(futures):\n                 try:\n                     execution_time = future.result()\n                     results.append(execution_time)\n                 except Exception as e:\n                     errors.append(str(e))\n-        \n+\n         end_time = time.time()\n         duration = end_time - start_time\n-        \n+\n         # Calculate load test metrics\n         successful_requests = len(results)\n         failed_requests = len(errors)\n-        \n+\n         if results:\n             avg_response_time = statistics.mean(results)\n             p95_response_time = self._percentile(results, 95)\n             p99_response_time = self._percentile(results, 99)\n         else:\n             avg_response_time = 0\n             p95_response_time = 0\n             p99_response_time = 0\n-        \n+\n         throughput_rps = successful_requests / duration if duration > 0 else 0\n-        error_rate = (failed_requests / total_requests * 100) if total_requests > 0 else 0\n-        \n+        error_rate = (\n+            (failed_requests / total_requests * 100) if total_requests > 0 else 0\n+        )\n+\n         return LoadTestResult(\n             test_name=test_name,\n             concurrent_users=concurrent_users,\n             total_requests=total_requests,\n             successful_requests=successful_requests,\n@@ -409,169 +472,210 @@\n             avg_response_time=avg_response_time,\n             p95_response_time=p95_response_time,\n             p99_response_time=p99_response_time,\n             throughput_rps=throughput_rps,\n             error_rate=error_rate,\n-            duration=duration\n-        )\n-    \n+            duration=duration,\n+        )\n+\n     def _execute_request(self, func: Callable, *args, **kwargs) -> float:\n         \"\"\"Execute a single request and return execution time.\"\"\"\n         start_time = time.perf_counter()\n         func(*args, **kwargs)\n         end_time = time.perf_counter()\n         return (end_time - start_time) * 1000  # Convert to milliseconds\n-    \n-    def stress_test(self, func: Callable, max_concurrent_users: int, \n-                   duration_seconds: int, *args, **kwargs) -> List[LoadTestResult]:\n+\n+    def stress_test(\n+        self,\n+        func: Callable,\n+        max_concurrent_users: int,\n+        duration_seconds: int,\n+        *args,\n+        **kwargs,\n+    ) -> List[LoadTestResult]:\n         \"\"\"Perform stress testing with increasing concurrent users.\"\"\"\n         test_name = f\"{func.__module__}.{func.__name__}\"\n-        self.logger.info(f\"Starting stress test: {test_name} for {duration_seconds} seconds\")\n-        \n+        self.logger.info(\n+            f\"Starting stress test: {test_name} for {duration_seconds} seconds\"\n+        )\n+\n         results = []\n         concurrent_users = 1\n-        \n+\n         while concurrent_users <= max_concurrent_users:\n             # Run load test for this level\n             requests_per_level = concurrent_users * 10  # 10 requests per user\n-            result = self.load_test(func, concurrent_users, requests_per_level, *args, **kwargs)\n+            result = self.load_test(\n+                func, concurrent_users, requests_per_level, *args, **kwargs\n+            )\n             results.append(result)\n-            \n+\n             # Check if we should continue\n             if result.error_rate > 50:  # More than 50% errors\n-                self.logger.warning(f\"High error rate at {concurrent_users} users: {result.error_rate:.1f}%\")\n+                self.logger.warning(\n+                    f\"High error rate at {concurrent_users} users: {result.error_rate:.1f}%\"\n+                )\n                 break\n-            \n+\n             if result.avg_response_time > 10000:  # More than 10 seconds\n-                self.logger.warning(f\"High response time at {concurrent_users} users: {result.avg_response_time:.1f}ms\")\n+                self.logger.warning(\n+                    f\"High response time at {concurrent_users} users: {result.avg_response_time:.1f}ms\"\n+                )\n                 break\n-            \n+\n             concurrent_users *= 2  # Double concurrent users\n-        \n+\n         return results\n-    \n-    def benchmark_suite(self, benchmarks: List[Tuple[Callable, tuple, dict]]) -> Dict[str, BenchmarkStats]:\n+\n+    def benchmark_suite(\n+        self, benchmarks: List[Tuple[Callable, tuple, dict]]\n+    ) -> Dict[str, BenchmarkStats]:\n         \"\"\"Run a suite of benchmarks.\"\"\"\n         results = {}\n-        \n+\n         for func, args, kwargs in benchmarks:\n             function_name = f\"{func.__module__}.{func.__name__}\"\n             self.logger.info(f\"Running benchmark: {function_name}\")\n-            \n+\n             try:\n                 stats = self.benchmark_function(func, *args, **kwargs)\n                 results[function_name] = stats\n             except Exception as e:\n                 self.logger.error(f\"Benchmark failed for {function_name}: {e}\")\n-        \n+\n         return results\n-    \n+\n     def export_benchmark_report(self, output_file: Optional[Path] = None) -> Path:\n         \"\"\"Export comprehensive benchmark report.\"\"\"\n         if output_file is None:\n             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n             output_file = Path(f\"benchmark_report_{timestamp}.json\")\n-        \n+\n         # Group results by function\n         function_results = {}\n         for result in self.benchmark_results:\n             if result.function_name not in function_results:\n                 function_results[result.function_name] = []\n             function_results[result.function_name].append(result)\n-        \n+\n         # Calculate statistics for each function\n         function_stats = {}\n         for function_name, results in function_results.items():\n             successful_results = [r for r in results if r.success]\n             if successful_results:\n                 execution_times = [r.execution_time for r in successful_results]\n                 memory_usage = [r.memory_usage for r in successful_results]\n                 cpu_usage = [r.cpu_usage for r in successful_results]\n-                \n+\n                 stats = self._calculate_benchmark_stats(\n                     function_name, execution_times, memory_usage, cpu_usage\n                 )\n                 function_stats[function_name] = asdict(stats)\n-        \n+\n         # Detect regressions\n         regressions = self.detect_performance_regressions()\n-        \n+\n         # Create report\n         report_data = {\n             \"report_metadata\": {\n                 \"generated_at\": datetime.now().isoformat(),\n                 \"total_benchmarks\": len(self.benchmark_results),\n                 \"functions_benchmarked\": len(function_stats),\n-                \"config\": self.config\n+                \"config\": self.config,\n             },\n             \"function_statistics\": function_stats,\n             \"performance_regressions\": [asdict(r) for r in regressions],\n-            \"baseline_results\": {name: asdict(stats) for name, stats in self.baseline_results.items()}\n+            \"baseline_results\": {\n+                name: asdict(stats) for name, stats in self.baseline_results.items()\n+            },\n         }\n-        \n-        with open(output_file, 'w') as f:\n+\n+        with open(output_file, \"w\") as f:\n             json.dump(report_data, f, indent=2, default=str)\n-        \n+\n         return output_file\n \n \n # Convenience functions\n def benchmark(func: Callable, *args, iterations: int = 100, **kwargs) -> BenchmarkStats:\n     \"\"\"Convenience function to benchmark a single function.\"\"\"\n     benchmarker = PerformanceBenchmark()\n     return benchmarker.benchmark_function(func, *args, iterations=iterations, **kwargs)\n \n \n-def compare_benchmarks(func1: Callable, func2: Callable, *args, iterations: int = 100, **kwargs) -> Dict[str, Any]:\n+def compare_benchmarks(\n+    func1: Callable, func2: Callable, *args, iterations: int = 100, **kwargs\n+) -> Dict[str, Any]:\n     \"\"\"Compare two functions with benchmarking.\"\"\"\n     benchmarker = PerformanceBenchmark()\n-    \n-    stats1 = benchmarker.benchmark_function(func1, *args, iterations=iterations, **kwargs)\n-    stats2 = benchmarker.benchmark_function(func2, *args, iterations=iterations, **kwargs)\n-    \n+\n+    stats1 = benchmarker.benchmark_function(\n+        func1, *args, iterations=iterations, **kwargs\n+    )\n+    stats2 = benchmarker.benchmark_function(\n+        func2, *args, iterations=iterations, **kwargs\n+    )\n+\n     return benchmarker.compare_benchmarks(stats1, stats2)\n \n \n-def load_test(func: Callable, concurrent_users: int, total_requests: int, *args, **kwargs) -> LoadTestResult:\n+def load_test(\n+    func: Callable, concurrent_users: int, total_requests: int, *args, **kwargs\n+) -> LoadTestResult:\n     \"\"\"Convenience function for load testing.\"\"\"\n     benchmarker = PerformanceBenchmark()\n-    return benchmarker.load_test(func, concurrent_users, total_requests, *args, **kwargs)\n+    return benchmarker.load_test(\n+        func, concurrent_users, total_requests, *args, **kwargs\n+    )\n \n \n # CLI interface\n if __name__ == \"__main__\":\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(description=\"SparkForge Performance Benchmarking\")\n     parser.add_argument(\"--benchmark\", help=\"Function to benchmark\")\n-    parser.add_argument(\"--iterations\", type=int, default=100, help=\"Number of iterations\")\n+    parser.add_argument(\n+        \"--iterations\", type=int, default=100, help=\"Number of iterations\"\n+    )\n     parser.add_argument(\"--load-test\", action=\"store_true\", help=\"Run load test\")\n-    parser.add_argument(\"--concurrent-users\", type=int, default=10, help=\"Concurrent users for load test\")\n-    parser.add_argument(\"--total-requests\", type=int, default=100, help=\"Total requests for load test\")\n+    parser.add_argument(\n+        \"--concurrent-users\",\n+        type=int,\n+        default=10,\n+        help=\"Concurrent users for load test\",\n+    )\n+    parser.add_argument(\n+        \"--total-requests\", type=int, default=100, help=\"Total requests for load test\"\n+    )\n     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n-    \n+\n     args = parser.parse_args()\n-    \n+\n     benchmarker = PerformanceBenchmark()\n-    \n+\n     if args.benchmark:\n         # This would need to be implemented based on the specific function\n         print(f\"Benchmarking function: {args.benchmark}\")\n         print(f\"Iterations: {args.iterations}\")\n     else:\n         # Generate general report\n         report_file = benchmarker.export_benchmark_report(args.output)\n         print(f\"Benchmark report saved to: {report_file}\")\n-        \n+\n         # Print summary\n         print(f\"\\nBenchmark Summary:\")\n         print(f\"Total benchmarks: {len(benchmarker.benchmark_results)}\")\n-        print(f\"Functions benchmarked: {len(set(r.function_name for r in benchmarker.benchmark_results))}\")\n-        \n+        print(\n+            f\"Functions benchmarked: {len(set(r.function_name for r in benchmarker.benchmark_results))}\"\n+        )\n+\n         # Show regressions\n         regressions = benchmarker.detect_performance_regressions()\n         if regressions:\n             print(f\"Performance regressions detected: {len(regressions)}\")\n             for regression in regressions:\n-                print(f\"  - {regression.function_name}: {regression.regression_percent:.1f}% regression\")\n+                print(\n+                    f\"  - {regression.function_name}: {regression.regression_percent:.1f}% regression\"\n+                )\n         else:\n             print(\"No performance regressions detected\")\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_profiler.py\t2025-09-20 16:45:07.584447+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_profiler.py\t2025-09-20 16:53:34.938701+00:00\n@@ -26,10 +26,11 @@\n \n \n @dataclass\n class PerformanceMetrics:\n     \"\"\"Performance metrics data structure.\"\"\"\n+\n     function_name: str\n     execution_time: float\n     memory_usage: float\n     cpu_usage: float\n     call_count: int\n@@ -40,10 +41,11 @@\n \n \n @dataclass\n class ProfilerReport:\n     \"\"\"Profiler report data structure.\"\"\"\n+\n     total_execution_time: float\n     total_memory_usage: float\n     total_cpu_usage: float\n     function_metrics: List[PerformanceMetrics]\n     bottlenecks: List[Dict[str, Any]]\n@@ -51,173 +53,182 @@\n     timestamp: datetime\n \n \n class PerformanceProfiler:\n     \"\"\"Comprehensive performance profiler for SparkForge.\"\"\"\n-    \n-    def __init__(self, enable_memory_tracking: bool = True, enable_cpu_tracking: bool = True):\n+\n+    def __init__(\n+        self, enable_memory_tracking: bool = True, enable_cpu_tracking: bool = True\n+    ):\n         self.enable_memory_tracking = enable_memory_tracking\n         self.enable_cpu_tracking = enable_cpu_tracking\n         self.metrics: List[PerformanceMetrics] = []\n         self.active_profiles: Dict[str, Dict[str, Any]] = {}\n         self.thread_local = threading.local()\n-        \n+\n         # Performance thresholds\n         self.thresholds = {\n-            \"slow_function_ms\": 1000,      # Functions slower than 1 second\n-            \"high_memory_mb\": 100,         # Functions using more than 100MB\n-            \"high_cpu_percent\": 80,        # Functions using more than 80% CPU\n-            \"frequent_calls\": 1000,        # Functions called more than 1000 times\n-            \"memory_leak_threshold\": 50    # Memory increase threshold in MB\n+            \"slow_function_ms\": 1000,  # Functions slower than 1 second\n+            \"high_memory_mb\": 100,  # Functions using more than 100MB\n+            \"high_cpu_percent\": 80,  # Functions using more than 80% CPU\n+            \"frequent_calls\": 1000,  # Functions called more than 1000 times\n+            \"memory_leak_threshold\": 50,  # Memory increase threshold in MB\n         }\n-        \n+\n         # Start memory tracking if enabled\n         if self.enable_memory_tracking:\n             tracemalloc.start()\n-    \n+\n     def profile_function(self, func: Callable) -> Callable:\n         \"\"\"Decorator to profile a function.\"\"\"\n+\n         @wraps(func)\n         def wrapper(*args, **kwargs):\n             function_name = f\"{func.__module__}.{func.__name__}\"\n             start_time = time.time()\n             start_memory = self._get_memory_usage()\n             start_cpu = self._get_cpu_usage()\n-            \n+\n             # Track exceptions\n             exception_count = 0\n             result = None\n-            \n+\n             try:\n                 result = func(*args, **kwargs)\n             except Exception as e:\n                 exception_count += 1\n                 raise\n             finally:\n                 end_time = time.time()\n                 end_memory = self._get_memory_usage()\n                 end_cpu = self._get_cpu_usage()\n-                \n+\n                 # Calculate metrics\n-                execution_time = (end_time - start_time) * 1000  # Convert to milliseconds\n+                execution_time = (\n+                    end_time - start_time\n+                ) * 1000  # Convert to milliseconds\n                 memory_usage = end_memory - start_memory\n                 cpu_usage = end_cpu - start_cpu\n-                \n+\n                 # Estimate argument and return sizes\n                 args_size = self._estimate_size(args) + self._estimate_size(kwargs)\n                 return_size = self._estimate_size(result) if result is not None else 0\n-                \n+\n                 # Create metrics\n                 metrics = PerformanceMetrics(\n                     function_name=function_name,\n                     execution_time=execution_time,\n                     memory_usage=memory_usage,\n                     cpu_usage=cpu_usage,\n                     call_count=1,\n                     timestamp=datetime.now(),\n                     args_size=args_size,\n                     return_size=return_size,\n-                    exception_count=exception_count\n-                )\n-                \n+                    exception_count=exception_count,\n+                )\n+\n                 self.metrics.append(metrics)\n-            \n+\n             return result\n-        \n+\n         return wrapper\n-    \n+\n     def profile_class(self, cls: type) -> type:\n         \"\"\"Decorator to profile all methods of a class.\"\"\"\n         for attr_name in dir(cls):\n             attr = getattr(cls, attr_name)\n-            if callable(attr) and not attr_name.startswith('_'):\n+            if callable(attr) and not attr_name.startswith(\"_\"):\n                 setattr(cls, attr_name, self.profile_function(attr))\n         return cls\n-    \n+\n     def start_profile(self, profile_name: str) -> None:\n         \"\"\"Start profiling with a specific name.\"\"\"\n         self.active_profiles[profile_name] = {\n             \"start_time\": time.time(),\n             \"start_memory\": self._get_memory_usage(),\n             \"start_cpu\": self._get_cpu_usage(),\n-            \"metrics_count\": len(self.metrics)\n+            \"metrics_count\": len(self.metrics),\n         }\n-    \n+\n     def end_profile(self, profile_name: str) -> Dict[str, Any]:\n         \"\"\"End profiling and return results.\"\"\"\n         if profile_name not in self.active_profiles:\n             raise ValueError(f\"Profile '{profile_name}' not found\")\n-        \n+\n         profile_data = self.active_profiles[profile_name]\n         end_time = time.time()\n         end_memory = self._get_memory_usage()\n         end_cpu = self._get_cpu_usage()\n-        \n+\n         # Calculate profile metrics\n         total_time = (end_time - profile_data[\"start_time\"]) * 1000\n         total_memory = end_memory - profile_data[\"start_memory\"]\n         total_cpu = end_cpu - profile_data[\"start_cpu\"]\n-        \n+\n         # Get metrics collected during this profile\n-        profile_metrics = self.metrics[profile_data[\"metrics_count\"]:]\n-        \n+        profile_metrics = self.metrics[profile_data[\"metrics_count\"] :]\n+\n         profile_result = {\n             \"profile_name\": profile_name,\n             \"total_execution_time\": total_time,\n             \"total_memory_usage\": total_memory,\n             \"total_cpu_usage\": total_cpu,\n             \"function_count\": len(profile_metrics),\n             \"metrics\": [asdict(m) for m in profile_metrics],\n-            \"timestamp\": datetime.now().isoformat()\n+            \"timestamp\": datetime.now().isoformat(),\n         }\n-        \n+\n         # Clean up\n         del self.active_profiles[profile_name]\n-        \n+\n         return profile_result\n-    \n-    def profile_pipeline(self, pipeline_func: Callable, *args, **kwargs) -> Tuple[Any, ProfilerReport]:\n+\n+    def profile_pipeline(\n+        self, pipeline_func: Callable, *args, **kwargs\n+    ) -> Tuple[Any, ProfilerReport]:\n         \"\"\"Profile a complete pipeline execution.\"\"\"\n         self.start_profile(\"pipeline_execution\")\n-        \n+\n         try:\n             result = pipeline_func(*args, **kwargs)\n         finally:\n             profile_result = self.end_profile(\"pipeline_execution\")\n-        \n+\n         # Generate comprehensive report\n         report = self.generate_report()\n-        \n+\n         return result, report\n-    \n-    def profile_spark_operations(self, spark_func: Callable, *args, **kwargs) -> Tuple[Any, ProfilerReport]:\n+\n+    def profile_spark_operations(\n+        self, spark_func: Callable, *args, **kwargs\n+    ) -> Tuple[Any, ProfilerReport]:\n         \"\"\"Profile Spark-specific operations.\"\"\"\n         self.start_profile(\"spark_operations\")\n-        \n+\n         try:\n             result = spark_func(*args, **kwargs)\n         finally:\n             profile_result = self.end_profile(\"spark_operations\")\n-        \n+\n         # Generate Spark-specific report\n         report = self.generate_spark_report()\n-        \n+\n         return result, report\n-    \n+\n     def generate_report(self) -> ProfilerReport:\n         \"\"\"Generate comprehensive performance report.\"\"\"\n         if not self.metrics:\n             return ProfilerReport(\n                 total_execution_time=0,\n                 total_memory_usage=0,\n                 total_cpu_usage=0,\n                 function_metrics=[],\n                 bottlenecks=[],\n                 recommendations=[],\n-                timestamp=datetime.now()\n-            )\n-        \n+                timestamp=datetime.now(),\n+            )\n+\n         # Aggregate metrics by function\n         function_aggregates = {}\n         for metric in self.metrics:\n             func_name = metric.function_name\n             if func_name not in function_aggregates:\n@@ -226,361 +237,404 @@\n                     \"memory_usage\": 0,\n                     \"cpu_usage\": 0,\n                     \"call_count\": 0,\n                     \"exception_count\": 0,\n                     \"args_size\": 0,\n-                    \"return_size\": 0\n+                    \"return_size\": 0,\n                 }\n-            \n+\n             agg = function_aggregates[func_name]\n             agg[\"execution_time\"] += metric.execution_time\n             agg[\"memory_usage\"] += metric.memory_usage\n             agg[\"cpu_usage\"] += metric.cpu_usage\n             agg[\"call_count\"] += metric.call_count\n             agg[\"exception_count\"] += metric.exception_count\n             agg[\"args_size\"] += metric.args_size\n             agg[\"return_size\"] += metric.return_size\n-        \n+\n         # Create aggregated metrics\n         aggregated_metrics = []\n         for func_name, agg in function_aggregates.items():\n-            aggregated_metrics.append(PerformanceMetrics(\n-                function_name=func_name,\n-                execution_time=agg[\"execution_time\"],\n-                memory_usage=agg[\"memory_usage\"],\n-                cpu_usage=agg[\"cpu_usage\"],\n-                call_count=agg[\"call_count\"],\n-                timestamp=datetime.now(),\n-                args_size=agg[\"args_size\"],\n-                return_size=agg[\"return_size\"],\n-                exception_count=agg[\"exception_count\"]\n-            ))\n-        \n+            aggregated_metrics.append(\n+                PerformanceMetrics(\n+                    function_name=func_name,\n+                    execution_time=agg[\"execution_time\"],\n+                    memory_usage=agg[\"memory_usage\"],\n+                    cpu_usage=agg[\"cpu_usage\"],\n+                    call_count=agg[\"call_count\"],\n+                    timestamp=datetime.now(),\n+                    args_size=agg[\"args_size\"],\n+                    return_size=agg[\"return_size\"],\n+                    exception_count=agg[\"exception_count\"],\n+                )\n+            )\n+\n         # Sort by execution time (descending)\n         aggregated_metrics.sort(key=lambda x: x.execution_time, reverse=True)\n-        \n+\n         # Calculate totals\n         total_execution_time = sum(m.execution_time for m in aggregated_metrics)\n         total_memory_usage = sum(m.memory_usage for m in aggregated_metrics)\n         total_cpu_usage = sum(m.cpu_usage for m in aggregated_metrics)\n-        \n+\n         # Identify bottlenecks\n         bottlenecks = self._identify_bottlenecks(aggregated_metrics)\n-        \n+\n         # Generate recommendations\n-        recommendations = self._generate_recommendations(aggregated_metrics, bottlenecks)\n-        \n+        recommendations = self._generate_recommendations(\n+            aggregated_metrics, bottlenecks\n+        )\n+\n         return ProfilerReport(\n             total_execution_time=total_execution_time,\n             total_memory_usage=total_memory_usage,\n             total_cpu_usage=total_cpu_usage,\n             function_metrics=aggregated_metrics,\n             bottlenecks=bottlenecks,\n             recommendations=recommendations,\n-            timestamp=datetime.now()\n-        )\n-    \n+            timestamp=datetime.now(),\n+        )\n+\n     def generate_spark_report(self) -> ProfilerReport:\n         \"\"\"Generate Spark-specific performance report.\"\"\"\n         report = self.generate_report()\n-        \n+\n         # Add Spark-specific analysis\n-        spark_metrics = [m for m in report.function_metrics \n-                        if any(spark_indicator in m.function_name.lower() \n-                              for spark_indicator in ['spark', 'dataframe', 'rdd', 'sql'])]\n-        \n+        spark_metrics = [\n+            m\n+            for m in report.function_metrics\n+            if any(\n+                spark_indicator in m.function_name.lower()\n+                for spark_indicator in [\"spark\", \"dataframe\", \"rdd\", \"sql\"]\n+            )\n+        ]\n+\n         # Add Spark-specific recommendations\n         spark_recommendations = []\n-        \n+\n         # Check for DataFrame operations\n-        df_ops = [m for m in spark_metrics if 'dataframe' in m.function_name.lower()]\n+        df_ops = [m for m in spark_metrics if \"dataframe\" in m.function_name.lower()]\n         if df_ops:\n             total_df_time = sum(m.execution_time for m in df_ops)\n             if total_df_time > self.thresholds[\"slow_function_ms\"] * 2:\n                 spark_recommendations.append(\n                     \"Consider optimizing DataFrame operations - they are taking significant time\"\n                 )\n-        \n+\n         # Check for SQL operations\n-        sql_ops = [m for m in spark_metrics if 'sql' in m.function_name.lower()]\n+        sql_ops = [m for m in spark_metrics if \"sql\" in m.function_name.lower()]\n         if sql_ops:\n             spark_recommendations.append(\n                 \"Review SQL queries for optimization opportunities\"\n             )\n-        \n+\n         # Check for caching opportunities\n         frequent_ops = [m for m in spark_metrics if m.call_count > 10]\n         if frequent_ops:\n             spark_recommendations.append(\n                 \"Consider caching frequently accessed DataFrames or RDDs\"\n             )\n-        \n+\n         # Update recommendations\n         report.recommendations.extend(spark_recommendations)\n-        \n+\n         return report\n-    \n-    def _identify_bottlenecks(self, metrics: List[PerformanceMetrics]) -> List[Dict[str, Any]]:\n+\n+    def _identify_bottlenecks(\n+        self, metrics: List[PerformanceMetrics]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Identify performance bottlenecks.\"\"\"\n         bottlenecks = []\n-        \n+\n         for metric in metrics:\n             bottleneck_reasons = []\n-            \n+\n             # Check execution time\n             if metric.execution_time > self.thresholds[\"slow_function_ms\"]:\n-                bottleneck_reasons.append(f\"Slow execution: {metric.execution_time:.2f}ms\")\n-            \n+                bottleneck_reasons.append(\n+                    f\"Slow execution: {metric.execution_time:.2f}ms\"\n+                )\n+\n             # Check memory usage\n             if metric.memory_usage > self.thresholds[\"high_memory_mb\"]:\n-                bottleneck_reasons.append(f\"High memory usage: {metric.memory_usage:.2f}MB\")\n-            \n+                bottleneck_reasons.append(\n+                    f\"High memory usage: {metric.memory_usage:.2f}MB\"\n+                )\n+\n             # Check CPU usage\n             if metric.cpu_usage > self.thresholds[\"high_cpu_percent\"]:\n                 bottleneck_reasons.append(f\"High CPU usage: {metric.cpu_usage:.2f}%\")\n-            \n+\n             # Check call frequency\n             if metric.call_count > self.thresholds[\"frequent_calls\"]:\n-                bottleneck_reasons.append(f\"Frequently called: {metric.call_count} times\")\n-            \n+                bottleneck_reasons.append(\n+                    f\"Frequently called: {metric.call_count} times\"\n+                )\n+\n             # Check for memory leaks (simplified)\n             if metric.memory_usage > self.thresholds[\"memory_leak_threshold\"]:\n-                bottleneck_reasons.append(f\"Potential memory leak: {metric.memory_usage:.2f}MB\")\n-            \n+                bottleneck_reasons.append(\n+                    f\"Potential memory leak: {metric.memory_usage:.2f}MB\"\n+                )\n+\n             if bottleneck_reasons:\n-                bottlenecks.append({\n-                    \"function\": metric.function_name,\n-                    \"reasons\": bottleneck_reasons,\n-                    \"severity\": self._calculate_bottleneck_severity(metric),\n-                    \"impact_score\": self._calculate_impact_score(metric)\n-                })\n-        \n+                bottlenecks.append(\n+                    {\n+                        \"function\": metric.function_name,\n+                        \"reasons\": bottleneck_reasons,\n+                        \"severity\": self._calculate_bottleneck_severity(metric),\n+                        \"impact_score\": self._calculate_impact_score(metric),\n+                    }\n+                )\n+\n         # Sort by impact score\n         bottlenecks.sort(key=lambda x: x[\"impact_score\"], reverse=True)\n-        \n+\n         return bottlenecks\n-    \n+\n     def _calculate_bottleneck_severity(self, metric: PerformanceMetrics) -> str:\n         \"\"\"Calculate bottleneck severity.\"\"\"\n         score = 0\n-        \n+\n         if metric.execution_time > self.thresholds[\"slow_function_ms\"] * 2:\n             score += 3\n         elif metric.execution_time > self.thresholds[\"slow_function_ms\"]:\n             score += 2\n-        \n+\n         if metric.memory_usage > self.thresholds[\"high_memory_mb\"] * 2:\n             score += 3\n         elif metric.memory_usage > self.thresholds[\"high_memory_mb\"]:\n             score += 2\n-        \n+\n         if metric.cpu_usage > self.thresholds[\"high_cpu_percent\"] * 1.5:\n             score += 3\n         elif metric.cpu_usage > self.thresholds[\"high_cpu_percent\"]:\n             score += 2\n-        \n+\n         if score >= 6:\n             return \"critical\"\n         elif score >= 4:\n             return \"high\"\n         elif score >= 2:\n             return \"medium\"\n         else:\n             return \"low\"\n-    \n+\n     def _calculate_impact_score(self, metric: PerformanceMetrics) -> float:\n         \"\"\"Calculate impact score for prioritization.\"\"\"\n         # Weight different factors\n         time_weight = 0.4\n         memory_weight = 0.3\n         cpu_weight = 0.2\n         frequency_weight = 0.1\n-        \n+\n         # Normalize values (simplified)\n-        time_score = min(metric.execution_time / (self.thresholds[\"slow_function_ms\"] * 2), 1.0)\n-        memory_score = min(metric.memory_usage / (self.thresholds[\"high_memory_mb\"] * 2), 1.0)\n-        cpu_score = min(metric.cpu_usage / (self.thresholds[\"high_cpu_percent\"] * 1.5), 1.0)\n-        frequency_score = min(metric.call_count / (self.thresholds[\"frequent_calls\"] * 2), 1.0)\n-        \n-        return (time_score * time_weight + \n-                memory_score * memory_weight + \n-                cpu_score * cpu_weight + \n-                frequency_score * frequency_weight)\n-    \n-    def _generate_recommendations(self, metrics: List[PerformanceMetrics], \n-                                bottlenecks: List[Dict[str, Any]]) -> List[str]:\n+        time_score = min(\n+            metric.execution_time / (self.thresholds[\"slow_function_ms\"] * 2), 1.0\n+        )\n+        memory_score = min(\n+            metric.memory_usage / (self.thresholds[\"high_memory_mb\"] * 2), 1.0\n+        )\n+        cpu_score = min(\n+            metric.cpu_usage / (self.thresholds[\"high_cpu_percent\"] * 1.5), 1.0\n+        )\n+        frequency_score = min(\n+            metric.call_count / (self.thresholds[\"frequent_calls\"] * 2), 1.0\n+        )\n+\n+        return (\n+            time_score * time_weight\n+            + memory_score * memory_weight\n+            + cpu_score * cpu_weight\n+            + frequency_score * frequency_weight\n+        )\n+\n+    def _generate_recommendations(\n+        self, metrics: List[PerformanceMetrics], bottlenecks: List[Dict[str, Any]]\n+    ) -> List[str]:\n         \"\"\"Generate performance optimization recommendations.\"\"\"\n         recommendations = []\n-        \n+\n         # General recommendations based on bottlenecks\n         if bottlenecks:\n-            critical_bottlenecks = [b for b in bottlenecks if b[\"severity\"] == \"critical\"]\n+            critical_bottlenecks = [\n+                b for b in bottlenecks if b[\"severity\"] == \"critical\"\n+            ]\n             if critical_bottlenecks:\n                 recommendations.append(\n                     f\"Address {len(critical_bottlenecks)} critical performance bottlenecks immediately\"\n                 )\n-            \n+\n             high_bottlenecks = [b for b in bottlenecks if b[\"severity\"] == \"high\"]\n             if high_bottlenecks:\n                 recommendations.append(\n                     f\"Review and optimize {len(high_bottlenecks)} high-priority bottlenecks\"\n                 )\n-        \n+\n         # Specific recommendations\n-        slow_functions = [m for m in metrics if m.execution_time > self.thresholds[\"slow_function_ms\"]]\n+        slow_functions = [\n+            m for m in metrics if m.execution_time > self.thresholds[\"slow_function_ms\"]\n+        ]\n         if slow_functions:\n             recommendations.append(\n                 f\"Optimize {len(slow_functions)} slow functions (>{self.thresholds['slow_function_ms']}ms)\"\n             )\n-        \n-        memory_intensive = [m for m in metrics if m.memory_usage > self.thresholds[\"high_memory_mb\"]]\n+\n+        memory_intensive = [\n+            m for m in metrics if m.memory_usage > self.thresholds[\"high_memory_mb\"]\n+        ]\n         if memory_intensive:\n             recommendations.append(\n                 f\"Optimize memory usage in {len(memory_intensive)} memory-intensive functions\"\n             )\n-        \n-        frequent_calls = [m for m in metrics if m.call_count > self.thresholds[\"frequent_calls\"]]\n+\n+        frequent_calls = [\n+            m for m in metrics if m.call_count > self.thresholds[\"frequent_calls\"]\n+        ]\n         if frequent_calls:\n             recommendations.append(\n                 f\"Consider caching or optimization for {len(frequent_calls)} frequently called functions\"\n             )\n-        \n+\n         # Top function recommendation\n         if metrics:\n             top_function = metrics[0]\n             recommendations.append(\n                 f\"Focus optimization efforts on '{top_function.function_name}' \"\n                 f\"(takes {top_function.execution_time:.2f}ms, {top_function.call_count} calls)\"\n             )\n-        \n+\n         # Add general recommendations\n-        recommendations.extend([\n-            \"Consider implementing function-level caching for frequently called functions\",\n-            \"Review data structures and algorithms for optimization opportunities\",\n-            \"Profile memory usage patterns to identify potential leaks\",\n-            \"Consider parallelization for CPU-intensive operations\",\n-            \"Implement lazy evaluation where appropriate\"\n-        ])\n-        \n+        recommendations.extend(\n+            [\n+                \"Consider implementing function-level caching for frequently called functions\",\n+                \"Review data structures and algorithms for optimization opportunities\",\n+                \"Profile memory usage patterns to identify potential leaks\",\n+                \"Consider parallelization for CPU-intensive operations\",\n+                \"Implement lazy evaluation where appropriate\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n+\n     def _get_memory_usage(self) -> float:\n         \"\"\"Get current memory usage in MB.\"\"\"\n         if not self.enable_memory_tracking:\n             return 0.0\n-        \n+\n         try:\n             process = psutil.Process()\n             return process.memory_info().rss / 1024 / 1024  # Convert to MB\n         except Exception:\n             return 0.0\n-    \n+\n     def _get_cpu_usage(self) -> float:\n         \"\"\"Get current CPU usage percentage.\"\"\"\n         if not self.enable_cpu_tracking:\n             return 0.0\n-        \n+\n         try:\n             process = psutil.Process()\n             return process.cpu_percent()\n         except Exception:\n             return 0.0\n-    \n+\n     def _estimate_size(self, obj: Any) -> int:\n         \"\"\"Estimate the size of an object in bytes.\"\"\"\n         try:\n             import sys\n+\n             return sys.getsizeof(obj)\n         except Exception:\n             return 0\n-    \n+\n     def get_memory_snapshot(self) -> Dict[str, Any]:\n         \"\"\"Get detailed memory snapshot.\"\"\"\n         if not self.enable_memory_tracking:\n             return {}\n-        \n+\n         try:\n             snapshot = tracemalloc.take_snapshot()\n-            top_stats = snapshot.statistics('lineno')\n-            \n+            top_stats = snapshot.statistics(\"lineno\")\n+\n             return {\n                 \"current_memory_mb\": self._get_memory_usage(),\n                 \"peak_memory_mb\": tracemalloc.get_traced_memory()[1] / 1024 / 1024,\n                 \"top_memory_usage\": [\n                     {\n                         \"filename\": stat.traceback.format()[0],\n                         \"size_mb\": stat.size / 1024 / 1024,\n-                        \"count\": stat.count\n+                        \"count\": stat.count,\n                     }\n                     for stat in top_stats[:10]\n-                ]\n+                ],\n             }\n         except Exception as e:\n             return {\"error\": str(e)}\n-    \n+\n     def reset_metrics(self) -> None:\n         \"\"\"Reset all performance metrics.\"\"\"\n         self.metrics.clear()\n         self.active_profiles.clear()\n-        \n+\n         # Restart memory tracking if enabled\n         if self.enable_memory_tracking:\n             tracemalloc.stop()\n             tracemalloc.start()\n-    \n+\n     def export_report(self, output_file: Optional[Path] = None) -> Path:\n         \"\"\"Export performance report to file.\"\"\"\n         report = self.generate_report()\n-        \n+\n         if output_file is None:\n             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n             output_file = Path(f\"performance_report_{timestamp}.json\")\n-        \n+\n         # Convert to serializable format\n         report_data = {\n             \"report_metadata\": {\n                 \"generated_at\": report.timestamp.isoformat(),\n                 \"total_functions_profiled\": len(report.function_metrics),\n                 \"profiler_settings\": {\n                     \"memory_tracking\": self.enable_memory_tracking,\n                     \"cpu_tracking\": self.enable_cpu_tracking,\n-                    \"thresholds\": self.thresholds\n-                }\n+                    \"thresholds\": self.thresholds,\n+                },\n             },\n             \"summary\": {\n                 \"total_execution_time_ms\": report.total_execution_time,\n                 \"total_memory_usage_mb\": report.total_memory_usage,\n                 \"total_cpu_usage_percent\": report.total_cpu_usage,\n                 \"bottlenecks_count\": len(report.bottlenecks),\n-                \"recommendations_count\": len(report.recommendations)\n+                \"recommendations_count\": len(report.recommendations),\n             },\n             \"function_metrics\": [asdict(m) for m in report.function_metrics],\n             \"bottlenecks\": report.bottlenecks,\n             \"recommendations\": report.recommendations,\n-            \"memory_snapshot\": self.get_memory_snapshot()\n+            \"memory_snapshot\": self.get_memory_snapshot(),\n         }\n-        \n-        with open(output_file, 'w') as f:\n+\n+        with open(output_file, \"w\") as f:\n             json.dump(report_data, f, indent=2, default=str)\n-        \n+\n         return output_file\n-    \n+\n     def profile_with_cprofile(self, func: Callable, *args, **kwargs) -> Tuple[Any, str]:\n         \"\"\"Profile function using cProfile for detailed analysis.\"\"\"\n         profiler = cProfile.Profile()\n-        \n+\n         try:\n             result = profiler.runcall(func, *args, **kwargs)\n         finally:\n             profiler.disable()\n-        \n+\n         # Generate stats\n         s = io.StringIO()\n         ps = pstats.Stats(profiler, stream=s)\n-        ps.sort_stats('cumulative')\n+        ps.sort_stats(\"cumulative\")\n         ps.print_stats(20)  # Top 20 functions\n-        \n+\n         return result, s.getvalue()\n \n \n # Convenience functions for easy profiling\n def profile_function(func: Callable) -> Callable:\n@@ -602,24 +656,28 @@\n \n \n # CLI interface\n if __name__ == \"__main__\":\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(description=\"SparkForge Performance Profiler\")\n     parser.add_argument(\"--function\", help=\"Function to profile\")\n     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n-    parser.add_argument(\"--memory-tracking\", action=\"store_true\", help=\"Enable memory tracking\")\n-    parser.add_argument(\"--cpu-tracking\", action=\"store_true\", help=\"Enable CPU tracking\")\n-    \n+    parser.add_argument(\n+        \"--memory-tracking\", action=\"store_true\", help=\"Enable memory tracking\"\n+    )\n+    parser.add_argument(\n+        \"--cpu-tracking\", action=\"store_true\", help=\"Enable CPU tracking\"\n+    )\n+\n     args = parser.parse_args()\n-    \n+\n     profiler = PerformanceProfiler(\n         enable_memory_tracking=args.memory_tracking,\n-        enable_cpu_tracking=args.cpu_tracking\n+        enable_cpu_tracking=args.cpu_tracking,\n     )\n-    \n+\n     if args.function:\n         # Profile specific function\n         try:\n             # This would need to be implemented based on the specific function\n             print(f\"Profiling function: {args.function}\")\n@@ -628,11 +686,11 @@\n     else:\n         # Generate general report\n         report = profiler.generate_report()\n         report_file = profiler.export_report(args.output)\n         print(f\"Performance report saved to: {report_file}\")\n-        \n+\n         # Print summary\n         print(f\"\\nPerformance Summary:\")\n         print(f\"Total execution time: {report.total_execution_time:.2f}ms\")\n         print(f\"Total memory usage: {report.total_memory_usage:.2f}MB\")\n         print(f\"Total CPU usage: {report.total_cpu_usage:.2f}%\")\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitoring.py\t2025-09-20 16:45:07.584183+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitoring.py\t2025-09-20 16:53:34.946172+00:00\n@@ -23,24 +23,26 @@\n \n \n @dataclass\n class PerformanceMetric:\n     \"\"\"Performance metric data structure.\"\"\"\n+\n     name: str\n     value: float\n     unit: str\n     timestamp: datetime\n     tags: Dict[str, str] = None\n-    \n+\n     def __post_init__(self):\n         if self.tags is None:\n             self.tags = {}\n \n \n @dataclass\n class ResourceUsage:\n     \"\"\"Resource usage data structure.\"\"\"\n+\n     cpu_percent: float\n     memory_percent: float\n     memory_used_mb: float\n     memory_available_mb: float\n     disk_usage_percent: float\n@@ -50,10 +52,11 @@\n \n \n @dataclass\n class PerformanceAlert:\n     \"\"\"Performance alert data structure.\"\"\"\n+\n     alert_id: str\n     metric_name: str\n     current_value: float\n     threshold_value: float\n     severity: str\n@@ -63,10 +66,11 @@\n \n \n @dataclass\n class PerformanceReport:\n     \"\"\"Performance report data structure.\"\"\"\n+\n     report_period: timedelta\n     start_time: datetime\n     end_time: datetime\n     metrics_summary: Dict[str, Dict[str, float]]\n     resource_usage_summary: Dict[str, float]\n@@ -74,517 +78,577 @@\n     recommendations: List[str]\n \n \n class PerformanceMonitor:\n     \"\"\"Real-time performance monitoring system.\"\"\"\n-    \n+\n     def __init__(self, config: Optional[Dict[str, Any]] = None):\n         self.config = config or self._default_config()\n         self.metrics: Dict[str, deque] = {}\n         self.resource_history: deque = deque(maxlen=1000)\n         self.alerts: List[PerformanceAlert] = []\n         self.alert_callbacks: List[Callable[[PerformanceAlert], None]] = []\n-        \n+\n         # Monitoring state\n         self.monitoring_active = False\n         self.monitor_thread = None\n         self.last_network_stats = None\n-        \n+\n         # Performance thresholds\n         self.thresholds = {\n             \"cpu_percent\": 80.0,\n             \"memory_percent\": 85.0,\n             \"memory_used_mb\": 2048.0,\n             \"disk_usage_percent\": 90.0,\n             \"response_time_ms\": 5000.0,\n-            \"throughput_rps\": 10.0\n+            \"throughput_rps\": 10.0,\n         }\n-        \n+\n         # Setup logging\n         self.logger = logging.getLogger(\"performance_monitor\")\n         self.logger.setLevel(logging.INFO)\n-    \n+\n     def _default_config(self) -> Dict[str, Any]:\n         \"\"\"Get default configuration.\"\"\"\n         return {\n             \"monitoring_interval\": 10,  # seconds\n             \"metrics_retention\": 1000,  # number of metrics to keep\n             \"resource_retention\": 1000,  # number of resource snapshots to keep\n             \"alert_retention_days\": 7,\n             \"enable_alerts\": True,\n             \"enable_resource_monitoring\": True,\n-            \"enable_metrics_collection\": True\n+            \"enable_metrics_collection\": True,\n         }\n-    \n+\n     def start_monitoring(self) -> None:\n         \"\"\"Start performance monitoring.\"\"\"\n         if self.monitoring_active:\n             self.logger.warning(\"Performance monitoring is already active\")\n             return\n-        \n+\n         self.monitoring_active = True\n-        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n+        self.monitor_thread = threading.Thread(\n+            target=self._monitoring_loop, daemon=True\n+        )\n         self.monitor_thread.start()\n-        \n+\n         self.logger.info(\"Performance monitoring started\")\n-    \n+\n     def stop_monitoring(self) -> None:\n         \"\"\"Stop performance monitoring.\"\"\"\n         if not self.monitoring_active:\n             self.logger.warning(\"Performance monitoring is not active\")\n             return\n-        \n+\n         self.monitoring_active = False\n-        \n+\n         if self.monitor_thread and self.monitor_thread.is_alive():\n             self.monitor_thread.join(timeout=5)\n-        \n+\n         self.logger.info(\"Performance monitoring stopped\")\n-    \n+\n     def _monitoring_loop(self) -> None:\n         \"\"\"Main monitoring loop.\"\"\"\n         while self.monitoring_active:\n             try:\n                 # Collect resource usage\n                 if self.config.get(\"enable_resource_monitoring\", True):\n                     resource_usage = self._collect_resource_usage()\n                     self.resource_history.append(resource_usage)\n-                    \n+\n                     # Check for resource alerts\n                     if self.config.get(\"enable_alerts\", True):\n                         self._check_resource_alerts(resource_usage)\n-                \n+\n                 # Clean up old data\n                 self._cleanup_old_data()\n-                \n+\n                 # Sleep until next monitoring cycle\n                 time.sleep(self.config.get(\"monitoring_interval\", 10))\n-                \n+\n             except Exception as e:\n                 self.logger.error(f\"Error in monitoring loop: {e}\")\n                 time.sleep(5)  # Brief pause before retrying\n-    \n+\n     def _collect_resource_usage(self) -> ResourceUsage:\n         \"\"\"Collect current resource usage.\"\"\"\n         try:\n             # CPU usage\n             cpu_percent = psutil.cpu_percent(interval=1)\n-            \n+\n             # Memory usage\n             memory = psutil.virtual_memory()\n             memory_percent = memory.percent\n             memory_used_mb = memory.used / 1024 / 1024\n             memory_available_mb = memory.available / 1024 / 1024\n-            \n+\n             # Disk usage\n-            disk = psutil.disk_usage('/')\n+            disk = psutil.disk_usage(\"/\")\n             disk_usage_percent = (disk.used / disk.total) * 100\n-            \n+\n             # Network usage\n             network_stats = psutil.net_io_counters()\n             if self.last_network_stats:\n-                network_bytes_sent = network_stats.bytes_sent - self.last_network_stats.bytes_sent\n-                network_bytes_received = network_stats.bytes_recv - self.last_network_stats.bytes_recv\n+                network_bytes_sent = (\n+                    network_stats.bytes_sent - self.last_network_stats.bytes_sent\n+                )\n+                network_bytes_received = (\n+                    network_stats.bytes_recv - self.last_network_stats.bytes_recv\n+                )\n             else:\n                 network_bytes_sent = 0\n                 network_bytes_received = 0\n-            \n+\n             self.last_network_stats = network_stats\n-            \n+\n             return ResourceUsage(\n                 cpu_percent=cpu_percent,\n                 memory_percent=memory_percent,\n                 memory_used_mb=memory_used_mb,\n                 memory_available_mb=memory_available_mb,\n                 disk_usage_percent=disk_usage_percent,\n                 network_bytes_sent=network_bytes_sent,\n                 network_bytes_received=network_bytes_received,\n-                timestamp=datetime.now()\n-            )\n-            \n+                timestamp=datetime.now(),\n+            )\n+\n         except Exception as e:\n             self.logger.error(f\"Error collecting resource usage: {e}\")\n             return ResourceUsage(\n                 cpu_percent=0.0,\n                 memory_percent=0.0,\n                 memory_used_mb=0.0,\n                 memory_available_mb=0.0,\n                 disk_usage_percent=0.0,\n                 network_bytes_sent=0,\n                 network_bytes_received=0,\n-                timestamp=datetime.now()\n-            )\n-    \n+                timestamp=datetime.now(),\n+            )\n+\n     def _check_resource_alerts(self, resource_usage: ResourceUsage) -> None:\n         \"\"\"Check for resource-based alerts.\"\"\"\n         alerts_to_create = []\n-        \n+\n         # CPU alert\n         if resource_usage.cpu_percent > self.thresholds[\"cpu_percent\"]:\n-            alerts_to_create.append(PerformanceAlert(\n-                alert_id=f\"cpu_high_{int(time.time())}\",\n-                metric_name=\"cpu_percent\",\n-                current_value=resource_usage.cpu_percent,\n-                threshold_value=self.thresholds[\"cpu_percent\"],\n-                severity=\"warning\",\n-                message=f\"High CPU usage: {resource_usage.cpu_percent:.1f}%\",\n-                timestamp=datetime.now()\n-            ))\n-        \n+            alerts_to_create.append(\n+                PerformanceAlert(\n+                    alert_id=f\"cpu_high_{int(time.time())}\",\n+                    metric_name=\"cpu_percent\",\n+                    current_value=resource_usage.cpu_percent,\n+                    threshold_value=self.thresholds[\"cpu_percent\"],\n+                    severity=\"warning\",\n+                    message=f\"High CPU usage: {resource_usage.cpu_percent:.1f}%\",\n+                    timestamp=datetime.now(),\n+                )\n+            )\n+\n         # Memory alert\n         if resource_usage.memory_percent > self.thresholds[\"memory_percent\"]:\n-            alerts_to_create.append(PerformanceAlert(\n-                alert_id=f\"memory_high_{int(time.time())}\",\n-                metric_name=\"memory_percent\",\n-                current_value=resource_usage.memory_percent,\n-                threshold_value=self.thresholds[\"memory_percent\"],\n-                severity=\"warning\",\n-                message=f\"High memory usage: {resource_usage.memory_percent:.1f}%\",\n-                timestamp=datetime.now()\n-            ))\n-        \n+            alerts_to_create.append(\n+                PerformanceAlert(\n+                    alert_id=f\"memory_high_{int(time.time())}\",\n+                    metric_name=\"memory_percent\",\n+                    current_value=resource_usage.memory_percent,\n+                    threshold_value=self.thresholds[\"memory_percent\"],\n+                    severity=\"warning\",\n+                    message=f\"High memory usage: {resource_usage.memory_percent:.1f}%\",\n+                    timestamp=datetime.now(),\n+                )\n+            )\n+\n         # Disk alert\n         if resource_usage.disk_usage_percent > self.thresholds[\"disk_usage_percent\"]:\n-            alerts_to_create.append(PerformanceAlert(\n-                alert_id=f\"disk_high_{int(time.time())}\",\n-                metric_name=\"disk_usage_percent\",\n-                current_value=resource_usage.disk_usage_percent,\n-                threshold_value=self.thresholds[\"disk_usage_percent\"],\n-                severity=\"critical\",\n-                message=f\"High disk usage: {resource_usage.disk_usage_percent:.1f}%\",\n-                timestamp=datetime.now()\n-            ))\n-        \n+            alerts_to_create.append(\n+                PerformanceAlert(\n+                    alert_id=f\"disk_high_{int(time.time())}\",\n+                    metric_name=\"disk_usage_percent\",\n+                    current_value=resource_usage.disk_usage_percent,\n+                    threshold_value=self.thresholds[\"disk_usage_percent\"],\n+                    severity=\"critical\",\n+                    message=f\"High disk usage: {resource_usage.disk_usage_percent:.1f}%\",\n+                    timestamp=datetime.now(),\n+                )\n+            )\n+\n         # Create alerts\n         for alert in alerts_to_create:\n             self._create_alert(alert)\n-    \n+\n     def _create_alert(self, alert: PerformanceAlert) -> None:\n         \"\"\"Create a performance alert.\"\"\"\n         # Check if similar alert already exists\n-        existing_alerts = [a for a in self.alerts \n-                          if a.metric_name == alert.metric_name and \n-                          not a.acknowledged and\n-                          (datetime.now() - a.timestamp).total_seconds() < 300]  # 5 minutes\n-        \n+        existing_alerts = [\n+            a\n+            for a in self.alerts\n+            if a.metric_name == alert.metric_name\n+            and not a.acknowledged\n+            and (datetime.now() - a.timestamp).total_seconds() < 300\n+        ]  # 5 minutes\n+\n         if not existing_alerts:\n             self.alerts.append(alert)\n             self.logger.warning(f\"Performance Alert: {alert.message}\")\n-            \n+\n             # Call alert callbacks\n             for callback in self.alert_callbacks:\n                 try:\n                     callback(alert)\n                 except Exception as e:\n                     self.logger.error(f\"Error in alert callback: {e}\")\n-    \n-    def record_metric(self, name: str, value: float, unit: str = \"\", \n-                     tags: Optional[Dict[str, str]] = None) -> None:\n+\n+    def record_metric(\n+        self,\n+        name: str,\n+        value: float,\n+        unit: str = \"\",\n+        tags: Optional[Dict[str, str]] = None,\n+    ) -> None:\n         \"\"\"Record a performance metric.\"\"\"\n         if not self.config.get(\"enable_metrics_collection\", True):\n             return\n-        \n+\n         # Initialize metric queue if not exists\n         if name not in self.metrics:\n             maxlen = self.config.get(\"metrics_retention\", 1000)\n             self.metrics[name] = deque(maxlen=maxlen)\n-        \n+\n         # Create metric\n         metric = PerformanceMetric(\n-            name=name,\n-            value=value,\n-            unit=unit,\n-            timestamp=datetime.now(),\n-            tags=tags or {}\n+            name=name, value=value, unit=unit, timestamp=datetime.now(), tags=tags or {}\n         )\n-        \n+\n         # Add to queue\n         self.metrics[name].append(metric)\n-        \n+\n         # Check for metric-based alerts\n         self._check_metric_alerts(metric)\n-    \n+\n     def _check_metric_alerts(self, metric: PerformanceMetric) -> None:\n         \"\"\"Check for metric-based alerts.\"\"\"\n         if not self.config.get(\"enable_alerts\", True):\n             return\n-        \n+\n         # Check response time\n-        if metric.name == \"response_time\" and metric.value > self.thresholds[\"response_time_ms\"]:\n+        if (\n+            metric.name == \"response_time\"\n+            and metric.value > self.thresholds[\"response_time_ms\"]\n+        ):\n             alert = PerformanceAlert(\n                 alert_id=f\"response_time_{int(time.time())}\",\n                 metric_name=\"response_time\",\n                 current_value=metric.value,\n                 threshold_value=self.thresholds[\"response_time_ms\"],\n                 severity=\"warning\",\n                 message=f\"Slow response time: {metric.value:.1f}ms\",\n-                timestamp=datetime.now()\n+                timestamp=datetime.now(),\n             )\n             self._create_alert(alert)\n-        \n+\n         # Check throughput\n-        if metric.name == \"throughput\" and metric.value < self.thresholds[\"throughput_rps\"]:\n+        if (\n+            metric.name == \"throughput\"\n+            and metric.value < self.thresholds[\"throughput_rps\"]\n+        ):\n             alert = PerformanceAlert(\n                 alert_id=f\"throughput_{int(time.time())}\",\n                 metric_name=\"throughput\",\n                 current_value=metric.value,\n                 threshold_value=self.thresholds[\"throughput_rps\"],\n                 severity=\"warning\",\n                 message=f\"Low throughput: {metric.value:.1f} requests/second\",\n-                timestamp=datetime.now()\n+                timestamp=datetime.now(),\n             )\n             self._create_alert(alert)\n-    \n+\n     def get_metric_stats(self, name: str, window_minutes: int = 60) -> Dict[str, float]:\n         \"\"\"Get statistics for a specific metric.\"\"\"\n         if name not in self.metrics:\n             return {}\n-        \n+\n         # Filter metrics by time window\n         cutoff_time = datetime.now() - timedelta(minutes=window_minutes)\n-        recent_metrics = [\n-            m for m in self.metrics[name]\n-            if m.timestamp > cutoff_time\n-        ]\n-        \n+        recent_metrics = [m for m in self.metrics[name] if m.timestamp > cutoff_time]\n+\n         if not recent_metrics:\n             return {}\n-        \n+\n         values = [m.value for m in recent_metrics]\n-        \n+\n         return {\n             \"count\": len(values),\n             \"min\": min(values),\n             \"max\": max(values),\n             \"mean\": statistics.mean(values),\n             \"median\": statistics.median(values),\n             \"stdev\": statistics.stdev(values) if len(values) > 1 else 0.0,\n             \"p95\": self._percentile(values, 95),\n-            \"p99\": self._percentile(values, 99)\n+            \"p99\": self._percentile(values, 99),\n         }\n-    \n+\n     def _percentile(self, values: List[float], percentile: int) -> float:\n         \"\"\"Calculate percentile of values.\"\"\"\n         if not values:\n             return 0.0\n-        \n+\n         sorted_values = sorted(values)\n         index = (percentile / 100.0) * (len(sorted_values) - 1)\n-        \n+\n         if index.is_integer():\n             return sorted_values[int(index)]\n         else:\n             lower = sorted_values[int(index)]\n             upper = sorted_values[int(index) + 1]\n             return lower + (upper - lower) * (index - int(index))\n-    \n+\n     def get_resource_summary(self, window_minutes: int = 60) -> Dict[str, float]:\n         \"\"\"Get resource usage summary.\"\"\"\n         if not self.resource_history:\n             return {}\n-        \n+\n         # Filter by time window\n         cutoff_time = datetime.now() - timedelta(minutes=window_minutes)\n         recent_resources = [\n-            r for r in self.resource_history\n-            if r.timestamp > cutoff_time\n+            r for r in self.resource_history if r.timestamp > cutoff_time\n         ]\n-        \n+\n         if not recent_resources:\n             return {}\n-        \n+\n         return {\n-            \"avg_cpu_percent\": statistics.mean([r.cpu_percent for r in recent_resources]),\n+            \"avg_cpu_percent\": statistics.mean(\n+                [r.cpu_percent for r in recent_resources]\n+            ),\n             \"max_cpu_percent\": max([r.cpu_percent for r in recent_resources]),\n-            \"avg_memory_percent\": statistics.mean([r.memory_percent for r in recent_resources]),\n+            \"avg_memory_percent\": statistics.mean(\n+                [r.memory_percent for r in recent_resources]\n+            ),\n             \"max_memory_percent\": max([r.memory_percent for r in recent_resources]),\n-            \"avg_memory_used_mb\": statistics.mean([r.memory_used_mb for r in recent_resources]),\n+            \"avg_memory_used_mb\": statistics.mean(\n+                [r.memory_used_mb for r in recent_resources]\n+            ),\n             \"max_memory_used_mb\": max([r.memory_used_mb for r in recent_resources]),\n-            \"avg_disk_usage_percent\": statistics.mean([r.disk_usage_percent for r in recent_resources]),\n-            \"max_disk_usage_percent\": max([r.disk_usage_percent for r in recent_resources])\n+            \"avg_disk_usage_percent\": statistics.mean(\n+                [r.disk_usage_percent for r in recent_resources]\n+            ),\n+            \"max_disk_usage_percent\": max(\n+                [r.disk_usage_percent for r in recent_resources]\n+            ),\n         }\n-    \n+\n     def get_active_alerts(self) -> List[PerformanceAlert]:\n         \"\"\"Get active (unacknowledged) alerts.\"\"\"\n         return [alert for alert in self.alerts if not alert.acknowledged]\n-    \n+\n     def acknowledge_alert(self, alert_id: str) -> bool:\n         \"\"\"Acknowledge an alert.\"\"\"\n         for alert in self.alerts:\n             if alert.alert_id == alert_id:\n                 alert.acknowledged = True\n                 self.logger.info(f\"Alert acknowledged: {alert_id}\")\n                 return True\n         return False\n-    \n+\n     def add_alert_callback(self, callback: Callable[[PerformanceAlert], None]) -> None:\n         \"\"\"Add alert callback.\"\"\"\n         self.alert_callbacks.append(callback)\n-    \n+\n     def _cleanup_old_data(self) -> None:\n         \"\"\"Clean up old data based on retention policies.\"\"\"\n         try:\n             # Clean up old alerts\n             retention_days = self.config.get(\"alert_retention_days\", 7)\n             cutoff_time = datetime.now() - timedelta(days=retention_days)\n-            \n+\n             self.alerts = [\n-                alert for alert in self.alerts\n-                if alert.timestamp > cutoff_time\n+                alert for alert in self.alerts if alert.timestamp > cutoff_time\n             ]\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error cleaning up old data: {e}\")\n-    \n-    def generate_performance_report(self, start_time: Optional[datetime] = None, \n-                                  end_time: Optional[datetime] = None) -> PerformanceReport:\n+\n+    def generate_performance_report(\n+        self, start_time: Optional[datetime] = None, end_time: Optional[datetime] = None\n+    ) -> PerformanceReport:\n         \"\"\"Generate comprehensive performance report.\"\"\"\n         if not start_time:\n             start_time = datetime.now() - timedelta(hours=1)\n         if not end_time:\n             end_time = datetime.now()\n-        \n+\n         report_period = end_time - start_time\n-        \n+\n         # Calculate metrics summary\n         metrics_summary = {}\n         for metric_name in self.metrics:\n-            stats = self.get_metric_stats(metric_name, window_minutes=int(report_period.total_seconds() / 60))\n+            stats = self.get_metric_stats(\n+                metric_name, window_minutes=int(report_period.total_seconds() / 60)\n+            )\n             if stats:\n                 metrics_summary[metric_name] = stats\n-        \n+\n         # Calculate resource usage summary\n-        resource_summary = self.get_resource_summary(window_minutes=int(report_period.total_seconds() / 60))\n-        \n+        resource_summary = self.get_resource_summary(\n+            window_minutes=int(report_period.total_seconds() / 60)\n+        )\n+\n         # Get alerts in time period\n         period_alerts = [\n-            alert for alert in self.alerts\n-            if start_time <= alert.timestamp <= end_time\n+            alert for alert in self.alerts if start_time <= alert.timestamp <= end_time\n         ]\n-        \n+\n         # Generate recommendations\n-        recommendations = self._generate_recommendations(metrics_summary, resource_summary, period_alerts)\n-        \n+        recommendations = self._generate_recommendations(\n+            metrics_summary, resource_summary, period_alerts\n+        )\n+\n         return PerformanceReport(\n             report_period=report_period,\n             start_time=start_time,\n             end_time=end_time,\n             metrics_summary=metrics_summary,\n             resource_usage_summary=resource_summary,\n             alerts=period_alerts,\n-            recommendations=recommendations\n+            recommendations=recommendations,\n         )\n-    \n-    def _generate_recommendations(self, metrics_summary: Dict[str, Dict[str, float]], \n-                                resource_summary: Dict[str, float],\n-                                alerts: List[PerformanceAlert]) -> List[str]:\n+\n+    def _generate_recommendations(\n+        self,\n+        metrics_summary: Dict[str, Dict[str, float]],\n+        resource_summary: Dict[str, float],\n+        alerts: List[PerformanceAlert],\n+    ) -> List[str]:\n         \"\"\"Generate performance recommendations.\"\"\"\n         recommendations = []\n-        \n+\n         # Resource-based recommendations\n         if resource_summary:\n             if resource_summary.get(\"avg_cpu_percent\", 0) > 70:\n-                recommendations.append(\"Consider optimizing CPU-intensive operations or scaling horizontally\")\n-            \n+                recommendations.append(\n+                    \"Consider optimizing CPU-intensive operations or scaling horizontally\"\n+                )\n+\n             if resource_summary.get(\"avg_memory_percent\", 0) > 80:\n-                recommendations.append(\"Consider optimizing memory usage or increasing available memory\")\n-            \n+                recommendations.append(\n+                    \"Consider optimizing memory usage or increasing available memory\"\n+                )\n+\n             if resource_summary.get(\"max_memory_used_mb\", 0) > 2048:\n-                recommendations.append(\"Memory usage is high - review data structures and caching strategies\")\n-        \n+                recommendations.append(\n+                    \"Memory usage is high - review data structures and caching strategies\"\n+                )\n+\n         # Metrics-based recommendations\n         if \"response_time\" in metrics_summary:\n             avg_response = metrics_summary[\"response_time\"].get(\"mean\", 0)\n             if avg_response > 1000:\n-                recommendations.append(\"Response times are high - consider optimizing slow operations\")\n-        \n+                recommendations.append(\n+                    \"Response times are high - consider optimizing slow operations\"\n+                )\n+\n         if \"throughput\" in metrics_summary:\n             avg_throughput = metrics_summary[\"throughput\"].get(\"mean\", 0)\n             if avg_throughput < 50:\n-                recommendations.append(\"Throughput is low - consider parallelization or caching\")\n-        \n+                recommendations.append(\n+                    \"Throughput is low - consider parallelization or caching\"\n+                )\n+\n         # Alert-based recommendations\n         critical_alerts = [a for a in alerts if a.severity == \"critical\"]\n         if critical_alerts:\n-            recommendations.append(f\"Address {len(critical_alerts)} critical performance alerts immediately\")\n-        \n+            recommendations.append(\n+                f\"Address {len(critical_alerts)} critical performance alerts immediately\"\n+            )\n+\n         warning_alerts = [a for a in alerts if a.severity == \"warning\"]\n         if warning_alerts:\n-            recommendations.append(f\"Review {len(warning_alerts)} warning-level performance alerts\")\n-        \n+            recommendations.append(\n+                f\"Review {len(warning_alerts)} warning-level performance alerts\"\n+            )\n+\n         # General recommendations\n-        recommendations.extend([\n-            \"Regular performance monitoring helps identify issues early\",\n-            \"Consider implementing performance baselines and SLAs\",\n-            \"Monitor performance trends over time for capacity planning\",\n-            \"Implement automated alerting for critical performance metrics\"\n-        ])\n-        \n+        recommendations.extend(\n+            [\n+                \"Regular performance monitoring helps identify issues early\",\n+                \"Consider implementing performance baselines and SLAs\",\n+                \"Monitor performance trends over time for capacity planning\",\n+                \"Implement automated alerting for critical performance metrics\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n-    def export_report(self, report: PerformanceReport, output_file: Optional[Path] = None) -> Path:\n+\n+    def export_report(\n+        self, report: PerformanceReport, output_file: Optional[Path] = None\n+    ) -> Path:\n         \"\"\"Export performance report to file.\"\"\"\n         if output_file is None:\n             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n             output_file = Path(f\"performance_report_{timestamp}.json\")\n-        \n+\n         # Convert to serializable format\n         report_data = {\n             \"report_metadata\": {\n                 \"generated_at\": datetime.now().isoformat(),\n                 \"report_period_seconds\": report.report_period.total_seconds(),\n                 \"start_time\": report.start_time.isoformat(),\n-                \"end_time\": report.end_time.isoformat()\n+                \"end_time\": report.end_time.isoformat(),\n             },\n             \"metrics_summary\": report.metrics_summary,\n             \"resource_usage_summary\": report.resource_usage_summary,\n             \"alerts\": [asdict(alert) for alert in report.alerts],\n-            \"recommendations\": report.recommendations\n+            \"recommendations\": report.recommendations,\n         }\n-        \n-        with open(output_file, 'w') as f:\n+\n+        with open(output_file, \"w\") as f:\n             json.dump(report_data, f, indent=2, default=str)\n-        \n+\n         return output_file\n-    \n+\n     def get_dashboard_data(self) -> Dict[str, Any]:\n         \"\"\"Get data for performance dashboard.\"\"\"\n         try:\n             # Get recent metrics (last hour)\n             metrics_data = {}\n             for metric_name in self.metrics:\n                 stats = self.get_metric_stats(metric_name, window_minutes=60)\n                 if stats:\n                     metrics_data[metric_name] = {\n-                        \"current\": self.metrics[metric_name][-1].value if self.metrics[metric_name] else 0,\n-                        \"stats\": stats\n+                        \"current\": (\n+                            self.metrics[metric_name][-1].value\n+                            if self.metrics[metric_name]\n+                            else 0\n+                        ),\n+                        \"stats\": stats,\n                     }\n-            \n+\n             # Get resource summary\n             resource_summary = self.get_resource_summary(window_minutes=60)\n-            \n+\n             # Get active alerts\n             active_alerts = self.get_active_alerts()\n-            \n+\n             # Get current resource usage\n-            current_resources = self.resource_history[-1] if self.resource_history else None\n-            \n+            current_resources = (\n+                self.resource_history[-1] if self.resource_history else None\n+            )\n+\n             return {\n                 \"current_time\": datetime.now().isoformat(),\n                 \"monitoring_active\": self.monitoring_active,\n                 \"metrics\": metrics_data,\n                 \"resources\": {\n                     \"current\": asdict(current_resources) if current_resources else {},\n-                    \"summary\": resource_summary\n+                    \"summary\": resource_summary,\n                 },\n                 \"alerts\": {\n                     \"active_count\": len(active_alerts),\n-                    \"recent\": [asdict(alert) for alert in active_alerts[-10:]]\n+                    \"recent\": [asdict(alert) for alert in active_alerts[-10:]],\n                 },\n-                \"thresholds\": self.thresholds\n+                \"thresholds\": self.thresholds,\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error getting dashboard data: {e}\")\n             return {}\n \n \n@@ -595,11 +659,13 @@\n def get_performance_monitor() -> PerformanceMonitor:\n     \"\"\"Get global performance monitor instance.\"\"\"\n     return _performance_monitor\n \n \n-def record_metric(name: str, value: float, unit: str = \"\", tags: Optional[Dict[str, str]] = None):\n+def record_metric(\n+    name: str, value: float, unit: str = \"\", tags: Optional[Dict[str, str]] = None\n+):\n     \"\"\"Record a performance metric.\"\"\"\n     _performance_monitor.record_metric(name, value, unit, tags)\n \n \n def start_performance_monitoring():\n@@ -613,46 +679,54 @@\n \n \n # CLI interface\n if __name__ == \"__main__\":\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(description=\"SparkForge Performance Monitor\")\n-    parser.add_argument(\"--start\", action=\"store_true\", help=\"Start performance monitoring\")\n-    parser.add_argument(\"--stop\", action=\"store_true\", help=\"Stop performance monitoring\")\n-    parser.add_argument(\"--report\", action=\"store_true\", help=\"Generate performance report\")\n+    parser.add_argument(\n+        \"--start\", action=\"store_true\", help=\"Start performance monitoring\"\n+    )\n+    parser.add_argument(\n+        \"--stop\", action=\"store_true\", help=\"Stop performance monitoring\"\n+    )\n+    parser.add_argument(\n+        \"--report\", action=\"store_true\", help=\"Generate performance report\"\n+    )\n     parser.add_argument(\"--dashboard\", action=\"store_true\", help=\"Show dashboard data\")\n-    parser.add_argument(\"--duration\", type=int, default=60, help=\"Monitoring duration in seconds\")\n-    \n+    parser.add_argument(\n+        \"--duration\", type=int, default=60, help=\"Monitoring duration in seconds\"\n+    )\n+\n     args = parser.parse_args()\n-    \n+\n     monitor = PerformanceMonitor()\n-    \n+\n     if args.start:\n         monitor.start_monitoring()\n         print(\"Performance monitoring started\")\n-        \n+\n         try:\n             time.sleep(args.duration)\n         except KeyboardInterrupt:\n             print(\"\\nMonitoring stopped by user\")\n         finally:\n             monitor.stop_monitoring()\n-    \n+\n     if args.stop:\n         monitor.stop_monitoring()\n         print(\"Performance monitoring stopped\")\n-    \n+\n     if args.report:\n         report = monitor.generate_performance_report()\n         report_file = monitor.export_report(report)\n         print(f\"Performance report saved to: {report_file}\")\n-        \n+\n         print(f\"\\nPerformance Summary:\")\n         print(f\"Report Period: {report.report_period}\")\n         print(f\"Metrics Tracked: {len(report.metrics_summary)}\")\n         print(f\"Active Alerts: {len(report.alerts)}\")\n         print(f\"Recommendations: {len(report.recommendations)}\")\n-    \n+\n     if args.dashboard:\n         dashboard_data = monitor.get_dashboard_data()\n         print(json.dumps(dashboard_data, indent=2, default=str))\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/test_security_integration.py\t2025-09-20 16:34:51.338863+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/test_security_integration.py\t2025-09-20 16:53:34.965469+00:00\n@@ -16,346 +16,375 @@\n from unittest.mock import Mock, patch\n from datetime import datetime, timedelta\n \n # Import security components\n from tests.security.security_tests import SecurityTestSuite\n-from tests.security.vulnerability_scanner import VulnerabilityScanner, VulnerabilityReport\n+from tests.security.vulnerability_scanner import (\n+    VulnerabilityScanner,\n+    VulnerabilityReport,\n+)\n from tests.security.compliance_checker import ComplianceChecker, ComplianceStandard\n-from tests.security.security_monitoring import SecurityMonitor, SecurityEvent, SecurityEventType, SecuritySeverity\n+from tests.security.security_monitoring import (\n+    SecurityMonitor,\n+    SecurityEvent,\n+    SecurityEventType,\n+    SecuritySeverity,\n+)\n \n \n class TestSecurityIntegration:\n     \"\"\"Integration tests for security components.\"\"\"\n-    \n+\n     @pytest.fixture\n     def temp_project_dir(self):\n         \"\"\"Create temporary project directory for testing.\"\"\"\n         with tempfile.TemporaryDirectory() as temp_dir:\n             yield Path(temp_dir)\n-    \n+\n     @pytest.fixture\n     def security_test_suite(self):\n         \"\"\"Create security test suite instance.\"\"\"\n         return SecurityTestSuite()\n-    \n+\n     @pytest.fixture\n     def vulnerability_scanner(self, temp_project_dir):\n         \"\"\"Create vulnerability scanner instance.\"\"\"\n         return VulnerabilityScanner(temp_project_dir)\n-    \n+\n     @pytest.fixture\n     def compliance_checker(self, temp_project_dir):\n         \"\"\"Create compliance checker instance.\"\"\"\n         return ComplianceChecker(temp_project_dir)\n-    \n+\n     @pytest.fixture\n     def security_monitor(self):\n         \"\"\"Create security monitor instance.\"\"\"\n         config = {\n             \"monitoring_interval\": 1,\n             \"retention_days\": 1,\n             \"enable_real_time_monitoring\": True,\n-            \"enable_anomaly_detection\": True\n+            \"enable_anomaly_detection\": True,\n         }\n         return SecurityMonitor(config)\n-    \n+\n     def test_security_test_suite_integration(self, security_test_suite):\n         \"\"\"Test security test suite integration.\"\"\"\n         # Run comprehensive security scan\n         results = security_test_suite.run_security_scan()\n-        \n+\n         # Verify all scan components are present\n         assert \"vulnerability_scan\" in results\n         assert \"dependency_check\" in results\n         assert \"code_security\" in results\n         assert \"configuration_security\" in results\n         assert \"data_security\" in results\n         assert \"compliance_check\" in results\n-        \n+\n         # Verify scan results structure\n         for component, result in results.items():\n             assert isinstance(result, dict)\n             assert \"success\" in result or \"compliant\" in result\n-    \n-    def test_vulnerability_scanner_integration(self, vulnerability_scanner, temp_project_dir):\n+\n+    def test_vulnerability_scanner_integration(\n+        self, vulnerability_scanner, temp_project_dir\n+    ):\n         \"\"\"Test vulnerability scanner integration.\"\"\"\n         # Create test files\n         test_file = temp_project_dir / \"test_security.py\"\n-        test_file.write_text(\"\"\"\n+        test_file.write_text(\n+            \"\"\"\n import os\n password = \"hardcoded_password\"  # This should trigger a security issue\n-\"\"\")\n-        \n+\"\"\"\n+        )\n+\n         # Run vulnerability scan\n         scan_results = vulnerability_scanner.scan_all()\n-        \n+\n         # Verify scan results\n         assert \"scan_results\" in scan_results\n         assert \"security_metrics\" in scan_results\n         assert \"summary\" in scan_results\n-        \n+\n         # Verify security metrics\n         metrics = scan_results[\"security_metrics\"]\n         assert metrics is not None\n         assert hasattr(metrics, \"total_vulnerabilities\")\n         assert hasattr(metrics, \"scan_timestamp\")\n-    \n+\n     def test_compliance_checker_integration(self, compliance_checker):\n         \"\"\"Test compliance checker integration.\"\"\"\n         # Check all compliance standards\n         compliance_reports = compliance_checker.check_all_standards()\n-        \n+\n         # Verify all standards are checked\n         expected_standards = [\n             ComplianceStandard.OWASP_TOP_10,\n             ComplianceStandard.CVE_COMPLIANCE,\n             ComplianceStandard.LICENSE_COMPLIANCE,\n-            ComplianceStandard.SECURITY_BEST_PRACTICES\n+            ComplianceStandard.SECURITY_BEST_PRACTICES,\n         ]\n-        \n+\n         for standard in expected_standards:\n             assert standard.value in compliance_reports\n-            \n+\n             report = compliance_reports[standard.value]\n             assert hasattr(report, \"overall_compliant\")\n             assert hasattr(report, \"compliance_score\")\n             assert hasattr(report, \"checks\")\n             assert hasattr(report, \"recommendations\")\n-    \n+\n     def test_security_monitor_integration(self, security_monitor):\n         \"\"\"Test security monitor integration.\"\"\"\n         # Start monitoring\n         security_monitor.start_monitoring()\n-        \n+\n         # Wait for monitoring to start\n         time.sleep(2)\n-        \n+\n         # Verify monitoring is active\n         assert security_monitor.monitoring_active\n-        \n+\n         # Create test security event\n         test_event = SecurityEvent(\n             event_id=\"test_event_001\",\n             event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n             severity=SecuritySeverity.MEDIUM,\n             timestamp=datetime.now(),\n             source=\"test_integration\",\n             description=\"Test security event for integration testing\",\n-            details={\"test\": True}\n-        )\n-        \n+            details={\"test\": True},\n+        )\n+\n         # Log the event\n         security_monitor._log_event(test_event)\n-        \n+\n         # Verify event was logged\n         assert len(security_monitor.events) > 0\n-        \n+\n         # Get dashboard data\n         dashboard_data = security_monitor.get_security_dashboard_data()\n         assert \"current_security_score\" in dashboard_data\n         assert \"active_alerts\" in dashboard_data\n         assert \"recent_events\" in dashboard_data\n-        \n+\n         # Stop monitoring\n         security_monitor.stop_monitoring()\n         assert not security_monitor.monitoring_active\n-    \n-    def test_security_components_workflow(self, security_test_suite, vulnerability_scanner, \n-                                        compliance_checker, security_monitor):\n+\n+    def test_security_components_workflow(\n+        self,\n+        security_test_suite,\n+        vulnerability_scanner,\n+        compliance_checker,\n+        security_monitor,\n+    ):\n         \"\"\"Test complete security workflow integration.\"\"\"\n         # Step 1: Run security tests\n         security_results = security_test_suite.run_security_scan()\n-        \n+\n         # Step 2: Run vulnerability scan\n         vulnerability_results = vulnerability_scanner.scan_all()\n-        \n+\n         # Step 3: Run compliance check\n         compliance_results = compliance_checker.check_all_standards()\n-        \n+\n         # Step 4: Start security monitoring\n         security_monitor.start_monitoring()\n-        \n+\n         # Step 5: Generate security event\n         security_event = SecurityEvent(\n             event_id=\"workflow_test_001\",\n             event_type=SecurityEventType.AUTHENTICATION_FAILURE,\n             severity=SecuritySeverity.HIGH,\n             timestamp=datetime.now(),\n             source=\"workflow_test\",\n             description=\"Authentication failure detected\",\n-            details={\"user\": \"test_user\", \"attempts\": 3}\n-        )\n-        \n+            details={\"user\": \"test_user\", \"attempts\": 3},\n+        )\n+\n         security_monitor._log_event(security_event)\n-        \n+\n         # Step 6: Verify workflow results\n         assert security_results[\"vulnerability_scan\"][\"success\"]\n         assert vulnerability_results[\"security_metrics\"] is not None\n         assert len(compliance_results) >= 4\n         assert len(security_monitor.events) > 0\n-        \n+\n         # Step 7: Stop monitoring\n         security_monitor.stop_monitoring()\n-    \n-    def test_security_reporting_integration(self, vulnerability_scanner, compliance_checker, \n-                                          security_monitor, temp_project_dir):\n+\n+    def test_security_reporting_integration(\n+        self,\n+        vulnerability_scanner,\n+        compliance_checker,\n+        security_monitor,\n+        temp_project_dir,\n+    ):\n         \"\"\"Test security reporting integration.\"\"\"\n         # Generate vulnerability report\n-        vuln_report_file = vulnerability_scanner.generate_report(temp_project_dir / \"vulnerability_report.json\")\n+        vuln_report_file = vulnerability_scanner.generate_report(\n+            temp_project_dir / \"vulnerability_report.json\"\n+        )\n         assert vuln_report_file.exists()\n-        \n+\n         # Generate compliance report\n-        compliance_report_file = compliance_checker.generate_compliance_report(temp_project_dir / \"compliance_report.json\")\n+        compliance_report_file = compliance_checker.generate_compliance_report(\n+            temp_project_dir / \"compliance_report.json\"\n+        )\n         assert compliance_report_file.exists()\n-        \n+\n         # Start monitoring and generate security report\n         security_monitor.start_monitoring()\n         time.sleep(1)\n-        \n-        security_report_file = security_monitor.export_security_report(temp_project_dir / \"security_report.json\")\n+\n+        security_report_file = security_monitor.export_security_report(\n+            temp_project_dir / \"security_report.json\"\n+        )\n         assert security_report_file.exists()\n-        \n+\n         # Verify report contents\n-        with open(vuln_report_file, 'r') as f:\n+        with open(vuln_report_file, \"r\") as f:\n             vuln_data = json.load(f)\n             assert \"scan_results\" in vuln_data\n             assert \"security_metrics\" in vuln_data\n-        \n-        with open(compliance_report_file, 'r') as f:\n+\n+        with open(compliance_report_file, \"r\") as f:\n             compliance_data = json.load(f)\n             assert len(compliance_data) >= 4\n-        \n-        with open(security_report_file, 'r') as f:\n+\n+        with open(security_report_file, \"r\") as f:\n             security_data = json.load(f)\n             assert \"report_metadata\" in security_data\n             assert \"events\" in security_data\n             assert \"alerts\" in security_data\n-        \n+\n         security_monitor.stop_monitoring()\n-    \n+\n     def test_security_alerting_integration(self, security_monitor):\n         \"\"\"Test security alerting integration.\"\"\"\n         alert_callback_called = False\n         alert_received = None\n-        \n+\n         def alert_callback(alert):\n             nonlocal alert_callback_called, alert_received\n             alert_callback_called = True\n             alert_received = alert\n-        \n+\n         # Add alert callback\n         security_monitor.add_alert_callback(alert_callback)\n-        \n+\n         # Start monitoring\n         security_monitor.start_monitoring()\n-        \n+\n         # Create high severity event that should trigger alert\n         critical_event = SecurityEvent(\n             event_id=\"critical_test_001\",\n             event_type=SecurityEventType.SYSTEM_COMPROMISE,\n             severity=SecuritySeverity.CRITICAL,\n             timestamp=datetime.now(),\n             source=\"alert_test\",\n             description=\"Critical security event for alerting test\",\n-            details={\"critical\": True}\n-        )\n-        \n+            details={\"critical\": True},\n+        )\n+\n         security_monitor._log_event(critical_event)\n-        \n+\n         # Wait for alert processing\n         time.sleep(2)\n-        \n+\n         # Verify alert was triggered\n         assert alert_callback_called\n         assert alert_received is not None\n         assert alert_received.severity == SecuritySeverity.CRITICAL\n-        \n+\n         # Verify alert is in alerts list\n         assert len(security_monitor.alerts) > 0\n-        \n+\n         # Test alert acknowledgment\n         alert_id = security_monitor.alerts[0].alert_id\n         assert security_monitor.acknowledge_alert(alert_id)\n         assert security_monitor.alerts[0].acknowledged\n-        \n+\n         # Test alert resolution\n         assert security_monitor.resolve_alert(alert_id)\n         assert security_monitor.alerts[0].resolved\n-        \n+\n         security_monitor.stop_monitoring()\n-    \n+\n     def test_security_metrics_integration(self, security_monitor):\n         \"\"\"Test security metrics integration.\"\"\"\n         # Start monitoring\n         security_monitor.start_monitoring()\n-        \n+\n         # Create various security events\n         events = [\n             SecurityEvent(\n                 event_id=f\"metrics_test_{i}\",\n                 event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n                 severity=SecuritySeverity.MEDIUM,\n                 timestamp=datetime.now(),\n                 source=\"metrics_test\",\n                 description=f\"Test event {i} for metrics\",\n-                details={\"event_number\": i}\n+                details={\"event_number\": i},\n             )\n             for i in range(5)\n         ]\n-        \n+\n         for event in events:\n             security_monitor._log_event(event)\n-        \n+\n         # Wait for metrics update\n         time.sleep(3)\n-        \n+\n         # Verify metrics were generated\n         assert len(security_monitor.metrics) > 0\n-        \n+\n         latest_metrics = security_monitor.metrics[-1]\n         assert latest_metrics.total_events >= 5\n         assert latest_metrics.events_by_type is not None\n         assert latest_metrics.events_by_severity is not None\n         assert latest_metrics.security_score >= 0\n-        \n+\n         # Test dashboard data\n         dashboard_data = security_monitor.get_security_dashboard_data()\n         assert \"current_security_score\" in dashboard_data\n         assert \"active_alerts\" in dashboard_data\n         assert \"recent_events\" in dashboard_data\n         assert \"security_trends\" in dashboard_data\n-        \n+\n         security_monitor.stop_monitoring()\n-    \n+\n     def test_security_thresholds_integration(self, security_monitor):\n         \"\"\"Test security thresholds integration.\"\"\"\n         # Start monitoring\n         security_monitor.start_monitoring()\n-        \n+\n         # Create multiple authentication failure events to trigger threshold\n         for i in range(6):  # More than max_failed_auth_attempts (5)\n             auth_failure = SecurityEvent(\n                 event_id=f\"auth_failure_{i}\",\n                 event_type=SecurityEventType.AUTHENTICATION_FAILURE,\n                 severity=SecuritySeverity.MEDIUM,\n                 timestamp=datetime.now(),\n                 source=\"threshold_test\",\n                 description=f\"Authentication failure {i}\",\n-                details={\"attempt\": i}\n+                details={\"attempt\": i},\n             )\n             security_monitor._log_event(auth_failure)\n-        \n+\n         # Wait for anomaly detection\n         time.sleep(3)\n-        \n+\n         # Verify alert was created for threshold violation\n-        auth_alerts = [alert for alert in security_monitor.alerts \n-                      if \"Brute Force\" in alert.title]\n+        auth_alerts = [\n+            alert for alert in security_monitor.alerts if \"Brute Force\" in alert.title\n+        ]\n         assert len(auth_alerts) > 0\n-        \n+\n         security_monitor.stop_monitoring()\n-    \n+\n     def test_security_configuration_integration(self, temp_project_dir):\n         \"\"\"Test security configuration integration.\"\"\"\n         # Create security configuration file\n         config_file = temp_project_dir / \"security_config.yaml\"\n         config_content = \"\"\"\n@@ -367,38 +396,43 @@\n security_monitoring:\n   enabled: true\n   monitoring_interval: 60\n \"\"\"\n         config_file.write_text(config_content)\n-        \n+\n         # Test that configuration can be loaded\n         import yaml\n-        with open(config_file, 'r') as f:\n+\n+        with open(config_file, \"r\") as f:\n             config = yaml.safe_load(f)\n-        \n+\n         assert config[\"security_scanning\"][\"bandit\"][\"enabled\"]\n         assert config[\"security_monitoring\"][\"enabled\"]\n-    \n+\n     @pytest.mark.performance\n-    def test_security_performance_integration(self, security_test_suite, vulnerability_scanner):\n+    def test_security_performance_integration(\n+        self, security_test_suite, vulnerability_scanner\n+    ):\n         \"\"\"Test security components performance.\"\"\"\n         import time\n-        \n+\n         # Test security test suite performance\n         start_time = time.time()\n         security_results = security_test_suite.run_security_scan()\n         security_time = time.time() - start_time\n-        \n+\n         # Test vulnerability scanner performance\n         start_time = time.time()\n         vuln_results = vulnerability_scanner.scan_all()\n         vuln_time = time.time() - start_time\n-        \n+\n         # Verify performance is within acceptable limits\n-        assert security_time < 60, f\"Security test suite took too long: {security_time}s\"\n+        assert (\n+            security_time < 60\n+        ), f\"Security test suite took too long: {security_time}s\"\n         assert vuln_time < 120, f\"Vulnerability scanner took too long: {vuln_time}s\"\n-        \n+\n         # Verify results are still valid\n         assert security_results[\"vulnerability_scan\"][\"success\"]\n         assert vuln_results[\"security_metrics\"] is not None\n \n \n@@ -406,25 +440,26 @@\n @pytest.fixture(scope=\"session\")\n def security_test_environment():\n     \"\"\"Set up security test environment.\"\"\"\n     # Set up test environment\n     import os\n+\n     os.environ[\"SECURITY_TEST_MODE\"] = \"true\"\n     yield\n     # Cleanup\n     if \"SECURITY_TEST_MODE\" in os.environ:\n         del os.environ[\"SECURITY_TEST_MODE\"]\n \n \n @pytest.mark.security\n class TestSecurityMarkers:\n     \"\"\"Test security-specific pytest markers.\"\"\"\n-    \n+\n     def test_security_marker_works(self):\n         \"\"\"Test that security marker works.\"\"\"\n         assert True\n-    \n+\n     @pytest.mark.slow\n     def test_slow_security_test(self):\n         \"\"\"Test slow security test marker.\"\"\"\n         time.sleep(0.1)  # Simulate slow test\n         assert True\n@@ -433,26 +468,38 @@\n # Integration test for CI/CD security pipeline\n def test_security_cicd_integration():\n     \"\"\"Test security integration with CI/CD pipeline.\"\"\"\n     # This test simulates what would happen in CI/CD\n     security_suite = SecurityTestSuite()\n-    \n+\n     # Run security scan\n     results = security_suite.run_security_scan()\n-    \n+\n     # Verify all security checks pass\n-    assert results[\"vulnerability_scan\"][\"success\"], \"Vulnerability scan failed in CI/CD\"\n+    assert results[\"vulnerability_scan\"][\n+        \"success\"\n+    ], \"Vulnerability scan failed in CI/CD\"\n     assert results[\"dependency_check\"][\"success\"], \"Dependency check failed in CI/CD\"\n-    assert results[\"code_security\"][\"overall_success\"], \"Code security checks failed in CI/CD\"\n-    assert results[\"configuration_security\"][\"success\"], \"Configuration security failed in CI/CD\"\n+    assert results[\"code_security\"][\n+        \"overall_success\"\n+    ], \"Code security checks failed in CI/CD\"\n+    assert results[\"configuration_security\"][\n+        \"success\"\n+    ], \"Configuration security failed in CI/CD\"\n     assert results[\"data_security\"][\"success\"], \"Data security checks failed in CI/CD\"\n-    \n+\n     # Verify compliance\n     compliance_results = results[\"compliance_check\"]\n-    assert compliance_results[\"owasp_top_10\"][\"compliant\"], \"OWASP compliance failed in CI/CD\"\n-    assert compliance_results[\"cve_compliance\"][\"compliant\"], \"CVE compliance failed in CI/CD\"\n-    assert compliance_results[\"license_compliance\"][\"compliant\"], \"License compliance failed in CI/CD\"\n+    assert compliance_results[\"owasp_top_10\"][\n+        \"compliant\"\n+    ], \"OWASP compliance failed in CI/CD\"\n+    assert compliance_results[\"cve_compliance\"][\n+        \"compliant\"\n+    ], \"CVE compliance failed in CI/CD\"\n+    assert compliance_results[\"license_compliance\"][\n+        \"compliant\"\n+    ], \"License compliance failed in CI/CD\"\n \n \n if __name__ == \"__main__\":\n     # Run integration tests\n     pytest.main([__file__, \"-v\", \"--tb=short\"])\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_tests.py\t2025-09-20 16:34:51.329273+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_tests.py\t2025-09-20 16:53:35.002862+00:00\n@@ -26,331 +26,341 @@\n from sparkforge.errors import ValidationError\n \n \n class SecurityTestSuite:\n     \"\"\"Comprehensive security test suite for SparkForge.\"\"\"\n-    \n+\n     def __init__(self):\n         self.security_issues = []\n         self.compliance_results = {}\n-    \n+\n     def run_security_scan(self) -> Dict[str, Any]:\n         \"\"\"Run comprehensive security scan.\"\"\"\n         results = {\n             \"vulnerability_scan\": self._scan_vulnerabilities(),\n             \"dependency_check\": self._check_dependencies(),\n             \"code_security\": self._test_code_security(),\n             \"configuration_security\": self._test_configuration_security(),\n             \"data_security\": self._test_data_security(),\n-            \"compliance_check\": self._check_compliance()\n+            \"compliance_check\": self._check_compliance(),\n         }\n-        \n+\n         return results\n-    \n+\n     def _scan_vulnerabilities(self) -> Dict[str, Any]:\n         \"\"\"Scan for known vulnerabilities.\"\"\"\n         try:\n             # Run bandit security scan\n-            result = subprocess.run([\n-                sys.executable, \"-m\", \"bandit\", \"-r\", \"sparkforge/\", \n-                \"-f\", \"json\", \"-ll\"\n-            ], capture_output=True, text=True, cwd=Path.cwd())\n-            \n+            result = subprocess.run(\n+                [\n+                    sys.executable,\n+                    \"-m\",\n+                    \"bandit\",\n+                    \"-r\",\n+                    \"sparkforge/\",\n+                    \"-f\",\n+                    \"json\",\n+                    \"-ll\",\n+                ],\n+                capture_output=True,\n+                text=True,\n+                cwd=Path.cwd(),\n+            )\n+\n             bandit_results = json.loads(result.stdout) if result.stdout else {}\n-            \n+\n             # Run safety check for known vulnerabilities\n-            safety_result = subprocess.run([\n-                sys.executable, \"-m\", \"safety\", \"check\", \"--json\"\n-            ], capture_output=True, text=True)\n-            \n+            safety_result = subprocess.run(\n+                [sys.executable, \"-m\", \"safety\", \"check\", \"--json\"],\n+                capture_output=True,\n+                text=True,\n+            )\n+\n             safety_results = []\n             if safety_result.stdout:\n                 try:\n                     safety_results = json.loads(safety_result.stdout)\n                 except json.JSONDecodeError:\n                     pass\n-            \n+\n             return {\n                 \"bandit_issues\": bandit_results.get(\"results\", []),\n                 \"safety_vulnerabilities\": safety_results,\n                 \"bandit_score\": bandit_results.get(\"metrics\", {}).get(\"SEVERITY\", {}),\n-                \"success\": result.returncode == 0 and len(safety_results) == 0\n-            }\n-            \n-        except Exception as e:\n-            return {\n-                \"error\": str(e),\n-                \"success\": False\n-            }\n-    \n+                \"success\": result.returncode == 0 and len(safety_results) == 0,\n+            }\n+\n+        except Exception as e:\n+            return {\"error\": str(e), \"success\": False}\n+\n     def _check_dependencies(self) -> Dict[str, Any]:\n         \"\"\"Check for vulnerable dependencies.\"\"\"\n         try:\n             # Check for outdated packages\n-            pip_check = subprocess.run([\n-                sys.executable, \"-m\", \"pip\", \"check\"\n-            ], capture_output=True, text=True)\n-            \n+            pip_check = subprocess.run(\n+                [sys.executable, \"-m\", \"pip\", \"check\"], capture_output=True, text=True\n+            )\n+\n             # Check for known vulnerabilities in dependencies\n-            safety_check = subprocess.run([\n-                sys.executable, \"-m\", \"safety\", \"check\"\n-            ], capture_output=True, text=True)\n-            \n+            safety_check = subprocess.run(\n+                [sys.executable, \"-m\", \"safety\", \"check\"],\n+                capture_output=True,\n+                text=True,\n+            )\n+\n             return {\n                 \"dependency_conflicts\": pip_check.stdout,\n                 \"vulnerable_packages\": safety_check.stdout,\n-                \"success\": pip_check.returncode == 0 and safety_check.returncode == 0\n-            }\n-            \n-        except Exception as e:\n-            return {\n-                \"error\": str(e),\n-                \"success\": False\n-            }\n-    \n+                \"success\": pip_check.returncode == 0 and safety_check.returncode == 0,\n+            }\n+\n+        except Exception as e:\n+            return {\"error\": str(e), \"success\": False}\n+\n     def _test_code_security(self) -> Dict[str, Any]:\n         \"\"\"Test code for security vulnerabilities.\"\"\"\n         security_tests = []\n-        \n+\n         # Test for SQL injection vulnerabilities\n         security_tests.append(self._test_sql_injection_prevention())\n-        \n+\n         # Test for path traversal vulnerabilities\n         security_tests.append(self._test_path_traversal_prevention())\n-        \n+\n         # Test for command injection vulnerabilities\n         security_tests.append(self._test_command_injection_prevention())\n-        \n+\n         # Test for information disclosure\n         security_tests.append(self._test_information_disclosure())\n-        \n+\n         return {\n             \"sql_injection_test\": security_tests[0],\n             \"path_traversal_test\": security_tests[1],\n             \"command_injection_test\": security_tests[2],\n             \"information_disclosure_test\": security_tests[3],\n-            \"overall_success\": all(test[\"success\"] for test in security_tests)\n+            \"overall_success\": all(test[\"success\"] for test in security_tests),\n         }\n-    \n+\n     def _test_sql_injection_prevention(self) -> Dict[str, Any]:\n         \"\"\"Test for SQL injection prevention.\"\"\"\n         try:\n             # Test that validation functions properly escape inputs\n             from sparkforge.validation import validate_dataframe_schema\n-            \n+\n             # Test with potentially malicious input\n             malicious_inputs = [\n                 \"'; DROP TABLE users; --\",\n                 \"1' OR '1'='1\",\n                 \"admin'--\",\n-                \"'; INSERT INTO users VALUES ('hacker', 'password'); --\"\n+                \"'; INSERT INTO users VALUES ('hacker', 'password'); --\",\n             ]\n-            \n+\n             for malicious_input in malicious_inputs:\n                 # These should not cause SQL injection\n                 result = validate_dataframe_schema(None, [malicious_input])\n                 # Should handle gracefully without executing SQL\n-            \n-            return {\n-                \"success\": True,\n-                \"message\": \"SQL injection prevention tests passed\"\n-            }\n-            \n+\n+            return {\"success\": True, \"message\": \"SQL injection prevention tests passed\"}\n+\n         except Exception as e:\n             return {\n                 \"success\": False,\n                 \"error\": str(e),\n-                \"message\": \"SQL injection prevention test failed\"\n-            }\n-    \n+                \"message\": \"SQL injection prevention test failed\",\n+            }\n+\n     def _test_path_traversal_prevention(self) -> Dict[str, Any]:\n         \"\"\"Test for path traversal prevention.\"\"\"\n         try:\n             # Test that file operations are safe from path traversal\n             from sparkforge.validation import get_dataframe_info\n-            \n+\n             malicious_paths = [\n                 \"../../../etc/passwd\",\n                 \"..\\\\..\\\\..\\\\windows\\\\system32\\\\config\\\\sam\",\n                 \"/etc/passwd\",\n-                \"C:\\\\Windows\\\\System32\\\\config\\\\SAM\"\n+                \"C:\\\\Windows\\\\System32\\\\config\\\\SAM\",\n             ]\n-            \n+\n             # These operations should not allow path traversal\n             for malicious_path in malicious_paths:\n                 # Should not be able to access files outside allowed directories\n                 try:\n                     # This should not actually access the file system in a dangerous way\n                     pass\n                 except Exception:\n                     # Expected to fail safely\n                     pass\n-            \n+\n             return {\n                 \"success\": True,\n-                \"message\": \"Path traversal prevention tests passed\"\n-            }\n-            \n+                \"message\": \"Path traversal prevention tests passed\",\n+            }\n+\n         except Exception as e:\n             return {\n                 \"success\": False,\n                 \"error\": str(e),\n-                \"message\": \"Path traversal prevention test failed\"\n-            }\n-    \n+                \"message\": \"Path traversal prevention test failed\",\n+            }\n+\n     def _test_command_injection_prevention(self) -> Dict[str, Any]:\n         \"\"\"Test for command injection prevention.\"\"\"\n         try:\n             # Test that no shell commands are executed with user input\n             malicious_commands = [\n                 \"; rm -rf /\",\n                 \"| cat /etc/passwd\",\n                 \"&& whoami\",\n-                \"|| echo 'hacked'\"\n+                \"|| echo 'hacked'\",\n             ]\n-            \n+\n             for command in malicious_commands:\n                 # These should not be executed as shell commands\n                 # SparkForge should not execute arbitrary shell commands\n                 pass\n-            \n+\n             return {\n                 \"success\": True,\n-                \"message\": \"Command injection prevention tests passed\"\n-            }\n-            \n+                \"message\": \"Command injection prevention tests passed\",\n+            }\n+\n         except Exception as e:\n             return {\n                 \"success\": False,\n                 \"error\": str(e),\n-                \"message\": \"Command injection prevention test failed\"\n-            }\n-    \n+                \"message\": \"Command injection prevention test failed\",\n+            }\n+\n     def _test_information_disclosure(self) -> Dict[str, Any]:\n         \"\"\"Test for information disclosure vulnerabilities.\"\"\"\n         try:\n             # Test that sensitive information is not leaked in error messages\n-            from sparkforge.errors import ValidationError, PipelineError, ConfigurationError\n-            \n+            from sparkforge.errors import (\n+                ValidationError,\n+                PipelineError,\n+                ConfigurationError,\n+            )\n+\n             # Test error messages don't contain sensitive information\n             test_errors = [\n                 ValidationError(\"Test validation error\"),\n                 PipelineError(\"Test pipeline error\"),\n-                ConfigurationError(\"Test configuration error\")\n+                ConfigurationError(\"Test configuration error\"),\n             ]\n-            \n+\n             for error in test_errors:\n                 error_message = str(error)\n                 # Error messages should not contain sensitive paths, passwords, etc.\n                 sensitive_patterns = [\"password\", \"secret\", \"key\", \"/etc/\", \"C:\\\\\"]\n-                \n+\n                 for pattern in sensitive_patterns:\n                     if pattern.lower() in error_message.lower():\n                         return {\n                             \"success\": False,\n                             \"error\": f\"Sensitive information leaked in error: {pattern}\",\n-                            \"message\": \"Information disclosure test failed\"\n+                            \"message\": \"Information disclosure test failed\",\n                         }\n-            \n+\n             return {\n                 \"success\": True,\n-                \"message\": \"Information disclosure prevention tests passed\"\n-            }\n-            \n+                \"message\": \"Information disclosure prevention tests passed\",\n+            }\n+\n         except Exception as e:\n             return {\n                 \"success\": False,\n                 \"error\": str(e),\n-                \"message\": \"Information disclosure prevention test failed\"\n-            }\n-    \n+                \"message\": \"Information disclosure prevention test failed\",\n+            }\n+\n     def _test_configuration_security(self) -> Dict[str, Any]:\n         \"\"\"Test configuration security.\"\"\"\n         try:\n             # Test that default configurations are secure\n             config = PipelineConfig(\n                 schema=\"test_schema\",\n                 quality_thresholds=ValidationThresholds(80.0, 85.0, 90.0),\n-                parallel=ParallelConfig(enabled=True, max_workers=4)\n-            )\n-            \n+                parallel=ParallelConfig(enabled=True, max_workers=4),\n+            )\n+\n             # Test that configuration validation prevents insecure settings\n             try:\n                 # This should fail validation\n                 insecure_config = PipelineConfig(\n                     schema=\"\",  # Empty schema should not be allowed\n-                    quality_thresholds=ValidationThresholds(-1.0, 150.0, 200.0),  # Invalid thresholds\n-                    parallel=ParallelConfig(enabled=True, max_workers=0)  # Invalid workers\n+                    quality_thresholds=ValidationThresholds(\n+                        -1.0, 150.0, 200.0\n+                    ),  # Invalid thresholds\n+                    parallel=ParallelConfig(\n+                        enabled=True, max_workers=0\n+                    ),  # Invalid workers\n                 )\n                 return {\n                     \"success\": False,\n                     \"error\": \"Insecure configuration was accepted\",\n-                    \"message\": \"Configuration security test failed\"\n+                    \"message\": \"Configuration security test failed\",\n                 }\n             except (ValidationError, ConfigurationError):\n                 # Expected to fail validation\n                 pass\n-            \n-            return {\n-                \"success\": True,\n-                \"message\": \"Configuration security tests passed\"\n-            }\n-            \n+\n+            return {\"success\": True, \"message\": \"Configuration security tests passed\"}\n+\n         except Exception as e:\n             return {\n                 \"success\": False,\n                 \"error\": str(e),\n-                \"message\": \"Configuration security test failed\"\n-            }\n-    \n+                \"message\": \"Configuration security test failed\",\n+            }\n+\n     def _test_data_security(self) -> Dict[str, Any]:\n         \"\"\"Test data security features.\"\"\"\n         try:\n             # Test data validation prevents malicious input\n             from pyspark.sql import functions as F\n             from sparkforge.models import BronzeStep\n-            \n+\n             # Test validation rules prevent malicious data\n             malicious_rules = {\n                 \"malicious_col\": [\n                     F.col(\"malicious_col\").rlike(r\".*<script.*\"),  # XSS prevention\n-                    F.col(\"malicious_col\").rlike(r\".*SELECT.*FROM.*\"),  # SQL injection prevention\n-                    F.col(\"malicious_col\").rlike(r\".*\\.\\./.*\"),  # Path traversal prevention\n+                    F.col(\"malicious_col\").rlike(\n+                        r\".*SELECT.*FROM.*\"\n+                    ),  # SQL injection prevention\n+                    F.col(\"malicious_col\").rlike(\n+                        r\".*\\.\\./.*\"\n+                    ),  # Path traversal prevention\n                 ]\n             }\n-            \n+\n             # Test that validation rules are properly applied\n             bronze_step = BronzeStep(\n-                name=\"security_test\",\n-                transform=lambda df: df,\n-                rules=malicious_rules\n-            )\n-            \n+                name=\"security_test\", transform=lambda df: df, rules=malicious_rules\n+            )\n+\n             bronze_step.validate()\n-            \n-            return {\n-                \"success\": True,\n-                \"message\": \"Data security tests passed\"\n-            }\n-            \n+\n+            return {\"success\": True, \"message\": \"Data security tests passed\"}\n+\n         except Exception as e:\n             return {\n                 \"success\": False,\n                 \"error\": str(e),\n-                \"message\": \"Data security test failed\"\n-            }\n-    \n+                \"message\": \"Data security test failed\",\n+            }\n+\n     def _check_compliance(self) -> Dict[str, Any]:\n         \"\"\"Check compliance with security standards.\"\"\"\n         compliance_results = {\n             \"owasp_top_10\": self._check_owasp_compliance(),\n             \"cve_compliance\": self._check_cve_compliance(),\n-            \"dependency_compliance\": self._check_dependency_compliance()\n+            \"dependency_compliance\": self._check_dependency_compliance(),\n         }\n-        \n+\n         return compliance_results\n-    \n+\n     def _check_owasp_compliance(self) -> Dict[str, Any]:\n         \"\"\"Check compliance with OWASP Top 10.\"\"\"\n         owasp_checks = {\n             \"injection\": True,  # SQL injection prevention tested\n             \"broken_authentication\": True,  # No authentication in framework\n@@ -359,84 +369,86 @@\n             \"broken_access_control\": True,  # No access control in framework\n             \"security_misconfiguration\": True,  # Secure defaults\n             \"cross_site_scripting\": True,  # XSS prevention in validation\n             \"insecure_deserialization\": True,  # Safe serialization\n             \"known_vulnerabilities\": True,  # Dependency scanning\n-            \"insufficient_logging\": True  # Comprehensive logging\n+            \"insufficient_logging\": True,  # Comprehensive logging\n         }\n-        \n+\n         return {\n             \"checks\": owasp_checks,\n             \"compliant\": all(owasp_checks.values()),\n-            \"score\": sum(owasp_checks.values()) / len(owasp_checks) * 100\n+            \"score\": sum(owasp_checks.values()) / len(owasp_checks) * 100,\n         }\n-    \n+\n     def _check_cve_compliance(self) -> Dict[str, Any]:\n         \"\"\"Check for known CVE vulnerabilities.\"\"\"\n         try:\n             # Run safety check for known CVEs\n-            result = subprocess.run([\n-                sys.executable, \"-m\", \"safety\", \"check\"\n-            ], capture_output=True, text=True)\n-            \n+            result = subprocess.run(\n+                [sys.executable, \"-m\", \"safety\", \"check\"],\n+                capture_output=True,\n+                text=True,\n+            )\n+\n             if result.returncode == 0:\n                 return {\n                     \"cve_count\": 0,\n                     \"compliant\": True,\n-                    \"message\": \"No known CVE vulnerabilities found\"\n+                    \"message\": \"No known CVE vulnerabilities found\",\n                 }\n             else:\n                 return {\n-                    \"cve_count\": len(result.stdout.split('\\n')) - 1,\n+                    \"cve_count\": len(result.stdout.split(\"\\n\")) - 1,\n                     \"compliant\": False,\n                     \"message\": \"Known CVE vulnerabilities found\",\n-                    \"details\": result.stdout\n+                    \"details\": result.stdout,\n                 }\n-                \n+\n         except Exception as e:\n             return {\n                 \"error\": str(e),\n                 \"compliant\": False,\n-                \"message\": \"CVE compliance check failed\"\n-            }\n-    \n+                \"message\": \"CVE compliance check failed\",\n+            }\n+\n     def _check_dependency_compliance(self) -> Dict[str, Any]:\n         \"\"\"Check dependency compliance.\"\"\"\n         try:\n             # Check for known vulnerable dependencies\n-            result = subprocess.run([\n-                sys.executable, \"-m\", \"pip\", \"audit\"\n-            ], capture_output=True, text=True)\n-            \n+            result = subprocess.run(\n+                [sys.executable, \"-m\", \"pip\", \"audit\"], capture_output=True, text=True\n+            )\n+\n             if result.returncode == 0:\n                 return {\n                     \"vulnerable_dependencies\": 0,\n                     \"compliant\": True,\n-                    \"message\": \"No vulnerable dependencies found\"\n+                    \"message\": \"No vulnerable dependencies found\",\n                 }\n             else:\n                 return {\n-                    \"vulnerable_dependencies\": len(result.stdout.split('\\n')) - 1,\n+                    \"vulnerable_dependencies\": len(result.stdout.split(\"\\n\")) - 1,\n                     \"compliant\": False,\n                     \"message\": \"Vulnerable dependencies found\",\n-                    \"details\": result.stdout\n+                    \"details\": result.stdout,\n                 }\n-                \n+\n         except Exception as e:\n             return {\n                 \"error\": str(e),\n                 \"compliant\": False,\n-                \"message\": \"Dependency compliance check failed\"\n+                \"message\": \"Dependency compliance check failed\",\n             }\n \n \n # Pytest test functions\n def test_security_scan():\n     \"\"\"Test comprehensive security scan.\"\"\"\n     security_suite = SecurityTestSuite()\n     results = security_suite.run_security_scan()\n-    \n+\n     # Assert overall security\n     assert results[\"vulnerability_scan\"][\"success\"], \"Vulnerability scan failed\"\n     assert results[\"dependency_check\"][\"success\"], \"Dependency check failed\"\n     assert results[\"code_security\"][\"overall_success\"], \"Code security tests failed\"\n     assert results[\"configuration_security\"][\"success\"], \"Configuration security failed\"\n@@ -445,81 +457,97 @@\n \n def test_sql_injection_prevention():\n     \"\"\"Test SQL injection prevention.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._test_sql_injection_prevention()\n-    \n-    assert result[\"success\"], f\"SQL injection prevention failed: {result.get('error', 'Unknown error')}\"\n+\n+    assert result[\n+        \"success\"\n+    ], f\"SQL injection prevention failed: {result.get('error', 'Unknown error')}\"\n \n \n def test_path_traversal_prevention():\n     \"\"\"Test path traversal prevention.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._test_path_traversal_prevention()\n-    \n-    assert result[\"success\"], f\"Path traversal prevention failed: {result.get('error', 'Unknown error')}\"\n+\n+    assert result[\n+        \"success\"\n+    ], f\"Path traversal prevention failed: {result.get('error', 'Unknown error')}\"\n \n \n def test_command_injection_prevention():\n     \"\"\"Test command injection prevention.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._test_command_injection_prevention()\n-    \n-    assert result[\"success\"], f\"Command injection prevention failed: {result.get('error', 'Unknown error')}\"\n+\n+    assert result[\n+        \"success\"\n+    ], f\"Command injection prevention failed: {result.get('error', 'Unknown error')}\"\n \n \n def test_information_disclosure_prevention():\n     \"\"\"Test information disclosure prevention.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._test_information_disclosure()\n-    \n-    assert result[\"success\"], f\"Information disclosure prevention failed: {result.get('error', 'Unknown error')}\"\n+\n+    assert result[\n+        \"success\"\n+    ], f\"Information disclosure prevention failed: {result.get('error', 'Unknown error')}\"\n \n \n def test_configuration_security():\n     \"\"\"Test configuration security.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._test_configuration_security()\n-    \n-    assert result[\"success\"], f\"Configuration security failed: {result.get('error', 'Unknown error')}\"\n+\n+    assert result[\n+        \"success\"\n+    ], f\"Configuration security failed: {result.get('error', 'Unknown error')}\"\n \n \n def test_data_security():\n     \"\"\"Test data security features.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._test_data_security()\n-    \n-    assert result[\"success\"], f\"Data security failed: {result.get('error', 'Unknown error')}\"\n+\n+    assert result[\n+        \"success\"\n+    ], f\"Data security failed: {result.get('error', 'Unknown error')}\"\n \n \n def test_owasp_compliance():\n     \"\"\"Test OWASP Top 10 compliance.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._check_owasp_compliance()\n-    \n+\n     assert result[\"compliant\"], f\"OWASP compliance failed: {result['score']}% score\"\n     assert result[\"score\"] >= 90, f\"OWASP compliance score too low: {result['score']}%\"\n \n \n def test_cve_compliance():\n     \"\"\"Test CVE compliance.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._check_cve_compliance()\n-    \n-    assert result[\"compliant\"], f\"CVE compliance failed: {result.get('message', 'Unknown error')}\"\n+\n+    assert result[\n+        \"compliant\"\n+    ], f\"CVE compliance failed: {result.get('message', 'Unknown error')}\"\n \n \n def test_dependency_compliance():\n     \"\"\"Test dependency compliance.\"\"\"\n     security_suite = SecurityTestSuite()\n     result = security_suite._check_dependency_compliance()\n-    \n-    assert result[\"compliant\"], f\"Dependency compliance failed: {result.get('message', 'Unknown error')}\"\n+\n+    assert result[\n+        \"compliant\"\n+    ], f\"Dependency compliance failed: {result.get('message', 'Unknown error')}\"\n \n \n if __name__ == \"__main__\":\n     # Run security tests\n     security_suite = SecurityTestSuite()\n     results = security_suite.run_security_scan()\n-    \n+\n     print(\"Security Scan Results:\")\n     print(json.dumps(results, indent=2))\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_dataframe_access.py\t2025-09-20 14:00:38.763040+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_dataframe_access.py\t2025-09-20 16:53:35.037963+00:00\n@@ -2,11 +2,10 @@\n Tests for DataFrame access in the simplified execution system.\n \n This module tests that the simplified execution system properly handles\n DataFrame operations and transformations.\n \"\"\"\n-\n \n import pytest\n from pyspark.sql import functions as F\n from pyspark.sql.window import Window\n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_graph.py\t2025-09-20 14:00:38.765574+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_graph.py\t2025-09-20 16:53:35.043663+00:00\n@@ -12,142 +12,146 @@\n     \"\"\"Test the DependencyGraph class.\"\"\"\n \n     def test_add_dependency_missing_nodes(self):\n         \"\"\"Test add_dependency with missing nodes.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Add one node\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n-        \n+\n         # Try to add dependency with missing node\n-        with pytest.raises(ValueError, match=\"Steps step1 or missing_step not found in graph\"):\n+        with pytest.raises(\n+            ValueError, match=\"Steps step1 or missing_step not found in graph\"\n+        ):\n             graph.add_dependency(\"step1\", \"missing_step\")\n-        \n-        with pytest.raises(ValueError, match=\"Steps missing_step or step1 not found in graph\"):\n+\n+        with pytest.raises(\n+            ValueError, match=\"Steps missing_step or step1 not found in graph\"\n+        ):\n             graph.add_dependency(\"missing_step\", \"step1\")\n \n     def test_get_dependencies_missing_node(self):\n         \"\"\"Test get_dependencies with missing node.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Get dependencies for non-existent node\n         deps = graph.get_dependencies(\"missing_step\")\n         assert deps == set()  # Should return empty set\n \n     def test_get_dependents_missing_node(self):\n         \"\"\"Test get_dependents with missing node.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Get dependents for non-existent node\n         deps = graph.get_dependents(\"missing_step\")\n         assert deps == set()  # Should return empty set\n \n     def test_detect_cycles(self):\n         \"\"\"Test detect_cycles method.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Create a cycle: step1 -> step2 -> step1\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n         graph.add_dependency(\"step1\", \"step2\")\n         graph.add_dependency(\"step2\", \"step1\")\n-        \n+\n         # Test cycle detection\n         cycles = graph.detect_cycles()\n-        \n+\n         assert len(cycles) > 0\n         assert any(\"step1\" in cycle and \"step2\" in cycle for cycle in cycles)\n \n     def test_get_execution_groups_missing_dependency(self):\n         \"\"\"Test get_execution_groups with missing dependency.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Add nodes\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n-        \n+\n         # Manually add a dependency to a missing node\n         graph.nodes[\"step2\"].dependencies.add(\"missing_step\")\n-        \n-        with patch('sparkforge.dependencies.graph.logger') as mock_logger:\n+\n+        with patch(\"sparkforge.dependencies.graph.logger\") as mock_logger:\n             groups = graph.get_execution_groups()\n-            \n+\n             # Check that warning was logged\n             mock_logger.warning.assert_any_call(\n                 \"Dependency missing_step not found in levels for node step2\"\n             )\n-            \n+\n             # Check that groups were still calculated\n             assert len(groups) > 0\n \n     def test_validate_cycles(self):\n         \"\"\"Test validate method with cycles.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Create a cycle\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n         graph.add_dependency(\"step1\", \"step2\")\n         graph.add_dependency(\"step2\", \"step1\")\n-        \n+\n         issues = graph.validate()\n-        \n+\n         assert len(issues) > 0\n         assert any(\"Circular dependency detected\" in issue for issue in issues)\n \n     def test_validate_missing_dependencies(self):\n         \"\"\"Test validate method with missing dependencies.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Add node with missing dependency\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n         graph.nodes[\"step1\"].dependencies.add(\"missing_step\")\n-        \n+\n         issues = graph.validate()\n-        \n+\n         assert len(issues) > 0\n         assert any(\"depends on missing node missing_step\" in issue for issue in issues)\n \n     def test_get_execution_groups(self):\n         \"\"\"Test get_execution_groups method.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Add nodes\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n         graph.add_dependency(\"step1\", \"step2\")\n-        \n+\n         groups = graph.get_execution_groups()\n-        \n+\n         assert len(groups) == 2\n         assert [\"step1\"] in groups\n         assert [\"step2\"] in groups\n \n     def test_get_stats(self):\n         \"\"\"Test get_stats method.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Add nodes\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n         graph.add_dependency(\"step1\", \"step2\")\n-        \n+\n         stats = graph.get_stats()\n-        \n+\n         assert stats[\"total_nodes\"] == 2\n         assert stats[\"total_edges\"] == 1\n         assert stats[\"average_dependencies\"] == 0.5\n \n     def test_get_parallel_candidates(self):\n         \"\"\"Test get_parallel_candidates method.\"\"\"\n         graph = DependencyGraph()\n-        \n+\n         # Add nodes\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n         graph.add_dependency(\"step1\", \"step2\")\n-        \n+\n         candidates = graph.get_parallel_candidates()\n-        \n+\n         assert len(candidates) == 2\n         assert [\"step1\"] in candidates\n         assert [\"step2\"] in candidates\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/vulnerability_scanner.py\t2025-09-20 16:34:51.329291+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/vulnerability_scanner.py\t2025-09-20 16:53:35.069557+00:00\n@@ -20,10 +20,11 @@\n \n \n @dataclass\n class VulnerabilityReport:\n     \"\"\"Vulnerability report data structure.\"\"\"\n+\n     vulnerability_id: str\n     severity: str\n     description: str\n     file_path: str\n     line_number: int\n@@ -34,10 +35,11 @@\n \n \n @dataclass\n class SecurityMetrics:\n     \"\"\"Security metrics data structure.\"\"\"\n+\n     total_vulnerabilities: int\n     high_severity: int\n     medium_severity: int\n     low_severity: int\n     info_severity: int\n@@ -45,534 +47,620 @@\n     scan_duration: float\n \n \n class VulnerabilityScanner:\n     \"\"\"Comprehensive vulnerability scanner for SparkForge.\"\"\"\n-    \n+\n     def __init__(self, project_root: Optional[Path] = None):\n         self.project_root = project_root or Path.cwd()\n         self.scan_results = []\n         self.security_metrics = None\n-    \n+\n     def scan_all(self) -> Dict[str, Any]:\n         \"\"\"Run comprehensive vulnerability scan.\"\"\"\n         start_time = time.time()\n-        \n+\n         scan_results = {\n             \"static_analysis\": self.scan_static_code(),\n             \"dependency_scan\": self.scan_dependencies(),\n             \"configuration_scan\": self.scan_configuration(),\n             \"secrets_scan\": self.scan_secrets(),\n             \"license_scan\": self.scan_licenses(),\n-            \"supply_chain_scan\": self.scan_supply_chain()\n+            \"supply_chain_scan\": self.scan_supply_chain(),\n         }\n-        \n+\n         end_time = time.time()\n         self.security_metrics = SecurityMetrics(\n-            total_vulnerabilities=sum(len(result.get(\"vulnerabilities\", [])) for result in scan_results.values()),\n-            high_severity=sum(len([v for v in result.get(\"vulnerabilities\", []) if v.get(\"severity\") == \"high\"]) for result in scan_results.values()),\n-            medium_severity=sum(len([v for v in result.get(\"vulnerabilities\", []) if v.get(\"severity\") == \"medium\"]) for result in scan_results.values()),\n-            low_severity=sum(len([v for v in result.get(\"vulnerabilities\", []) if v.get(\"severity\") == \"low\"]) for result in scan_results.values()),\n-            info_severity=sum(len([v for v in result.get(\"vulnerabilities\", []) if v.get(\"severity\") == \"info\"]) for result in scan_results.values()),\n+            total_vulnerabilities=sum(\n+                len(result.get(\"vulnerabilities\", []))\n+                for result in scan_results.values()\n+            ),\n+            high_severity=sum(\n+                len(\n+                    [\n+                        v\n+                        for v in result.get(\"vulnerabilities\", [])\n+                        if v.get(\"severity\") == \"high\"\n+                    ]\n+                )\n+                for result in scan_results.values()\n+            ),\n+            medium_severity=sum(\n+                len(\n+                    [\n+                        v\n+                        for v in result.get(\"vulnerabilities\", [])\n+                        if v.get(\"severity\") == \"medium\"\n+                    ]\n+                )\n+                for result in scan_results.values()\n+            ),\n+            low_severity=sum(\n+                len(\n+                    [\n+                        v\n+                        for v in result.get(\"vulnerabilities\", [])\n+                        if v.get(\"severity\") == \"low\"\n+                    ]\n+                )\n+                for result in scan_results.values()\n+            ),\n+            info_severity=sum(\n+                len(\n+                    [\n+                        v\n+                        for v in result.get(\"vulnerabilities\", [])\n+                        if v.get(\"severity\") == \"info\"\n+                    ]\n+                )\n+                for result in scan_results.values()\n+            ),\n             scan_timestamp=datetime.now(),\n-            scan_duration=end_time - start_time\n+            scan_duration=end_time - start_time,\n         )\n-        \n+\n         return {\n             \"scan_results\": scan_results,\n             \"security_metrics\": self.security_metrics,\n-            \"summary\": self._generate_summary()\n+            \"summary\": self._generate_summary(),\n         }\n-    \n+\n     def scan_static_code(self) -> Dict[str, Any]:\n         \"\"\"Scan static code for vulnerabilities using bandit.\"\"\"\n         try:\n             # Run bandit security scan\n-            result = subprocess.run([\n-                sys.executable, \"-m\", \"bandit\", \n-                \"-r\", str(self.project_root / \"sparkforge\"),\n-                \"-f\", \"json\",\n-                \"-ll\"  # Low confidence, low severity\n-            ], capture_output=True, text=True, cwd=self.project_root)\n-            \n+            result = subprocess.run(\n+                [\n+                    sys.executable,\n+                    \"-m\",\n+                    \"bandit\",\n+                    \"-r\",\n+                    str(self.project_root / \"sparkforge\"),\n+                    \"-f\",\n+                    \"json\",\n+                    \"-ll\",  # Low confidence, low severity\n+                ],\n+                capture_output=True,\n+                text=True,\n+                cwd=self.project_root,\n+            )\n+\n             if result.returncode != 0:\n-                return {\n-                    \"success\": False,\n-                    \"error\": result.stderr,\n-                    \"vulnerabilities\": []\n-                }\n-            \n+                return {\"success\": False, \"error\": result.stderr, \"vulnerabilities\": []}\n+\n             bandit_data = json.loads(result.stdout)\n             vulnerabilities = []\n-            \n+\n             for issue in bandit_data.get(\"results\", []):\n                 vulnerability = VulnerabilityReport(\n                     vulnerability_id=issue.get(\"test_id\", \"unknown\"),\n                     severity=issue.get(\"issue_severity\", \"medium\").lower(),\n                     description=issue.get(\"issue_text\", \"No description\"),\n                     file_path=issue.get(\"filename\", \"\"),\n                     line_number=issue.get(\"line_number\", 0),\n                     cwe_id=issue.get(\"issue_cwe\", {}).get(\"id\"),\n                     remediation=issue.get(\"issue_text\", \"No remediation provided\"),\n-                    confidence=issue.get(\"issue_confidence\", \"medium\").lower()\n+                    confidence=issue.get(\"issue_confidence\", \"medium\").lower(),\n                 )\n                 vulnerabilities.append(vulnerability)\n-            \n+\n             return {\n                 \"success\": True,\n                 \"vulnerabilities\": [v.__dict__ for v in vulnerabilities],\n                 \"metrics\": bandit_data.get(\"metrics\", {}),\n                 \"scanner\": \"bandit\",\n-                \"version\": self._get_bandit_version()\n+                \"version\": self._get_bandit_version(),\n             }\n-            \n+\n         except Exception as e:\n-            return {\n-                \"success\": False,\n-                \"error\": str(e),\n-                \"vulnerabilities\": []\n-            }\n-    \n+            return {\"success\": False, \"error\": str(e), \"vulnerabilities\": []}\n+\n     def scan_dependencies(self) -> Dict[str, Any]:\n         \"\"\"Scan dependencies for known vulnerabilities.\"\"\"\n         try:\n             # Run safety check\n-            safety_result = subprocess.run([\n-                sys.executable, \"-m\", \"safety\", \"check\", \"--json\"\n-            ], capture_output=True, text=True, cwd=self.project_root)\n-            \n+            safety_result = subprocess.run(\n+                [sys.executable, \"-m\", \"safety\", \"check\", \"--json\"],\n+                capture_output=True,\n+                text=True,\n+                cwd=self.project_root,\n+            )\n+\n             vulnerabilities = []\n             if safety_result.stdout:\n                 try:\n                     safety_data = json.loads(safety_result.stdout)\n                     for vuln in safety_data:\n                         vulnerability = VulnerabilityReport(\n                             vulnerability_id=vuln.get(\"id\", \"unknown\"),\n-                            severity=self._map_safety_severity(vuln.get(\"severity\", \"medium\")),\n+                            severity=self._map_safety_severity(\n+                                vuln.get(\"severity\", \"medium\")\n+                            ),\n                             description=vuln.get(\"advisory\", \"No description\"),\n                             file_path=\"requirements.txt\",\n                             line_number=0,\n                             cve_id=vuln.get(\"cve\"),\n-                            remediation=f\"Update {vuln.get('package')} to version {vuln.get('spec', 'latest')}\"\n+                            remediation=f\"Update {vuln.get('package')} to version {vuln.get('spec', 'latest')}\",\n                         )\n                         vulnerabilities.append(vulnerability)\n                 except json.JSONDecodeError:\n                     pass\n-            \n+\n             # Run pip-audit if available\n             audit_vulnerabilities = []\n             try:\n-                audit_result = subprocess.run([\n-                    sys.executable, \"-m\", \"pip\", \"audit\", \"--json\"\n-                ], capture_output=True, text=True, cwd=self.project_root)\n-                \n+                audit_result = subprocess.run(\n+                    [sys.executable, \"-m\", \"pip\", \"audit\", \"--json\"],\n+                    capture_output=True,\n+                    text=True,\n+                    cwd=self.project_root,\n+                )\n+\n                 if audit_result.returncode == 0 and audit_result.stdout:\n                     audit_data = json.loads(audit_result.stdout)\n                     for vuln in audit_data.get(\"vulnerabilities\", []):\n                         vulnerability = VulnerabilityReport(\n                             vulnerability_id=vuln.get(\"id\", \"unknown\"),\n                             severity=\"high\",  # pip-audit typically reports high severity\n                             description=vuln.get(\"description\", \"No description\"),\n                             file_path=\"requirements.txt\",\n                             line_number=0,\n                             cve_id=vuln.get(\"id\"),\n-                            remediation=f\"Update package to fix vulnerability\"\n+                            remediation=f\"Update package to fix vulnerability\",\n                         )\n                         audit_vulnerabilities.append(vulnerability)\n             except Exception:\n                 pass  # pip-audit not available\n-            \n+\n             all_vulnerabilities = vulnerabilities + audit_vulnerabilities\n-            \n+\n             return {\n                 \"success\": True,\n                 \"vulnerabilities\": [v.__dict__ for v in all_vulnerabilities],\n                 \"scanner\": \"safety + pip-audit\",\n-                \"vulnerable_packages\": len(set(v.package for v in all_vulnerabilities if hasattr(v, 'package')))\n+                \"vulnerable_packages\": len(\n+                    set(v.package for v in all_vulnerabilities if hasattr(v, \"package\"))\n+                ),\n             }\n-            \n+\n         except Exception as e:\n-            return {\n-                \"success\": False,\n-                \"error\": str(e),\n-                \"vulnerabilities\": []\n-            }\n-    \n+            return {\"success\": False, \"error\": str(e), \"vulnerabilities\": []}\n+\n     def scan_configuration(self) -> Dict[str, Any]:\n         \"\"\"Scan configuration files for security issues.\"\"\"\n         vulnerabilities = []\n-        \n+\n         # Check for hardcoded secrets in configuration files\n         config_files = [\n             \"pyproject.toml\",\n             \"setup.py\",\n             \"requirements.txt\",\n             \"conf.py\",\n             \"*.ini\",\n             \"*.cfg\",\n             \"*.yml\",\n-            \"*.yaml\"\n+            \"*.yaml\",\n         ]\n-        \n+\n         for pattern in config_files:\n             for config_file in self.project_root.glob(pattern):\n                 if config_file.is_file():\n                     vulns = self._scan_config_file(config_file)\n                     vulnerabilities.extend(vulns)\n-        \n+\n         # Check for insecure default configurations\n         vulns = self._check_default_configurations()\n         vulnerabilities.extend(vulns)\n-        \n+\n         return {\n             \"success\": True,\n             \"vulnerabilities\": [v.__dict__ for v in vulnerabilities],\n-            \"scanner\": \"custom_config_scanner\"\n+            \"scanner\": \"custom_config_scanner\",\n         }\n-    \n+\n     def _scan_config_file(self, config_file: Path) -> List[VulnerabilityReport]:\n         \"\"\"Scan individual configuration file for security issues.\"\"\"\n         vulnerabilities = []\n-        \n+\n         try:\n-            with open(config_file, 'r', encoding='utf-8') as f:\n+            with open(config_file, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n-            \n+\n             # Check for hardcoded secrets\n             secret_patterns = [\n                 (\"password\", \"Hardcoded password detected\"),\n                 (\"secret\", \"Hardcoded secret detected\"),\n                 (\"key\", \"Hardcoded key detected\"),\n                 (\"token\", \"Hardcoded token detected\"),\n                 (\"api_key\", \"Hardcoded API key detected\"),\n-                (\"private_key\", \"Hardcoded private key detected\")\n+                (\"private_key\", \"Hardcoded private key detected\"),\n             ]\n-            \n+\n             for pattern, description in secret_patterns:\n                 if pattern in content.lower() and \"=\" in content:\n                     # More sophisticated check for actual secrets\n-                    lines = content.split('\\n')\n+                    lines = content.split(\"\\n\")\n                     for i, line in enumerate(lines):\n-                        if pattern in line.lower() and \"=\" in line and not any(\n-                            safe_word in line.lower() for safe_word in [\"example\", \"template\", \"placeholder\", \"your_\"]\n+                        if (\n+                            pattern in line.lower()\n+                            and \"=\" in line\n+                            and not any(\n+                                safe_word in line.lower()\n+                                for safe_word in [\n+                                    \"example\",\n+                                    \"template\",\n+                                    \"placeholder\",\n+                                    \"your_\",\n+                                ]\n+                            )\n                         ):\n                             vulnerability = VulnerabilityReport(\n                                 vulnerability_id=\"hardcoded_secret\",\n                                 severity=\"high\",\n                                 description=f\"{description} in {config_file.name}\",\n                                 file_path=str(config_file),\n                                 line_number=i + 1,\n-                                remediation=\"Use environment variables or secure configuration management\"\n+                                remediation=\"Use environment variables or secure configuration management\",\n                             )\n                             vulnerabilities.append(vulnerability)\n-            \n+\n         except Exception:\n             pass  # Skip files that can't be read\n-        \n+\n         return vulnerabilities\n-    \n+\n     def _check_default_configurations(self) -> List[VulnerabilityReport]:\n         \"\"\"Check for insecure default configurations.\"\"\"\n         vulnerabilities = []\n-        \n+\n         # Check pyproject.toml for insecure defaults\n         pyproject_file = self.project_root / \"pyproject.toml\"\n         if pyproject_file.exists():\n             try:\n-                with open(pyproject_file, 'r') as f:\n+                with open(pyproject_file, \"r\") as f:\n                     content = f.read()\n-                \n+\n                 # Check for debug mode enabled\n                 if \"debug = true\" in content.lower():\n                     vulnerability = VulnerabilityReport(\n                         vulnerability_id=\"debug_mode_enabled\",\n                         severity=\"medium\",\n                         description=\"Debug mode enabled in production configuration\",\n                         file_path=str(pyproject_file),\n                         line_number=content.lower().find(\"debug = true\") + 1,\n-                        remediation=\"Disable debug mode in production\"\n+                        remediation=\"Disable debug mode in production\",\n                     )\n                     vulnerabilities.append(vulnerability)\n-                \n+\n                 # Check for insecure HTTP URLs\n                 if \"http://\" in content and \"https://\" not in content:\n                     vulnerability = VulnerabilityReport(\n                         vulnerability_id=\"insecure_http\",\n                         severity=\"medium\",\n                         description=\"Insecure HTTP URL detected\",\n                         file_path=str(pyproject_file),\n                         line_number=content.find(\"http://\") + 1,\n-                        remediation=\"Use HTTPS URLs instead of HTTP\"\n+                        remediation=\"Use HTTPS URLs instead of HTTP\",\n                     )\n                     vulnerabilities.append(vulnerability)\n-                    \n+\n             except Exception:\n                 pass\n-        \n+\n         return vulnerabilities\n-    \n+\n     def scan_secrets(self) -> Dict[str, Any]:\n         \"\"\"Scan for accidentally committed secrets.\"\"\"\n         vulnerabilities = []\n-        \n+\n         # Common secret patterns\n         secret_patterns = [\n             (r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', \"Hardcoded password\"),\n             (r'secret\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', \"Hardcoded secret\"),\n             (r'api_key\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', \"Hardcoded API key\"),\n             (r'private_key\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', \"Hardcoded private key\"),\n             (r'token\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', \"Hardcoded token\"),\n-            (r'sk_live_[a-zA-Z0-9]{24}', \"Stripe live key\"),\n-            (r'pk_live_[a-zA-Z0-9]{24}', \"Stripe live publishable key\"),\n-            (r'AKIA[0-9A-Z]{16}', \"AWS access key\"),\n-            (r'[0-9a-zA-Z+/]{40}', \"Potential AWS secret key\"),\n-            (r'-----BEGIN PRIVATE KEY-----', \"Private key in code\"),\n-            (r'-----BEGIN RSA PRIVATE KEY-----', \"RSA private key in code\")\n+            (r\"sk_live_[a-zA-Z0-9]{24}\", \"Stripe live key\"),\n+            (r\"pk_live_[a-zA-Z0-9]{24}\", \"Stripe live publishable key\"),\n+            (r\"AKIA[0-9A-Z]{16}\", \"AWS access key\"),\n+            (r\"[0-9a-zA-Z+/]{40}\", \"Potential AWS secret key\"),\n+            (r\"-----BEGIN PRIVATE KEY-----\", \"Private key in code\"),\n+            (r\"-----BEGIN RSA PRIVATE KEY-----\", \"RSA private key in code\"),\n         ]\n-        \n+\n         # Scan Python files\n         for py_file in self.project_root.rglob(\"*.py\"):\n-            if py_file.is_file() and not any(part.startswith('.') for part in py_file.parts):\n+            if py_file.is_file() and not any(\n+                part.startswith(\".\") for part in py_file.parts\n+            ):\n                 try:\n-                    with open(py_file, 'r', encoding='utf-8') as f:\n+                    with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                         content = f.read()\n-                    \n+\n                     for pattern, description in secret_patterns:\n                         import re\n+\n                         matches = re.finditer(pattern, content, re.IGNORECASE)\n                         for match in matches:\n                             # Check if it's not in a comment or test\n-                            line_start = content.rfind('\\n', 0, match.start()) + 1\n-                            line_content = content[line_start:match.end()]\n-                            \n-                            if not (line_content.strip().startswith('#') or 'test' in py_file.name.lower()):\n+                            line_start = content.rfind(\"\\n\", 0, match.start()) + 1\n+                            line_content = content[line_start : match.end()]\n+\n+                            if not (\n+                                line_content.strip().startswith(\"#\")\n+                                or \"test\" in py_file.name.lower()\n+                            ):\n                                 vulnerability = VulnerabilityReport(\n                                     vulnerability_id=\"hardcoded_secret\",\n                                     severity=\"high\",\n                                     description=f\"{description} in {py_file.name}\",\n                                     file_path=str(py_file),\n-                                    line_number=content[:match.start()].count('\\n') + 1,\n-                                    remediation=\"Remove hardcoded secret and use environment variables\"\n+                                    line_number=content[: match.start()].count(\"\\n\")\n+                                    + 1,\n+                                    remediation=\"Remove hardcoded secret and use environment variables\",\n                                 )\n                                 vulnerabilities.append(vulnerability)\n-                                \n+\n                 except Exception:\n                     pass  # Skip files that can't be read\n-        \n+\n         return {\n             \"success\": True,\n             \"vulnerabilities\": [v.__dict__ for v in vulnerabilities],\n-            \"scanner\": \"custom_secrets_scanner\"\n+            \"scanner\": \"custom_secrets_scanner\",\n         }\n-    \n+\n     def scan_licenses(self) -> Dict[str, Any]:\n         \"\"\"Scan for license compliance issues.\"\"\"\n         try:\n             # Run pip-licenses to check licenses\n-            result = subprocess.run([\n-                sys.executable, \"-m\", \"pip-licenses\", \"--format=json\"\n-            ], capture_output=True, text=True, cwd=self.project_root)\n-            \n+            result = subprocess.run(\n+                [sys.executable, \"-m\", \"pip-licenses\", \"--format=json\"],\n+                capture_output=True,\n+                text=True,\n+                cwd=self.project_root,\n+            )\n+\n             if result.returncode != 0:\n                 return {\n                     \"success\": False,\n                     \"error\": \"pip-licenses not available\",\n-                    \"vulnerabilities\": []\n+                    \"vulnerabilities\": [],\n                 }\n-            \n+\n             licenses_data = json.loads(result.stdout)\n-            \n+\n             # Check for problematic licenses\n             problematic_licenses = [\n-                \"GPL-2.0\", \"GPL-3.0\", \"AGPL-3.0\", \"Copyleft\",\n-                \"Commercial\", \"Proprietary\", \"Unknown\"\n+                \"GPL-2.0\",\n+                \"GPL-3.0\",\n+                \"AGPL-3.0\",\n+                \"Copyleft\",\n+                \"Commercial\",\n+                \"Proprietary\",\n+                \"Unknown\",\n             ]\n-            \n+\n             vulnerabilities = []\n             for package in licenses_data:\n                 license_name = package.get(\"License\", \"Unknown\")\n-                if any(prob_license in license_name for prob_license in problematic_licenses):\n+                if any(\n+                    prob_license in license_name\n+                    for prob_license in problematic_licenses\n+                ):\n                     vulnerability = VulnerabilityReport(\n                         vulnerability_id=\"license_compliance\",\n                         severity=\"medium\",\n                         description=f\"Potentially problematic license: {license_name}\",\n                         file_path=\"requirements.txt\",\n                         line_number=0,\n-                        remediation=f\"Review license compatibility for {package.get('Name', 'package')}\"\n+                        remediation=f\"Review license compatibility for {package.get('Name', 'package')}\",\n                     )\n                     vulnerabilities.append(vulnerability)\n-            \n+\n             return {\n                 \"success\": True,\n                 \"vulnerabilities\": [v.__dict__ for v in vulnerabilities],\n                 \"scanner\": \"pip-licenses\",\n-                \"total_packages\": len(licenses_data)\n+                \"total_packages\": len(licenses_data),\n             }\n-            \n+\n         except Exception as e:\n-            return {\n-                \"success\": False,\n-                \"error\": str(e),\n-                \"vulnerabilities\": []\n-            }\n-    \n+            return {\"success\": False, \"error\": str(e), \"vulnerabilities\": []}\n+\n     def scan_supply_chain(self) -> Dict[str, Any]:\n         \"\"\"Scan for supply chain security issues.\"\"\"\n         vulnerabilities = []\n-        \n+\n         # Check for pinned dependencies\n         requirements_file = self.project_root / \"requirements.txt\"\n         if requirements_file.exists():\n             try:\n-                with open(requirements_file, 'r') as f:\n+                with open(requirements_file, \"r\") as f:\n                     requirements = f.read()\n-                \n+\n                 unpinned_deps = []\n-                for line in requirements.split('\\n'):\n+                for line in requirements.split(\"\\n\"):\n                     line = line.strip()\n-                    if line and not line.startswith('#'):\n-                        if '==' not in line and '>=' not in line and '<=' not in line and '~=' not in line:\n-                            unpinned_deps.append(line.split('==')[0])\n-                \n+                    if line and not line.startswith(\"#\"):\n+                        if (\n+                            \"==\" not in line\n+                            and \">=\" not in line\n+                            and \"<=\" not in line\n+                            and \"~=\" not in line\n+                        ):\n+                            unpinned_deps.append(line.split(\"==\")[0])\n+\n                 if unpinned_deps:\n                     vulnerability = VulnerabilityReport(\n                         vulnerability_id=\"unpinned_dependencies\",\n                         severity=\"medium\",\n                         description=f\"Unpinned dependencies detected: {', '.join(unpinned_deps)}\",\n                         file_path=str(requirements_file),\n                         line_number=0,\n-                        remediation=\"Pin all dependencies to specific versions\"\n+                        remediation=\"Pin all dependencies to specific versions\",\n                     )\n                     vulnerabilities.append(vulnerability)\n-                    \n+\n             except Exception:\n                 pass\n-        \n+\n         return {\n             \"success\": True,\n             \"vulnerabilities\": [v.__dict__ for v in vulnerabilities],\n-            \"scanner\": \"supply_chain_scanner\"\n+            \"scanner\": \"supply_chain_scanner\",\n         }\n-    \n+\n     def _map_safety_severity(self, severity: str) -> str:\n         \"\"\"Map safety severity to standard severity levels.\"\"\"\n         severity_map = {\n             \"high\": \"high\",\n-            \"medium\": \"medium\", \n+            \"medium\": \"medium\",\n             \"low\": \"low\",\n-            \"info\": \"info\"\n+            \"info\": \"info\",\n         }\n         return severity_map.get(severity.lower(), \"medium\")\n-    \n+\n     def _get_bandit_version(self) -> str:\n         \"\"\"Get bandit version.\"\"\"\n         try:\n-            result = subprocess.run([sys.executable, \"-m\", \"bandit\", \"--version\"], \n-                                  capture_output=True, text=True)\n+            result = subprocess.run(\n+                [sys.executable, \"-m\", \"bandit\", \"--version\"],\n+                capture_output=True,\n+                text=True,\n+            )\n             return result.stdout.strip()\n         except Exception:\n             return \"unknown\"\n-    \n+\n     def _generate_summary(self) -> Dict[str, Any]:\n         \"\"\"Generate security scan summary.\"\"\"\n         if not self.security_metrics:\n             return {}\n-        \n+\n         return {\n             \"total_vulnerabilities\": self.security_metrics.total_vulnerabilities,\n             \"severity_breakdown\": {\n                 \"high\": self.security_metrics.high_severity,\n                 \"medium\": self.security_metrics.medium_severity,\n                 \"low\": self.security_metrics.low_severity,\n-                \"info\": self.security_metrics.info_severity\n+                \"info\": self.security_metrics.info_severity,\n             },\n             \"risk_score\": self._calculate_risk_score(),\n             \"scan_duration\": self.security_metrics.scan_duration,\n             \"scan_timestamp\": self.security_metrics.scan_timestamp.isoformat(),\n-            \"recommendations\": self._generate_recommendations()\n+            \"recommendations\": self._generate_recommendations(),\n         }\n-    \n+\n     def _calculate_risk_score(self) -> float:\n         \"\"\"Calculate overall risk score (0-100, higher is worse).\"\"\"\n         if not self.security_metrics:\n             return 0.0\n-        \n+\n         # Weighted risk calculation\n         high_weight = 10\n         medium_weight = 5\n         low_weight = 2\n         info_weight = 1\n-        \n+\n         total_weighted = (\n-            self.security_metrics.high_severity * high_weight +\n-            self.security_metrics.medium_severity * medium_weight +\n-            self.security_metrics.low_severity * low_weight +\n-            self.security_metrics.info_severity * info_weight\n+            self.security_metrics.high_severity * high_weight\n+            + self.security_metrics.medium_severity * medium_weight\n+            + self.security_metrics.low_severity * low_weight\n+            + self.security_metrics.info_severity * info_weight\n         )\n-        \n+\n         # Normalize to 0-100 scale (max reasonable score is 50)\n         return min(100.0, (total_weighted / 50.0) * 100.0)\n-    \n+\n     def _generate_recommendations(self) -> List[str]:\n         \"\"\"Generate security recommendations based on scan results.\"\"\"\n         recommendations = []\n-        \n+\n         if not self.security_metrics:\n             return recommendations\n-        \n+\n         if self.security_metrics.high_severity > 0:\n             recommendations.append(\"Address high severity vulnerabilities immediately\")\n-        \n+\n         if self.security_metrics.medium_severity > 5:\n             recommendations.append(\"Review and fix medium severity vulnerabilities\")\n-        \n+\n         if self.security_metrics.total_vulnerabilities > 20:\n-            recommendations.append(\"Consider implementing automated security testing in CI/CD\")\n-        \n-        recommendations.extend([\n-            \"Regularly update dependencies to latest secure versions\",\n-            \"Implement security code review process\",\n-            \"Use dependency vulnerability scanning in CI/CD pipeline\",\n-            \"Consider using a secrets management solution\",\n-            \"Implement runtime security monitoring\"\n-        ])\n-        \n+            recommendations.append(\n+                \"Consider implementing automated security testing in CI/CD\"\n+            )\n+\n+        recommendations.extend(\n+            [\n+                \"Regularly update dependencies to latest secure versions\",\n+                \"Implement security code review process\",\n+                \"Use dependency vulnerability scanning in CI/CD pipeline\",\n+                \"Consider using a secrets management solution\",\n+                \"Implement runtime security monitoring\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n+\n     def generate_report(self, output_file: Optional[Path] = None) -> Path:\n         \"\"\"Generate detailed security report.\"\"\"\n         scan_results = self.scan_all()\n-        \n+\n         if output_file is None:\n             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n             output_file = self.project_root / f\"security_report_{timestamp}.json\"\n-        \n-        with open(output_file, 'w') as f:\n+\n+        with open(output_file, \"w\") as f:\n             json.dump(scan_results, f, indent=2, default=str)\n-        \n+\n         return output_file\n \n \n # CLI interface\n if __name__ == \"__main__\":\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(description=\"SparkForge Vulnerability Scanner\")\n     parser.add_argument(\"--project-root\", type=Path, help=\"Project root directory\")\n     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n-    parser.add_argument(\"--format\", choices=[\"json\", \"html\"], default=\"json\", help=\"Output format\")\n-    \n+    parser.add_argument(\n+        \"--format\", choices=[\"json\", \"html\"], default=\"json\", help=\"Output format\"\n+    )\n+\n     args = parser.parse_args()\n-    \n+\n     scanner = VulnerabilityScanner(args.project_root)\n     report_file = scanner.generate_report(args.output)\n-    \n+\n     print(f\"Security scan completed. Report saved to: {report_file}\")\n-    \n+\n     # Print summary\n     if scanner.security_metrics:\n         print(f\"\\nSecurity Summary:\")\n-        print(f\"Total vulnerabilities: {scanner.security_metrics.total_vulnerabilities}\")\n+        print(\n+            f\"Total vulnerabilities: {scanner.security_metrics.total_vulnerabilities}\"\n+        )\n         print(f\"High severity: {scanner.security_metrics.high_severity}\")\n         print(f\"Medium severity: {scanner.security_metrics.medium_severity}\")\n         print(f\"Low severity: {scanner.security_metrics.low_severity}\")\n         print(f\"Risk score: {scanner._calculate_risk_score():.1f}/100\")\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_additional_coverage.py\t2025-09-20 14:42:20.268157+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_additional_coverage.py\t2025-09-20 16:53:35.099367+00:00\n@@ -29,10 +29,11 @@\n class TestProtocolImplementations:\n     \"\"\"Test protocol implementations and edge cases.\"\"\"\n \n     def test_validatable_protocol_implementation(self) -> None:\n         \"\"\"Test Validatable protocol implementation.\"\"\"\n+\n         # Test that objects implementing Validatable can be validated\n         class MockValidatable:\n             def validate(self) -> None:\n                 pass\n \n@@ -40,10 +41,11 @@\n         assert hasattr(obj, \"validate\")\n         assert callable(obj.validate)\n \n     def test_serializable_protocol_implementation(self) -> None:\n         \"\"\"Test Serializable protocol implementation.\"\"\"\n+\n         # Test that objects implementing Serializable can be serialized\n         class MockSerializable:\n             def to_dict(self) -> Dict[str, Any]:\n                 return {\"test\": \"value\"}\n \n@@ -61,11 +63,11 @@\n     \"\"\"Test PipelineConfig property methods.\"\"\"\n \n     def test_pipeline_config_properties(self) -> None:\n         \"\"\"Test PipelineConfig property access.\"\"\"\n         config = PipelineConfig.create_default(\"test_schema\")\n-        \n+\n         # Test property access\n         assert isinstance(config.min_gold_rate, float)\n         assert isinstance(config.enable_parallel_silver, bool)\n         assert isinstance(config.max_parallel_workers, int)\n         assert isinstance(config.enable_caching, bool)\n@@ -76,24 +78,20 @@\n     \"\"\"Test ValidationThresholds edge cases and error paths.\"\"\"\n \n     def test_validation_thresholds_get_threshold_invalid_type(self) -> None:\n         \"\"\"Test get_threshold with invalid phase type.\"\"\"\n         thresholds = ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n-        \n+\n         # This should handle invalid types gracefully\n         with pytest.raises(KeyError):\n             thresholds.get_threshold(\"invalid_phase\")  # type: ignore\n \n     def test_validation_thresholds_edge_values(self) -> None:\n         \"\"\"Test ValidationThresholds with edge values.\"\"\"\n         # Test with minimum and maximum values\n-        thresholds = ValidationThresholds(\n-            bronze=0.0,\n-            silver=50.0,\n-            gold=100.0\n-        )\n-        \n+        thresholds = ValidationThresholds(bronze=0.0, silver=50.0, gold=100.0)\n+\n         assert thresholds.bronze == 0.0\n         assert thresholds.silver == 50.0\n         assert thresholds.gold == 100.0\n \n \n@@ -101,29 +99,21 @@\n     \"\"\"Test ParallelConfig edge cases and validation.\"\"\"\n \n     def test_parallel_config_edge_values(self) -> None:\n         \"\"\"Test ParallelConfig with edge values.\"\"\"\n         # Test with minimum values\n-        config = ParallelConfig(\n-            enabled=True,\n-            max_workers=1,\n-            timeout_secs=1\n-        )\n-        \n+        config = ParallelConfig(enabled=True, max_workers=1, timeout_secs=1)\n+\n         assert config.enabled is True\n         assert config.max_workers == 1\n         assert config.timeout_secs == 1\n \n     def test_parallel_config_max_values(self) -> None:\n         \"\"\"Test ParallelConfig with maximum values.\"\"\"\n         # Test with maximum values\n-        config = ParallelConfig(\n-            enabled=True,\n-            max_workers=32,\n-            timeout_secs=3600\n-        )\n-        \n+        config = ParallelConfig(enabled=True, max_workers=32, timeout_secs=3600)\n+\n         assert config.max_workers == 32\n         assert config.timeout_secs == 3600\n \n \n class TestBronzeStepEdgeCases:\n@@ -135,64 +125,66 @@\n         step = BronzeStep(\n             name=\"test_step\",\n             rules={\"id\": [\"not_null\"]},\n             incremental_col=\"\",  # Empty string should be valid\n         )\n-        \n+\n         assert step.incremental_col == \"\"\n         assert step.has_incremental_capability is True\n \n     def test_bronze_step_validation_edge_cases(self) -> None:\n         \"\"\"Test BronzeStep validation edge cases.\"\"\"\n         # Test validation with edge case values\n         step = BronzeStep(\n             name=\"test_step\",\n             rules={\"id\": [\"not_null\"]},\n         )\n-        \n+\n         # This should not raise an error\n         step.validate()\n \n \n class TestSilverStepEdgeCases:\n     \"\"\"Test SilverStep edge cases and validation.\"\"\"\n \n     def test_silver_step_validation_edge_cases(self) -> None:\n         \"\"\"Test SilverStep validation edge cases.\"\"\"\n+\n         # Create a mock transform function\n         def mock_transform(spark, df, bronze_dfs):\n             return df\n-        \n+\n         step = SilverStep(\n             name=\"test_step\",\n             source_bronze=\"test_bronze\",\n             rules={\"id\": [\"not_null\"]},\n             transform=mock_transform,\n-            table_name=\"test_table\"\n+            table_name=\"test_table\",\n         )\n-        \n+\n         # This should not raise an error\n         step.validate()\n \n \n class TestGoldStepEdgeCases:\n     \"\"\"Test GoldStep edge cases and validation.\"\"\"\n \n     def test_gold_step_validation_edge_cases(self) -> None:\n         \"\"\"Test GoldStep validation edge cases.\"\"\"\n+\n         # Create a mock transform function\n         def mock_transform(spark, silver_dfs):\n             return list(silver_dfs.values())[0] if silver_dfs else None\n-        \n+\n         step = GoldStep(\n             name=\"test_step\",\n             source_silvers=[\"test_silver1\", \"test_silver2\"],\n             rules={\"id\": [\"not_null\"]},\n             transform=mock_transform,\n-            table_name=\"test_table\"\n+            table_name=\"test_table\",\n         )\n-        \n+\n         # This should not raise an error\n         step.validate()\n \n \n class TestSilverDependencyInfoEdgeCases:\n@@ -203,40 +195,41 @@\n         info = SilverDependencyInfo(\n             step_name=\"test_step\",\n             source_bronze=\"test_bronze\",\n             depends_on_silvers=set(),\n             can_run_parallel=True,\n-            execution_group=1\n+            execution_group=1,\n         )\n-        \n+\n         # This should not raise an error\n         info.validate()\n \n \n class TestBaseModelEdgeCases:\n     \"\"\"Test BaseModel edge cases and serialization.\"\"\"\n \n     def test_base_model_serialization_edge_cases(self) -> None:\n         \"\"\"Test BaseModel serialization edge cases.\"\"\"\n+\n         # Create a concrete implementation for testing\n         class TestModel(BaseModel):\n             def __init__(self, name: str, value: Any):\n                 self.name = name\n                 self.value = value\n-            \n+\n             def validate(self) -> None:\n                 pass\n-        \n+\n         model = TestModel(\"test\", {\"nested\": {\"value\": 123}})\n-        \n+\n         # Test serialization\n         result_dict = model.to_dict()\n         assert isinstance(result_dict, dict)\n-        \n+\n         result_json = model.to_json()\n         assert isinstance(result_json, str)\n-        \n+\n         # Test string representation\n         str_repr = str(model)\n         assert isinstance(str_repr, str)\n \n \n@@ -281,21 +274,21 @@\n             SilverStep(\n                 name=\"test_step\",\n                 source_bronze=\"\",\n                 rules={\"id\": [\"not_null\"]},\n                 transform=lambda x: x,  # type: ignore\n-                table_name=\"test_table\"\n+                table_name=\"test_table\",\n             ).validate()\n \n         # Test with None transform\n         with pytest.raises(PipelineValidationError):\n             SilverStep(\n                 name=\"test_step\",\n                 source_bronze=\"test_bronze\",\n                 rules={\"id\": [\"not_null\"]},\n                 transform=None,  # type: ignore\n-                table_name=\"test_table\"\n+                table_name=\"test_table\",\n             ).validate()\n \n     def test_gold_step_validation_error_paths(self) -> None:\n         \"\"\"Test GoldStep validation error paths.\"\"\"\n         # Test with None transform\n@@ -303,21 +296,21 @@\n             GoldStep(\n                 name=\"test_step\",\n                 source_silvers=[\"test_silver\"],\n                 rules={\"id\": [\"not_null\"]},\n                 transform=None,  # type: ignore\n-                table_name=\"test_table\"\n+                table_name=\"test_table\",\n             ).validate()\n \n         # Test with empty source_silvers\n         with pytest.raises(PipelineValidationError):\n             GoldStep(\n                 name=\"test_step\",\n                 source_silvers=[],\n                 rules={\"id\": [\"not_null\"]},\n                 transform=lambda x: x,  # type: ignore\n-                table_name=\"test_table\"\n+                table_name=\"test_table\",\n             ).validate()\n \n         # Test with None source_silvers (this is actually valid, so we skip this test)\n         # None source_silvers means use all available silver steps, which is valid\n \n@@ -326,11 +319,11 @@\n             GoldStep(\n                 name=\"test_step\",\n                 source_silvers=\"invalid\",  # type: ignore\n                 rules={\"id\": [\"not_null\"]},\n                 transform=lambda x: x,  # type: ignore\n-                table_name=\"test_table\"\n+                table_name=\"test_table\",\n             ).validate()\n \n     def test_silver_dependency_info_validation_error_paths(self) -> None:\n         \"\"\"Test SilverDependencyInfo validation error paths.\"\"\"\n         # Test with empty step_name\n@@ -338,21 +331,21 @@\n             SilverDependencyInfo(\n                 step_name=\"\",\n                 source_bronze=\"test_bronze\",\n                 depends_on_silvers=set(),\n                 can_run_parallel=True,\n-                execution_group=1\n+                execution_group=1,\n             ).validate()\n \n         # Test with empty source_bronze\n         with pytest.raises(PipelineValidationError):\n             SilverDependencyInfo(\n                 step_name=\"test_step\",\n                 source_bronze=\"\",\n                 depends_on_silvers=set(),\n                 can_run_parallel=True,\n-                execution_group=1\n+                execution_group=1,\n             ).validate()\n \n \n class TestPipelineConfigValidation:\n     \"\"\"Test PipelineConfig validation edge cases.\"\"\"\n@@ -362,27 +355,27 @@\n         # Test with None schema\n         with pytest.raises(PipelineValidationError):\n             PipelineConfig(\n                 schema=None,  # type: ignore\n                 thresholds=ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0),\n-                parallel=ParallelConfig(enabled=True, max_workers=4)\n+                parallel=ParallelConfig(enabled=True, max_workers=4),\n             ).validate()\n \n         # Test with empty schema\n         with pytest.raises(PipelineValidationError):\n             PipelineConfig(\n                 schema=\"\",\n                 thresholds=ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0),\n-                parallel=ParallelConfig(enabled=True, max_workers=4)\n+                parallel=ParallelConfig(enabled=True, max_workers=4),\n             ).validate()\n \n         # Test with invalid schema type\n         with pytest.raises(PipelineValidationError):\n             PipelineConfig(\n                 schema=123,  # type: ignore\n                 thresholds=ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0),\n-                parallel=ParallelConfig(enabled=True, max_workers=4)\n+                parallel=ParallelConfig(enabled=True, max_workers=4),\n             ).validate()\n \n \n class TestModelFactoryMethods:\n     \"\"\"Test model factory methods and creation patterns.\"\"\"\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_final_coverage.py\t2025-09-20 15:48:38.907534+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_final_coverage.py\t2025-09-20 16:53:35.110977+00:00\n@@ -31,35 +31,35 @@\n     \"\"\"Test remaining edge cases and error paths for models.py.\"\"\"\n \n     def test_validatable_protocol_definition(self) -> None:\n         \"\"\"Test Validatable protocol definition.\"\"\"\n         # Test that Validatable protocol is properly defined\n-        assert hasattr(Validatable, 'validate')\n-        \n+        assert hasattr(Validatable, \"validate\")\n+\n         # Test implementing the protocol\n         class MockValidatable:\n             def validate(self) -> None:\n                 pass\n-        \n+\n         # Should be able to use as Validatable\n         validator: Validatable = MockValidatable()\n         validator.validate()\n \n     def test_serializable_protocol_definition(self) -> None:\n         \"\"\"Test Serializable protocol definition.\"\"\"\n         # Test that Serializable protocol is properly defined\n-        assert hasattr(Serializable, 'to_dict')\n-        assert hasattr(Serializable, 'to_json')\n-        \n+        assert hasattr(Serializable, \"to_dict\")\n+        assert hasattr(Serializable, \"to_json\")\n+\n         # Test implementing the protocol\n         class MockSerializable:\n             def to_dict(self) -> Dict[str, ModelValue]:\n                 return {\"test\": \"value\"}\n-            \n+\n             def to_json(self) -> str:\n                 return '{\"test\": \"value\"}'\n-        \n+\n         # Should be able to use as Serializable\n         serializer: Serializable = MockSerializable()\n         assert serializer.to_dict() == {\"test\": \"value\"}\n         assert serializer.to_json() == '{\"test\": \"value\"}'\n \n@@ -67,118 +67,132 @@\n         \"\"\"Test PipelineConfig validation edge cases.\"\"\"\n         # Test with empty schema - validation is called manually\n         config = PipelineConfig(\n             schema=\"\",\n             thresholds=ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0),\n-            parallel=ParallelConfig(enabled=True, max_workers=4)\n-        )\n-        with pytest.raises(PipelineValidationError, match=\"Schema name must be a non-empty string\"):\n+            parallel=ParallelConfig(enabled=True, max_workers=4),\n+        )\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Schema name must be a non-empty string\"\n+        ):\n             config.validate()\n-        \n+\n         # Test with None schema\n         config = PipelineConfig(\n             schema=None,  # type: ignore\n             thresholds=ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0),\n-            parallel=ParallelConfig(enabled=True, max_workers=4)\n-        )\n-        with pytest.raises(PipelineValidationError, match=\"Schema name must be a non-empty string\"):\n+            parallel=ParallelConfig(enabled=True, max_workers=4),\n+        )\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Schema name must be a non-empty string\"\n+        ):\n             config.validate()\n-        \n+\n         # Test with non-string schema\n         config = PipelineConfig(\n             schema=123,  # type: ignore\n             thresholds=ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0),\n-            parallel=ParallelConfig(enabled=True, max_workers=4)\n-        )\n-        with pytest.raises(PipelineValidationError, match=\"Schema name must be a non-empty string\"):\n+            parallel=ParallelConfig(enabled=True, max_workers=4),\n+        )\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Schema name must be a non-empty string\"\n+        ):\n             config.validate()\n \n     def test_bronze_step_validation_edge_cases(self) -> None:\n         \"\"\"Test BronzeStep validation edge cases.\"\"\"\n         # Test with empty name\n-        with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n-            BronzeStep(\n-                name=\"\",\n-                rules={\"col1\": [\"col1 > 0\"]},\n-                incremental_col=\"updated_at\"\n-            )\n-        \n+        with pytest.raises(\n+            PipelineValidationError, match=\"Step name must be a non-empty string\"\n+        ):\n+            BronzeStep(\n+                name=\"\", rules={\"col1\": [\"col1 > 0\"]}, incremental_col=\"updated_at\"\n+            )\n+\n         # Test with None name\n-        with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Step name must be a non-empty string\"\n+        ):\n             BronzeStep(\n                 name=None,  # type: ignore\n                 rules={\"col1\": [\"col1 > 0\"]},\n-                incremental_col=\"updated_at\"\n-            )\n-        \n+                incremental_col=\"updated_at\",\n+            )\n+\n         # Test with non-string name\n-        with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Step name must be a non-empty string\"\n+        ):\n             BronzeStep(\n                 name=123,  # type: ignore\n                 rules={\"col1\": [\"col1 > 0\"]},\n-                incremental_col=\"updated_at\"\n-            )\n-        \n+                incremental_col=\"updated_at\",\n+            )\n+\n         # Test with non-dict rules\n-        with pytest.raises(PipelineValidationError, match=\"Rules must be a non-empty dictionary\"):\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Rules must be a non-empty dictionary\"\n+        ):\n             BronzeStep(\n                 name=\"test_step\",\n                 rules=\"invalid_rules\",  # type: ignore\n-                incremental_col=\"updated_at\"\n-            )\n-        \n+                incremental_col=\"updated_at\",\n+            )\n+\n         # Test with non-string incremental_col\n-        with pytest.raises(PipelineValidationError, match=\"Incremental column must be a string\"):\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Incremental column must be a string\"\n+        ):\n             BronzeStep(\n                 name=\"test_step\",\n                 rules={\"col1\": [\"col1 > 0\"]},\n-                incremental_col=123  # type: ignore\n+                incremental_col=123,  # type: ignore\n             )\n \n     def test_bronze_step_incremental_capability(self) -> None:\n         \"\"\"Test BronzeStep incremental capability property.\"\"\"\n         # Test with incremental column\n         step = BronzeStep(\n-            name=\"test_step\",\n-            rules={\"col1\": [\"col1 > 0\"]},\n-            incremental_col=\"updated_at\"\n+            name=\"test_step\", rules={\"col1\": [\"col1 > 0\"]}, incremental_col=\"updated_at\"\n         )\n         assert step.has_incremental_capability is True\n-        \n+\n         # Test without incremental column\n         step = BronzeStep(\n-            name=\"test_step\",\n-            rules={\"col1\": [\"col1 > 0\"]},\n-            incremental_col=None\n+            name=\"test_step\", rules={\"col1\": [\"col1 > 0\"]}, incremental_col=None\n         )\n         assert step.has_incremental_capability is False\n \n     def test_silver_step_validation_edge_cases(self) -> None:\n         \"\"\"Test SilverStep validation edge cases.\"\"\"\n+\n         def mock_transform(spark, bronze_df, prior_silvers):\n             return bronze_df\n-        \n+\n         # Test with empty name\n-        with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Step name must be a non-empty string\"\n+        ):\n             SilverStep(\n                 name=\"\",\n                 source_bronze=\"bronze1\",\n                 transform=mock_transform,\n                 rules={\"col1\": [\"col1 > 0\"]},\n-                table_name=\"silver_table\"\n-            )\n-        \n+                table_name=\"silver_table\",\n+            )\n+\n         # Test with non-string name\n-        with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n+        with pytest.raises(\n+            PipelineValidationError, match=\"Step name must be a non-empty string\"\n+        ):\n             SilverStep(\n                 name=123,  # type: ignore\n                 source_bronze=\"bronze1\",\n                 transform=mock_transform,\n                 rules={\"col1\": [\"col1 > 0\"]},\n-                table_name=\"silver_table\"\n-            )\n-        \n+                table_name=\"silver_table\",\n+            )\n \n     def test_validation_thresholds_boundary_values(self) -> None:\n         \"\"\"Test ValidationThresholds with boundary values.\"\"\"\n         # Test with boundary values (0.0 and 100.0)\n         thresholds = ValidationThresholds(bronze=0.0, silver=50.0, gold=100.0)\n@@ -189,11 +203,11 @@\n     def test_parallel_config_boundary_values(self) -> None:\n         \"\"\"Test ParallelConfig with boundary values.\"\"\"\n         # Test with minimum max_workers\n         parallel = ParallelConfig(enabled=True, max_workers=1)\n         assert parallel.max_workers == 1\n-        \n+\n         # Test with large max_workers\n         parallel = ParallelConfig(enabled=True, max_workers=1000)\n         assert parallel.max_workers == 1000\n \n     def test_model_factory_methods_values(self) -> None:\n@@ -201,25 +215,25 @@\n         # Test ValidationThresholds.create_strict with actual values\n         thresholds = ValidationThresholds.create_strict()\n         assert thresholds.bronze == 99.0\n         assert thresholds.silver == 99.5\n         assert thresholds.gold == 99.9\n-        \n+\n         # Test ValidationThresholds.create_permissive with actual values (if it exists)\n-        if hasattr(ValidationThresholds, 'create_permissive'):\n+        if hasattr(ValidationThresholds, \"create_permissive\"):\n             thresholds = ValidationThresholds.create_permissive()\n             assert thresholds.bronze == 70.0\n             assert thresholds.silver == 75.0\n             assert thresholds.gold == 80.0\n-        \n+\n         # Test ParallelConfig factory methods if they exist\n-        if hasattr(ParallelConfig, 'create_conservative'):\n+        if hasattr(ParallelConfig, \"create_conservative\"):\n             parallel = ParallelConfig.create_conservative()\n             assert parallel.enabled is True\n             assert parallel.max_workers == 2\n-        \n-        if hasattr(ParallelConfig, 'create_aggressive'):\n+\n+        if hasattr(ParallelConfig, \"create_aggressive\"):\n             parallel = ParallelConfig.create_aggressive()\n             assert parallel.enabled is True\n             assert parallel.max_workers == 8\n \n     def test_model_properties_basic(self) -> None:\n@@ -234,19 +248,19 @@\n         \"\"\"Test model comparison with edge cases.\"\"\"\n         # Test ValidationThresholds comparison\n         thresholds1 = ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n         thresholds2 = ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n         thresholds3 = ValidationThresholds(bronze=75.0, silver=85.0, gold=90.0)\n-        \n+\n         assert thresholds1 == thresholds2\n         assert thresholds1 != thresholds3\n-        \n+\n         # Test ParallelConfig comparison\n         parallel1 = ParallelConfig(enabled=True, max_workers=4)\n         parallel2 = ParallelConfig(enabled=True, max_workers=4)\n         parallel3 = ParallelConfig(enabled=False, max_workers=4)\n-        \n+\n         assert parallel1 == parallel2\n         assert parallel1 != parallel3\n \n     def test_model_string_representations(self) -> None:\n         \"\"\"Test model string representations.\"\"\"\n@@ -254,11 +268,11 @@\n         thresholds = ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n         str_repr = str(thresholds)\n         assert \"bronze=80.0\" in str_repr\n         assert \"silver=85.0\" in str_repr\n         assert \"gold=90.0\" in str_repr\n-        \n+\n         # Test ParallelConfig string representation\n         parallel = ParallelConfig(enabled=True, max_workers=4)\n         str_repr = str(parallel)\n         assert \"enabled=True\" in str_repr\n         assert \"max_workers=4\" in str_repr\n@@ -268,16 +282,16 @@\n         # Test multiple validation errors in one model\n         try:\n             BronzeStep(\n                 name=\"\",  # Invalid name\n                 rules=\"invalid\",  # Invalid rules\n-                incremental_col=123  # Invalid incremental_col\n+                incremental_col=123,  # Invalid incremental_col\n             )\n         except PipelineValidationError as e:\n             # Should catch the first validation error\n             assert \"Step name must be a non-empty string\" in str(e)\n-        \n+\n         # Test ValidationThresholds with multiple invalid values\n         try:\n             ValidationThresholds(bronze=-10.0, silver=150.0, gold=-5.0)\n         except PipelineValidationError as e:\n             # Should catch the first validation error\n@@ -288,13 +302,13 @@\n         # Test ValidationThresholds boundary values\n         thresholds = ValidationThresholds(bronze=0.0, silver=50.0, gold=100.0)\n         assert thresholds.bronze == 0.0\n         assert thresholds.silver == 50.0\n         assert thresholds.gold == 100.0\n-        \n+\n         # Test ParallelConfig boundary values\n         parallel = ParallelConfig(enabled=True, max_workers=1)\n         assert parallel.max_workers == 1\n-        \n+\n         # Test with very large max_workers\n         parallel = ParallelConfig(enabled=True, max_workers=1000)\n         assert parallel.max_workers == 1000\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_logging.py\t2025-09-20 14:00:38.766148+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_logging.py\t2025-09-20 16:53:35.111755+00:00\n@@ -86,11 +86,13 @@\n     def test_format_message_with_kwargs(self):\n         \"\"\"Test _format_message method with kwargs.\"\"\"\n         logger = PipelineLogger()\n \n         # Test with kwargs\n-        result = logger._format_message(\"Test message\", {\"key1\": \"value1\", \"key2\": \"value2\"})\n+        result = logger._format_message(\n+            \"Test message\", {\"key1\": \"value1\", \"key2\": \"value2\"}\n+        )\n         assert result == \"Test message | key1=value1, key2=value2\"\n \n     def test_format_message_without_kwargs(self):\n         \"\"\"Test _format_message method without kwargs.\"\"\"\n         logger = PipelineLogger()\n@@ -139,11 +141,13 @@\n         \"\"\"Test step start logging for different stages.\"\"\"\n         logger = PipelineLogger()\n \n         with patch.object(logger.logger, \"info\") as mock_info:\n             logger.step_start(\"silver\", \"enriched_events\")\n-            mock_info.assert_called_once_with(\"\ud83d\ude80 Starting SILVER step: enriched_events\")\n+            mock_info.assert_called_once_with(\n+                \"\ud83d\ude80 Starting SILVER step: enriched_events\"\n+            )\n \n     def test_step_complete(self):\n         \"\"\"Test step completion logging.\"\"\"\n         logger = PipelineLogger()\n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/compliance_checker.py\t2025-09-20 16:34:51.329429+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/compliance_checker.py\t2025-09-20 16:53:35.139951+00:00\n@@ -20,10 +20,11 @@\n from enum import Enum\n \n \n class ComplianceStandard(Enum):\n     \"\"\"Supported compliance standards.\"\"\"\n+\n     OWASP_TOP_10 = \"owasp_top_10\"\n     CVE_COMPLIANCE = \"cve_compliance\"\n     LICENSE_COMPLIANCE = \"license_compliance\"\n     SECURITY_BEST_PRACTICES = \"security_best_practices\"\n     SOC2 = \"soc2\"\n@@ -31,10 +32,11 @@\n \n \n @dataclass\n class ComplianceCheck:\n     \"\"\"Individual compliance check result.\"\"\"\n+\n     check_id: str\n     standard: ComplianceStandard\n     name: str\n     description: str\n     passed: bool\n@@ -45,39 +47,40 @@\n \n \n @dataclass\n class ComplianceReport:\n     \"\"\"Comprehensive compliance report.\"\"\"\n+\n     standard: ComplianceStandard\n     overall_compliant: bool\n     compliance_score: float\n     checks: List[ComplianceCheck]\n     timestamp: datetime\n     recommendations: List[str]\n \n \n class ComplianceChecker:\n     \"\"\"Comprehensive compliance checker for SparkForge.\"\"\"\n-    \n+\n     def __init__(self, project_root: Optional[Path] = None):\n         self.project_root = project_root or Path.cwd()\n         self.compliance_reports = {}\n-    \n+\n     def check_all_standards(self) -> Dict[str, ComplianceReport]:\n         \"\"\"Check compliance against all supported standards.\"\"\"\n         standards = [\n             ComplianceStandard.OWASP_TOP_10,\n             ComplianceStandard.CVE_COMPLIANCE,\n             ComplianceStandard.LICENSE_COMPLIANCE,\n-            ComplianceStandard.SECURITY_BEST_PRACTICES\n+            ComplianceStandard.SECURITY_BEST_PRACTICES,\n         ]\n-        \n+\n         for standard in standards:\n             self.compliance_reports[standard.value] = self.check_standard(standard)\n-        \n+\n         return self.compliance_reports\n-    \n+\n     def check_standard(self, standard: ComplianceStandard) -> ComplianceReport:\n         \"\"\"Check compliance against a specific standard.\"\"\"\n         if standard == ComplianceStandard.OWASP_TOP_10:\n             return self._check_owasp_top_10()\n         elif standard == ComplianceStandard.CVE_COMPLIANCE:\n@@ -90,11 +93,11 @@\n             return self._check_soc2_compliance()\n         elif standard == ComplianceStandard.ISO27001:\n             return self._check_iso27001_compliance()\n         else:\n             raise ValueError(f\"Unsupported compliance standard: {standard}\")\n-    \n+\n     def _check_owasp_top_10(self) -> ComplianceReport:\n         \"\"\"Check OWASP Top 10 compliance.\"\"\"\n         checks = [\n             self._check_injection_prevention(),\n             self._check_broken_authentication(),\n@@ -103,1066 +106,1110 @@\n             self._check_broken_access_control(),\n             self._check_security_misconfiguration(),\n             self._check_cross_site_scripting(),\n             self._check_insecure_deserialization(),\n             self._check_known_vulnerabilities(),\n-            self._check_insufficient_logging()\n+            self._check_insufficient_logging(),\n         ]\n-        \n+\n         passed_checks = sum(1 for check in checks if check.passed)\n         compliance_score = (passed_checks / len(checks)) * 100\n-        \n+\n         return ComplianceReport(\n             standard=ComplianceStandard.OWASP_TOP_10,\n             overall_compliant=compliance_score >= 90,\n             compliance_score=compliance_score,\n             checks=checks,\n             timestamp=datetime.now(),\n-            recommendations=self._generate_owasp_recommendations(checks)\n-        )\n-    \n+            recommendations=self._generate_owasp_recommendations(checks),\n+        )\n+\n     def _check_injection_prevention(self) -> ComplianceCheck:\n         \"\"\"Check A01:2021 - Broken Access Control.\"\"\"\n         evidence = []\n-        \n+\n         # Check for SQL injection prevention\n         if self._has_sql_injection_prevention():\n             evidence.append(\"SQL injection prevention mechanisms in place\")\n-        \n+\n         # Check for NoSQL injection prevention\n         if self._has_nosql_injection_prevention():\n             evidence.append(\"NoSQL injection prevention mechanisms in place\")\n-        \n+\n         # Check for command injection prevention\n         if self._has_command_injection_prevention():\n             evidence.append(\"Command injection prevention mechanisms in place\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"owasp_a01_injection\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Injection Prevention\",\n             description=\"Prevent injection attacks through input validation and sanitization\",\n             passed=passed,\n             severity=\"critical\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_broken_authentication(self) -> ComplianceCheck:\n         \"\"\"Check A02:2021 - Cryptographic Failures.\"\"\"\n         evidence = []\n-        \n+\n         # SparkForge doesn't implement authentication, so this is N/A\n         evidence.append(\"No authentication system implemented (framework level)\")\n         evidence.append(\"Authentication should be implemented at application level\")\n-        \n+\n         return ComplianceCheck(\n             check_id=\"owasp_a02_authentication\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Authentication Security\",\n             description=\"Implement secure authentication mechanisms\",\n             passed=True,  # N/A for framework\n             severity=\"high\",\n             evidence=evidence,\n             score=100.0,\n-            remediation=\"Implement authentication at application level using secure frameworks\"\n-        )\n-    \n+            remediation=\"Implement authentication at application level using secure frameworks\",\n+        )\n+\n     def _check_sensitive_data_exposure(self) -> ComplianceCheck:\n         \"\"\"Check A03:2021 - Injection.\"\"\"\n         evidence = []\n-        \n+\n         # Check for data encryption in transit\n         if self._has_transit_encryption():\n             evidence.append(\"Data encryption in transit implemented\")\n-        \n+\n         # Check for data encryption at rest\n         if self._has_at_rest_encryption():\n             evidence.append(\"Data encryption at rest implemented\")\n-        \n+\n         # Check for sensitive data handling\n         if self._has_sensitive_data_protection():\n             evidence.append(\"Sensitive data protection mechanisms in place\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"owasp_a03_data_exposure\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Sensitive Data Exposure Prevention\",\n             description=\"Protect sensitive data from exposure\",\n             passed=passed,\n             severity=\"high\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_xml_external_entities(self) -> ComplianceCheck:\n         \"\"\"Check A04:2021 - Insecure Design.\"\"\"\n         evidence = []\n-        \n+\n         # SparkForge doesn't process XML, so this is N/A\n         evidence.append(\"No XML processing in framework\")\n-        \n+\n         return ComplianceCheck(\n             check_id=\"owasp_a04_xxe\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"XML External Entities Prevention\",\n             description=\"Prevent XXE attacks\",\n             passed=True,  # N/A for framework\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0\n-        )\n-    \n+            score=100.0,\n+        )\n+\n     def _check_broken_access_control(self) -> ComplianceCheck:\n         \"\"\"Check A05:2021 - Security Misconfiguration.\"\"\"\n         evidence = []\n-        \n+\n         # Check for access control mechanisms\n         if self._has_access_control():\n             evidence.append(\"Access control mechanisms in place\")\n-        \n+\n         # Check for privilege escalation prevention\n         if self._has_privilege_escalation_prevention():\n             evidence.append(\"Privilege escalation prevention mechanisms in place\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"owasp_a05_access_control\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Access Control Security\",\n             description=\"Implement proper access controls\",\n             passed=passed,\n             severity=\"high\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_security_misconfiguration(self) -> ComplianceCheck:\n         \"\"\"Check A06:2021 - Vulnerable and Outdated Components.\"\"\"\n         evidence = []\n-        \n+\n         # Check for secure defaults\n         if self._has_secure_defaults():\n             evidence.append(\"Secure default configurations\")\n-        \n+\n         # Check for security headers\n         if self._has_security_headers():\n             evidence.append(\"Security headers configured\")\n-        \n+\n         # Check for error handling\n         if self._has_secure_error_handling():\n             evidence.append(\"Secure error handling implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"owasp_a06_misconfiguration\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Security Configuration\",\n             description=\"Implement secure configuration practices\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_cross_site_scripting(self) -> ComplianceCheck:\n         \"\"\"Check A07:2021 - Identification and Authentication Failures.\"\"\"\n         evidence = []\n-        \n+\n         # Check for XSS prevention\n         if self._has_xss_prevention():\n             evidence.append(\"XSS prevention mechanisms in place\")\n-        \n+\n         # Check for input sanitization\n         if self._has_input_sanitization():\n             evidence.append(\"Input sanitization implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"owasp_a07_xss\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Cross-Site Scripting Prevention\",\n             description=\"Prevent XSS attacks through input validation\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_insecure_deserialization(self) -> ComplianceCheck:\n         \"\"\"Check A08:2021 - Software and Data Integrity Failures.\"\"\"\n         evidence = []\n-        \n+\n         # Check for secure serialization\n         if self._has_secure_serialization():\n             evidence.append(\"Secure serialization mechanisms in place\")\n-        \n+\n         # Check for deserialization security\n         if self._has_secure_deserialization():\n             evidence.append(\"Secure deserialization implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"owasp_a08_deserialization\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Insecure Deserialization Prevention\",\n             description=\"Prevent insecure deserialization attacks\",\n             passed=passed,\n             severity=\"high\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_known_vulnerabilities(self) -> ComplianceCheck:\n         \"\"\"Check A09:2021 - Security Logging and Monitoring Failures.\"\"\"\n         evidence = []\n-        \n+\n         # Check for dependency vulnerability scanning\n         if self._has_dependency_scanning():\n             evidence.append(\"Dependency vulnerability scanning implemented\")\n-        \n+\n         # Check for known CVE scanning\n         if self._has_cve_scanning():\n             evidence.append(\"CVE scanning implemented\")\n-        \n+\n         # Check for outdated dependencies\n         if self._has_updated_dependencies():\n             evidence.append(\"Dependencies are up to date\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"owasp_a09_vulnerabilities\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Known Vulnerabilities Prevention\",\n             description=\"Prevent known vulnerabilities\",\n             passed=passed,\n             severity=\"high\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_insufficient_logging(self) -> ComplianceCheck:\n         \"\"\"Check A10:2021 - Server-Side Request Forgery.\"\"\"\n         evidence = []\n-        \n+\n         # Check for security logging\n         if self._has_security_logging():\n             evidence.append(\"Security logging implemented\")\n-        \n+\n         # Check for audit trails\n         if self._has_audit_trails():\n             evidence.append(\"Audit trails implemented\")\n-        \n+\n         # Check for monitoring\n         if self._has_security_monitoring():\n             evidence.append(\"Security monitoring implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"owasp_a10_logging\",\n             standard=ComplianceStandard.OWASP_TOP_10,\n             name=\"Security Logging and Monitoring\",\n             description=\"Implement comprehensive security logging and monitoring\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_cve_compliance(self) -> ComplianceReport:\n         \"\"\"Check CVE compliance.\"\"\"\n         checks = [\n             self._check_no_known_cves(),\n             self._check_dependency_cves(),\n-            self._check_runtime_cves()\n+            self._check_runtime_cves(),\n         ]\n-        \n+\n         passed_checks = sum(1 for check in checks if check.passed)\n         compliance_score = (passed_checks / len(checks)) * 100\n-        \n+\n         return ComplianceReport(\n             standard=ComplianceStandard.CVE_COMPLIANCE,\n             overall_compliant=compliance_score >= 95,\n             compliance_score=compliance_score,\n             checks=checks,\n             timestamp=datetime.now(),\n-            recommendations=self._generate_cve_recommendations(checks)\n-        )\n-    \n+            recommendations=self._generate_cve_recommendations(checks),\n+        )\n+\n     def _check_no_known_cves(self) -> ComplianceCheck:\n         \"\"\"Check for known CVE vulnerabilities.\"\"\"\n         try:\n             # Run safety check\n-            result = subprocess.run([\n-                sys.executable, \"-m\", \"safety\", \"check\", \"--json\"\n-            ], capture_output=True, text=True, cwd=self.project_root)\n-            \n+            result = subprocess.run(\n+                [sys.executable, \"-m\", \"safety\", \"check\", \"--json\"],\n+                capture_output=True,\n+                text=True,\n+                cwd=self.project_root,\n+            )\n+\n             if result.returncode == 0:\n                 evidence = [\"No known CVE vulnerabilities found\"]\n                 passed = True\n             else:\n                 evidence = [f\"Known CVE vulnerabilities found: {result.stdout}\"]\n                 passed = False\n-            \n+\n             return ComplianceCheck(\n                 check_id=\"cve_known_vulnerabilities\",\n                 standard=ComplianceStandard.CVE_COMPLIANCE,\n                 name=\"Known CVE Vulnerabilities\",\n                 description=\"Check for known CVE vulnerabilities in dependencies\",\n                 passed=passed,\n                 severity=\"critical\",\n                 evidence=evidence,\n-                score=100.0 if passed else 0.0\n+                score=100.0 if passed else 0.0,\n             )\n-            \n+\n         except Exception as e:\n             return ComplianceCheck(\n                 check_id=\"cve_known_vulnerabilities\",\n                 standard=ComplianceStandard.CVE_COMPLIANCE,\n                 name=\"Known CVE Vulnerabilities\",\n                 description=\"Check for known CVE vulnerabilities in dependencies\",\n                 passed=False,\n                 severity=\"critical\",\n                 evidence=[f\"CVE check failed: {str(e)}\"],\n-                score=0.0\n+                score=0.0,\n             )\n-    \n+\n     def _check_dependency_cves(self) -> ComplianceCheck:\n         \"\"\"Check for dependency CVE vulnerabilities.\"\"\"\n         evidence = []\n-        \n+\n         # Check for dependency vulnerability scanning\n         if self._has_dependency_scanning():\n             evidence.append(\"Dependency vulnerability scanning implemented\")\n-        \n+\n         # Check for automated updates\n         if self._has_automated_updates():\n             evidence.append(\"Automated dependency updates configured\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"cve_dependency_scanning\",\n             standard=ComplianceStandard.CVE_COMPLIANCE,\n             name=\"Dependency CVE Scanning\",\n             description=\"Implement dependency CVE scanning\",\n             passed=passed,\n             severity=\"high\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_runtime_cves(self) -> ComplianceCheck:\n         \"\"\"Check for runtime CVE vulnerabilities.\"\"\"\n         evidence = []\n-        \n+\n         # Check for runtime security monitoring\n         if self._has_runtime_monitoring():\n             evidence.append(\"Runtime security monitoring implemented\")\n-        \n+\n         # Check for intrusion detection\n         if self._has_intrusion_detection():\n             evidence.append(\"Intrusion detection implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"cve_runtime_monitoring\",\n             standard=ComplianceStandard.CVE_COMPLIANCE,\n             name=\"Runtime CVE Monitoring\",\n             description=\"Implement runtime CVE monitoring\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_license_compliance(self) -> ComplianceReport:\n         \"\"\"Check license compliance.\"\"\"\n         checks = [\n             self._check_license_compatibility(),\n             self._check_license_documentation(),\n-            self._check_copyleft_licenses()\n+            self._check_copyleft_licenses(),\n         ]\n-        \n+\n         passed_checks = sum(1 for check in checks if check.passed)\n         compliance_score = (passed_checks / len(checks)) * 100\n-        \n+\n         return ComplianceReport(\n             standard=ComplianceStandard.LICENSE_COMPLIANCE,\n             overall_compliant=compliance_score >= 80,\n             compliance_score=compliance_score,\n             checks=checks,\n             timestamp=datetime.now(),\n-            recommendations=self._generate_license_recommendations(checks)\n-        )\n-    \n+            recommendations=self._generate_license_recommendations(checks),\n+        )\n+\n     def _check_license_compatibility(self) -> ComplianceCheck:\n         \"\"\"Check license compatibility.\"\"\"\n         evidence = []\n-        \n+\n         # Check for compatible licenses\n         if self._has_compatible_licenses():\n             evidence.append(\"All dependencies have compatible licenses\")\n-        \n+\n         # Check for license documentation\n         if self._has_license_documentation():\n             evidence.append(\"License documentation is complete\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"license_compatibility\",\n             standard=ComplianceStandard.LICENSE_COMPLIANCE,\n             name=\"License Compatibility\",\n             description=\"Ensure all licenses are compatible\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_license_documentation(self) -> ComplianceCheck:\n         \"\"\"Check license documentation.\"\"\"\n         evidence = []\n-        \n+\n         # Check for LICENSE file\n         license_file = self.project_root / \"LICENSE\"\n         if license_file.exists():\n             evidence.append(\"LICENSE file present\")\n-        \n+\n         # Check for license information in setup\n         if self._has_setup_license_info():\n             evidence.append(\"License information in setup files\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"license_documentation\",\n             standard=ComplianceStandard.LICENSE_COMPLIANCE,\n             name=\"License Documentation\",\n             description=\"Maintain proper license documentation\",\n             passed=passed,\n             severity=\"low\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_copyleft_licenses(self) -> ComplianceCheck:\n         \"\"\"Check for copyleft licenses.\"\"\"\n         evidence = []\n-        \n+\n         # Check for copyleft licenses\n         copyleft_licenses = self._find_copyleft_licenses()\n         if not copyleft_licenses:\n             evidence.append(\"No copyleft licenses detected\")\n             passed = True\n         else:\n-            evidence.append(f\"Copyleft licenses detected: {', '.join(copyleft_licenses)}\")\n+            evidence.append(\n+                f\"Copyleft licenses detected: {', '.join(copyleft_licenses)}\"\n+            )\n             passed = False\n-        \n+\n         return ComplianceCheck(\n             check_id=\"license_copyleft\",\n             standard=ComplianceStandard.LICENSE_COMPLIANCE,\n             name=\"Copyleft License Check\",\n             description=\"Check for problematic copyleft licenses\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_security_best_practices(self) -> ComplianceReport:\n         \"\"\"Check security best practices compliance.\"\"\"\n         checks = [\n             self._check_input_validation(),\n             self._check_output_encoding(),\n             self._check_error_handling(),\n             self._check_logging_practices(),\n-            self._check_cryptographic_practices()\n+            self._check_cryptographic_practices(),\n         ]\n-        \n+\n         passed_checks = sum(1 for check in checks if check.passed)\n         compliance_score = (passed_checks / len(checks)) * 100\n-        \n+\n         return ComplianceReport(\n             standard=ComplianceStandard.SECURITY_BEST_PRACTICES,\n             overall_compliant=compliance_score >= 80,\n             compliance_score=compliance_score,\n             checks=checks,\n             timestamp=datetime.now(),\n-            recommendations=self._generate_best_practices_recommendations(checks)\n-        )\n-    \n+            recommendations=self._generate_best_practices_recommendations(checks),\n+        )\n+\n     def _check_input_validation(self) -> ComplianceCheck:\n         \"\"\"Check input validation practices.\"\"\"\n         evidence = []\n-        \n+\n         # Check for input validation\n         if self._has_input_validation():\n             evidence.append(\"Input validation implemented\")\n-        \n+\n         # Check for sanitization\n         if self._has_input_sanitization():\n             evidence.append(\"Input sanitization implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"best_practice_input_validation\",\n             standard=ComplianceStandard.SECURITY_BEST_PRACTICES,\n             name=\"Input Validation\",\n             description=\"Implement proper input validation\",\n             passed=passed,\n             severity=\"high\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_output_encoding(self) -> ComplianceCheck:\n         \"\"\"Check output encoding practices.\"\"\"\n         evidence = []\n-        \n+\n         # Check for output encoding\n         if self._has_output_encoding():\n             evidence.append(\"Output encoding implemented\")\n-        \n+\n         # Check for XSS prevention\n         if self._has_xss_prevention():\n             evidence.append(\"XSS prevention mechanisms in place\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"best_practice_output_encoding\",\n             standard=ComplianceStandard.SECURITY_BEST_PRACTICES,\n             name=\"Output Encoding\",\n             description=\"Implement proper output encoding\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_error_handling(self) -> ComplianceCheck:\n         \"\"\"Check error handling practices.\"\"\"\n         evidence = []\n-        \n+\n         # Check for secure error handling\n         if self._has_secure_error_handling():\n             evidence.append(\"Secure error handling implemented\")\n-        \n+\n         # Check for error logging\n         if self._has_error_logging():\n             evidence.append(\"Error logging implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"best_practice_error_handling\",\n             standard=ComplianceStandard.SECURITY_BEST_PRACTICES,\n             name=\"Error Handling\",\n             description=\"Implement secure error handling\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_logging_practices(self) -> ComplianceCheck:\n         \"\"\"Check logging practices.\"\"\"\n         evidence = []\n-        \n+\n         # Check for security logging\n         if self._has_security_logging():\n             evidence.append(\"Security logging implemented\")\n-        \n+\n         # Check for audit logging\n         if self._has_audit_logging():\n             evidence.append(\"Audit logging implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"best_practice_logging\",\n             standard=ComplianceStandard.SECURITY_BEST_PRACTICES,\n             name=\"Logging Practices\",\n             description=\"Implement proper logging practices\",\n             passed=passed,\n             severity=\"medium\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_cryptographic_practices(self) -> ComplianceCheck:\n         \"\"\"Check cryptographic practices.\"\"\"\n         evidence = []\n-        \n+\n         # Check for secure random number generation\n         if self._has_secure_random():\n             evidence.append(\"Secure random number generation implemented\")\n-        \n+\n         # Check for proper hashing\n         if self._has_secure_hashing():\n             evidence.append(\"Secure hashing implemented\")\n-        \n-        passed = len(evidence) > 0\n-        \n+\n+        passed = len(evidence) > 0\n+\n         return ComplianceCheck(\n             check_id=\"best_practice_cryptography\",\n             standard=ComplianceStandard.SECURITY_BEST_PRACTICES,\n             name=\"Cryptographic Practices\",\n             description=\"Implement proper cryptographic practices\",\n             passed=passed,\n             severity=\"high\",\n             evidence=evidence,\n-            score=100.0 if passed else 0.0\n-        )\n-    \n+            score=100.0 if passed else 0.0,\n+        )\n+\n     def _check_soc2_compliance(self) -> ComplianceReport:\n         \"\"\"Check SOC 2 compliance.\"\"\"\n         checks = [\n             self._check_availability_controls(),\n             self._check_processing_integrity(),\n             self._check_confidentiality_controls(),\n-            self._check_privacy_controls()\n+            self._check_privacy_controls(),\n         ]\n-        \n+\n         passed_checks = sum(1 for check in checks if check.passed)\n         compliance_score = (passed_checks / len(checks)) * 100\n-        \n+\n         return ComplianceReport(\n             standard=ComplianceStandard.SOC2,\n             overall_compliant=compliance_score >= 80,\n             compliance_score=compliance_score,\n             checks=checks,\n             timestamp=datetime.now(),\n-            recommendations=self._generate_soc2_recommendations(checks)\n-        )\n-    \n+            recommendations=self._generate_soc2_recommendations(checks),\n+        )\n+\n     def _check_iso27001_compliance(self) -> ComplianceReport:\n         \"\"\"Check ISO 27001 compliance.\"\"\"\n         checks = [\n             self._check_information_security_policy(),\n             self._check_risk_management(),\n             self._check_incident_management(),\n-            self._check_business_continuity()\n+            self._check_business_continuity(),\n         ]\n-        \n+\n         passed_checks = sum(1 for check in checks if check.passed)\n         compliance_score = (passed_checks / len(checks)) * 100\n-        \n+\n         return ComplianceReport(\n             standard=ComplianceStandard.ISO27001,\n             overall_compliant=compliance_score >= 80,\n             compliance_score=compliance_score,\n             checks=checks,\n             timestamp=datetime.now(),\n-            recommendations=self._generate_iso27001_recommendations(checks)\n-        )\n-    \n+            recommendations=self._generate_iso27001_recommendations(checks),\n+        )\n+\n     # Helper methods for checking specific security features\n     def _has_sql_injection_prevention(self) -> bool:\n         \"\"\"Check if SQL injection prevention is implemented.\"\"\"\n         # SparkForge uses parameterized queries and validation\n         return True\n-    \n+\n     def _has_nosql_injection_prevention(self) -> bool:\n         \"\"\"Check if NoSQL injection prevention is implemented.\"\"\"\n         # SparkForge doesn't use NoSQL databases\n         return True\n-    \n+\n     def _has_command_injection_prevention(self) -> bool:\n         \"\"\"Check if command injection prevention is implemented.\"\"\"\n         # SparkForge doesn't execute shell commands\n         return True\n-    \n+\n     def _has_transit_encryption(self) -> bool:\n         \"\"\"Check if data encryption in transit is implemented.\"\"\"\n         # Should be implemented at infrastructure level\n         return True\n-    \n+\n     def _has_at_rest_encryption(self) -> bool:\n         \"\"\"Check if data encryption at rest is implemented.\"\"\"\n         # Should be implemented at infrastructure level\n         return True\n-    \n+\n     def _has_sensitive_data_protection(self) -> bool:\n         \"\"\"Check if sensitive data protection is implemented.\"\"\"\n         # SparkForge includes validation for sensitive data\n         return True\n-    \n+\n     def _has_access_control(self) -> bool:\n         \"\"\"Check if access control is implemented.\"\"\"\n         # Should be implemented at application level\n         return True\n-    \n+\n     def _has_privilege_escalation_prevention(self) -> bool:\n         \"\"\"Check if privilege escalation prevention is implemented.\"\"\"\n         # Should be implemented at application level\n         return True\n-    \n+\n     def _has_secure_defaults(self) -> bool:\n         \"\"\"Check if secure defaults are configured.\"\"\"\n         return True\n-    \n+\n     def _has_security_headers(self) -> bool:\n         \"\"\"Check if security headers are configured.\"\"\"\n         # Should be configured at web server level\n         return True\n-    \n+\n     def _has_secure_error_handling(self) -> bool:\n         \"\"\"Check if secure error handling is implemented.\"\"\"\n         # SparkForge has proper error handling\n         return True\n-    \n+\n     def _has_xss_prevention(self) -> bool:\n         \"\"\"Check if XSS prevention is implemented.\"\"\"\n         # SparkForge includes input validation\n         return True\n-    \n+\n     def _has_input_sanitization(self) -> bool:\n         \"\"\"Check if input sanitization is implemented.\"\"\"\n         # SparkForge includes validation rules\n         return True\n-    \n+\n     def _has_secure_serialization(self) -> bool:\n         \"\"\"Check if secure serialization is implemented.\"\"\"\n         # SparkForge uses safe serialization methods\n         return True\n-    \n+\n     def _has_secure_deserialization(self) -> bool:\n         \"\"\"Check if secure deserialization is implemented.\"\"\"\n         # SparkForge uses safe deserialization methods\n         return True\n-    \n+\n     def _has_dependency_scanning(self) -> bool:\n         \"\"\"Check if dependency scanning is implemented.\"\"\"\n         # This security test module provides dependency scanning\n         return True\n-    \n+\n     def _has_cve_scanning(self) -> bool:\n         \"\"\"Check if CVE scanning is implemented.\"\"\"\n         # This security test module provides CVE scanning\n         return True\n-    \n+\n     def _has_updated_dependencies(self) -> bool:\n         \"\"\"Check if dependencies are up to date.\"\"\"\n         try:\n-            result = subprocess.run([\n-                sys.executable, \"-m\", \"pip\", \"list\", \"--outdated\"\n-            ], capture_output=True, text=True, cwd=self.project_root)\n-            \n+            result = subprocess.run(\n+                [sys.executable, \"-m\", \"pip\", \"list\", \"--outdated\"],\n+                capture_output=True,\n+                text=True,\n+                cwd=self.project_root,\n+            )\n+\n             # If no output, all dependencies are up to date\n             return len(result.stdout.strip()) == 0\n         except Exception:\n             return False\n-    \n+\n     def _has_security_logging(self) -> bool:\n         \"\"\"Check if security logging is implemented.\"\"\"\n         # SparkForge includes comprehensive logging\n         return True\n-    \n+\n     def _has_audit_trails(self) -> bool:\n         \"\"\"Check if audit trails are implemented.\"\"\"\n         # Should be implemented at application level\n         return True\n-    \n+\n     def _has_security_monitoring(self) -> bool:\n         \"\"\"Check if security monitoring is implemented.\"\"\"\n         # Should be implemented at infrastructure level\n         return True\n-    \n+\n     def _has_automated_updates(self) -> bool:\n         \"\"\"Check if automated updates are configured.\"\"\"\n         # Should be configured in CI/CD\n         return True\n-    \n+\n     def _has_runtime_monitoring(self) -> bool:\n         \"\"\"Check if runtime monitoring is implemented.\"\"\"\n         # Should be implemented at infrastructure level\n         return True\n-    \n+\n     def _has_intrusion_detection(self) -> bool:\n         \"\"\"Check if intrusion detection is implemented.\"\"\"\n         # Should be implemented at infrastructure level\n         return True\n-    \n+\n     def _has_compatible_licenses(self) -> bool:\n         \"\"\"Check if all licenses are compatible.\"\"\"\n         return True  # MIT license is permissive\n-    \n+\n     def _has_license_documentation(self) -> bool:\n         \"\"\"Check if license documentation is complete.\"\"\"\n         license_file = self.project_root / \"LICENSE\"\n         return license_file.exists()\n-    \n+\n     def _has_setup_license_info(self) -> bool:\n         \"\"\"Check if setup files contain license information.\"\"\"\n         pyproject_file = self.project_root / \"pyproject.toml\"\n         if pyproject_file.exists():\n-            with open(pyproject_file, 'r') as f:\n+            with open(pyproject_file, \"r\") as f:\n                 content = f.read()\n                 return \"license\" in content.lower()\n         return False\n-    \n+\n     def _find_copyleft_licenses(self) -> List[str]:\n         \"\"\"Find copyleft licenses in dependencies.\"\"\"\n         try:\n-            result = subprocess.run([\n-                sys.executable, \"-m\", \"pip-licenses\", \"--format=json\"\n-            ], capture_output=True, text=True, cwd=self.project_root)\n-            \n+            result = subprocess.run(\n+                [sys.executable, \"-m\", \"pip-licenses\", \"--format=json\"],\n+                capture_output=True,\n+                text=True,\n+                cwd=self.project_root,\n+            )\n+\n             if result.returncode == 0:\n                 licenses_data = json.loads(result.stdout)\n                 copyleft_licenses = []\n-                \n+\n                 for package in licenses_data:\n                     license_name = package.get(\"License\", \"\")\n-                    if any(license_type in license_name for license_type in [\"GPL\", \"AGPL\", \"Copyleft\"]):\n-                        copyleft_licenses.append(f\"{package.get('Name', 'package')}: {license_name}\")\n-                \n+                    if any(\n+                        license_type in license_name\n+                        for license_type in [\"GPL\", \"AGPL\", \"Copyleft\"]\n+                    ):\n+                        copyleft_licenses.append(\n+                            f\"{package.get('Name', 'package')}: {license_name}\"\n+                        )\n+\n                 return copyleft_licenses\n         except Exception:\n             pass\n-        \n+\n         return []\n-    \n+\n     def _has_input_validation(self) -> bool:\n         \"\"\"Check if input validation is implemented.\"\"\"\n         return True\n-    \n+\n     def _has_output_encoding(self) -> bool:\n         \"\"\"Check if output encoding is implemented.\"\"\"\n         return True\n-    \n+\n     def _has_error_logging(self) -> bool:\n         \"\"\"Check if error logging is implemented.\"\"\"\n         return True\n-    \n+\n     def _has_audit_logging(self) -> bool:\n         \"\"\"Check if audit logging is implemented.\"\"\"\n         return True\n-    \n+\n     def _has_secure_random(self) -> bool:\n         \"\"\"Check if secure random number generation is implemented.\"\"\"\n         return True\n-    \n+\n     def _has_secure_hashing(self) -> bool:\n         \"\"\"Check if secure hashing is implemented.\"\"\"\n         return True\n-    \n+\n     # Placeholder methods for SOC2 and ISO27001 checks\n     def _check_availability_controls(self) -> ComplianceCheck:\n         \"\"\"Check availability controls.\"\"\"\n         return ComplianceCheck(\n             check_id=\"soc2_availability\",\n             standard=ComplianceStandard.SOC2,\n             name=\"Availability Controls\",\n             description=\"Implement availability controls\",\n             passed=True,\n             severity=\"high\",\n-            evidence=[\"Availability controls should be implemented at infrastructure level\"],\n-            score=100.0\n-        )\n-    \n+            evidence=[\n+                \"Availability controls should be implemented at infrastructure level\"\n+            ],\n+            score=100.0,\n+        )\n+\n     def _check_processing_integrity(self) -> ComplianceCheck:\n         \"\"\"Check processing integrity.\"\"\"\n         return ComplianceCheck(\n             check_id=\"soc2_integrity\",\n             standard=ComplianceStandard.SOC2,\n             name=\"Processing Integrity\",\n             description=\"Implement processing integrity controls\",\n             passed=True,\n             severity=\"high\",\n             evidence=[\"Processing integrity controls implemented in SparkForge\"],\n-            score=100.0\n-        )\n-    \n+            score=100.0,\n+        )\n+\n     def _check_confidentiality_controls(self) -> ComplianceCheck:\n         \"\"\"Check confidentiality controls.\"\"\"\n         return ComplianceCheck(\n             check_id=\"soc2_confidentiality\",\n             standard=ComplianceStandard.SOC2,\n             name=\"Confidentiality Controls\",\n             description=\"Implement confidentiality controls\",\n             passed=True,\n             severity=\"high\",\n-            evidence=[\"Confidentiality controls should be implemented at application level\"],\n-            score=100.0\n-        )\n-    \n+            evidence=[\n+                \"Confidentiality controls should be implemented at application level\"\n+            ],\n+            score=100.0,\n+        )\n+\n     def _check_privacy_controls(self) -> ComplianceCheck:\n         \"\"\"Check privacy controls.\"\"\"\n         return ComplianceCheck(\n             check_id=\"soc2_privacy\",\n             standard=ComplianceStandard.SOC2,\n             name=\"Privacy Controls\",\n             description=\"Implement privacy controls\",\n             passed=True,\n             severity=\"medium\",\n             evidence=[\"Privacy controls should be implemented at application level\"],\n-            score=100.0\n-        )\n-    \n+            score=100.0,\n+        )\n+\n     def _check_information_security_policy(self) -> ComplianceCheck:\n         \"\"\"Check information security policy.\"\"\"\n         return ComplianceCheck(\n             check_id=\"iso27001_policy\",\n             standard=ComplianceStandard.ISO27001,\n             name=\"Information Security Policy\",\n             description=\"Implement information security policy\",\n             passed=True,\n             severity=\"high\",\n-            evidence=[\"Information security policy should be implemented at organizational level\"],\n-            score=100.0\n-        )\n-    \n+            evidence=[\n+                \"Information security policy should be implemented at organizational level\"\n+            ],\n+            score=100.0,\n+        )\n+\n     def _check_risk_management(self) -> ComplianceCheck:\n         \"\"\"Check risk management.\"\"\"\n         return ComplianceCheck(\n             check_id=\"iso27001_risk\",\n             standard=ComplianceStandard.ISO27001,\n             name=\"Risk Management\",\n             description=\"Implement risk management\",\n             passed=True,\n             severity=\"high\",\n             evidence=[\"Risk management should be implemented at organizational level\"],\n-            score=100.0\n-        )\n-    \n+            score=100.0,\n+        )\n+\n     def _check_incident_management(self) -> ComplianceCheck:\n         \"\"\"Check incident management.\"\"\"\n         return ComplianceCheck(\n             check_id=\"iso27001_incident\",\n             standard=ComplianceStandard.ISO27001,\n             name=\"Incident Management\",\n             description=\"Implement incident management\",\n             passed=True,\n             severity=\"medium\",\n-            evidence=[\"Incident management should be implemented at organizational level\"],\n-            score=100.0\n-        )\n-    \n+            evidence=[\n+                \"Incident management should be implemented at organizational level\"\n+            ],\n+            score=100.0,\n+        )\n+\n     def _check_business_continuity(self) -> ComplianceCheck:\n         \"\"\"Check business continuity.\"\"\"\n         return ComplianceCheck(\n             check_id=\"iso27001_continuity\",\n             standard=ComplianceStandard.ISO27001,\n             name=\"Business Continuity\",\n             description=\"Implement business continuity\",\n             passed=True,\n             severity=\"medium\",\n-            evidence=[\"Business continuity should be implemented at organizational level\"],\n-            score=100.0\n-        )\n-    \n+            evidence=[\n+                \"Business continuity should be implemented at organizational level\"\n+            ],\n+            score=100.0,\n+        )\n+\n     # Recommendation generation methods\n-    def _generate_owasp_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n+    def _generate_owasp_recommendations(\n+        self, checks: List[ComplianceCheck]\n+    ) -> List[str]:\n         \"\"\"Generate OWASP recommendations.\"\"\"\n         recommendations = []\n-        \n+\n         for check in checks:\n             if not check.passed:\n                 recommendations.append(f\"Address {check.name}: {check.description}\")\n-        \n-        recommendations.extend([\n-            \"Implement comprehensive security testing in CI/CD pipeline\",\n-            \"Regular security code reviews\",\n-            \"Automated vulnerability scanning\",\n-            \"Security awareness training for developers\"\n-        ])\n-        \n+\n+        recommendations.extend(\n+            [\n+                \"Implement comprehensive security testing in CI/CD pipeline\",\n+                \"Regular security code reviews\",\n+                \"Automated vulnerability scanning\",\n+                \"Security awareness training for developers\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n+\n     def _generate_cve_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n         \"\"\"Generate CVE recommendations.\"\"\"\n         recommendations = []\n-        \n+\n         for check in checks:\n             if not check.passed:\n                 recommendations.append(f\"Address {check.name}: {check.description}\")\n-        \n-        recommendations.extend([\n-            \"Implement automated dependency scanning\",\n-            \"Regular security updates\",\n-            \"Monitor security advisories\",\n-            \"Implement runtime security monitoring\"\n-        ])\n-        \n+\n+        recommendations.extend(\n+            [\n+                \"Implement automated dependency scanning\",\n+                \"Regular security updates\",\n+                \"Monitor security advisories\",\n+                \"Implement runtime security monitoring\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n-    def _generate_license_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n+\n+    def _generate_license_recommendations(\n+        self, checks: List[ComplianceCheck]\n+    ) -> List[str]:\n         \"\"\"Generate license recommendations.\"\"\"\n         recommendations = []\n-        \n+\n         for check in checks:\n             if not check.passed:\n                 recommendations.append(f\"Address {check.name}: {check.description}\")\n-        \n-        recommendations.extend([\n-            \"Regular license compliance reviews\",\n-            \"Automated license scanning\",\n-            \"Legal review of license changes\",\n-            \"Document license obligations\"\n-        ])\n-        \n+\n+        recommendations.extend(\n+            [\n+                \"Regular license compliance reviews\",\n+                \"Automated license scanning\",\n+                \"Legal review of license changes\",\n+                \"Document license obligations\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n-    def _generate_best_practices_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n+\n+    def _generate_best_practices_recommendations(\n+        self, checks: List[ComplianceCheck]\n+    ) -> List[str]:\n         \"\"\"Generate best practices recommendations.\"\"\"\n         recommendations = []\n-        \n+\n         for check in checks:\n             if not check.passed:\n                 recommendations.append(f\"Address {check.name}: {check.description}\")\n-        \n-        recommendations.extend([\n-            \"Implement security coding standards\",\n-            \"Regular security training\",\n-            \"Automated security testing\",\n-            \"Security code review process\"\n-        ])\n-        \n+\n+        recommendations.extend(\n+            [\n+                \"Implement security coding standards\",\n+                \"Regular security training\",\n+                \"Automated security testing\",\n+                \"Security code review process\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n-    def _generate_soc2_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n+\n+    def _generate_soc2_recommendations(\n+        self, checks: List[ComplianceCheck]\n+    ) -> List[str]:\n         \"\"\"Generate SOC2 recommendations.\"\"\"\n         return [\n             \"Implement comprehensive access controls\",\n             \"Regular security assessments\",\n             \"Incident response procedures\",\n-            \"Business continuity planning\"\n+            \"Business continuity planning\",\n         ]\n-    \n-    def _generate_iso27001_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n+\n+    def _generate_iso27001_recommendations(\n+        self, checks: List[ComplianceCheck]\n+    ) -> List[str]:\n         \"\"\"Generate ISO27001 recommendations.\"\"\"\n         return [\n             \"Implement information security management system\",\n             \"Regular risk assessments\",\n             \"Security awareness training\",\n-            \"Continuous improvement processes\"\n+            \"Continuous improvement processes\",\n         ]\n-    \n+\n     def generate_compliance_report(self, output_file: Optional[Path] = None) -> Path:\n         \"\"\"Generate comprehensive compliance report.\"\"\"\n         compliance_reports = self.check_all_standards()\n-        \n+\n         if output_file is None:\n             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n             output_file = self.project_root / f\"compliance_report_{timestamp}.json\"\n-        \n+\n         # Convert to serializable format\n         serializable_reports = {}\n         for standard, report in compliance_reports.items():\n             serializable_reports[standard] = {\n                 \"standard\": report.standard.value,\n@@ -1175,48 +1222,53 @@\n                         \"description\": check.description,\n                         \"passed\": check.passed,\n                         \"severity\": check.severity,\n                         \"evidence\": check.evidence,\n                         \"remediation\": check.remediation,\n-                        \"score\": check.score\n+                        \"score\": check.score,\n                     }\n                     for check in report.checks\n                 ],\n                 \"timestamp\": report.timestamp.isoformat(),\n-                \"recommendations\": report.recommendations\n+                \"recommendations\": report.recommendations,\n             }\n-        \n-        with open(output_file, 'w') as f:\n+\n+        with open(output_file, \"w\") as f:\n             json.dump(serializable_reports, f, indent=2)\n-        \n+\n         return output_file\n \n \n # CLI interface\n if __name__ == \"__main__\":\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(description=\"SparkForge Compliance Checker\")\n     parser.add_argument(\"--project-root\", type=Path, help=\"Project root directory\")\n     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n-    parser.add_argument(\"--standard\", choices=[s.value for s in ComplianceStandard], \n-                       help=\"Specific standard to check\")\n-    \n+    parser.add_argument(\n+        \"--standard\",\n+        choices=[s.value for s in ComplianceStandard],\n+        help=\"Specific standard to check\",\n+    )\n+\n     args = parser.parse_args()\n-    \n+\n     checker = ComplianceChecker(args.project_root)\n-    \n+\n     if args.standard:\n         standard = ComplianceStandard(args.standard)\n         report = checker.check_standard(standard)\n         print(f\"\\nCompliance Report for {standard.value}:\")\n         print(f\"Overall Compliant: {report.overall_compliant}\")\n         print(f\"Compliance Score: {report.compliance_score:.1f}%\")\n-        print(f\"Checks Passed: {sum(1 for c in report.checks if c.passed)}/{len(report.checks)}\")\n+        print(\n+            f\"Checks Passed: {sum(1 for c in report.checks if c.passed)}/{len(report.checks)}\"\n+        )\n     else:\n         reports = checker.check_all_standards()\n         print(f\"\\nCompliance Reports:\")\n         for standard, report in reports.items():\n             print(f\"{standard}: {report.compliance_score:.1f}% compliant\")\n-        \n+\n         report_file = checker.generate_compliance_report(args.output)\n         print(f\"\\nDetailed compliance report saved to: {report_file}\")\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_monitoring.py\t2025-09-20 16:34:51.329490+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_monitoring.py\t2025-09-20 16:53:35.098485+00:00\n@@ -22,10 +22,11 @@\n import subprocess\n \n \n class SecurityEventType(Enum):\n     \"\"\"Types of security events.\"\"\"\n+\n     AUTHENTICATION_FAILURE = \"authentication_failure\"\n     AUTHORIZATION_FAILURE = \"authorization_failure\"\n     SUSPICIOUS_ACTIVITY = \"suspicious_activity\"\n     DATA_BREACH = \"data_breach\"\n     MALWARE_DETECTION = \"malware_detection\"\n@@ -36,20 +37,22 @@\n     CONFIGURATION_CHANGE = \"configuration_change\"\n \n \n class SecuritySeverity(Enum):\n     \"\"\"Security event severity levels.\"\"\"\n+\n     CRITICAL = \"critical\"\n     HIGH = \"high\"\n     MEDIUM = \"medium\"\n     LOW = \"low\"\n     INFO = \"info\"\n \n \n @dataclass\n class SecurityEvent:\n     \"\"\"Security event data structure.\"\"\"\n+\n     event_id: str\n     event_type: SecurityEventType\n     severity: SecuritySeverity\n     timestamp: datetime\n     source: str\n@@ -58,19 +61,20 @@\n     user_id: Optional[str] = None\n     ip_address: Optional[str] = None\n     user_agent: Optional[str] = None\n     session_id: Optional[str] = None\n     tags: List[str] = None\n-    \n+\n     def __post_init__(self):\n         if self.tags is None:\n             self.tags = []\n \n \n @dataclass\n class SecurityAlert:\n     \"\"\"Security alert data structure.\"\"\"\n+\n     alert_id: str\n     event_id: str\n     severity: SecuritySeverity\n     timestamp: datetime\n     title: str\n@@ -82,10 +86,11 @@\n \n \n @dataclass\n class SecurityMetrics:\n     \"\"\"Security metrics data structure.\"\"\"\n+\n     timestamp: datetime\n     total_events: int\n     events_by_type: Dict[str, int]\n     events_by_severity: Dict[str, int]\n     active_alerts: int\n@@ -95,415 +100,469 @@\n     security_score: float\n \n \n class SecurityMonitor:\n     \"\"\"Real-time security monitoring system.\"\"\"\n-    \n+\n     def __init__(self, config: Optional[Dict[str, Any]] = None):\n         self.config = config or self._default_config()\n         self.events: List[SecurityEvent] = []\n         self.alerts: List[SecurityAlert] = []\n         self.metrics: List[SecurityMetrics] = []\n         self.monitoring_active = False\n         self.monitor_thread = None\n         self.alert_callbacks: List[Callable[[SecurityAlert], None]] = []\n         self.event_callbacks: List[Callable[[SecurityEvent], None]] = []\n-        \n+\n         # Setup logging\n         self.logger = logging.getLogger(\"security_monitor\")\n         self.logger.setLevel(logging.INFO)\n-        \n+\n         # Security thresholds\n         self.thresholds = {\n             \"max_failed_auth_attempts\": 5,\n             \"max_suspicious_requests\": 10,\n             \"max_data_access_violations\": 3,\n-            \"max_privilege_escalation_attempts\": 2\n+            \"max_privilege_escalation_attempts\": 2,\n         }\n-        \n+\n         # Rate limiting\n-        self.rate_limits = {\n-            \"events_per_minute\": 100,\n-            \"alerts_per_hour\": 50\n-        }\n-        \n+        self.rate_limits = {\"events_per_minute\": 100, \"alerts_per_hour\": 50}\n+\n         # Event counters for rate limiting\n         self.event_counters = {\n             \"minute\": 0,\n             \"hour\": 0,\n             \"last_minute_reset\": datetime.now(),\n-            \"last_hour_reset\": datetime.now()\n+            \"last_hour_reset\": datetime.now(),\n         }\n-    \n+\n     def _default_config(self) -> Dict[str, Any]:\n         \"\"\"Get default configuration.\"\"\"\n         return {\n             \"monitoring_interval\": 60,  # seconds\n             \"retention_days\": 30,\n             \"alert_threshold\": 10,\n             \"enable_real_time_monitoring\": True,\n             \"enable_anomaly_detection\": True,\n             \"enable_threat_intelligence\": True,\n             \"log_file\": \"security_monitor.log\",\n-            \"metrics_file\": \"security_metrics.json\"\n+            \"metrics_file\": \"security_metrics.json\",\n         }\n-    \n+\n     def start_monitoring(self) -> None:\n         \"\"\"Start security monitoring.\"\"\"\n         if self.monitoring_active:\n             self.logger.warning(\"Security monitoring is already active\")\n             return\n-        \n+\n         self.monitoring_active = True\n-        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n+        self.monitor_thread = threading.Thread(\n+            target=self._monitoring_loop, daemon=True\n+        )\n         self.monitor_thread.start()\n-        \n+\n         self.logger.info(\"Security monitoring started\")\n-        \n+\n         # Log startup event\n-        self._log_event(SecurityEvent(\n-            event_id=f\"startup_{int(time.time())}\",\n-            event_type=SecurityEventType.CONFIGURATION_CHANGE,\n-            severity=SecuritySeverity.INFO,\n-            timestamp=datetime.now(),\n-            source=\"security_monitor\",\n-            description=\"Security monitoring started\",\n-            details={\"config\": self.config}\n-        ))\n-    \n+        self._log_event(\n+            SecurityEvent(\n+                event_id=f\"startup_{int(time.time())}\",\n+                event_type=SecurityEventType.CONFIGURATION_CHANGE,\n+                severity=SecuritySeverity.INFO,\n+                timestamp=datetime.now(),\n+                source=\"security_monitor\",\n+                description=\"Security monitoring started\",\n+                details={\"config\": self.config},\n+            )\n+        )\n+\n     def stop_monitoring(self) -> None:\n         \"\"\"Stop security monitoring.\"\"\"\n         if not self.monitoring_active:\n             self.logger.warning(\"Security monitoring is not active\")\n             return\n-        \n+\n         self.monitoring_active = False\n-        \n+\n         if self.monitor_thread and self.monitor_thread.is_alive():\n             self.monitor_thread.join(timeout=5)\n-        \n+\n         self.logger.info(\"Security monitoring stopped\")\n-        \n+\n         # Log shutdown event\n-        self._log_event(SecurityEvent(\n-            event_id=f\"shutdown_{int(time.time())}\",\n-            event_type=SecurityEventType.CONFIGURATION_CHANGE,\n-            severity=SecuritySeverity.INFO,\n-            timestamp=datetime.now(),\n-            source=\"security_monitor\",\n-            description=\"Security monitoring stopped\",\n-            details={}\n-        ))\n-    \n+        self._log_event(\n+            SecurityEvent(\n+                event_id=f\"shutdown_{int(time.time())}\",\n+                event_type=SecurityEventType.CONFIGURATION_CHANGE,\n+                severity=SecuritySeverity.INFO,\n+                timestamp=datetime.now(),\n+                source=\"security_monitor\",\n+                description=\"Security monitoring stopped\",\n+                details={},\n+            )\n+        )\n+\n     def _monitoring_loop(self) -> None:\n         \"\"\"Main monitoring loop.\"\"\"\n         while self.monitoring_active:\n             try:\n                 # Monitor system resources\n                 self._monitor_system_resources()\n-                \n+\n                 # Monitor network activity\n                 self._monitor_network_activity()\n-                \n+\n                 # Monitor file system changes\n                 self._monitor_file_system()\n-                \n+\n                 # Monitor process activity\n                 self._monitor_process_activity()\n-                \n+\n                 # Check for anomalies\n                 if self.config.get(\"enable_anomaly_detection\", True):\n                     self._detect_anomalies()\n-                \n+\n                 # Update metrics\n                 self._update_metrics()\n-                \n+\n                 # Cleanup old data\n                 self._cleanup_old_data()\n-                \n+\n                 time.sleep(self.config.get(\"monitoring_interval\", 60))\n-                \n+\n             except Exception as e:\n                 self.logger.error(f\"Error in monitoring loop: {e}\")\n                 time.sleep(5)  # Brief pause before retrying\n-    \n+\n     def _monitor_system_resources(self) -> None:\n         \"\"\"Monitor system resource usage.\"\"\"\n         try:\n             # CPU usage\n             cpu_percent = psutil.cpu_percent(interval=1)\n             if cpu_percent > 90:\n-                self._log_event(SecurityEvent(\n-                    event_id=f\"high_cpu_{int(time.time())}\",\n-                    event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n-                    severity=SecuritySeverity.MEDIUM,\n-                    timestamp=datetime.now(),\n-                    source=\"system_monitor\",\n-                    description=f\"High CPU usage detected: {cpu_percent}%\",\n-                    details={\"cpu_percent\": cpu_percent}\n-                ))\n-            \n+                self._log_event(\n+                    SecurityEvent(\n+                        event_id=f\"high_cpu_{int(time.time())}\",\n+                        event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n+                        severity=SecuritySeverity.MEDIUM,\n+                        timestamp=datetime.now(),\n+                        source=\"system_monitor\",\n+                        description=f\"High CPU usage detected: {cpu_percent}%\",\n+                        details={\"cpu_percent\": cpu_percent},\n+                    )\n+                )\n+\n             # Memory usage\n             memory = psutil.virtual_memory()\n             if memory.percent > 90:\n-                self._log_event(SecurityEvent(\n-                    event_id=f\"high_memory_{int(time.time())}\",\n-                    event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n-                    severity=SecuritySeverity.MEDIUM,\n-                    timestamp=datetime.now(),\n-                    source=\"system_monitor\",\n-                    description=f\"High memory usage detected: {memory.percent}%\",\n-                    details={\"memory_percent\": memory.percent}\n-                ))\n-            \n+                self._log_event(\n+                    SecurityEvent(\n+                        event_id=f\"high_memory_{int(time.time())}\",\n+                        event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n+                        severity=SecuritySeverity.MEDIUM,\n+                        timestamp=datetime.now(),\n+                        source=\"system_monitor\",\n+                        description=f\"High memory usage detected: {memory.percent}%\",\n+                        details={\"memory_percent\": memory.percent},\n+                    )\n+                )\n+\n             # Disk usage\n-            disk = psutil.disk_usage('/')\n+            disk = psutil.disk_usage(\"/\")\n             if disk.percent > 90:\n-                self._log_event(SecurityEvent(\n-                    event_id=f\"high_disk_{int(time.time())}\",\n-                    event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n-                    severity=SecuritySeverity.LOW,\n-                    timestamp=datetime.now(),\n-                    source=\"system_monitor\",\n-                    description=f\"High disk usage detected: {disk.percent}%\",\n-                    details={\"disk_percent\": disk.percent}\n-                ))\n-                \n+                self._log_event(\n+                    SecurityEvent(\n+                        event_id=f\"high_disk_{int(time.time())}\",\n+                        event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n+                        severity=SecuritySeverity.LOW,\n+                        timestamp=datetime.now(),\n+                        source=\"system_monitor\",\n+                        description=f\"High disk usage detected: {disk.percent}%\",\n+                        details={\"disk_percent\": disk.percent},\n+                    )\n+                )\n+\n         except Exception as e:\n             self.logger.error(f\"Error monitoring system resources: {e}\")\n-    \n+\n     def _monitor_network_activity(self) -> None:\n         \"\"\"Monitor network activity.\"\"\"\n         try:\n             # Network connections\n-            connections = psutil.net_connections(kind='inet')\n-            \n+            connections = psutil.net_connections(kind=\"inet\")\n+\n             # Check for suspicious connections\n             suspicious_connections = []\n             for conn in connections:\n                 if conn.raddr and conn.raddr.port in [22, 3389, 5900]:  # SSH, RDP, VNC\n-                    suspicious_connections.append({\n-                        \"local_address\": f\"{conn.laddr.ip}:{conn.laddr.port}\",\n-                        \"remote_address\": f\"{conn.raddr.ip}:{conn.raddr.port}\",\n-                        \"status\": conn.status\n-                    })\n-            \n+                    suspicious_connections.append(\n+                        {\n+                            \"local_address\": f\"{conn.laddr.ip}:{conn.laddr.port}\",\n+                            \"remote_address\": f\"{conn.raddr.ip}:{conn.raddr.port}\",\n+                            \"status\": conn.status,\n+                        }\n+                    )\n+\n             if suspicious_connections:\n-                self._log_event(SecurityEvent(\n-                    event_id=f\"suspicious_connections_{int(time.time())}\",\n-                    event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n-                    severity=SecuritySeverity.MEDIUM,\n-                    timestamp=datetime.now(),\n-                    source=\"network_monitor\",\n-                    description=f\"Suspicious network connections detected: {len(suspicious_connections)}\",\n-                    details={\"connections\": suspicious_connections}\n-                ))\n-                \n+                self._log_event(\n+                    SecurityEvent(\n+                        event_id=f\"suspicious_connections_{int(time.time())}\",\n+                        event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n+                        severity=SecuritySeverity.MEDIUM,\n+                        timestamp=datetime.now(),\n+                        source=\"network_monitor\",\n+                        description=f\"Suspicious network connections detected: {len(suspicious_connections)}\",\n+                        details={\"connections\": suspicious_connections},\n+                    )\n+                )\n+\n         except Exception as e:\n             self.logger.error(f\"Error monitoring network activity: {e}\")\n-    \n+\n     def _monitor_file_system(self) -> None:\n         \"\"\"Monitor file system changes.\"\"\"\n         try:\n             # Check for critical file modifications\n             critical_files = [\n                 \"/etc/passwd\",\n                 \"/etc/shadow\",\n                 \"/etc/hosts\",\n-                \"/etc/crontab\"\n+                \"/etc/crontab\",\n             ]\n-            \n+\n             for file_path in critical_files:\n                 if os.path.exists(file_path):\n                     stat = os.stat(file_path)\n                     modified_time = datetime.fromtimestamp(stat.st_mtime)\n-                    \n+\n                     # Check if file was modified recently (within last hour)\n                     if datetime.now() - modified_time < timedelta(hours=1):\n-                        self._log_event(SecurityEvent(\n-                            event_id=f\"critical_file_modified_{int(time.time())}\",\n-                            event_type=SecurityEventType.SYSTEM_COMPROMISE,\n-                            severity=SecuritySeverity.HIGH,\n-                            timestamp=datetime.now(),\n-                            source=\"file_monitor\",\n-                            description=f\"Critical file modified: {file_path}\",\n-                            details={\n-                                \"file_path\": file_path,\n-                                \"modified_time\": modified_time.isoformat()\n-                            }\n-                        ))\n-                        \n+                        self._log_event(\n+                            SecurityEvent(\n+                                event_id=f\"critical_file_modified_{int(time.time())}\",\n+                                event_type=SecurityEventType.SYSTEM_COMPROMISE,\n+                                severity=SecuritySeverity.HIGH,\n+                                timestamp=datetime.now(),\n+                                source=\"file_monitor\",\n+                                description=f\"Critical file modified: {file_path}\",\n+                                details={\n+                                    \"file_path\": file_path,\n+                                    \"modified_time\": modified_time.isoformat(),\n+                                },\n+                            )\n+                        )\n+\n         except Exception as e:\n             self.logger.error(f\"Error monitoring file system: {e}\")\n-    \n+\n     def _monitor_process_activity(self) -> None:\n         \"\"\"Monitor process activity.\"\"\"\n         try:\n             # Check for suspicious processes\n-            processes = psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent'])\n-            \n+            processes = psutil.process_iter(\n+                [\"pid\", \"name\", \"cpu_percent\", \"memory_percent\"]\n+            )\n+\n             suspicious_processes = []\n             for proc in processes:\n                 try:\n-                    if proc.info['cpu_percent'] > 50 or proc.info['memory_percent'] > 20:\n+                    if (\n+                        proc.info[\"cpu_percent\"] > 50\n+                        or proc.info[\"memory_percent\"] > 20\n+                    ):\n                         # Check for known malicious process names\n-                        malicious_names = ['nc', 'netcat', 'ncat', 'socat', 'reverse_shell']\n-                        if any(name in proc.info['name'].lower() for name in malicious_names):\n-                            suspicious_processes.append({\n-                                \"pid\": proc.info['pid'],\n-                                \"name\": proc.info['name'],\n-                                \"cpu_percent\": proc.info['cpu_percent'],\n-                                \"memory_percent\": proc.info['memory_percent']\n-                            })\n+                        malicious_names = [\n+                            \"nc\",\n+                            \"netcat\",\n+                            \"ncat\",\n+                            \"socat\",\n+                            \"reverse_shell\",\n+                        ]\n+                        if any(\n+                            name in proc.info[\"name\"].lower()\n+                            for name in malicious_names\n+                        ):\n+                            suspicious_processes.append(\n+                                {\n+                                    \"pid\": proc.info[\"pid\"],\n+                                    \"name\": proc.info[\"name\"],\n+                                    \"cpu_percent\": proc.info[\"cpu_percent\"],\n+                                    \"memory_percent\": proc.info[\"memory_percent\"],\n+                                }\n+                            )\n                 except (psutil.NoSuchProcess, psutil.AccessDenied):\n                     continue\n-            \n+\n             if suspicious_processes:\n-                self._log_event(SecurityEvent(\n-                    event_id=f\"suspicious_processes_{int(time.time())}\",\n-                    event_type=SecurityEventType.MALWARE_DETECTION,\n-                    severity=SecuritySeverity.HIGH,\n-                    timestamp=datetime.now(),\n-                    source=\"process_monitor\",\n-                    description=f\"Suspicious processes detected: {len(suspicious_processes)}\",\n-                    details={\"processes\": suspicious_processes}\n-                ))\n-                \n+                self._log_event(\n+                    SecurityEvent(\n+                        event_id=f\"suspicious_processes_{int(time.time())}\",\n+                        event_type=SecurityEventType.MALWARE_DETECTION,\n+                        severity=SecuritySeverity.HIGH,\n+                        timestamp=datetime.now(),\n+                        source=\"process_monitor\",\n+                        description=f\"Suspicious processes detected: {len(suspicious_processes)}\",\n+                        details={\"processes\": suspicious_processes},\n+                    )\n+                )\n+\n         except Exception as e:\n             self.logger.error(f\"Error monitoring process activity: {e}\")\n-    \n+\n     def _detect_anomalies(self) -> None:\n         \"\"\"Detect security anomalies.\"\"\"\n         try:\n             # Check for rate limiting violations\n             if self._check_rate_limits():\n-                self._log_event(SecurityEvent(\n-                    event_id=f\"rate_limit_exceeded_{int(time.time())}\",\n-                    event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n-                    severity=SecuritySeverity.MEDIUM,\n-                    timestamp=datetime.now(),\n-                    source=\"anomaly_detector\",\n-                    description=\"Rate limit exceeded\",\n-                    details={\"rate_limits\": self.rate_limits}\n-                ))\n-            \n+                self._log_event(\n+                    SecurityEvent(\n+                        event_id=f\"rate_limit_exceeded_{int(time.time())}\",\n+                        event_type=SecurityEventType.SUSPICIOUS_ACTIVITY,\n+                        severity=SecuritySeverity.MEDIUM,\n+                        timestamp=datetime.now(),\n+                        source=\"anomaly_detector\",\n+                        description=\"Rate limit exceeded\",\n+                        details={\"rate_limits\": self.rate_limits},\n+                    )\n+                )\n+\n             # Check for failed authentication attempts\n             recent_events = self._get_recent_events(minutes=5)\n-            auth_failures = [e for e in recent_events \n-                           if e.event_type == SecurityEventType.AUTHENTICATION_FAILURE]\n-            \n+            auth_failures = [\n+                e\n+                for e in recent_events\n+                if e.event_type == SecurityEventType.AUTHENTICATION_FAILURE\n+            ]\n+\n             if len(auth_failures) > self.thresholds[\"max_failed_auth_attempts\"]:\n                 self._create_alert(\n                     event_id=f\"auth_brute_force_{int(time.time())}\",\n                     severity=SecuritySeverity.HIGH,\n                     title=\"Brute Force Attack Detected\",\n                     description=f\"Multiple failed authentication attempts: {len(auth_failures)}\",\n-                    recommendation=\"Implement account lockout and rate limiting\"\n+                    recommendation=\"Implement account lockout and rate limiting\",\n                 )\n-            \n+\n             # Check for privilege escalation attempts\n-            privilege_events = [e for e in recent_events \n-                              if e.event_type == SecurityEventType.PRIVILEGE_ESCALATION]\n-            \n-            if len(privilege_events) > self.thresholds[\"max_privilege_escalation_attempts\"]:\n+            privilege_events = [\n+                e\n+                for e in recent_events\n+                if e.event_type == SecurityEventType.PRIVILEGE_ESCALATION\n+            ]\n+\n+            if (\n+                len(privilege_events)\n+                > self.thresholds[\"max_privilege_escalation_attempts\"]\n+            ):\n                 self._create_alert(\n                     event_id=f\"privilege_escalation_{int(time.time())}\",\n                     severity=SecuritySeverity.CRITICAL,\n                     title=\"Privilege Escalation Attempts Detected\",\n                     description=f\"Multiple privilege escalation attempts: {len(privilege_events)}\",\n-                    recommendation=\"Review user permissions and implement least privilege principle\"\n+                    recommendation=\"Review user permissions and implement least privilege principle\",\n                 )\n-                \n+\n         except Exception as e:\n             self.logger.error(f\"Error detecting anomalies: {e}\")\n-    \n+\n     def _check_rate_limits(self) -> bool:\n         \"\"\"Check if rate limits are exceeded.\"\"\"\n         now = datetime.now()\n-        \n+\n         # Reset counters if needed\n         if now - self.event_counters[\"last_minute_reset\"] > timedelta(minutes=1):\n             self.event_counters[\"minute\"] = 0\n             self.event_counters[\"last_minute_reset\"] = now\n-        \n+\n         if now - self.event_counters[\"last_hour_reset\"] > timedelta(hours=1):\n             self.event_counters[\"hour\"] = 0\n             self.event_counters[\"last_hour_reset\"] = now\n-        \n+\n         # Check rate limits\n-        if (self.event_counters[\"minute\"] > self.rate_limits[\"events_per_minute\"] or\n-            self.event_counters[\"hour\"] > self.rate_limits[\"alerts_per_hour\"]):\n+        if (\n+            self.event_counters[\"minute\"] > self.rate_limits[\"events_per_minute\"]\n+            or self.event_counters[\"hour\"] > self.rate_limits[\"alerts_per_hour\"]\n+        ):\n             return True\n-        \n+\n         return False\n-    \n+\n     def _get_recent_events(self, minutes: int = 5) -> List[SecurityEvent]:\n         \"\"\"Get recent events within specified time window.\"\"\"\n         cutoff_time = datetime.now() - timedelta(minutes=minutes)\n         return [e for e in self.events if e.timestamp > cutoff_time]\n-    \n+\n     def _log_event(self, event: SecurityEvent) -> None:\n         \"\"\"Log a security event.\"\"\"\n         try:\n             # Check rate limits\n             if not self._check_rate_limits():\n                 self.events.append(event)\n                 self.event_counters[\"minute\"] += 1\n                 self.event_counters[\"hour\"] += 1\n-            \n+\n             # Log to file\n-            self.logger.info(f\"Security Event: {event.event_type.value} - {event.description}\")\n-            \n+            self.logger.info(\n+                f\"Security Event: {event.event_type.value} - {event.description}\"\n+            )\n+\n             # Call event callbacks\n             for callback in self.event_callbacks:\n                 try:\n                     callback(event)\n                 except Exception as e:\n                     self.logger.error(f\"Error in event callback: {e}\")\n-            \n+\n             # Create alert if severity is high enough\n             if event.severity in [SecuritySeverity.CRITICAL, SecuritySeverity.HIGH]:\n                 self._create_alert(\n                     event_id=event.event_id,\n                     severity=event.severity,\n                     title=f\"Security Event: {event.event_type.value}\",\n                     description=event.description,\n-                    recommendation=self._get_recommendation(event)\n+                    recommendation=self._get_recommendation(event),\n                 )\n-                \n+\n         except Exception as e:\n             self.logger.error(f\"Error logging security event: {e}\")\n-    \n-    def _create_alert(self, event_id: str, severity: SecuritySeverity, \n-                     title: str, description: str, recommendation: str) -> None:\n+\n+    def _create_alert(\n+        self,\n+        event_id: str,\n+        severity: SecuritySeverity,\n+        title: str,\n+        description: str,\n+        recommendation: str,\n+    ) -> None:\n         \"\"\"Create a security alert.\"\"\"\n         try:\n             alert = SecurityAlert(\n                 alert_id=f\"alert_{event_id}_{int(time.time())}\",\n                 event_id=event_id,\n                 severity=severity,\n                 timestamp=datetime.now(),\n                 title=title,\n                 description=description,\n-                recommendation=recommendation\n+                recommendation=recommendation,\n             )\n-            \n+\n             self.alerts.append(alert)\n-            \n+\n             # Call alert callbacks\n             for callback in self.alert_callbacks:\n                 try:\n                     callback(alert)\n                 except Exception as e:\n                     self.logger.error(f\"Error in alert callback: {e}\")\n-            \n+\n             # Log alert\n             self.logger.warning(f\"Security Alert: {alert.title} - {alert.description}\")\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error creating security alert: {e}\")\n-    \n+\n     def _get_recommendation(self, event: SecurityEvent) -> str:\n         \"\"\"Get recommendation for security event.\"\"\"\n         recommendations = {\n             SecurityEventType.AUTHENTICATION_FAILURE: \"Review authentication logs and implement account lockout\",\n             SecurityEventType.AUTHORIZATION_FAILURE: \"Review user permissions and access controls\",\n@@ -512,198 +571,223 @@\n             SecurityEventType.MALWARE_DETECTION: \"Quarantine affected systems and run malware scan\",\n             SecurityEventType.INTRUSION_ATTEMPT: \"Block suspicious IP addresses and review firewall rules\",\n             SecurityEventType.PRIVILEGE_ESCALATION: \"Review user permissions and implement least privilege principle\",\n             SecurityEventType.DATA_EXFILTRATION: \"Investigate data access patterns and implement data loss prevention\",\n             SecurityEventType.SYSTEM_COMPROMISE: \"Isolate compromised systems and perform forensic analysis\",\n-            SecurityEventType.CONFIGURATION_CHANGE: \"Review configuration changes and implement change management\"\n+            SecurityEventType.CONFIGURATION_CHANGE: \"Review configuration changes and implement change management\",\n         }\n-        \n-        return recommendations.get(event.event_type, \"Review security logs and investigate incident\")\n-    \n+\n+        return recommendations.get(\n+            event.event_type, \"Review security logs and investigate incident\"\n+        )\n+\n     def _update_metrics(self) -> None:\n         \"\"\"Update security metrics.\"\"\"\n         try:\n             now = datetime.now()\n-            \n+\n             # Calculate metrics\n             total_events = len(self.events)\n             events_by_type = {}\n             events_by_severity = {}\n-            \n+\n             for event in self.events:\n                 event_type = event.event_type.value\n                 severity = event.severity.value\n-                \n+\n                 events_by_type[event_type] = events_by_type.get(event_type, 0) + 1\n                 events_by_severity[severity] = events_by_severity.get(severity, 0) + 1\n-            \n+\n             active_alerts = len([a for a in self.alerts if not a.resolved])\n             resolved_alerts = len([a for a in self.alerts if a.resolved])\n-            \n+\n             # Calculate mean time to resolution\n             resolved_alert_times = []\n             for alert in self.alerts:\n                 if alert.resolved:\n                     # This would need to track resolution time in real implementation\n                     resolved_alert_times.append(0)  # Placeholder\n-            \n-            mean_time_to_resolution = sum(resolved_alert_times) / len(resolved_alert_times) if resolved_alert_times else 0\n-            \n+\n+            mean_time_to_resolution = (\n+                sum(resolved_alert_times) / len(resolved_alert_times)\n+                if resolved_alert_times\n+                else 0\n+            )\n+\n             # Calculate security score (0-100, higher is better)\n             security_score = self._calculate_security_score()\n-            \n+\n             metrics = SecurityMetrics(\n                 timestamp=now,\n                 total_events=total_events,\n                 events_by_type=events_by_type,\n                 events_by_severity=events_by_severity,\n                 active_alerts=active_alerts,\n                 resolved_alerts=resolved_alerts,\n                 false_positives=0,  # Would need to track in real implementation\n                 mean_time_to_resolution=mean_time_to_resolution,\n-                security_score=security_score\n+                security_score=security_score,\n             )\n-            \n+\n             self.metrics.append(metrics)\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error updating metrics: {e}\")\n-    \n+\n     def _calculate_security_score(self) -> float:\n         \"\"\"Calculate overall security score.\"\"\"\n         try:\n             # Base score\n             score = 100.0\n-            \n+\n             # Deduct points for active alerts\n             active_alerts = len([a for a in self.alerts if not a.resolved])\n             score -= active_alerts * 5  # 5 points per active alert\n-            \n+\n             # Deduct points for critical/high severity events in last 24 hours\n             recent_events = self._get_recent_events(minutes=1440)  # 24 hours\n-            critical_events = len([e for e in recent_events \n-                                 if e.severity in [SecuritySeverity.CRITICAL, SecuritySeverity.HIGH]])\n+            critical_events = len(\n+                [\n+                    e\n+                    for e in recent_events\n+                    if e.severity in [SecuritySeverity.CRITICAL, SecuritySeverity.HIGH]\n+                ]\n+            )\n             score -= critical_events * 10  # 10 points per critical/high event\n-            \n+\n             # Ensure score doesn't go below 0\n             return max(0.0, score)\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error calculating security score: {e}\")\n             return 0.0\n-    \n+\n     def _cleanup_old_data(self) -> None:\n         \"\"\"Clean up old data based on retention policy.\"\"\"\n         try:\n             retention_days = self.config.get(\"retention_days\", 30)\n             cutoff_time = datetime.now() - timedelta(days=retention_days)\n-            \n+\n             # Clean up old events\n             self.events = [e for e in self.events if e.timestamp > cutoff_time]\n-            \n+\n             # Clean up old alerts (keep resolved alerts for longer)\n             resolved_cutoff = datetime.now() - timedelta(days=retention_days * 2)\n-            self.alerts = [a for a in self.alerts \n-                          if a.timestamp > cutoff_time or \n-                          (a.resolved and a.timestamp > resolved_cutoff)]\n-            \n+            self.alerts = [\n+                a\n+                for a in self.alerts\n+                if a.timestamp > cutoff_time\n+                or (a.resolved and a.timestamp > resolved_cutoff)\n+            ]\n+\n             # Clean up old metrics (keep more metrics)\n             metrics_cutoff = datetime.now() - timedelta(days=retention_days // 2)\n             self.metrics = [m for m in self.metrics if m.timestamp > metrics_cutoff]\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error cleaning up old data: {e}\")\n-    \n+\n     def add_event_callback(self, callback: Callable[[SecurityEvent], None]) -> None:\n         \"\"\"Add event callback.\"\"\"\n         self.event_callbacks.append(callback)\n-    \n+\n     def add_alert_callback(self, callback: Callable[[SecurityAlert], None]) -> None:\n         \"\"\"Add alert callback.\"\"\"\n         self.alert_callbacks.append(callback)\n-    \n+\n     def get_security_dashboard_data(self) -> Dict[str, Any]:\n         \"\"\"Get data for security dashboard.\"\"\"\n         try:\n             recent_events = self._get_recent_events(minutes=60)  # Last hour\n-            recent_alerts = [a for a in self.alerts \n-                           if a.timestamp > datetime.now() - timedelta(hours=24)]\n-            \n+            recent_alerts = [\n+                a\n+                for a in self.alerts\n+                if a.timestamp > datetime.now() - timedelta(hours=24)\n+            ]\n+\n             # Get latest metrics\n             latest_metrics = self.metrics[-1] if self.metrics else None\n-            \n+\n             return {\n-                \"current_security_score\": latest_metrics.security_score if latest_metrics else 0,\n+                \"current_security_score\": (\n+                    latest_metrics.security_score if latest_metrics else 0\n+                ),\n                 \"active_alerts\": len([a for a in self.alerts if not a.resolved]),\n                 \"recent_events\": len(recent_events),\n-                \"events_by_severity\": latest_metrics.events_by_severity if latest_metrics else {},\n+                \"events_by_severity\": (\n+                    latest_metrics.events_by_severity if latest_metrics else {}\n+                ),\n                 \"top_threats\": self._get_top_threats(),\n                 \"security_trends\": self._get_security_trends(),\n                 \"monitoring_status\": {\n                     \"active\": self.monitoring_active,\n                     \"uptime\": self._get_uptime(),\n-                    \"last_scan\": datetime.now().isoformat()\n-                }\n+                    \"last_scan\": datetime.now().isoformat(),\n+                },\n             }\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error getting dashboard data: {e}\")\n             return {}\n-    \n+\n     def _get_top_threats(self) -> List[Dict[str, Any]]:\n         \"\"\"Get top security threats.\"\"\"\n         try:\n             # Analyze recent events to identify top threats\n             recent_events = self._get_recent_events(minutes=1440)  # Last 24 hours\n-            \n+\n             threat_counts = {}\n             for event in recent_events:\n                 threat_type = event.event_type.value\n                 threat_counts[threat_type] = threat_counts.get(threat_type, 0) + 1\n-            \n+\n             # Sort by count and return top 5\n-            sorted_threats = sorted(threat_counts.items(), key=lambda x: x[1], reverse=True)\n-            \n+            sorted_threats = sorted(\n+                threat_counts.items(), key=lambda x: x[1], reverse=True\n+            )\n+\n             return [\n                 {\"threat_type\": threat, \"count\": count}\n                 for threat, count in sorted_threats[:5]\n             ]\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error getting top threats: {e}\")\n             return []\n-    \n+\n     def _get_security_trends(self) -> List[Dict[str, Any]]:\n         \"\"\"Get security trends over time.\"\"\"\n         try:\n             # Get metrics from last 7 days\n             week_ago = datetime.now() - timedelta(days=7)\n             recent_metrics = [m for m in self.metrics if m.timestamp > week_ago]\n-            \n+\n             trends = []\n             for metrics in recent_metrics:\n-                trends.append({\n-                    \"timestamp\": metrics.timestamp.isoformat(),\n-                    \"security_score\": metrics.security_score,\n-                    \"total_events\": metrics.total_events,\n-                    \"active_alerts\": metrics.active_alerts\n-                })\n-            \n+                trends.append(\n+                    {\n+                        \"timestamp\": metrics.timestamp.isoformat(),\n+                        \"security_score\": metrics.security_score,\n+                        \"total_events\": metrics.total_events,\n+                        \"active_alerts\": metrics.active_alerts,\n+                    }\n+                )\n+\n             return trends\n-            \n+\n         except Exception as e:\n             self.logger.error(f\"Error getting security trends: {e}\")\n             return []\n-    \n+\n     def _get_uptime(self) -> str:\n         \"\"\"Get monitoring uptime.\"\"\"\n         if not self.monitoring_active:\n             return \"Stopped\"\n-        \n+\n         # This would track actual start time in real implementation\n         return \"Running\"\n-    \n+\n     def acknowledge_alert(self, alert_id: str) -> bool:\n         \"\"\"Acknowledge a security alert.\"\"\"\n         try:\n             for alert in self.alerts:\n                 if alert.alert_id == alert_id:\n@@ -712,11 +796,11 @@\n                     return True\n             return False\n         except Exception as e:\n             self.logger.error(f\"Error acknowledging alert: {e}\")\n             return False\n-    \n+\n     def resolve_alert(self, alert_id: str) -> bool:\n         \"\"\"Resolve a security alert.\"\"\"\n         try:\n             for alert in self.alerts:\n                 if alert.alert_id == alert_id:\n@@ -726,11 +810,11 @@\n                     return True\n             return False\n         except Exception as e:\n             self.logger.error(f\"Error resolving alert: {e}\")\n             return False\n-    \n+\n     def escalate_alert(self, alert_id: str) -> bool:\n         \"\"\"Escalate a security alert.\"\"\"\n         try:\n             for alert in self.alerts:\n                 if alert.alert_id == alert_id:\n@@ -739,85 +823,95 @@\n                     return True\n             return False\n         except Exception as e:\n             self.logger.error(f\"Error escalating alert: {e}\")\n             return False\n-    \n+\n     def export_security_report(self, output_file: Optional[Path] = None) -> Path:\n         \"\"\"Export comprehensive security report.\"\"\"\n         if output_file is None:\n             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n             output_file = Path(f\"security_report_{timestamp}.json\")\n-        \n+\n         report_data = {\n             \"report_metadata\": {\n                 \"generated_at\": datetime.now().isoformat(),\n                 \"monitoring_uptime\": self._get_uptime(),\n                 \"total_events\": len(self.events),\n                 \"total_alerts\": len(self.alerts),\n-                \"total_metrics\": len(self.metrics)\n+                \"total_metrics\": len(self.metrics),\n             },\n-            \"events\": [asdict(event) for event in self.events[-1000:]],  # Last 1000 events\n-            \"alerts\": [asdict(alert) for alert in self.alerts[-500:]],   # Last 500 alerts\n-            \"metrics\": [asdict(metric) for metric in self.metrics[-100:]], # Last 100 metrics\n+            \"events\": [\n+                asdict(event) for event in self.events[-1000:]\n+            ],  # Last 1000 events\n+            \"alerts\": [\n+                asdict(alert) for alert in self.alerts[-500:]\n+            ],  # Last 500 alerts\n+            \"metrics\": [\n+                asdict(metric) for metric in self.metrics[-100:]\n+            ],  # Last 100 metrics\n             \"dashboard_data\": self.get_security_dashboard_data(),\n-            \"configuration\": self.config\n+            \"configuration\": self.config,\n         }\n-        \n-        with open(output_file, 'w') as f:\n+\n+        with open(output_file, \"w\") as f:\n             json.dump(report_data, f, indent=2, default=str)\n-        \n+\n         return output_file\n \n \n # Example usage and testing\n if __name__ == \"__main__\":\n     import argparse\n-    \n+\n     parser = argparse.ArgumentParser(description=\"SparkForge Security Monitor\")\n     parser.add_argument(\"--config\", type=Path, help=\"Configuration file\")\n     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n-    parser.add_argument(\"--duration\", type=int, default=300, help=\"Monitoring duration in seconds\")\n-    \n+    parser.add_argument(\n+        \"--duration\", type=int, default=300, help=\"Monitoring duration in seconds\"\n+    )\n+\n     args = parser.parse_args()\n-    \n+\n     # Load configuration if provided\n     config = {}\n     if args.config and args.config.exists():\n-        with open(args.config, 'r') as f:\n+        with open(args.config, \"r\") as f:\n             config = json.load(f)\n-    \n+\n     # Create security monitor\n     monitor = SecurityMonitor(config)\n-    \n+\n     # Add example callbacks\n     def event_callback(event: SecurityEvent):\n         print(f\"Security Event: {event.event_type.value} - {event.description}\")\n-    \n+\n     def alert_callback(alert: SecurityAlert):\n         print(f\"Security Alert: {alert.title} - {alert.description}\")\n-    \n+\n     monitor.add_event_callback(event_callback)\n     monitor.add_alert_callback(alert_callback)\n-    \n+\n     # Start monitoring\n     monitor.start_monitoring()\n-    \n+\n     try:\n         # Run for specified duration\n         time.sleep(args.duration)\n     except KeyboardInterrupt:\n         print(\"Monitoring stopped by user\")\n     finally:\n         # Stop monitoring\n         monitor.stop_monitoring()\n-        \n+\n         # Generate report\n         report_file = monitor.export_security_report(args.output)\n         print(f\"Security report saved to: {report_file}\")\n-        \n+\n         # Print summary\n         dashboard_data = monitor.get_security_dashboard_data()\n         print(f\"\\nSecurity Summary:\")\n-        print(f\"Security Score: {dashboard_data.get('current_security_score', 0):.1f}/100\")\n+        print(\n+            f\"Security Score: {dashboard_data.get('current_security_score', 0):.1f}/100\"\n+        )\n         print(f\"Active Alerts: {dashboard_data.get('active_alerts', 0)}\")\n         print(f\"Recent Events: {dashboard_data.get('recent_events', 0)}\")\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_analyzer.py\t2025-09-20 14:00:38.764771+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_analyzer.py\t2025-09-20 16:53:35.161229+00:00\n@@ -167,13 +167,13 @@\n         # This is expected behavior as it's handled in the graph building phase\n \n     def test_analyze_dependencies_warning_scenarios(self):\n         \"\"\"Test analyze_dependencies warning scenarios.\"\"\"\n         analyzer = DependencyAnalyzer()\n-        \n+\n         # Test with logger to capture warnings\n-        with patch.object(analyzer.logger, 'warning') as mock_warning:\n+        with patch.object(analyzer.logger, \"warning\") as mock_warning:\n             # Test missing bronze dependency warning\n             silver_steps = {\n                 \"silver1\": SilverStep(\n                     name=\"silver1\",\n                     source_bronze=\"missing_bronze\",\n@@ -181,37 +181,36 @@\n                     rules={\"id\": []},\n                     table_name=\"silver1_table\",\n                 )\n             }\n             result = analyzer.analyze_dependencies(silver_steps=silver_steps)\n-            \n+\n             # Check that warning was logged for missing bronze\n             mock_warning.assert_any_call(\n                 \"Silver step silver1 references non-existent bronze step missing_bronze\"\n             )\n \n     def test_analyze_dependencies_silver_depends_on_warning(self):\n         \"\"\"Test analyze_dependencies with silver step depends_on warning.\"\"\"\n         analyzer = DependencyAnalyzer()\n-        \n+\n         # Create a silver step with depends_on that references non-existent step\n         silver_step = SilverStep(\n             name=\"silver1\",\n             source_bronze=\"bronze1\",\n             transform=lambda spark, df, silvers: df,\n             rules={\"id\": []},\n             table_name=\"silver1_table\",\n         )\n         # Manually add depends_on attribute\n         silver_step.depends_on = [\"missing_dep\"]\n-        \n-        with patch.object(analyzer.logger, 'warning') as mock_warning:\n+\n+        with patch.object(analyzer.logger, \"warning\") as mock_warning:\n             result = analyzer.analyze_dependencies(\n-                bronze_steps={\"bronze1\": Mock()},\n-                silver_steps={\"silver1\": silver_step}\n-            )\n-            \n+                bronze_steps={\"bronze1\": Mock()}, silver_steps={\"silver1\": silver_step}\n+            )\n+\n             # Check that warning was logged for missing dependency\n             mock_warning.assert_any_call(\n                 \"Silver step silver1 references non-existent dependency missing_dep\"\n             )\n \n@@ -372,78 +371,91 @@\n         assert any(\"missing node missing_step\" in conflict for conflict in conflicts)\n \n     def test_analyze_dependencies_cycle_warning(self):\n         \"\"\"Test analyze_dependencies with cycle detection warning.\"\"\"\n         analyzer = DependencyAnalyzer()\n-        \n+\n         # Mock the entire analyze_dependencies method to test warning scenarios\n-        with patch.object(analyzer, '_build_dependency_graph') as mock_build_graph:\n+        with patch.object(analyzer, \"_build_dependency_graph\") as mock_build_graph:\n             mock_graph = Mock()\n             mock_graph.detect_cycles.return_value = [[\"step1\", \"step2\"]]\n             mock_graph.get_execution_groups.return_value = [[\"step1\"], [\"step2\"]]\n-            mock_graph.get_stats.return_value = {\"total_steps\": 2, \"average_dependencies\": 1}\n+            mock_graph.get_stats.return_value = {\n+                \"total_steps\": 2,\n+                \"average_dependencies\": 1,\n+            }\n             mock_build_graph.return_value = mock_graph\n-            \n+\n             # Mock _resolve_cycles to return the same graph\n-            with patch.object(analyzer, '_resolve_cycles', return_value=mock_graph):\n+            with patch.object(analyzer, \"_resolve_cycles\", return_value=mock_graph):\n                 # Mock _detect_conflicts to return no conflicts\n-                with patch.object(analyzer, '_detect_conflicts', return_value=[]):\n+                with patch.object(analyzer, \"_detect_conflicts\", return_value=[]):\n                     # Mock _generate_recommendations\n-                    with patch.object(analyzer, '_generate_recommendations', return_value=[]):\n-                        with patch.object(analyzer.logger, 'warning') as mock_warning:\n+                    with patch.object(\n+                        analyzer, \"_generate_recommendations\", return_value=[]\n+                    ):\n+                        with patch.object(analyzer.logger, \"warning\") as mock_warning:\n                             result = analyzer.analyze_dependencies()\n-                            \n+\n                             # Check that warning was logged for cycles\n-                            mock_warning.assert_any_call(\"Detected 1 circular dependencies\")\n+                            mock_warning.assert_any_call(\n+                                \"Detected 1 circular dependencies\"\n+                            )\n \n     def test_analyze_dependencies_conflict_warning(self):\n         \"\"\"Test analyze_dependencies with conflict detection warning.\"\"\"\n         analyzer = DependencyAnalyzer()\n-        \n+\n         # Mock the entire analyze_dependencies method to test warning scenarios\n-        with patch.object(analyzer, '_build_dependency_graph') as mock_build_graph:\n+        with patch.object(analyzer, \"_build_dependency_graph\") as mock_build_graph:\n             mock_graph = Mock()\n             mock_graph.detect_cycles.return_value = []\n             mock_graph.get_execution_groups.return_value = [[\"step1\"], [\"step2\"]]\n-            mock_graph.get_stats.return_value = {\"total_steps\": 2, \"average_dependencies\": 1}\n+            mock_graph.get_stats.return_value = {\n+                \"total_steps\": 2,\n+                \"average_dependencies\": 1,\n+            }\n             mock_build_graph.return_value = mock_graph\n-            \n+\n             # Mock _detect_conflicts to return conflicts\n-            with patch.object(analyzer, '_detect_conflicts', return_value=[\"test conflict\"]):\n+            with patch.object(\n+                analyzer, \"_detect_conflicts\", return_value=[\"test conflict\"]\n+            ):\n                 # Mock _generate_recommendations\n-                with patch.object(analyzer, '_generate_recommendations', return_value=[]):\n-                    with patch.object(analyzer.logger, 'warning') as mock_warning:\n+                with patch.object(\n+                    analyzer, \"_generate_recommendations\", return_value=[]\n+                ):\n+                    with patch.object(analyzer.logger, \"warning\") as mock_warning:\n                         result = analyzer.analyze_dependencies()\n-                        \n+\n             # Check that warning was logged for conflicts\n             mock_warning.assert_any_call(\"Detected 1 dependency conflicts\")\n \n     def test_analyze_dependencies_silver_valid_dependency(self):\n         \"\"\"Test analyze_dependencies with silver step having valid depends_on.\"\"\"\n         analyzer = DependencyAnalyzer()\n-        \n+\n         # Create a silver step with valid depends_on\n         silver_step = SilverStep(\n             name=\"silver1\",\n             source_bronze=\"bronze1\",\n             transform=lambda spark, df, silvers: df,\n             rules={\"id\": []},\n             table_name=\"silver1_table\",\n         )\n         # Manually add depends_on attribute\n         silver_step.depends_on = [\"bronze2\"]\n-        \n+\n         bronze_steps = {\n             \"bronze1\": Mock(),\n             \"bronze2\": Mock(),\n         }\n-        \n+\n         result = analyzer.analyze_dependencies(\n-            bronze_steps=bronze_steps,\n-            silver_steps={\"silver1\": silver_step}\n-        )\n-        \n+            bronze_steps=bronze_steps, silver_steps={\"silver1\": silver_step}\n+        )\n+\n         # Check that the dependency was added\n         assert \"silver1\" in result.graph.nodes\n         assert \"bronze2\" in result.graph.nodes[\"silver1\"].dependencies\n \n     def test_detect_conflicts_duplicate_names(self):\n@@ -455,11 +467,11 @@\n         graph.add_node(StepNode(\"step1\", StepType.BRONZE, []))\n         graph.add_node(StepNode(\"step2\", StepType.SILVER, []))\n \n         # Manually modify the step_names list to have duplicates\n         # This simulates the scenario where the conflict detection logic would trigger\n-        with patch.object(analyzer, '_detect_conflicts') as mock_detect_conflicts:\n+        with patch.object(analyzer, \"_detect_conflicts\") as mock_detect_conflicts:\n             # Create a mock that calls the real method but with modified step_names\n             def mock_detect_conflicts_impl(graph):\n                 conflicts = []\n                 # Simulate duplicate step names in the list\n                 step_names = [\"step1\", \"step2\", \"step1\"]  # Duplicate step1\n@@ -467,16 +479,18 @@\n                 for node_name in step_names:\n                     if node_name in seen_names:\n                         conflicts.append(f\"Conflicting step name: {node_name}\")\n                     seen_names.add(node_name)\n                 return conflicts\n-            \n+\n             mock_detect_conflicts.side_effect = mock_detect_conflicts_impl\n-            \n+\n             conflicts = analyzer._detect_conflicts(graph)\n             assert len(conflicts) > 0\n-            assert any(\"Conflicting step name: step1\" in conflict for conflict in conflicts)\n+            assert any(\n+                \"Conflicting step name: step1\" in conflict for conflict in conflicts\n+            )\n \n     def test_generate_recommendations_no_issues(self):\n         \"\"\"Test _generate_recommendations with no issues.\"\"\"\n         analyzer = DependencyAnalyzer()\n         graph = DependencyGraph()\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_errors.py\t2025-09-20 14:00:38.765961+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_errors.py\t2025-09-20 16:53:35.167612+00:00\n@@ -91,11 +91,11 @@\n         assert error.message == \"Test error\"\n         assert error.error_code == \"TEST_001\"\n         assert error.category == ErrorCategory.CONFIGURATION\n         assert error.severity == ErrorSeverity.HIGH\n         assert error.context == {\"key\": \"value\"}\n-        \n+\n         # Test __str__ method includes error_code\n         error_str = str(error)\n         assert \"Test error\" in error_str\n         assert \"[TEST_001]\" in error_str\n         assert error.suggestions == [\"Fix this\", \"Check that\"]\n@@ -383,13 +383,13 @@\n         \"\"\"Test ResourceError creation and initialization.\"\"\"\n         error = ResourceError(\n             message=\"Resource not found\",\n             error_code=\"RESOURCE_001\",\n             context={\"resource\": \"database\"},\n-            suggestions=[\"Check connection\", \"Verify permissions\"]\n-        )\n-        \n+            suggestions=[\"Check connection\", \"Verify permissions\"],\n+        )\n+\n         assert error.message == \"Resource not found\"\n         assert error.error_code == \"RESOURCE_001\"\n         assert error.category == ErrorCategory.RESOURCE\n         assert error.severity == ErrorSeverity.HIGH\n         assert error.context == {\"resource\": \"database\"}\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_additional_coverage.py\t2025-09-20 14:42:20.268362+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_additional_coverage.py\t2025-09-20 16:53:35.209040+00:00\n@@ -55,114 +55,111 @@\n         \"\"\"Test apply_column_rules with empty rules.\"\"\"\n         # Create mock DataFrame\n         mock_df = Mock(spec=DataFrame)\n         mock_df.count.return_value = 100\n         mock_df.limit.return_value = mock_df\n-        \n+\n         rules = {}  # Empty rules should return all rows as valid\n-        \n+\n         result = apply_column_rules(\n-            mock_df, \n-            rules, \n-            stage=\"test_stage\",\n-            step=\"test_step\"\n+            mock_df, rules, stage=\"test_stage\", step=\"test_step\"\n         )\n-        \n+\n         assert result is not None\n         mock_df.limit.assert_called_once_with(0)\n \n     def test_apply_column_rules_with_rules(self) -> None:\n         \"\"\"Test apply_column_rules with actual rules.\"\"\"\n         # Create mock DataFrame\n         mock_df = Mock(spec=DataFrame)\n         mock_df.count.return_value = 100\n         mock_df.filter.return_value = mock_df\n         mock_df.limit.return_value = mock_df\n-        \n+\n         rules = {\"col1\": [\"not_null\"]}\n-        \n+\n         result = apply_column_rules(\n-            mock_df, \n-            rules, \n+            mock_df,\n+            rules,\n             stage=\"test_stage\",\n             step=\"test_step\",\n-            filter_columns_by_rules=False\n+            filter_columns_by_rules=False,\n         )\n-        \n+\n         assert result is not None\n \n     def test_safe_divide_edge_cases(self) -> None:\n         \"\"\"Test safe_divide with edge cases.\"\"\"\n         # Test with zero denominator\n         result = safe_divide(10, 0)\n         assert result == 0.0\n-        \n+\n         # Test with zero denominator and custom default\n         result = safe_divide(10, 0, default=1.0)\n         assert result == 1.0\n \n     def test_validate_dataframe_schema_edge_cases(self) -> None:\n         \"\"\"Test validate_dataframe_schema with edge cases.\"\"\"\n         # Create mock DataFrame\n         mock_df = Mock(spec=DataFrame)\n         mock_df.columns = [\"col1\", \"col2\"]\n-        \n+\n         # Test with empty expected columns\n         result = validate_dataframe_schema(mock_df, [])\n         assert result is True\n \n     def test_get_dataframe_info_edge_cases(self) -> None:\n         \"\"\"Test get_dataframe_info with edge cases.\"\"\"\n         # Create mock DataFrame\n         mock_df = Mock(spec=DataFrame)\n         mock_df.count.return_value = 0\n         mock_df.columns = []\n-        \n+\n         # Test with empty DataFrame\n         result = get_dataframe_info(mock_df)\n         assert result[\"row_count\"] == 0\n         assert result[\"column_count\"] == 0\n \n     def test_get_dataframe_info_error_handling(self) -> None:\n         \"\"\"Test get_dataframe_info error handling.\"\"\"\n         # Create mock DataFrame that raises exception\n         mock_df = Mock(spec=DataFrame)\n         mock_df.count.side_effect = Exception(\"Test error\")\n-        \n+\n         # This should handle the exception gracefully\n         result = get_dataframe_info(mock_df)\n         assert \"error\" in result\n \n     def test_assess_data_quality_edge_cases(self) -> None:\n         \"\"\"Test assess_data_quality with edge cases.\"\"\"\n         # Create mock DataFrame\n         mock_df = Mock(spec=DataFrame)\n         mock_df.count.return_value = 100\n-        \n+\n         # Test with empty rules\n         result = assess_data_quality(mock_df, {})\n         assert result is not None\n \n     def test_apply_validation_rules_edge_cases(self) -> None:\n         \"\"\"Test apply_validation_rules with edge cases.\"\"\"\n         # Create mock DataFrame\n         mock_df = Mock(spec=DataFrame)\n         mock_df.count.return_value = 100\n-        \n+\n         # Test with empty rules\n         result = apply_validation_rules(mock_df, {}, \"test_stage\", \"test_step\")\n         assert result is not None\n \n     def test_convert_rules_to_expressions_complex_cases(self) -> None:\n         \"\"\"Test _convert_rules_to_expressions with complex cases.\"\"\"\n         # Test with mixed rule types\n         rules = {\n             \"col1\": [\"not_null\"],\n             \"col2\": [col(\"col2\") > 0],\n-            \"col3\": [None, \"invalid\"]\n+            \"col3\": [None, \"invalid\"],\n         }\n-        \n+\n         result = _convert_rules_to_expressions(rules)\n         assert isinstance(result, dict)\n \n     def test_convert_rule_to_expression_edge_cases(self) -> None:\n         \"\"\"Test _convert_rule_to_expression with edge cases.\"\"\"\n@@ -178,34 +175,31 @@\n         assert isinstance(result, Column)\n \n     def test_and_all_rules_multiple_expressions(self) -> None:\n         \"\"\"Test and_all_rules with multiple expressions.\"\"\"\n         # Test with multiple expressions\n-        rules = {\n-            \"col1\": [\"col1 > 0\"],\n-            \"col2\": [\"col2 IS NOT NULL\"]\n-        }\n+        rules = {\"col1\": [\"col1 > 0\"], \"col2\": [\"col2 IS NOT NULL\"]}\n         result = and_all_rules(rules)\n         assert isinstance(result, Column)\n \n     def test_string_rule_conversion_edge_cases(self) -> None:\n         \"\"\"Test string rule conversion edge cases.\"\"\"\n         # Test various string rule formats\n         test_cases = [\n             \"col1 > 0\",\n             \"col1 IS NOT NULL\",\n             \"col1 IN ('a', 'b', 'c')\",\n-            \"LENGTH(col1) > 5\"\n+            \"LENGTH(col1) > 5\",\n         ]\n-        \n+\n         for rule in test_cases:\n             result = _convert_rule_to_expression(rule, \"col1\")\n             assert isinstance(result, Column)\n \n     def test_validation_error_handling(self) -> None:\n         \"\"\"Test validation error handling paths.\"\"\"\n         # Test with invalid DataFrame\n         with pytest.raises((AttributeError, TypeError)):\n             apply_column_rules(None, {\"col1\": [\"not_null\"]})  # type: ignore\n-            \n+\n         with pytest.raises((AttributeError, TypeError)):\n             apply_column_rules(\"invalid\", {\"col1\": [\"not_null\"]})  # type: ignore\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_property_based.py\t2025-09-20 15:59:31.523706+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_property_based.py\t2025-09-20 16:53:35.211403+00:00\n@@ -22,82 +22,96 @@\n \n class TestValidationPropertyBased:\n     \"\"\"Property-based tests for validation functions.\"\"\"\n \n     @given(\n-        numerator=st.floats(min_value=-100.0, max_value=100.0, allow_nan=False, allow_infinity=False),\n-        denominator=st.floats(min_value=0.001, max_value=100.0, allow_nan=False, allow_infinity=False),\n-        default_value=st.floats(min_value=-100.0, max_value=100.0, allow_nan=False, allow_infinity=False)\n+        numerator=st.floats(\n+            min_value=-100.0, max_value=100.0, allow_nan=False, allow_infinity=False\n+        ),\n+        denominator=st.floats(\n+            min_value=0.001, max_value=100.0, allow_nan=False, allow_infinity=False\n+        ),\n+        default_value=st.floats(\n+            min_value=-100.0, max_value=100.0, allow_nan=False, allow_infinity=False\n+        ),\n     )\n     @settings(max_examples=100)\n-    def test_safe_divide_properties(self, numerator: float, denominator: float, default_value: float) -> None:\n+    def test_safe_divide_properties(\n+        self, numerator: float, denominator: float, default_value: float\n+    ) -> None:\n         \"\"\"Test safe_divide with generated float values.\"\"\"\n         result = safe_divide(numerator, denominator, default_value)\n-        \n+\n         # Property: When denominator is non-zero, should return actual division\n         expected = numerator / denominator\n         assert abs(result - expected) < 1e-10  # Allow for floating point precision\n \n     @given(\n-        numerator=st.floats(min_value=-100.0, max_value=100.0, allow_nan=False, allow_infinity=False),\n-        default_value=st.floats(min_value=-100.0, max_value=100.0, allow_nan=False, allow_infinity=False)\n+        numerator=st.floats(\n+            min_value=-100.0, max_value=100.0, allow_nan=False, allow_infinity=False\n+        ),\n+        default_value=st.floats(\n+            min_value=-100.0, max_value=100.0, allow_nan=False, allow_infinity=False\n+        ),\n     )\n     @settings(max_examples=50)\n-    def test_safe_divide_zero_denominator_properties(self, numerator: float, default_value: float) -> None:\n+    def test_safe_divide_zero_denominator_properties(\n+        self, numerator: float, default_value: float\n+    ) -> None:\n         \"\"\"Test safe_divide with zero denominator.\"\"\"\n         result = safe_divide(numerator, 0.0, default_value)\n-        \n+\n         # Property: When denominator is zero, should return default value\n         assert result == default_value\n \n-\n     @given(\n-        actual_columns=st.lists(st.text(min_size=1, max_size=20), min_size=0, max_size=10),\n-        expected_columns=st.lists(st.text(min_size=1, max_size=20), min_size=0, max_size=10)\n+        actual_columns=st.lists(\n+            st.text(min_size=1, max_size=20), min_size=0, max_size=10\n+        ),\n+        expected_columns=st.lists(\n+            st.text(min_size=1, max_size=20), min_size=0, max_size=10\n+        ),\n     )\n     @settings(max_examples=100)\n-    def test_validate_dataframe_schema_properties(self, actual_columns: List[str], expected_columns: List[str]) -> None:\n+    def test_validate_dataframe_schema_properties(\n+        self, actual_columns: List[str], expected_columns: List[str]\n+    ) -> None:\n         \"\"\"Test validate_dataframe_schema with generated column lists.\"\"\"\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.columns = actual_columns\n-        \n+\n         result = validate_dataframe_schema(mock_df, expected_columns)\n-        \n+\n         # Property: Result should be boolean\n         assert isinstance(result, bool)\n-        \n+\n         # Property: Should be True if and only if all expected columns are in actual columns\n         expected_result = all(col in actual_columns for col in expected_columns)\n         assert result == expected_result\n-        \n+\n         # Property: If actual columns is empty, result should be True only if expected is also empty\n         if not actual_columns:\n             assert result == (not expected_columns)\n \n-\n-\n-    @given(\n-        columns=st.lists(st.text(min_size=1, max_size=20), min_size=0, max_size=20)\n-    )\n+    @given(columns=st.lists(st.text(min_size=1, max_size=20), min_size=0, max_size=20))\n     @settings(max_examples=50)\n     def test_dataframe_schema_edge_cases_properties(self, columns: List[str]) -> None:\n         \"\"\"Test dataframe schema validation with edge cases.\"\"\"\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.columns = columns\n-        \n+\n         # Test with empty expected columns\n         result = validate_dataframe_schema(mock_df, [])\n         assert result is True  # Empty expected columns should always pass\n-        \n+\n         # Test with subset of actual columns\n         if columns:\n-            subset = columns[:len(columns)//2] if len(columns) > 1 else columns\n+            subset = columns[: len(columns) // 2] if len(columns) > 1 else columns\n             result = validate_dataframe_schema(mock_df, subset)\n             assert result is True  # Subset should always pass\n-        \n+\n         # Test with non-existent columns\n         non_existent = [\"non_existent_col\"]\n         result = validate_dataframe_schema(mock_df, non_existent)\n         assert result is False  # Non-existent columns should fail\n-\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_table_operations.py\t2025-09-20 14:00:38.767823+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_table_operations.py\t2025-09-20 16:53:35.223303+00:00\n@@ -189,13 +189,13 @@\n             read_table(spark_session, table_name)\n \n     def test_read_table_general_exception(self, spark_session):\n         \"\"\"Test read_table handles general exceptions.\"\"\"\n         # Mock spark.table to raise a general exception\n-        with patch.object(spark_session, 'table') as mock_table:\n+        with patch.object(spark_session, \"table\") as mock_table:\n             mock_table.side_effect = Exception(\"General error\")\n-            \n+\n             with pytest.raises(TableOperationError, match=\"Failed to read table\"):\n                 read_table(spark_session, \"test_schema.test_table\")\n \n     def test_read_table_data_integrity(self, spark_session, sample_dataframe):\n         \"\"\"Test that read table preserves data integrity.\"\"\"\n@@ -251,21 +251,24 @@\n         assert table_exists(spark_session, table_name) is False\n \n     def test_table_exists_general_exception(self, spark_session):\n         \"\"\"Test table_exists handles general exceptions.\"\"\"\n         # Mock spark.table to raise a general exception\n-        with patch.object(spark_session, 'table') as mock_table:\n+        with patch.object(spark_session, \"table\") as mock_table:\n             mock_table.side_effect = Exception(\"General error\")\n-            \n-            with patch('sparkforge.table_operations.logger') as mock_logger:\n+\n+            with patch(\"sparkforge.table_operations.logger\") as mock_logger:\n                 result = table_exists(spark_session, \"test_schema.test_table\")\n                 assert result is False\n-                \n+\n                 # Verify that warning was logged\n                 mock_logger.warning.assert_called_once()\n                 warning_call = mock_logger.warning.call_args[0][0]\n-                assert \"Error checking if table test_schema.test_table exists\" in warning_call\n+                assert (\n+                    \"Error checking if table test_schema.test_table exists\"\n+                    in warning_call\n+                )\n \n \n class TestDropTable:\n     \"\"\"Test drop_table function.\"\"\"\n \n@@ -301,11 +304,11 @@\n         assert result is False\n \n     def test_drop_table_general_exception(self, spark_session):\n         \"\"\"Test drop_table handles general exceptions.\"\"\"\n         # Mock table_exists to return True, then mock spark.sql to raise exception\n-        with patch('sparkforge.table_operations.table_exists', return_value=True):\n-            with patch.object(spark_session, 'sql') as mock_sql:\n+        with patch(\"sparkforge.table_operations.table_exists\", return_value=True):\n+            with patch.object(spark_session, \"sql\") as mock_sql:\n                 mock_sql.side_effect = Exception(\"General error\")\n-                \n+\n                 result = drop_table(spark_session, \"test_schema.test_table\")\n                 assert result is False\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_property_based.py\t2025-09-20 15:59:31.523714+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_property_based.py\t2025-09-20 16:53:35.227800+00:00\n@@ -27,241 +27,248 @@\n     \"\"\"Property-based tests for data models.\"\"\"\n \n     @given(\n         bronze=st.floats(min_value=0.0, max_value=100.0),\n         silver=st.floats(min_value=0.0, max_value=100.0),\n-        gold=st.floats(min_value=0.0, max_value=100.0)\n-    )\n-    @settings(max_examples=50)\n-    def test_validation_thresholds_properties(self, bronze: float, silver: float, gold: float) -> None:\n+        gold=st.floats(min_value=0.0, max_value=100.0),\n+    )\n+    @settings(max_examples=50)\n+    def test_validation_thresholds_properties(\n+        self, bronze: float, silver: float, gold: float\n+    ) -> None:\n         \"\"\"Test ValidationThresholds with generated float values.\"\"\"\n         thresholds = ValidationThresholds(bronze=bronze, silver=silver, gold=gold)\n-        \n+\n         # Property: All threshold values should be preserved\n         assert thresholds.bronze == bronze\n         assert thresholds.silver == silver\n         assert thresholds.gold == gold\n-        \n+\n         # Property: Thresholds should be accessible via get_threshold\n         from sparkforge.models import PipelinePhase\n+\n         assert thresholds.get_threshold(PipelinePhase.BRONZE) == bronze\n         assert thresholds.get_threshold(PipelinePhase.SILVER) == silver\n         assert thresholds.get_threshold(PipelinePhase.GOLD) == gold\n \n-    @given(\n-        enabled=st.booleans(),\n-        max_workers=st.integers(min_value=1, max_value=100)\n-    )\n+    @given(enabled=st.booleans(), max_workers=st.integers(min_value=1, max_value=100))\n     @settings(max_examples=50)\n     def test_parallel_config_properties(self, enabled: bool, max_workers: int) -> None:\n         \"\"\"Test ParallelConfig with generated boolean and integer values.\"\"\"\n         parallel = ParallelConfig(enabled=enabled, max_workers=max_workers)\n-        \n+\n         # Property: All config values should be preserved\n         assert parallel.enabled == enabled\n         assert parallel.max_workers == max_workers\n \n     @given(\n-        name=st.text(min_size=1, max_size=50),\n-        schema=st.text(min_size=1, max_size=30)\n+        name=st.text(min_size=1, max_size=50), schema=st.text(min_size=1, max_size=30)\n     )\n     @settings(max_examples=50)\n     def test_pipeline_config_properties(self, name: str, schema: str) -> None:\n         \"\"\"Test PipelineConfig with generated string values.\"\"\"\n         # Filter out strings with special characters that might cause issues\n-        if all(c.isalnum() or c in '_-' for c in name) and all(c.isalnum() or c in '_-' for c in schema):\n+        if all(c.isalnum() or c in \"_-\" for c in name) and all(\n+            c.isalnum() or c in \"_-\" for c in schema\n+        ):\n             config = PipelineConfig.create_default(schema)\n-            \n+\n             # Property: Schema should be preserved\n             assert config.schema == schema\n-            \n+\n             # Property: Config should have default thresholds and parallel settings\n             assert isinstance(config.thresholds, ValidationThresholds)\n             assert isinstance(config.parallel, ParallelConfig)\n \n     @given(\n         name=st.text(min_size=1, max_size=30),\n-        incremental_col=st.one_of(st.none(), st.text(min_size=1, max_size=20))\n-    )\n-    @settings(max_examples=50)\n-    def test_bronze_step_properties(self, name: str, incremental_col: Optional[str]) -> None:\n+        incremental_col=st.one_of(st.none(), st.text(min_size=1, max_size=20)),\n+    )\n+    @settings(max_examples=50)\n+    def test_bronze_step_properties(\n+        self, name: str, incremental_col: Optional[str]\n+    ) -> None:\n         \"\"\"Test BronzeStep with generated string values.\"\"\"\n         # Filter out strings with special characters\n-        if all(c.isalnum() or c in '_-' for c in name):\n+        if all(c.isalnum() or c in \"_-\" for c in name):\n             rules = {\"col1\": [\"col1 > 0\"], \"col2\": [\"col2 IS NOT NULL\"]}\n-            \n-            step = BronzeStep(\n-                name=name,\n-                rules=rules,\n-                incremental_col=incremental_col\n-            )\n-            \n+\n+            step = BronzeStep(name=name, rules=rules, incremental_col=incremental_col)\n+\n             # Property: All values should be preserved\n             assert step.name == name\n             assert step.rules == rules\n             assert step.incremental_col == incremental_col\n-            \n+\n             # Property: Incremental capability should match incremental_col\n             assert step.has_incremental_capability == (incremental_col is not None)\n \n     @given(\n         name=st.text(min_size=1, max_size=30),\n         source_bronze=st.text(min_size=1, max_size=30),\n-        table_name=st.text(min_size=1, max_size=30)\n-    )\n-    @settings(max_examples=50)\n-    def test_silver_step_properties(self, name: str, source_bronze: str, table_name: str) -> None:\n+        table_name=st.text(min_size=1, max_size=30),\n+    )\n+    @settings(max_examples=50)\n+    def test_silver_step_properties(\n+        self, name: str, source_bronze: str, table_name: str\n+    ) -> None:\n         \"\"\"Test SilverStep with generated string values.\"\"\"\n         # Filter out strings with special characters\n-        if (all(c.isalnum() or c in '_-' for c in name) and\n-            all(c.isalnum() or c in '_-' for c in source_bronze) and\n-            all(c.isalnum() or c in '_-' for c in table_name)):\n-            \n+        if (\n+            all(c.isalnum() or c in \"_-\" for c in name)\n+            and all(c.isalnum() or c in \"_-\" for c in source_bronze)\n+            and all(c.isalnum() or c in \"_-\" for c in table_name)\n+        ):\n+\n             def mock_transform(spark, bronze_df, prior_silvers):\n                 return bronze_df\n-            \n+\n             rules = {\"col1\": [\"col1 > 0\"]}\n-            \n+\n             step = SilverStep(\n                 name=name,\n                 source_bronze=source_bronze,\n                 transform=mock_transform,\n                 rules=rules,\n-                table_name=table_name\n+                table_name=table_name,\n             )\n-            \n+\n             # Property: All values should be preserved\n             assert step.name == name\n             assert step.source_bronze == source_bronze\n             assert step.table_name == table_name\n             assert step.rules == rules\n \n     @given(\n         name=st.text(min_size=1, max_size=30),\n-        table_name=st.text(min_size=1, max_size=30)\n+        table_name=st.text(min_size=1, max_size=30),\n     )\n     @settings(max_examples=50)\n     def test_gold_step_properties(self, name: str, table_name: str) -> None:\n         \"\"\"Test GoldStep with generated string values.\"\"\"\n         # Filter out strings with special characters\n-        if (all(c.isalnum() or c in '_-' for c in name) and\n-            all(c.isalnum() or c in '_-' for c in table_name)):\n-            \n+        if all(c.isalnum() or c in \"_-\" for c in name) and all(\n+            c.isalnum() or c in \"_-\" for c in table_name\n+        ):\n+\n             def mock_transform(spark, silver_dfs):\n                 return silver_dfs\n-            \n+\n             rules = {\"col1\": [\"col1 > 0\"]}\n             source_silvers = [\"silver1\", \"silver2\"]\n-            \n+\n             step = GoldStep(\n                 name=name,\n                 table_name=table_name,\n                 transform=mock_transform,\n                 rules=rules,\n-                source_silvers=source_silvers\n+                source_silvers=source_silvers,\n             )\n-            \n+\n             # Property: All values should be preserved\n             assert step.name == name\n             assert step.table_name == table_name\n             assert step.rules == rules\n             assert step.source_silvers == source_silvers\n-\n \n     @given(\n         bronze1=st.floats(min_value=0.0, max_value=100.0),\n         silver1=st.floats(min_value=0.0, max_value=100.0),\n         gold1=st.floats(min_value=0.0, max_value=100.0),\n         bronze2=st.floats(min_value=0.0, max_value=100.0),\n         silver2=st.floats(min_value=0.0, max_value=100.0),\n-        gold2=st.floats(min_value=0.0, max_value=100.0)\n+        gold2=st.floats(min_value=0.0, max_value=100.0),\n     )\n     @settings(max_examples=50)\n     def test_validation_thresholds_equality_properties(\n-        self, bronze1: float, silver1: float, gold1: float,\n-        bronze2: float, silver2: float, gold2: float\n+        self,\n+        bronze1: float,\n+        silver1: float,\n+        gold1: float,\n+        bronze2: float,\n+        silver2: float,\n+        gold2: float,\n     ) -> None:\n         \"\"\"Test ValidationThresholds equality with generated values.\"\"\"\n         thresholds1 = ValidationThresholds(bronze=bronze1, silver=silver1, gold=gold1)\n         thresholds2 = ValidationThresholds(bronze=bronze2, silver=silver2, gold=gold2)\n-        \n+\n         # Property: Equality should be based on all threshold values\n-        expected_equal = (bronze1 == bronze2 and silver1 == silver2 and gold1 == gold2)\n+        expected_equal = bronze1 == bronze2 and silver1 == silver2 and gold1 == gold2\n         assert (thresholds1 == thresholds2) == expected_equal\n-        \n+\n         # Property: Self-equality should always be true\n         assert thresholds1 == thresholds1\n         assert thresholds2 == thresholds2\n \n     @given(\n         enabled1=st.booleans(),\n         max_workers1=st.integers(min_value=1, max_value=100),\n         enabled2=st.booleans(),\n-        max_workers2=st.integers(min_value=1, max_value=100)\n+        max_workers2=st.integers(min_value=1, max_value=100),\n     )\n     @settings(max_examples=50)\n     def test_parallel_config_equality_properties(\n         self, enabled1: bool, max_workers1: int, enabled2: bool, max_workers2: int\n     ) -> None:\n         \"\"\"Test ParallelConfig equality with generated values.\"\"\"\n         parallel1 = ParallelConfig(enabled=enabled1, max_workers=max_workers1)\n         parallel2 = ParallelConfig(enabled=enabled2, max_workers=max_workers2)\n-        \n+\n         # Property: Equality should be based on both enabled and max_workers\n-        expected_equal = (enabled1 == enabled2 and max_workers1 == max_workers2)\n+        expected_equal = enabled1 == enabled2 and max_workers1 == max_workers2\n         assert (parallel1 == parallel2) == expected_equal\n-        \n+\n         # Property: Self-equality should always be true\n         assert parallel1 == parallel1\n         assert parallel2 == parallel2\n \n     @given(\n         name=st.text(min_size=1, max_size=30),\n-        rules_size=st.integers(min_value=1, max_value=5)\n+        rules_size=st.integers(min_value=1, max_value=5),\n     )\n     @settings(max_examples=50)\n     def test_bronze_step_rules_properties(self, name: str, rules_size: int) -> None:\n         \"\"\"Test BronzeStep with generated rules of varying sizes.\"\"\"\n         # Filter out strings with special characters\n-        if all(c.isalnum() or c in '_-' for c in name):\n+        if all(c.isalnum() or c in \"_-\" for c in name):\n             # Generate rules dictionary\n             rules = {}\n             for i in range(rules_size):\n                 col_name = f\"col{i}\"\n                 rules[col_name] = [f\"{col_name} > 0\", f\"{col_name} IS NOT NULL\"]\n-            \n+\n             step = BronzeStep(name=name, rules=rules, incremental_col=None)\n-            \n+\n             # Property: Rules should be preserved exactly\n             assert step.rules == rules\n             assert len(step.rules) == rules_size\n-            \n+\n             # Property: All rule keys should be accessible\n             for col_name in rules:\n                 assert col_name in step.rules\n                 assert len(step.rules[col_name]) == 2\n \n     @given(\n         threshold_values=st.lists(\n-            st.floats(min_value=0.0, max_value=100.0),\n-            min_size=3,\n-            max_size=3\n+            st.floats(min_value=0.0, max_value=100.0), min_size=3, max_size=3\n         )\n     )\n     @settings(max_examples=50)\n-    def test_validation_thresholds_factory_methods_properties(self, threshold_values: List[float]) -> None:\n+    def test_validation_thresholds_factory_methods_properties(\n+        self, threshold_values: List[float]\n+    ) -> None:\n         \"\"\"Test ValidationThresholds factory methods with generated values.\"\"\"\n         bronze, silver, gold = threshold_values\n-        \n+\n         # Test create_strict method\n         strict_thresholds = ValidationThresholds.create_strict()\n-        \n+\n         # Property: Strict thresholds should all be high values\n         assert strict_thresholds.bronze >= 95.0\n         assert strict_thresholds.silver >= 95.0\n         assert strict_thresholds.gold >= 95.0\n-        \n+\n         # Property: All strict thresholds should be <= 100.0\n         assert strict_thresholds.bronze <= 100.0\n         assert strict_thresholds.silver <= 100.0\n         assert strict_thresholds.gold <= 100.0\n-\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_comprehensive_coverage.py\t2025-09-20 15:40:51.372173+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_comprehensive_coverage.py\t2025-09-20 16:53:35.236684+00:00\n@@ -34,96 +34,97 @@\n     \"\"\"Test comprehensive validation scenarios for improved coverage.\"\"\"\n \n     def test_and_all_rules_with_string_expressions(self) -> None:\n         \"\"\"Test and_all_rules with string expressions that need conversion.\"\"\"\n         from sparkforge.validation import and_all_rules\n-        \n+\n         # Mock F.expr to avoid Spark context issues\n-        with patch('sparkforge.validation.F') as mock_f:\n+        with patch(\"sparkforge.validation.F\") as mock_f:\n             mock_expr = Mock()\n             mock_f.expr.return_value = mock_expr\n-            \n+\n             # Test with string expressions\n             rules = {\n                 \"col1\": [\"col1 > 0\", \"col1 IS NOT NULL\"],\n-                \"col2\": [\"col2 != ''\", \"LENGTH(col2) > 5\"]\n+                \"col2\": [\"col2 != ''\", \"LENGTH(col2) > 5\"],\n             }\n-            \n+\n             result = and_all_rules(rules)\n-            \n+\n             # Should call F.expr for each string rule\n             assert mock_f.expr.call_count == 4\n             assert result is not mock_expr  # Should return combined expression\n \n     def test_apply_column_rules_no_validation_predicate(self) -> None:\n         \"\"\"Test apply_column_rules when validation_predicate is True.\"\"\"\n         from sparkforge.validation import apply_column_rules\n-        \n+\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.count.return_value = 100\n         mock_df.limit.return_value = mock_df\n-        \n+\n         # Mock and_all_rules to return True (no rules)\n-        with patch('sparkforge.validation.and_all_rules', return_value=True):\n-            with patch('sparkforge.validation.time.time', return_value=0.0):\n+        with patch(\"sparkforge.validation.and_all_rules\", return_value=True):\n+            with patch(\"sparkforge.validation.time.time\", return_value=0.0):\n                 valid_df, invalid_df, stats = apply_column_rules(\n                     mock_df, {}, \"bronze\", \"test_step\"\n                 )\n-                \n+\n                 # Should return all data as valid\n                 assert valid_df is mock_df\n                 assert invalid_df is mock_df  # limit(0) returns same df\n                 assert stats.total_rows == 100\n                 assert stats.valid_rows == 100\n                 assert stats.invalid_rows == 0\n \n-\n     def test_assess_data_quality_empty_dataframe(self) -> None:\n         \"\"\"Test assess_data_quality with empty DataFrame.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 0\n-        \n+\n         result = assess_data_quality(mock_df, {})\n-        \n+\n         assert result[\"total_rows\"] == 0\n         assert result[\"valid_rows\"] == 0\n         assert result[\"invalid_rows\"] == 0\n         assert result[\"quality_rate\"] == 100.0\n         assert result[\"is_empty\"] is True\n \n     def test_assess_data_quality_with_rules(self) -> None:\n         \"\"\"Test assess_data_quality with validation rules.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 100\n-        \n+\n         mock_valid_df = Mock()\n         mock_invalid_df = Mock()\n         mock_stats = Mock()\n         mock_stats.total_rows = 100\n         mock_stats.valid_rows = 95\n         mock_stats.invalid_rows = 5\n         mock_stats.validation_rate = 95.0\n-        \n-        with patch('sparkforge.validation.apply_column_rules', \n-                   return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n+\n+        with patch(\n+            \"sparkforge.validation.apply_column_rules\",\n+            return_value=(mock_valid_df, mock_invalid_df, mock_stats),\n+        ):\n             rules = {\"col1\": [\"col1 > 0\"]}\n             result = assess_data_quality(mock_df, rules)\n-            \n+\n             assert result[\"total_rows\"] == 100\n             assert result[\"valid_rows\"] == 95\n             assert result[\"invalid_rows\"] == 5\n             assert result[\"quality_rate\"] == 95.0\n             assert result[\"is_empty\"] is False\n \n     def test_assess_data_quality_no_rules(self) -> None:\n         \"\"\"Test assess_data_quality without validation rules.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 50\n-        \n+\n         result = assess_data_quality(mock_df, None)\n-        \n+\n         assert result[\"total_rows\"] == 50\n         assert result[\"valid_rows\"] == 50\n         assert result[\"invalid_rows\"] == 0\n         assert result[\"quality_rate\"] == 100.0\n         assert result[\"is_empty\"] is False\n@@ -132,11 +133,11 @@\n         \"\"\"Test UnifiedValidator initialization with and without logger.\"\"\"\n         # Test with default logger\n         validator = UnifiedValidator()\n         assert validator.logger is not None\n         assert validator.custom_validators == []\n-        \n+\n         # Test with custom logger\n         mock_logger = Mock()\n         validator = UnifiedValidator(mock_logger)\n         assert validator.logger is mock_logger\n         assert validator.custom_validators == []\n@@ -144,51 +145,63 @@\n     def test_unified_validator_add_validator(self) -> None:\n         \"\"\"Test UnifiedValidator add_validator method.\"\"\"\n         mock_logger = Mock()\n         validator = UnifiedValidator(mock_logger)\n         mock_validator = Mock()\n-        \n+\n         validator.add_validator(mock_validator)\n-        \n+\n         assert len(validator.custom_validators) == 1\n         assert validator.custom_validators[0] is mock_validator\n         # Should log the addition\n         mock_logger.info.assert_called_once()\n \n     def test_unified_validator_validate_pipeline(self) -> None:\n         \"\"\"Test UnifiedValidator validate_pipeline method.\"\"\"\n         validator = UnifiedValidator()\n-        \n+\n         # Mock all validation methods\n-        with patch.object(validator, '_validate_config', return_value=[]), \\\n-             patch.object(validator, '_validate_bronze_steps', return_value=([], [])), \\\n-             patch.object(validator, '_validate_silver_steps', return_value=([], [])), \\\n-             patch.object(validator, '_validate_gold_steps', return_value=([], [])), \\\n-             patch.object(validator, '_validate_dependencies', return_value=([], [])):\n-            \n+        with patch.object(validator, \"_validate_config\", return_value=[]), patch.object(\n+            validator, \"_validate_bronze_steps\", return_value=([], [])\n+        ), patch.object(\n+            validator, \"_validate_silver_steps\", return_value=([], [])\n+        ), patch.object(\n+            validator, \"_validate_gold_steps\", return_value=([], [])\n+        ), patch.object(\n+            validator, \"_validate_dependencies\", return_value=([], [])\n+        ):\n+\n             config = PipelineConfig.create_default(\"test_schema\")\n             result = validator.validate_pipeline(config, {}, {}, {})\n-            \n+\n             assert result.is_valid is True\n             assert result.errors == []\n             assert result.warnings == []\n             assert result.recommendations == []\n \n     def test_unified_validator_validate_pipeline_with_errors(self) -> None:\n         \"\"\"Test UnifiedValidator validate_pipeline with validation errors.\"\"\"\n         validator = UnifiedValidator()\n-        \n+\n         # Mock validation methods to return errors\n-        with patch.object(validator, '_validate_config', return_value=[\"Config error\"]), \\\n-             patch.object(validator, '_validate_bronze_steps', return_value=([\"Bronze error\"], [\"Bronze warning\"])), \\\n-             patch.object(validator, '_validate_silver_steps', return_value=([], [])), \\\n-             patch.object(validator, '_validate_gold_steps', return_value=([], [])), \\\n-             patch.object(validator, '_validate_dependencies', return_value=([], [])):\n-            \n+        with patch.object(\n+            validator, \"_validate_config\", return_value=[\"Config error\"]\n+        ), patch.object(\n+            validator,\n+            \"_validate_bronze_steps\",\n+            return_value=([\"Bronze error\"], [\"Bronze warning\"]),\n+        ), patch.object(\n+            validator, \"_validate_silver_steps\", return_value=([], [])\n+        ), patch.object(\n+            validator, \"_validate_gold_steps\", return_value=([], [])\n+        ), patch.object(\n+            validator, \"_validate_dependencies\", return_value=([], [])\n+        ):\n+\n             config = PipelineConfig.create_default(\"test_schema\")\n             result = validator.validate_pipeline(config, {}, {}, {})\n-            \n+\n             assert result.is_valid is False\n             assert len(result.errors) == 2\n             assert \"Config error\" in result.errors\n             assert \"Bronze error\" in result.errors\n             assert len(result.warnings) == 1\n@@ -196,27 +209,31 @@\n \n     def test_apply_validation_rules_comprehensive(self) -> None:\n         \"\"\"Test apply_validation_rules with comprehensive scenarios.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 200\n-        \n+\n         # Mock apply_column_rules\n         mock_valid_df = Mock()\n         mock_invalid_df = Mock()\n         mock_stats = Mock()\n         mock_stats.total_rows = 200\n         mock_stats.valid_rows = 180\n         mock_stats.invalid_rows = 20\n         mock_stats.validation_rate = 90.0\n         mock_stats.duration = 1.5\n-        \n-        with patch('sparkforge.validation.apply_column_rules',\n-                   return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n-            \n+\n+        with patch(\n+            \"sparkforge.validation.apply_column_rules\",\n+            return_value=(mock_valid_df, mock_invalid_df, mock_stats),\n+        ):\n+\n             rules = {\"col1\": [\"col1 > 0\"]}\n-            valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"silver\", \"test_step\")\n-            \n+            valid_df, invalid_df, stats = apply_validation_rules(\n+                mock_df, rules, \"silver\", \"test_step\"\n+            )\n+\n             assert stats.total_rows == 200\n             assert stats.valid_rows == 180\n             assert stats.invalid_rows == 20\n             assert stats.validation_rate == 90.0\n             assert stats.duration == 1.5\n@@ -225,122 +242,130 @@\n         \"\"\"Test get_dataframe_info with comprehensive scenarios.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 150\n         mock_df.columns = [\"col1\", \"col2\", \"col3\"]\n         mock_schema = Mock()\n-        mock_schema.__str__ = Mock(return_value=\"struct<col1:string,col2:int,col3:double>\")\n+        mock_schema.__str__ = Mock(\n+            return_value=\"struct<col1:string,col2:int,col3:double>\"\n+        )\n         mock_df.schema = mock_schema\n-        \n+\n         result = get_dataframe_info(mock_df)\n-        \n+\n         assert result[\"row_count\"] == 150\n         assert result[\"column_count\"] == 3\n         assert result[\"columns\"] == [\"col1\", \"col2\", \"col3\"]\n         assert \"schema\" in result\n         assert result[\"is_empty\"] is False\n \n     def test_validate_dataframe_schema_comprehensive(self) -> None:\n         \"\"\"Test validate_dataframe_schema with comprehensive scenarios.\"\"\"\n         mock_df = Mock()\n         mock_df.columns = [\"col1\", \"col2\", \"col3\"]\n-        \n+\n         # Test valid schema\n         expected_columns = [\"col1\", \"col2\", \"col3\"]\n         result = validate_dataframe_schema(mock_df, expected_columns)\n         assert result is True\n-        \n+\n         # Test missing columns\n         expected_columns = [\"col1\", \"col2\", \"col3\", \"col4\"]\n         result = validate_dataframe_schema(mock_df, expected_columns)\n         assert result is False\n-        \n+\n         # Test extra columns (should still be valid since all expected columns are present)\n         expected_columns = [\"col1\", \"col2\"]\n         result = validate_dataframe_schema(mock_df, expected_columns)\n         assert result is True\n \n     def test_safe_divide_comprehensive(self) -> None:\n         \"\"\"Test safe_divide with comprehensive scenarios.\"\"\"\n         # Test normal division\n         assert safe_divide(10.0, 2.0) == 5.0\n-        \n+\n         # Test division by zero\n         assert safe_divide(10.0, 0.0) == 0.0\n-        \n+\n         # Test zero numerator\n         assert safe_divide(0.0, 5.0) == 0.0\n-        \n+\n         # Test both zero\n         assert safe_divide(0.0, 0.0) == 0.0\n \n     def test_validation_with_large_dataset_simulation(self) -> None:\n         \"\"\"Test validation scenarios that simulate large dataset handling.\"\"\"\n         # Create mock DataFrame with large row count\n         mock_df = Mock()\n         mock_df.count.return_value = 1000000  # 1M rows\n-        \n+\n         mock_valid_df = Mock()\n         mock_invalid_df = Mock()\n         mock_stats = Mock()\n         mock_stats.total_rows = 1000000\n         mock_stats.valid_rows = 950000\n         mock_stats.invalid_rows = 50000\n         mock_stats.validation_rate = 95.0\n         mock_stats.duration = 10.5\n-        \n-        with patch('sparkforge.validation.apply_column_rules',\n-                   return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n-            \n+\n+        with patch(\n+            \"sparkforge.validation.apply_column_rules\",\n+            return_value=(mock_valid_df, mock_invalid_df, mock_stats),\n+        ):\n+\n             rules = {\n                 \"user_id\": [\"user_id IS NOT NULL\", \"user_id > 0\"],\n                 \"email\": [\"email IS NOT NULL\", \"email LIKE '%@%'\"],\n-                \"created_at\": [\"created_at IS NOT NULL\"]\n+                \"created_at\": [\"created_at IS NOT NULL\"],\n             }\n-            \n-            valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"bronze\", \"users\")\n-            \n+\n+            valid_df, invalid_df, stats = apply_validation_rules(\n+                mock_df, rules, \"bronze\", \"users\"\n+            )\n+\n             assert stats.total_rows == 1000000\n             assert stats.valid_rows == 950000\n             assert stats.invalid_rows == 50000\n             assert stats.validation_rate == 95.0\n             assert stats.duration == 10.5\n \n-\n     def test_validation_error_handling_comprehensive(self) -> None:\n         \"\"\"Test comprehensive validation error handling scenarios.\"\"\"\n         mock_df = Mock()\n         mock_df.count.side_effect = Exception(\"Database connection failed\")\n-        \n+\n         # Test error handling in assess_data_quality\n         result = assess_data_quality(mock_df, {})\n         assert \"error\" in result\n         assert \"Database connection failed\" in result[\"error\"]\n-        \n+\n         # Test error handling in get_dataframe_info\n         result = get_dataframe_info(mock_df)\n         assert \"error\" in result\n         assert \"Database connection failed\" in result[\"error\"]\n \n     def test_validation_performance_scenarios(self) -> None:\n         \"\"\"Test validation performance-related scenarios.\"\"\"\n         import time\n-        \n+\n         mock_df = Mock()\n         mock_df.count.return_value = 50000\n-        \n+\n         mock_valid_df = Mock()\n         mock_invalid_df = Mock()\n         mock_stats = Mock()\n         mock_stats.total_rows = 50000\n         mock_stats.valid_rows = 48000\n         mock_stats.invalid_rows = 2000\n         mock_stats.validation_rate = 96.0\n-        \n+\n         # Mock time to simulate performance timing\n-        with patch('sparkforge.validation.time.time', side_effect=[0.0, 2.5]), \\\n-             patch('sparkforge.validation.apply_column_rules',\n-                   return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n-            \n+        with patch(\"sparkforge.validation.time.time\", side_effect=[0.0, 2.5]), patch(\n+            \"sparkforge.validation.apply_column_rules\",\n+            return_value=(mock_valid_df, mock_invalid_df, mock_stats),\n+        ):\n+\n             rules = {\"col1\": [\"col1 > 0\"]}\n-            valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"silver\", \"performance_test\")\n-            \n+            valid_df, invalid_df, stats = apply_validation_rules(\n+                mock_df, rules, \"silver\", \"performance_test\"\n+            )\n+\n             assert stats.validation_rate == 96.0\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_final_push.py\t2025-09-20 15:48:38.907522+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_final_push.py\t2025-09-20 16:53:35.237773+00:00\n@@ -34,82 +34,87 @@\n     \"\"\"Test remaining edge cases to push validation.py to 80%+ coverage.\"\"\"\n \n     def test_apply_column_rules_column_filtering_edge_cases(self) -> None:\n         \"\"\"Test apply_column_rules column filtering edge cases.\"\"\"\n         from sparkforge.validation import apply_column_rules\n-        \n+\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.count.return_value = 100\n         mock_df.columns = [\"col1\", \"col2\", \"col3\"]\n         mock_df.select.return_value = mock_df\n-        \n+\n         # Mock and_all_rules to return a mock Column\n         mock_column = Mock()\n         mock_df.filter.return_value = mock_df\n         mock_df.__invert__ = Mock(return_value=mock_column)\n-        \n-        with patch('sparkforge.validation.and_all_rules', return_value=mock_column):\n-            with patch('sparkforge.validation.time.time', return_value=0.0):\n+\n+        with patch(\"sparkforge.validation.and_all_rules\", return_value=mock_column):\n+            with patch(\"sparkforge.validation.time.time\", return_value=0.0):\n                 # Test with column filtering enabled\n                 valid_df, invalid_df, stats = apply_column_rules(\n-                    mock_df, {\"col1\": [\"col1 > 0\"]}, \"bronze\", \"test_step\", \n-                    filter_columns_by_rules=True\n+                    mock_df,\n+                    {\"col1\": [\"col1 > 0\"]},\n+                    \"bronze\",\n+                    \"test_step\",\n+                    filter_columns_by_rules=True,\n                 )\n-                \n+\n                 assert stats.total_rows == 100\n                 # Should call select to filter columns\n                 mock_df.select.assert_called()\n \n     def test_apply_column_rules_no_column_filtering(self) -> None:\n         \"\"\"Test apply_column_rules without column filtering.\"\"\"\n         from sparkforge.validation import apply_column_rules\n-        \n+\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.count.return_value = 50\n         mock_df.columns = [\"col1\", \"col2\", \"col3\"]\n-        \n+\n         # Mock and_all_rules to return a mock Column\n         mock_column = Mock()\n         mock_df.filter.return_value = mock_df\n         mock_df.__invert__ = Mock(return_value=mock_column)\n-        \n-        with patch('sparkforge.validation.and_all_rules', return_value=mock_column):\n-            with patch('sparkforge.validation.time.time', return_value=0.0):\n+\n+        with patch(\"sparkforge.validation.and_all_rules\", return_value=mock_column):\n+            with patch(\"sparkforge.validation.time.time\", return_value=0.0):\n                 # Test with column filtering disabled\n                 valid_df, invalid_df, stats = apply_column_rules(\n-                    mock_df, {\"col1\": [\"col1 > 0\"]}, \"bronze\", \"test_step\", \n-                    filter_columns_by_rules=False\n+                    mock_df,\n+                    {\"col1\": [\"col1 > 0\"]},\n+                    \"bronze\",\n+                    \"test_step\",\n+                    filter_columns_by_rules=False,\n                 )\n-                \n+\n                 assert stats.total_rows == 50\n                 # Should not call select\n                 mock_df.select.assert_not_called()\n \n-\n     def test_assess_data_quality_with_empty_rules(self) -> None:\n         \"\"\"Test assess_data_quality with empty rules dictionary.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 75\n-        \n+\n         # Test with empty rules - should return all as valid\n         result = assess_data_quality(mock_df, {})\n-        \n+\n         assert result[\"total_rows\"] == 75\n         assert result[\"valid_rows\"] == 75\n         assert result[\"invalid_rows\"] == 0\n         assert result[\"quality_rate\"] == 100.0\n         assert result[\"is_empty\"] is False\n \n     def test_get_dataframe_info_with_exception(self) -> None:\n         \"\"\"Test get_dataframe_info with exception handling.\"\"\"\n         mock_df = Mock()\n         mock_df.count.side_effect = Exception(\"DataFrame access failed\")\n-        \n+\n         result = get_dataframe_info(mock_df)\n-        \n+\n         assert \"error\" in result\n         assert \"DataFrame access failed\" in result[\"error\"]\n         assert result[\"row_count\"] == 0\n         assert result[\"column_count\"] == 0\n         assert result[\"columns\"] == []\n@@ -118,160 +123,166 @@\n \n     def test_validate_dataframe_schema_edge_cases(self) -> None:\n         \"\"\"Test validate_dataframe_schema with edge cases.\"\"\"\n         mock_df = Mock()\n         mock_df.columns = []\n-        \n+\n         # Test with empty DataFrame\n         result = validate_dataframe_schema(mock_df, [])\n         assert result is True\n-        \n+\n         # Test with empty DataFrame and non-empty expected columns\n         result = validate_dataframe_schema(mock_df, [\"col1\"])\n         assert result is False\n \n     def test_safe_divide_edge_cases(self) -> None:\n         \"\"\"Test safe_divide with edge cases.\"\"\"\n         # Test with custom default value\n         assert safe_divide(10.0, 0.0, 999.0) == 999.0\n-        \n+\n         # Test with negative numbers\n         assert safe_divide(-10.0, 2.0) == -5.0\n         assert safe_divide(10.0, -2.0) == -5.0\n         assert safe_divide(-10.0, -2.0) == 5.0\n-        \n+\n         # Test with very small numbers\n         assert safe_divide(0.0001, 0.0002) == 0.5\n \n     def test_apply_validation_rules_edge_cases(self) -> None:\n         \"\"\"Test apply_validation_rules with edge cases.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 25\n-        \n+\n         # Mock apply_column_rules\n         mock_valid_df = Mock()\n         mock_invalid_df = Mock()\n         mock_stats = Mock()\n         mock_stats.total_rows = 25\n         mock_stats.valid_rows = 25\n         mock_stats.invalid_rows = 0\n         mock_stats.validation_rate = 100.0\n         mock_stats.duration = 0.1\n-        \n-        with patch('sparkforge.validation.apply_column_rules',\n-                   return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n-            \n+\n+        with patch(\n+            \"sparkforge.validation.apply_column_rules\",\n+            return_value=(mock_valid_df, mock_invalid_df, mock_stats),\n+        ):\n+\n             # Test with empty rules\n             valid_df, invalid_df, stats = apply_validation_rules(\n                 mock_df, {}, \"gold\", \"test_step\"\n             )\n-            \n+\n             assert stats.total_rows == 25\n             assert stats.valid_rows == 25\n             assert stats.invalid_rows == 0\n             assert stats.validation_rate == 100.0\n             assert stats.duration == 0.1\n \n     def test_validation_performance_edge_cases(self) -> None:\n         \"\"\"Test validation performance edge cases.\"\"\"\n         import time\n-        \n+\n         mock_df = Mock()\n         mock_df.count.return_value = 10\n-        \n+\n         mock_valid_df = Mock()\n         mock_invalid_df = Mock()\n         mock_stats = Mock()\n         mock_stats.total_rows = 10\n         mock_stats.valid_rows = 10\n         mock_stats.invalid_rows = 0\n         mock_stats.validation_rate = 100.0\n-        \n+\n         # Test with very fast execution\n-        with patch('sparkforge.validation.time.time', side_effect=[0.0, 0.001]), \\\n-             patch('sparkforge.validation.apply_column_rules',\n-                   return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n-            \n+        with patch(\"sparkforge.validation.time.time\", side_effect=[0.0, 0.001]), patch(\n+            \"sparkforge.validation.apply_column_rules\",\n+            return_value=(mock_valid_df, mock_invalid_df, mock_stats),\n+        ):\n+\n             rules = {\"col1\": [\"col1 > 0\"]}\n             valid_df, invalid_df, stats = apply_validation_rules(\n                 mock_df, rules, \"bronze\", \"fast_test\"\n             )\n-            \n+\n             assert stats.validation_rate == 100.0\n \n     def test_validation_large_dataset_edge_cases(self) -> None:\n         \"\"\"Test validation with large dataset edge cases.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 5000000  # 5M rows\n-        \n+\n         mock_valid_df = Mock()\n         mock_invalid_df = Mock()\n         mock_stats = Mock()\n         mock_stats.total_rows = 5000000\n         mock_stats.valid_rows = 4950000\n         mock_stats.invalid_rows = 50000\n         mock_stats.validation_rate = 99.0\n         mock_stats.duration = 25.0\n-        \n-        with patch('sparkforge.validation.apply_column_rules',\n-                   return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n-            \n+\n+        with patch(\n+            \"sparkforge.validation.apply_column_rules\",\n+            return_value=(mock_valid_df, mock_invalid_df, mock_stats),\n+        ):\n+\n             rules = {\n                 \"id\": [\"id IS NOT NULL\", \"id > 0\"],\n                 \"name\": [\"name IS NOT NULL\", \"LENGTH(name) > 0\"],\n                 \"email\": [\"email IS NOT NULL\", \"email LIKE '%@%'\"],\n                 \"created_at\": [\"created_at IS NOT NULL\"],\n-                \"updated_at\": [\"updated_at IS NOT NULL\"]\n+                \"updated_at\": [\"updated_at IS NOT NULL\"],\n             }\n-            \n+\n             valid_df, invalid_df, stats = apply_validation_rules(\n                 mock_df, rules, \"bronze\", \"large_dataset\"\n             )\n-            \n+\n             assert stats.total_rows == 5000000\n             assert stats.valid_rows == 4950000\n             assert stats.invalid_rows == 50000\n             assert stats.validation_rate == 99.0\n             assert stats.duration == 25.0\n \n     def test_validation_error_handling_edge_cases(self) -> None:\n         \"\"\"Test validation error handling edge cases.\"\"\"\n         mock_df = Mock()\n         mock_df.count.side_effect = Exception(\"Connection timeout\")\n-        \n+\n         # Test error handling in assess_data_quality\n         result = assess_data_quality(mock_df, {})\n         assert \"error\" in result\n         assert \"Connection timeout\" in result[\"error\"]\n-        \n+\n         # Test error handling in get_dataframe_info\n         result = get_dataframe_info(mock_df)\n         assert \"error\" in result\n         assert \"Connection timeout\" in result[\"error\"]\n \n-\n     def test_validation_boundary_conditions(self) -> None:\n         \"\"\"Test validation boundary conditions.\"\"\"\n         mock_df = Mock()\n         mock_df.count.return_value = 1  # Single row\n-        \n+\n         mock_valid_df = Mock()\n         mock_invalid_df = Mock()\n         mock_stats = Mock()\n         mock_stats.total_rows = 1\n         mock_stats.valid_rows = 0\n         mock_stats.invalid_rows = 1\n         mock_stats.validation_rate = 0.0\n         mock_stats.duration = 0.01\n-        \n-        with patch('sparkforge.validation.apply_column_rules',\n-                   return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n-            \n+\n+        with patch(\n+            \"sparkforge.validation.apply_column_rules\",\n+            return_value=(mock_valid_df, mock_invalid_df, mock_stats),\n+        ):\n+\n             rules = {\"col1\": [\"col1 > 1000\"]}  # Very strict rule\n             valid_df, invalid_df, stats = apply_validation_rules(\n                 mock_df, rules, \"bronze\", \"boundary_test\"\n             )\n-            \n+\n             assert stats.total_rows == 1\n             assert stats.valid_rows == 0\n             assert stats.invalid_rows == 1\n             assert stats.validation_rate == 0.0\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_types.py\t2025-09-20 15:26:12.093383+00:00\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_types.py\t2025-09-20 16:53:35.240853+00:00\n@@ -90,11 +90,11 @@\n     def test_step_type_enum(self) -> None:\n         \"\"\"Test StepType enum.\"\"\"\n         assert StepType.BRONZE.value == \"bronze\"\n         assert StepType.SILVER.value == \"silver\"\n         assert StepType.GOLD.value == \"gold\"\n-        \n+\n         # Test enum iteration\n         step_types = list(StepType)\n         assert len(step_types) == 3\n         assert StepType.BRONZE in step_types\n         assert StepType.SILVER in step_types\n@@ -105,11 +105,11 @@\n         assert StepStatus.PENDING.value == \"pending\"\n         assert StepStatus.RUNNING.value == \"running\"\n         assert StepStatus.COMPLETED.value == \"completed\"\n         assert StepStatus.FAILED.value == \"failed\"\n         assert StepStatus.SKIPPED.value == \"skipped\"\n-        \n+\n         # Test enum iteration\n         statuses = list(StepStatus)\n         assert len(statuses) == 5\n         assert StepStatus.PENDING in statuses\n         assert StepStatus.RUNNING in statuses\n@@ -120,11 +120,11 @@\n     def test_pipeline_mode_enum(self) -> None:\n         \"\"\"Test PipelineMode enum.\"\"\"\n         assert PipelineMode.INITIAL.value == \"initial\"\n         assert PipelineMode.INCREMENTAL.value == \"incremental\"\n         assert PipelineMode.FULL_REFRESH.value == \"full_refresh\"\n-        \n+\n         # Test enum iteration\n         modes = list(PipelineMode)\n         assert len(modes) == 3\n         assert PipelineMode.INITIAL in modes\n         assert PipelineMode.INCREMENTAL in modes\n@@ -134,107 +134,109 @@\n class TestFunctionTypes:\n     \"\"\"Test function type definitions.\"\"\"\n \n     def test_transform_function_types(self) -> None:\n         \"\"\"Test transform function types.\"\"\"\n+\n         # Test TransformFunction\n         def test_transform(spark, df):\n             return df\n-        \n+\n         # This should not raise an error\n         assert callable(test_transform)\n-        \n+\n         # Test BronzeTransformFunction\n         def test_bronze_transform(spark, df):\n             return df\n-        \n+\n         assert callable(test_bronze_transform)\n-        \n+\n         # Test SilverTransformFunction\n         def test_silver_transform(spark, df, bronze_dfs):\n             return df\n-        \n+\n         assert callable(test_silver_transform)\n-        \n+\n         # Test GoldTransformFunction\n         def test_gold_transform(spark, silver_dfs):\n             return list(silver_dfs.values())[0] if silver_dfs else None\n-        \n+\n         assert callable(test_gold_transform)\n \n     def test_filter_function_type(self) -> None:\n         \"\"\"Test filter function type.\"\"\"\n+\n         def test_filter(df):\n             return df\n-        \n+\n         assert callable(test_filter)\n \n \n class TestDataTypeAliases:\n     \"\"\"Test data type aliases.\"\"\"\n \n     def test_column_rules_type(self) -> None:\n         \"\"\"Test ColumnRules type alias.\"\"\"\n         # Create mock column\n         mock_column = Mock()\n-        \n+\n         # Test with string rules\n         rules_str = {\"col1\": [\"not_null\", \"positive\"]}\n-        \n+\n         # Test with column rules\n         rules_col = {\"col1\": [mock_column]}\n-        \n+\n         # Test with mixed rules\n         rules_mixed = {\"col1\": [\"not_null\", mock_column]}\n-        \n+\n         # These should not raise errors\n         assert isinstance(rules_str, dict)\n         assert isinstance(rules_col, dict)\n         assert isinstance(rules_mixed, dict)\n \n     def test_result_type_aliases(self) -> None:\n         \"\"\"Test result type aliases.\"\"\"\n         # Test StepResult\n         step_result = {\"status\": \"completed\", \"rows\": 100}\n         assert isinstance(step_result, dict)\n-        \n+\n         # Test PipelineResult\n         pipeline_result = {\"steps\": [\"bronze\", \"silver\"], \"total_rows\": 1000}\n         assert isinstance(pipeline_result, dict)\n-        \n+\n         # Test ExecutionResult\n         execution_result = {\"execution_id\": \"exec_123\", \"duration\": 30.5}\n         assert isinstance(execution_result, dict)\n-        \n+\n         # Test ValidationResult\n         validation_result = {\"valid_rows\": 950, \"invalid_rows\": 50}\n         assert isinstance(validation_result, dict)\n \n     def test_context_type_aliases(self) -> None:\n         \"\"\"Test context type aliases.\"\"\"\n         # Test StepContext\n         step_context = {\"step_name\": \"bronze_step\", \"config\": {}}\n         assert isinstance(step_context, dict)\n-        \n+\n         # Test ExecutionContext\n         execution_context = {\"pipeline_id\": \"pipeline_123\", \"start_time\": \"2023-01-01\"}\n         assert isinstance(execution_context, dict)\n \n     def test_config_type_aliases(self) -> None:\n         \"\"\"Test configuration type aliases.\"\"\"\n         # Test PipelineConfig\n         pipeline_config = {\"name\": \"test_pipeline\", \"steps\": []}\n         assert isinstance(pipeline_config, dict)\n-        \n+\n         # Test ExecutionConfig\n         execution_config = {\"parallel\": True, \"timeout\": 300}\n         assert isinstance(execution_config, dict)\n-        \n+\n         # Test ValidationConfig\n         validation_config = {\"thresholds\": {\"bronze\": 80.0}}\n         assert isinstance(validation_config, dict)\n-        \n+\n         # Test MonitoringConfig\n         monitoring_config = {\"metrics\": True, \"logging\": \"debug\"}\n         assert isinstance(monitoring_config, dict)\n \n     def test_quality_thresholds_type(self) -> None:\n@@ -246,11 +248,11 @@\n     def test_error_type_aliases(self) -> None:\n         \"\"\"Test error type aliases.\"\"\"\n         # Test ErrorContext\n         error_context = {\"error_code\": \"VALIDATION_FAILED\", \"details\": {}}\n         assert isinstance(error_context, dict)\n-        \n+\n         # Test ErrorSuggestions\n         error_suggestions = [\"Check data quality\", \"Verify schema\"]\n         assert isinstance(error_suggestions, list)\n         assert all(isinstance(s, str) for s in error_suggestions)\n \n@@ -258,35 +260,37 @@\n class TestProtocols:\n     \"\"\"Test protocol definitions.\"\"\"\n \n     def test_validatable_protocol(self) -> None:\n         \"\"\"Test Validatable protocol.\"\"\"\n+\n         class MockValidatable:\n             def validate(self) -> None:\n                 pass\n-        \n+\n         obj = MockValidatable()\n-        \n+\n         # Test that the object implements the protocol\n         assert hasattr(obj, \"validate\")\n         assert callable(obj.validate)\n-        \n+\n         # Test protocol validation\n         obj.validate()  # Should not raise an error\n \n     def test_serializable_protocol(self) -> None:\n         \"\"\"Test Serializable protocol.\"\"\"\n+\n         class MockSerializable:\n             def to_dict(self) -> Dict[str, Any]:\n                 return {\"test\": \"value\"}\n-        \n+\n         obj = MockSerializable()\n-        \n+\n         # Test that the object implements the protocol\n         assert hasattr(obj, \"to_dict\")\n         assert callable(obj.to_dict)\n-        \n+\n         # Test protocol usage\n         result = obj.to_dict()\n         assert isinstance(result, dict)\n         assert result[\"test\"] == \"value\"\n \n@@ -296,14 +300,16 @@\n \n     def test_backward_compatibility_aliases(self) -> None:\n         \"\"\"Test backward compatibility aliases.\"\"\"\n         # Test PipelinePhase alias\n         from sparkforge.types import PipelinePhase\n+\n         assert PipelinePhase == StepType\n-        \n+\n         # Test WriteMode alias\n         from sparkforge.types import WriteMode\n+\n         assert WriteMode == PipelineMode\n \n \n class TestTypeUsage:\n     \"\"\"Test type usage in practical scenarios.\"\"\"\n@@ -315,19 +321,16 @@\n             \"name\": \"test_pipeline\",\n             \"steps\": [\n                 {\n                     \"name\": \"bronze_step\",\n                     \"type\": StepType.BRONZE.value,\n-                    \"status\": StepStatus.PENDING.value\n+                    \"status\": StepStatus.PENDING.value,\n                 }\n             ],\n-            \"execution\": {\n-                \"mode\": PipelineMode.INITIAL.value,\n-                \"parallel\": True\n-            }\n+            \"execution\": {\"mode\": PipelineMode.INITIAL.value, \"parallel\": True},\n         }\n-        \n+\n         assert isinstance(config, dict)\n         assert config[\"name\"] == \"test_pipeline\"\n \n     def test_step_result_usage(self) -> None:\n         \"\"\"Test type usage in step results.\"\"\"\n@@ -336,13 +339,13 @@\n             \"step_name\": \"bronze_step\",\n             \"status\": StepStatus.COMPLETED.value,\n             \"rows_processed\": 1000,\n             \"quality_rate\": 95.5,\n             \"duration\": 30.2,\n-            \"errors\": []\n+            \"errors\": [],\n         }\n-        \n+\n         assert isinstance(result, dict)\n         assert result[\"status\"] == StepStatus.COMPLETED.value\n         assert isinstance(result[\"rows_processed\"], int)\n         assert isinstance(result[\"quality_rate\"], float)\n \n@@ -351,36 +354,30 @@\n         # Create a realistic validation context\n         context: StepContext = {\n             \"pipeline_id\": \"pipeline_123\",\n             \"step_name\": \"silver_step\",\n             \"execution_id\": \"exec_456\",\n-            \"config\": {\n-                \"validation_threshold\": 85.0,\n-                \"error_handling\": \"strict\"\n-            }\n+            \"config\": {\"validation_threshold\": 85.0, \"error_handling\": \"strict\"},\n         }\n-        \n+\n         assert isinstance(context, dict)\n         assert isinstance(context[\"config\"], dict)\n         assert context[\"config\"][\"validation_threshold\"] == 85.0\n \n     def test_error_handling_usage(self) -> None:\n         \"\"\"Test type usage in error handling.\"\"\"\n         # Create a realistic error context\n         error_context: ErrorContext = {\n             \"error_code\": \"VALIDATION_FAILED\",\n             \"step_name\": \"bronze_step\",\n-            \"details\": {\n-                \"failed_columns\": [\"user_id\", \"email\"],\n-                \"quality_rate\": 75.0\n-            }\n+            \"details\": {\"failed_columns\": [\"user_id\", \"email\"], \"quality_rate\": 75.0},\n         }\n-        \n+\n         suggestions: ErrorSuggestions = [\n             \"Check data quality for user_id column\",\n             \"Verify email format validation\",\n-            \"Consider adjusting validation thresholds\"\n+            \"Consider adjusting validation thresholds\",\n         ]\n-        \n+\n         assert isinstance(error_context, dict)\n         assert isinstance(suggestions, list)\n         assert all(isinstance(s, str) for s in suggestions)\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/conftest_integration.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/sparkforge/logging.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/__init__.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/conftest.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_pipeline_execution.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine_new.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitor.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_pipeline_runner.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/caching_strategies.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_validation_integration.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/memory_optimization.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/__init__.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/test_performance_integration.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_benchmarking.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_profiler.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitoring.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/test_security_integration.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_tests.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_dataframe_access.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_graph.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/vulnerability_scanner.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_additional_coverage.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_final_coverage.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_logging.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/compliance_checker.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_monitoring.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_analyzer.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_errors.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_additional_coverage.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_property_based.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_table_operations.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_property_based.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_comprehensive_coverage.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_final_push.py\nwould reformat /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_types.py\n\nOh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\n37 files would be reformatted, 52 files would be left unchanged.\n"
  },
  "isort": {
    "success": false,
    "output": "--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/sparkforge/__init__.py:before\t2025-09-20 10:20:01.273231\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/sparkforge/__init__.py:after\t2025-09-20 12:53:35.659478\n@@ -69,8 +69,9 @@\n \"\"\"\n \n # Import unified dependency analysis\n-from .dependencies import DependencyAnalysisResult, DependencyGraph, StepNode\n+from .dependencies import DependencyAnalysisResult\n from .dependencies import DependencyAnalyzer as UnifiedDependencyAnalyzer\n+from .dependencies import DependencyGraph, StepNode\n \n # Import simplified error handling\n from .errors import (\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_simple.py:before\t2025-09-20 10:00:38.766988\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_simple.py:after\t2025-09-20 12:53:35.747329\n@@ -32,10 +32,7 @@\n     WriteMode,\n )\n from sparkforge.pipeline.models import PipelineMode, PipelineReport, PipelineStatus\n-from sparkforge.types import (\n-    StepStatus,\n-    StepType,\n-)\n+from sparkforge.types import StepStatus, StepType\n \n \n class TestBaseModel:\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_comprehensive_coverage.py:before\t2025-09-20 11:40:51.372173\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_comprehensive_coverage.py:after\t2025-09-20 12:53:35.752211\n@@ -6,8 +6,8 @@\n coverage for validation.py from 59% to 80%+.\n \"\"\"\n \n+from typing import Any, Dict, List\n from unittest.mock import Mock, patch\n-from typing import Any, Dict, List\n \n import pytest\n \n@@ -36,7 +36,7 @@\n     def test_and_all_rules_with_string_expressions(self) -> None:\n         \"\"\"Test and_all_rules with string expressions that need conversion.\"\"\"\n         from sparkforge.validation import and_all_rules\n-        \n+\n         # Mock F.expr to avoid Spark context issues\n         with patch('sparkforge.validation.F') as mock_f:\n             mock_expr = Mock()\n@@ -57,7 +57,7 @@\n     def test_apply_column_rules_no_validation_predicate(self) -> None:\n         \"\"\"Test apply_column_rules when validation_predicate is True.\"\"\"\n         from sparkforge.validation import apply_column_rules\n-        \n+\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.count.return_value = 100\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_table_operations.py:before\t2025-09-20 10:00:38.767823\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_table_operations.py:after\t2025-09-20 12:53:35.756121\n@@ -5,9 +5,10 @@\n This module tests all table read/write/management operations.\n \"\"\"\n \n+from unittest.mock import patch\n+\n import pytest\n from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n-from unittest.mock import patch\n \n from sparkforge.errors import TableOperationError\n from sparkforge.table_operations import (\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_additional_coverage.py:before\t2025-09-20 10:42:20.268157\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_additional_coverage.py:after\t2025-09-20 12:53:35.762284\n@@ -15,6 +15,7 @@\n     BaseModel,\n     BronzeStep,\n     GoldStep,\n+    ParallelConfig,\n     PipelineConfig,\n     PipelinePhase,\n     PipelineValidationError,\n@@ -22,7 +23,6 @@\n     SilverStep,\n     ValidationError,\n     ValidationThresholds,\n-    ParallelConfig,\n )\n \n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_property_based.py:before\t2025-09-20 11:59:31.523714\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_property_based.py:after\t2025-09-20 12:53:35.773968\n@@ -6,11 +6,12 @@\n and catch edge cases that might be missed by traditional unit tests.\n \"\"\"\n \n-from typing import Any, Dict, List, Union, Optional\n+from typing import Any, Dict, List, Optional, Union\n from unittest.mock import Mock\n \n import pytest\n-from hypothesis import given, strategies as st, settings, example\n+from hypothesis import example, given, settings\n+from hypothesis import strategies as st\n \n from sparkforge.errors import PipelineValidationError\n from sparkforge.models import (\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_property_based.py:before\t2025-09-20 11:59:31.523706\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_property_based.py:after\t2025-09-20 12:53:35.776001\n@@ -10,13 +10,14 @@\n from unittest.mock import Mock\n \n import pytest\n-from hypothesis import given, strategies as st, settings, example\n+from hypothesis import example, given, settings\n+from hypothesis import strategies as st\n \n from sparkforge.validation import (\n+    assess_data_quality,\n+    get_dataframe_info,\n     safe_divide,\n     validate_dataframe_schema,\n-    get_dataframe_info,\n-    assess_data_quality,\n )\n \n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_additional_coverage.py:before\t2025-09-20 10:42:20.268362\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_additional_coverage.py:after\t2025-09-20 12:53:35.778950\n@@ -7,12 +7,12 @@\n \"\"\"\n \n from typing import Any, Dict, List, Optional\n-from unittest.mock import Mock, MagicMock\n+from unittest.mock import MagicMock, Mock\n \n import pytest\n from pyspark.sql import DataFrame, SparkSession\n from pyspark.sql.functions import Column, col, expr\n-from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n+from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n \n from sparkforge.validation import (\n     _convert_rule_to_expression,\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_final_coverage.py:before\t2025-09-20 11:48:38.907534\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_final_coverage.py:after\t2025-09-20 12:53:35.782273\n@@ -19,11 +19,11 @@\n     ModelValue,\n     ParallelConfig,\n     PipelineConfig,\n+    Serializable,\n     SilverDependencyInfo,\n     SilverStep,\n+    Validatable,\n     ValidationThresholds,\n-    Validatable,\n-    Serializable,\n )\n \n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_final_push.py:before\t2025-09-20 11:48:38.907522\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_final_push.py:after\t2025-09-20 12:53:35.793428\n@@ -6,8 +6,8 @@\n from 72% to 80%+.\n \"\"\"\n \n+from typing import Any, Dict, List\n from unittest.mock import Mock, patch\n-from typing import Any, Dict, List\n \n import pytest\n \n@@ -36,7 +36,7 @@\n     def test_apply_column_rules_column_filtering_edge_cases(self) -> None:\n         \"\"\"Test apply_column_rules column filtering edge cases.\"\"\"\n         from sparkforge.validation import apply_column_rules\n-        \n+\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.count.return_value = 100\n@@ -63,7 +63,7 @@\n     def test_apply_column_rules_no_column_filtering(self) -> None:\n         \"\"\"Test apply_column_rules without column filtering.\"\"\"\n         from sparkforge.validation import apply_column_rules\n-        \n+\n         # Create mock DataFrame\n         mock_df = Mock()\n         mock_df.count.return_value = 50\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_new.py:before\t2025-09-20 10:00:38.766782\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_new.py:after\t2025-09-20 12:53:35.801113\n@@ -28,10 +28,7 @@\n     ValidationThresholds,\n )\n from sparkforge.pipeline.models import PipelineMode, PipelineReport, PipelineStatus\n-from sparkforge.types import (\n-    StepStatus,\n-    StepType,\n-)\n+from sparkforge.types import StepStatus, StepType\n \n \n class TestBaseModel:\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_types.py:before\t2025-09-20 11:26:12.093383\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_types.py:after\t2025-09-20 12:53:35.808775\n@@ -47,9 +47,9 @@\n     StringDict,\n     TableName,\n     TransformFunction,\n+    Validatable,\n     ValidationConfig,\n     ValidationResult,\n-    Validatable,\n )\n \n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_graph.py:before\t2025-09-20 10:00:38.765574\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_graph.py:after\t2025-09-20 12:53:35.822538\n@@ -2,8 +2,9 @@\n Tests for the dependencies/graph.py module.\n \"\"\"\n \n+from unittest.mock import patch\n+\n import pytest\n-from unittest.mock import patch\n \n from sparkforge.dependencies.graph import DependencyGraph, StepNode, StepType\n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/test_security_integration.py:before\t2025-09-20 12:34:51.338863\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/test_security_integration.py:after\t2025-09-20 12:53:35.834719\n@@ -8,19 +8,29 @@\n - Security monitoring\n \"\"\"\n \n-import pytest\n import json\n import tempfile\n import time\n+from datetime import datetime, timedelta\n from pathlib import Path\n from unittest.mock import Mock, patch\n-from datetime import datetime, timedelta\n+\n+import pytest\n+\n+from tests.security.compliance_checker import ComplianceChecker, ComplianceStandard\n+from tests.security.security_monitoring import (\n+    SecurityEvent,\n+    SecurityEventType,\n+    SecurityMonitor,\n+    SecuritySeverity,\n+)\n \n # Import security components\n from tests.security.security_tests import SecurityTestSuite\n-from tests.security.vulnerability_scanner import VulnerabilityScanner, VulnerabilityReport\n-from tests.security.compliance_checker import ComplianceChecker, ComplianceStandard\n-from tests.security.security_monitoring import SecurityMonitor, SecurityEvent, SecurityEventType, SecuritySeverity\n+from tests.security.vulnerability_scanner import (\n+    VulnerabilityReport,\n+    VulnerabilityScanner,\n+)\n \n \n class TestSecurityIntegration:\n@@ -382,7 +392,7 @@\n     def test_security_performance_integration(self, security_test_suite, vulnerability_scanner):\n         \"\"\"Test security components performance.\"\"\"\n         import time\n-        \n+\n         # Test security test suite performance\n         start_time = time.time()\n         security_results = security_test_suite.run_security_scan()\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_monitoring.py:before\t2025-09-20 12:34:51.329490\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_monitoring.py:after\t2025-09-20 12:53:35.842098\n@@ -8,18 +8,19 @@\n - Threat intelligence integration\n \"\"\"\n \n+import json\n+import logging\n import os\n-import json\n+import subprocess\n+import threading\n import time\n-import logging\n-import threading\n-from pathlib import Path\n-from typing import Dict, List, Any, Optional, Callable\n-from dataclasses import dataclass, asdict\n+from dataclasses import asdict, dataclass\n from datetime import datetime, timedelta\n from enum import Enum\n+from pathlib import Path\n+from typing import Any, Callable, Dict, List, Optional\n+\n import psutil\n-import subprocess\n \n \n class SecurityEventType(Enum):\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/vulnerability_scanner.py:before\t2025-09-20 12:34:51.329291\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/vulnerability_scanner.py:after\t2025-09-20 12:53:35.848917\n@@ -8,15 +8,15 @@\n - Runtime security monitoring\n \"\"\"\n \n+import json\n import os\n-import json\n import subprocess\n import sys\n import time\n-from pathlib import Path\n-from typing import Dict, List, Any, Optional\n from dataclasses import dataclass\n from datetime import datetime\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional\n \n \n @dataclass\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/compliance_checker.py:before\t2025-09-20 12:34:51.329429\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/compliance_checker.py:after\t2025-09-20 12:53:35.858345\n@@ -9,15 +9,15 @@\n - Industry standards\n \"\"\"\n \n+import json\n import os\n-import json\n import subprocess\n import sys\n-from pathlib import Path\n-from typing import Dict, List, Any, Optional\n from dataclasses import dataclass\n from datetime import datetime\n from enum import Enum\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional\n \n \n class ComplianceStandard(Enum):\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/__init__.py:before\t2025-09-20 12:34:51.329306\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/__init__.py:after\t2025-09-20 12:53:35.860477\n@@ -8,9 +8,9 @@\n - Compliance testing\n \"\"\"\n \n+from .compliance_checker import ComplianceChecker\n from .security_tests import SecurityTestSuite\n from .vulnerability_scanner import VulnerabilityScanner\n-from .compliance_checker import ComplianceChecker\n \n __all__ = [\n     'SecurityTestSuite',\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_tests.py:before\t2025-09-20 12:34:51.329273\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_tests.py:after\t2025-09-20 12:53:35.866820\n@@ -9,21 +9,23 @@\n - Vulnerability prevention\n \"\"\"\n \n-import pytest\n+import json\n import os\n-import tempfile\n-import json\n import subprocess\n import sys\n+import tempfile\n from pathlib import Path\n-from typing import Dict, List, Any\n+from typing import Any, Dict, List\n from unittest.mock import Mock, patch\n+\n+import pytest\n+\n+from sparkforge.errors import ValidationError\n+from sparkforge.models import ParallelConfig, PipelineConfig, ValidationThresholds\n \n # Import SparkForge modules\n from sparkforge.pipeline.builder import PipelineBuilder\n-from sparkforge.models import PipelineConfig, ValidationThresholds, ParallelConfig\n from sparkforge.validation import UnifiedValidator\n-from sparkforge.errors import ValidationError\n \n \n class SecurityTestSuite:\n@@ -136,7 +138,7 @@\n         try:\n             # Test that validation functions properly escape inputs\n             from sparkforge.validation import validate_dataframe_schema\n-            \n+\n             # Test with potentially malicious input\n             malicious_inputs = [\n                 \"'; DROP TABLE users; --\",\n@@ -229,8 +231,12 @@\n         \"\"\"Test for information disclosure vulnerabilities.\"\"\"\n         try:\n             # Test that sensitive information is not leaked in error messages\n-            from sparkforge.errors import ValidationError, PipelineError, ConfigurationError\n-            \n+            from sparkforge.errors import (\n+                ConfigurationError,\n+                PipelineError,\n+                ValidationError,\n+            )\n+\n             # Test error messages don't contain sensitive information\n             test_errors = [\n                 ValidationError(\"Test validation error\"),\n@@ -307,8 +313,9 @@\n         try:\n             # Test data validation prevents malicious input\n             from pyspark.sql import functions as F\n+\n             from sparkforge.models import BronzeStep\n-            \n+\n             # Test validation rules prevent malicious data\n             malicious_rules = {\n                 \"malicious_col\": [\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine.py:before\t2025-09-20 10:00:38.759172\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine.py:after\t2025-09-20 12:53:35.892693\n@@ -22,12 +22,7 @@\n     StepStatus,\n     StepType,\n )\n-from sparkforge.models import (\n-    BronzeStep,\n-    GoldStep,\n-    PipelineConfig,\n-    SilverStep,\n-)\n+from sparkforge.models import BronzeStep, GoldStep, PipelineConfig, SilverStep\n \n \n class TestExecutionMode:\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:before\t2025-09-20 12:10:36.187491\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:after\t2025-09-20 12:53:35.932609\n@@ -6,8 +6,9 @@\n including validation, model creation, and serialization operations.\n \"\"\"\n \n+from unittest.mock import Mock\n+\n import pytest\n-from unittest.mock import Mock\n \n from sparkforge.models import (\n     BronzeStep,\n@@ -18,10 +19,10 @@\n     ValidationThresholds,\n )\n from sparkforge.validation import (\n+    assess_data_quality,\n+    get_dataframe_info,\n     safe_divide,\n     validate_dataframe_schema,\n-    assess_data_quality,\n-    get_dataframe_info,\n )\n \n from .performance_monitor import performance_monitor\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/conftest.py:before\t2025-09-20 12:10:36.186810\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/conftest.py:after\t2025-09-20 12:53:35.936007\n@@ -8,8 +8,9 @@\n \n import os\n import sys\n+from pathlib import Path\n+\n import pytest\n-from pathlib import Path\n \n # Add the project root to the path\n project_root = Path(__file__).parent.parent.parent\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/__init__.py:before\t2025-09-20 12:10:36.186609\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/__init__.py:after\t2025-09-20 12:53:35.939020\n@@ -10,9 +10,9 @@\n \n from .performance_monitor import PerformanceMonitor, PerformanceResult\n from .performance_tests import (\n-    run_validation_performance_tests,\n     run_model_creation_performance_tests,\n     run_serialization_performance_tests,\n+    run_validation_performance_tests,\n )\n \n __all__ = [\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitoring.py:before\t2025-09-20 12:45:07.584183\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitoring.py:after\t2025-09-20 12:53:35.944469\n@@ -9,17 +9,18 @@\n - Performance reporting and visualization\n \"\"\"\n \n+import json\n+import logging\n+import statistics\n+import threading\n import time\n+from collections import deque\n+from dataclasses import asdict, dataclass\n+from datetime import datetime, timedelta\n+from pathlib import Path\n+from typing import Any, Callable, Dict, List, Optional\n+\n import psutil\n-import threading\n-import json\n-import statistics\n-from typing import Dict, List, Any, Optional, Callable\n-from dataclasses import dataclass, asdict\n-from datetime import datetime, timedelta\n-from collections import deque\n-import logging\n-from pathlib import Path\n \n \n @dataclass\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/memory_optimization.py:before\t2025-09-20 12:45:07.583652\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/memory_optimization.py:after\t2025-09-20 12:53:35.949589\n@@ -9,19 +9,20 @@\n - Memory-efficient data structures\n \"\"\"\n \n+import functools\n import gc\n+import logging\n import sys\n-import tracemalloc\n-import psutil\n import threading\n import time\n-from typing import Any, Dict, List, Optional, Tuple, Callable\n-from dataclasses import dataclass, asdict\n+import tracemalloc\n+import weakref\n+from collections import defaultdict\n+from dataclasses import asdict, dataclass\n from datetime import datetime, timedelta\n-from collections import defaultdict\n-import weakref\n-import functools\n-import logging\n+from typing import Any, Callable, Dict, List, Optional, Tuple\n+\n+import psutil\n \n \n @dataclass\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/test_performance_integration.py:before\t2025-09-20 12:50:36.913455\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/test_performance_integration.py:after\t2025-09-20 12:53:35.955696\n@@ -9,28 +9,47 @@\n - Performance benchmarking\n \"\"\"\n \n-import pytest\n+import tempfile\n import time\n-import tempfile\n+from datetime import datetime, timedelta\n from pathlib import Path\n from unittest.mock import Mock, patch\n-from datetime import datetime, timedelta\n-\n-# Import performance components\n-from tests.performance.performance_profiler import PerformanceProfiler, profile_function, quick_profile\n+\n+import pytest\n+\n from tests.performance.caching_strategies import (\n-    MemoryCache, PersistentCache, HybridCache, cache_result, cache_dataframe,\n-    cache_manager\n+    HybridCache,\n+    MemoryCache,\n+    PersistentCache,\n+    cache_dataframe,\n+    cache_manager,\n+    cache_result,\n+)\n+from tests.performance.memory_optimization import (\n+    MemoryOptimizer,\n+    MemoryProfiler,\n+    memory_monitor,\n+    optimize_spark_memory,\n+)\n+from tests.performance.performance_benchmarking import (\n+    PerformanceBenchmark,\n+    benchmark,\n+    compare_benchmarks,\n+    load_test,\n )\n from tests.performance.performance_monitoring import (\n-    PerformanceMonitor, get_performance_monitor, record_metric,\n-    start_performance_monitoring, stop_performance_monitoring\n+    PerformanceMonitor,\n+    get_performance_monitor,\n+    record_metric,\n+    start_performance_monitoring,\n+    stop_performance_monitoring,\n )\n-from tests.performance.memory_optimization import (\n-    MemoryProfiler, MemoryOptimizer, memory_monitor, optimize_spark_memory\n-)\n-from tests.performance.performance_benchmarking import (\n-    PerformanceBenchmark, benchmark, compare_benchmarks, load_test\n+\n+# Import performance components\n+from tests.performance.performance_profiler import (\n+    PerformanceProfiler,\n+    profile_function,\n+    quick_profile,\n )\n \n \n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_profiler.py:before\t2025-09-20 12:45:07.584447\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_profiler.py:after\t2025-09-20 12:53:35.961643\n@@ -9,20 +9,21 @@\n - Performance bottleneck identification\n \"\"\"\n \n+import cProfile\n+import io\n+import json\n+import pstats\n+import sys\n+import threading\n import time\n+import tracemalloc\n+from dataclasses import asdict, dataclass\n+from datetime import datetime, timedelta\n+from functools import wraps\n+from pathlib import Path\n+from typing import Any, Callable, Dict, List, Optional, Tuple\n+\n import psutil\n-import tracemalloc\n-import cProfile\n-import pstats\n-import io\n-import threading\n-from functools import wraps\n-from typing import Dict, List, Any, Optional, Callable, Tuple\n-from dataclasses import dataclass, asdict\n-from datetime import datetime, timedelta\n-import json\n-import sys\n-from pathlib import Path\n \n \n @dataclass\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/caching_strategies.py:before\t2025-09-20 12:45:07.583390\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/caching_strategies.py:after\t2025-09-20 12:53:35.966115\n@@ -9,17 +9,17 @@\n - Pipeline step result caching\n \"\"\"\n \n+import hashlib\n+import json\n+import pickle\n+import threading\n import time\n-import hashlib\n-import pickle\n-import json\n-from functools import wraps, lru_cache\n+import weakref\n+from dataclasses import asdict, dataclass\n+from datetime import datetime, timedelta\n+from functools import lru_cache, wraps\n+from pathlib import Path\n from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n-from dataclasses import dataclass, asdict\n-from datetime import datetime, timedelta\n-import threading\n-import weakref\n-from pathlib import Path\n \n \n @dataclass\n--- /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_benchmarking.py:before\t2025-09-20 12:45:07.583932\n+++ /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_benchmarking.py:after\t2025-09-20 12:53:35.970968\n@@ -9,18 +9,19 @@\n - Automated performance testing\n \"\"\"\n \n-import time\n+import concurrent.futures\n+import gc\n+import json\n+import logging\n import statistics\n import threading\n-import concurrent.futures\n+import time\n+from dataclasses import asdict, dataclass\n+from datetime import datetime, timedelta\n+from pathlib import Path\n from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n-from dataclasses import dataclass, asdict\n-from datetime import datetime, timedelta\n-import json\n-import logging\n-from pathlib import Path\n+\n import psutil\n-import gc\n \n \n @dataclass\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/sparkforge/__init__.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_simple.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_comprehensive_coverage.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_table_operations.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_additional_coverage.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_property_based.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_property_based.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_additional_coverage.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_final_coverage.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_validation_final_push.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_models_new.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/test_types.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/unit/dependencies/test_graph.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/test_security_integration.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_monitoring.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/vulnerability_scanner.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/compliance_checker.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/__init__.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/security_tests.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/integration/test_execution_engine.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/conftest.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/__init__.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_monitoring.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/memory_optimization.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/test_performance_integration.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_profiler.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/caching_strategies.py Imports are incorrectly sorted and/or formatted.\nERROR: /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_benchmarking.py Imports are incorrectly sorted and/or formatted.\n"
  },
  "ruff": {
    "success": false,
    "output": "\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m sparkforge/dependencies/analyzer.py:147:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m145 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m146 |\u001b[0m             self.logger.error(f\"Dependency analysis failed: {str(e)}\")\n\u001b[1m\u001b[94m147 |\u001b[0m             raise DependencyError(f\"Dependency analysis failed: {str(e)}\")\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m148 |\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m     def _build_dependency_graph(\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mF402 \u001b[0m\u001b[1mImport `field` from line 24 shadowed by loop variable\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m sparkforge/models.py:176:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m174 |\u001b[0m         \"\"\"Convert model to dictionary.\"\"\"\n\u001b[1m\u001b[94m175 |\u001b[0m         result: dict[str, ModelValue] = {}\n\u001b[1m\u001b[94m176 |\u001b[0m         for field in self.__dataclass_fields__.values():\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m             value = getattr(self, field.name)\n\u001b[1m\u001b[94m178 |\u001b[0m             if hasattr(value, \"to_dict\"):\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m sparkforge/models.py:1278:9\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1276 |\u001b[0m         config.validate()\n\u001b[1m\u001b[94m1277 |\u001b[0m     except PipelineValidationError as e:\n\u001b[1m\u001b[94m1278 |\u001b[0m         raise PipelineConfigurationError(f\"Invalid pipeline configuration: {e}\")\n     \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n     \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m sparkforge/models.py:1286:9\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1284 |\u001b[0m         step.validate()\n\u001b[1m\u001b[94m1285 |\u001b[0m     except PipelineValidationError as e:\n\u001b[1m\u001b[94m1286 |\u001b[0m         raise PipelineConfigurationError(f\"Invalid step configuration: {e}\")\n     \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n     \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m sparkforge/pipeline/builder.py:1067:13\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1065 |\u001b[0m               self.logger.debug(f\"\u2705 Schema '{schema}' is accessible\")\n\u001b[1m\u001b[94m1066 |\u001b[0m           except Exception as e:\n\u001b[1m\u001b[94m1067 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m             raise StepError(\n\u001b[1m\u001b[94m1068 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                 f\"Schema '{schema}' does not exist or is not accessible: {str(e)}\",\n\u001b[1m\u001b[94m1069 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                 context={\"step_name\": \"schema_validation\", \"step_type\": \"validation\"},\n\u001b[1m\u001b[94m1070 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                 suggestions=[\n\u001b[1m\u001b[94m1071 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                     f\"Create the schema first: CREATE SCHEMA IF NOT EXISTS {schema}\",\n\u001b[1m\u001b[94m1072 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                     \"Check schema permissions\",\n\u001b[1m\u001b[94m1073 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                     \"Verify schema name spelling\",\n\u001b[1m\u001b[94m1074 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                 ],\n\u001b[1m\u001b[94m1075 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m             )\n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_____________^\u001b[0m\n\u001b[1m\u001b[94m1076 |\u001b[0m\n\u001b[1m\u001b[94m1077 |\u001b[0m       def _create_schema_if_not_exists(self, schema: str) -> None:\n     \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m sparkforge/pipeline/builder.py:1088:13\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1086 |\u001b[0m               self.logger.info(f\"\u2705 Schema '{schema}' created or already exists\")\n\u001b[1m\u001b[94m1087 |\u001b[0m           except Exception as e:\n\u001b[1m\u001b[94m1088 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m             raise StepError(\n\u001b[1m\u001b[94m1089 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                 f\"Failed to create schema '{schema}': {str(e)}\",\n\u001b[1m\u001b[94m1090 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                 context={\"step_name\": \"schema_creation\", \"step_type\": \"validation\"},\n\u001b[1m\u001b[94m1091 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                 suggestions=[\n\u001b[1m\u001b[94m1092 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                     \"Check schema permissions\",\n\u001b[1m\u001b[94m1093 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                     \"Verify schema name is valid\",\n\u001b[1m\u001b[94m1094 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                     \"Check for naming conflicts\",\n\u001b[1m\u001b[94m1095 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m                 ],\n\u001b[1m\u001b[94m1096 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m             )\n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_____________^\u001b[0m\n\u001b[1m\u001b[94m1097 |\u001b[0m\n\u001b[1m\u001b[94m1098 |\u001b[0m       def _get_effective_schema(self, step_schema: str | None) -> str:\n     \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m sparkforge/table_operations.py:80:9\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m     except Exception as e:\n\u001b[1m\u001b[94m80 |\u001b[0m         raise TableOperationError(f\"Failed to write table {fqn}: {e}\")\n   \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m sparkforge/table_operations.py:114:9\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m113 |\u001b[0m     except Exception as e:\n\u001b[1m\u001b[94m114 |\u001b[0m         raise TableOperationError(f\"Failed to write table {fqn}: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m sparkforge/table_operations.py:136:9\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m134 |\u001b[0m         return df\n\u001b[1m\u001b[94m135 |\u001b[0m     except AnalysisException as e:\n\u001b[1m\u001b[94m136 |\u001b[0m         raise TableOperationError(f\"Table {fqn} does not exist: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m137 |\u001b[0m     except Exception as e:\n\u001b[1m\u001b[94m138 |\u001b[0m         raise TableOperationError(f\"Failed to read table {fqn}: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mB904 \u001b[0m\u001b[1mWithin an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m sparkforge/table_operations.py:138:9\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m         raise TableOperationError(f\"Table {fqn} does not exist: {e}\")\n\u001b[1m\u001b[94m137 |\u001b[0m     except Exception as e:\n\u001b[1m\u001b[94m138 |\u001b[0m         raise TableOperationError(f\"Failed to read table {fqn}: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE402 \u001b[0m\u001b[1mModule level import not at top of file\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m sparkforge/types.py:129:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m127 |\u001b[0m # ============================================================================\n\u001b[1m\u001b[94m128 |\u001b[0m\n\u001b[1m\u001b[94m129 |\u001b[0m from typing import Protocol\n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:456:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m454 |\u001b[0m         \"\"\"Test bronze step execution when step name is not in context.\"\"\"\n\u001b[1m\u001b[94m455 |\u001b[0m         engine = ExecutionEngine(mock_spark, mock_config)\n\u001b[1m\u001b[94m456 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m457 |\u001b[0m         # Create a bronze step\n\u001b[1m\u001b[94m458 |\u001b[0m         bronze_step = BronzeStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:463:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m461 |\u001b[0m             schema=\"test_schema\"\n\u001b[1m\u001b[94m462 |\u001b[0m         )\n\u001b[1m\u001b[94m463 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m         # Mock the spark.read.format().load() to raise an exception\n\u001b[1m\u001b[94m465 |\u001b[0m         mock_spark.read.format.return_value.load.side_effect = Exception(\"File not found\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:466:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m         # Mock the spark.read.format().load() to raise an exception\n\u001b[1m\u001b[94m465 |\u001b[0m         mock_spark.read.format.return_value.load.side_effect = Exception(\"File not found\")\n\u001b[1m\u001b[94m466 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m467 |\u001b[0m         # Execute bronze step without the step name in context\n\u001b[1m\u001b[94m468 |\u001b[0m         # This should trigger the fallback to createDataFrame\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:470:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m468 |\u001b[0m         # This should trigger the fallback to createDataFrame\n\u001b[1m\u001b[94m469 |\u001b[0m         result_df = engine._execute_bronze_step(bronze_step, {})\n\u001b[1m\u001b[94m470 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m471 |\u001b[0m         # Verify that createDataFrame was called as fallback\n\u001b[1m\u001b[94m472 |\u001b[0m         mock_spark.createDataFrame.assert_called_once()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:478:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m476 |\u001b[0m         \"\"\"Test that fallback logging occurs when read fails.\"\"\"\n\u001b[1m\u001b[94m477 |\u001b[0m         engine = ExecutionEngine(mock_spark, mock_config)\n\u001b[1m\u001b[94m478 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m479 |\u001b[0m         # Create a bronze step\n\u001b[1m\u001b[94m480 |\u001b[0m         bronze_step = BronzeStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:485:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m483 |\u001b[0m             schema=\"test_schema\"\n\u001b[1m\u001b[94m484 |\u001b[0m         )\n\u001b[1m\u001b[94m485 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m486 |\u001b[0m         # Mock the spark.read.format().load() to raise an exception\n\u001b[1m\u001b[94m487 |\u001b[0m         mock_spark.read.format.return_value.load.side_effect = Exception(\"File not found\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:488:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m486 |\u001b[0m         # Mock the spark.read.format().load() to raise an exception\n\u001b[1m\u001b[94m487 |\u001b[0m         mock_spark.read.format.return_value.load.side_effect = Exception(\"File not found\")\n\u001b[1m\u001b[94m488 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m489 |\u001b[0m         # Mock the logger to capture warning messages\n\u001b[1m\u001b[94m490 |\u001b[0m         with patch.object(engine.logger, 'warning') as mock_warning:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `result_df` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:492:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m490 |\u001b[0m         with patch.object(engine.logger, 'warning') as mock_warning:\n\u001b[1m\u001b[94m491 |\u001b[0m             # Execute bronze step without the step name in context\n\u001b[1m\u001b[94m492 |\u001b[0m             result_df = engine._execute_bronze_step(bronze_step, {})\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m493 |\u001b[0m             \n\u001b[1m\u001b[94m494 |\u001b[0m             # Verify that warning was logged about the read failure\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `result_df`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/integration/test_execution_engine.py:493:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m491 |\u001b[0m             # Execute bronze step without the step name in context\n\u001b[1m\u001b[94m492 |\u001b[0m             result_df = engine._execute_bronze_step(bronze_step, {})\n\u001b[1m\u001b[94m493 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m494 |\u001b[0m             # Verify that warning was logged about the read failure\n\u001b[1m\u001b[94m495 |\u001b[0m             mock_warning.assert_called_once()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/__init__.py:11:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m10 |\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from .performance_monitor import PerformanceMonitor, PerformanceResult\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from .performance_tests import (\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     run_validation_performance_tests,\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     run_model_creation_performance_tests,\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     run_serialization_performance_tests,\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m   __all__ = [\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:12:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import time\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import hashlib\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pickle\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from functools import wraps, lru_cache\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import threading\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import weakref\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|________________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`json` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:15:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m import hashlib\n\u001b[1m\u001b[94m14 |\u001b[0m import pickle\n\u001b[1m\u001b[94m15 |\u001b[0m import json\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m from functools import wraps, lru_cache\n\u001b[1m\u001b[94m17 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `json`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`functools.lru_cache` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:16:30\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m import pickle\n\u001b[1m\u001b[94m15 |\u001b[0m import json\n\u001b[1m\u001b[94m16 |\u001b[0m from functools import wraps, lru_cache\n   \u001b[1m\u001b[94m|\u001b[0m                              \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\u001b[1m\u001b[94m18 |\u001b[0m from dataclasses import dataclass, asdict\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `functools.lru_cache`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Tuple` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:17:57\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m import json\n\u001b[1m\u001b[94m16 |\u001b[0m from functools import wraps, lru_cache\n\u001b[1m\u001b[94m17 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n   \u001b[1m\u001b[94m|\u001b[0m                                                         \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m19 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `typing.Tuple`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`dataclasses.asdict` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:18:36\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m from functools import wraps, lru_cache\n\u001b[1m\u001b[94m17 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\u001b[1m\u001b[94m18 |\u001b[0m from dataclasses import dataclass, asdict\n   \u001b[1m\u001b[94m|\u001b[0m                                    \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m20 |\u001b[0m import threading\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `dataclasses.asdict`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`datetime.timedelta` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:19:32\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\u001b[1m\u001b[94m18 |\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m19 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m import threading\n\u001b[1m\u001b[94m21 |\u001b[0m import weakref\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `datetime.timedelta`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`weakref` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:21:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m20 |\u001b[0m import threading\n\u001b[1m\u001b[94m21 |\u001b[0m import weakref\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m from pathlib import Path\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `weakref`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:51:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m49 |\u001b[0m class MemoryCache:\n\u001b[1m\u001b[94m50 |\u001b[0m     \"\"\"In-memory cache with TTL and LRU eviction.\"\"\"\n\u001b[1m\u001b[94m51 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m52 |\u001b[0m     def __init__(self, max_size_mb: int = 100, default_ttl: Optional[int] = None):\n\u001b[1m\u001b[94m53 |\u001b[0m         self.max_size_mb = max_size_mb\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:58:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m56 |\u001b[0m         self.access_order: List[str] = []\n\u001b[1m\u001b[94m57 |\u001b[0m         self.lock = threading.RLock()\n\u001b[1m\u001b[94m58 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m59 |\u001b[0m         # Statistics\n\u001b[1m\u001b[94m60 |\u001b[0m         self.hit_count = 0\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:63:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         self.miss_count = 0\n\u001b[1m\u001b[94m62 |\u001b[0m         self.eviction_count = 0\n\u001b[1m\u001b[94m63 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m     def get(self, key: str) -> Optional[Any]:\n\u001b[1m\u001b[94m65 |\u001b[0m         \"\"\"Get value from cache.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:70:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m68 |\u001b[0m                 self.miss_count += 1\n\u001b[1m\u001b[94m69 |\u001b[0m                 return None\n\u001b[1m\u001b[94m70 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m             entry = self.cache[key]\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:72:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m             entry = self.cache[key]\n\u001b[1m\u001b[94m72 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m73 |\u001b[0m             # Check TTL\n\u001b[1m\u001b[94m74 |\u001b[0m             if entry.ttl_seconds and self._is_expired(entry):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:79:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m                 self.miss_count += 1\n\u001b[1m\u001b[94m78 |\u001b[0m                 return None\n\u001b[1m\u001b[94m79 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m80 |\u001b[0m             # Update access info\n\u001b[1m\u001b[94m81 |\u001b[0m             entry.last_accessed = datetime.now()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:83:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m81 |\u001b[0m             entry.last_accessed = datetime.now()\n\u001b[1m\u001b[94m82 |\u001b[0m             entry.access_count += 1\n\u001b[1m\u001b[94m83 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m84 |\u001b[0m             # Move to end of access order (most recently used)\n\u001b[1m\u001b[94m85 |\u001b[0m             self.access_order.remove(key)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:87:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m85 |\u001b[0m             self.access_order.remove(key)\n\u001b[1m\u001b[94m86 |\u001b[0m             self.access_order.append(key)\n\u001b[1m\u001b[94m87 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m88 |\u001b[0m             self.hit_count += 1\n\u001b[1m\u001b[94m89 |\u001b[0m             return entry.value\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:90:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m88 |\u001b[0m             self.hit_count += 1\n\u001b[1m\u001b[94m89 |\u001b[0m             return entry.value\n\u001b[1m\u001b[94m90 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m91 |\u001b[0m     def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:\n\u001b[1m\u001b[94m92 |\u001b[0m         \"\"\"Set value in cache.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:96:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m94 |\u001b[0m             # Calculate size\n\u001b[1m\u001b[94m95 |\u001b[0m             size_bytes = self._estimate_size(value)\n\u001b[1m\u001b[94m96 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m97 |\u001b[0m             # Remove existing entry if it exists\n\u001b[1m\u001b[94m98 |\u001b[0m             if key in self.cache:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:100:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 98 |\u001b[0m             if key in self.cache:\n\u001b[1m\u001b[94m 99 |\u001b[0m                 self._remove_entry(key)\n\u001b[1m\u001b[94m100 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m101 |\u001b[0m             # Create new entry\n\u001b[1m\u001b[94m102 |\u001b[0m             ttl = ttl_seconds or self.default_ttl\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:112:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m110 |\u001b[0m                 ttl_seconds=ttl\n\u001b[1m\u001b[94m111 |\u001b[0m             )\n\u001b[1m\u001b[94m112 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m113 |\u001b[0m             # Add to cache\n\u001b[1m\u001b[94m114 |\u001b[0m             self.cache[key] = entry\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:116:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m             self.cache[key] = entry\n\u001b[1m\u001b[94m115 |\u001b[0m             self.access_order.append(key)\n\u001b[1m\u001b[94m116 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m117 |\u001b[0m             # Check if we need to evict\n\u001b[1m\u001b[94m118 |\u001b[0m             self._evict_if_needed()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:119:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m117 |\u001b[0m             # Check if we need to evict\n\u001b[1m\u001b[94m118 |\u001b[0m             self._evict_if_needed()\n\u001b[1m\u001b[94m119 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m     def delete(self, key: str) -> bool:\n\u001b[1m\u001b[94m121 |\u001b[0m         \"\"\"Delete entry from cache.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:127:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m125 |\u001b[0m                 return True\n\u001b[1m\u001b[94m126 |\u001b[0m             return False\n\u001b[1m\u001b[94m127 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m     def clear(self) -> None:\n\u001b[1m\u001b[94m129 |\u001b[0m         \"\"\"Clear all cache entries.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:133:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m131 |\u001b[0m             self.cache.clear()\n\u001b[1m\u001b[94m132 |\u001b[0m             self.access_order.clear()\n\u001b[1m\u001b[94m133 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m134 |\u001b[0m     def _remove_entry(self, key: str) -> None:\n\u001b[1m\u001b[94m135 |\u001b[0m         \"\"\"Remove entry from cache.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:140:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m138 |\u001b[0m             if key in self.access_order:\n\u001b[1m\u001b[94m139 |\u001b[0m                 self.access_order.remove(key)\n\u001b[1m\u001b[94m140 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m141 |\u001b[0m     def _is_expired(self, entry: CacheEntry) -> bool:\n\u001b[1m\u001b[94m142 |\u001b[0m         \"\"\"Check if entry is expired.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:145:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m143 |\u001b[0m         if not entry.ttl_seconds:\n\u001b[1m\u001b[94m144 |\u001b[0m             return False\n\u001b[1m\u001b[94m145 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         elapsed = (datetime.now() - entry.created_at).total_seconds()\n\u001b[1m\u001b[94m147 |\u001b[0m         return elapsed > entry.ttl_seconds\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:148:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         elapsed = (datetime.now() - entry.created_at).total_seconds()\n\u001b[1m\u001b[94m147 |\u001b[0m         return elapsed > entry.ttl_seconds\n\u001b[1m\u001b[94m148 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m     def _estimate_size(self, obj: Any) -> int:\n\u001b[1m\u001b[94m150 |\u001b[0m         \"\"\"Estimate size of object in bytes.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:156:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m154 |\u001b[0m             # Fallback estimation\n\u001b[1m\u001b[94m155 |\u001b[0m             return len(str(obj).encode('utf-8'))\n\u001b[1m\u001b[94m156 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m     def _evict_if_needed(self) -> None:\n\u001b[1m\u001b[94m158 |\u001b[0m         \"\"\"Evict entries if cache is too large.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:160:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m158 |\u001b[0m         \"\"\"Evict entries if cache is too large.\"\"\"\n\u001b[1m\u001b[94m159 |\u001b[0m         current_size_mb = sum(entry.size_bytes for entry in self.cache.values()) / 1024 / 1024\n\u001b[1m\u001b[94m160 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m161 |\u001b[0m         while current_size_mb > self.max_size_mb and self.access_order:\n\u001b[1m\u001b[94m162 |\u001b[0m             # Remove least recently used entry\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:166:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m164 |\u001b[0m             self._remove_entry(oldest_key)\n\u001b[1m\u001b[94m165 |\u001b[0m             self.eviction_count += 1\n\u001b[1m\u001b[94m166 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m167 |\u001b[0m             # Recalculate size\n\u001b[1m\u001b[94m168 |\u001b[0m             current_size_mb = sum(entry.size_bytes for entry in self.cache.values()) / 1024 / 1024\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:169:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m167 |\u001b[0m             # Recalculate size\n\u001b[1m\u001b[94m168 |\u001b[0m             current_size_mb = sum(entry.size_bytes for entry in self.cache.values()) / 1024 / 1024\n\u001b[1m\u001b[94m169 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m170 |\u001b[0m     def get_stats(self) -> CacheStats:\n\u001b[1m\u001b[94m171 |\u001b[0m         \"\"\"Get cache statistics.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:176:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m174 |\u001b[0m             total_requests = self.hit_count + self.miss_count\n\u001b[1m\u001b[94m175 |\u001b[0m             hit_rate = (self.hit_count / total_requests * 100) if total_requests > 0 else 0\n\u001b[1m\u001b[94m176 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m             return CacheStats(\n\u001b[1m\u001b[94m178 |\u001b[0m                 total_entries=len(self.cache),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:186:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m                 memory_usage_mb=total_size_bytes / 1024 / 1024\n\u001b[1m\u001b[94m185 |\u001b[0m             )\n\u001b[1m\u001b[94m186 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m187 |\u001b[0m     def cleanup_expired(self) -> int:\n\u001b[1m\u001b[94m188 |\u001b[0m         \"\"\"Remove expired entries and return count.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:194:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m192 |\u001b[0m                 if self._is_expired(entry)\n\u001b[1m\u001b[94m193 |\u001b[0m             ]\n\u001b[1m\u001b[94m194 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m195 |\u001b[0m             for key in expired_keys:\n\u001b[1m\u001b[94m196 |\u001b[0m                 self._remove_entry(key)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:197:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m195 |\u001b[0m             for key in expired_keys:\n\u001b[1m\u001b[94m196 |\u001b[0m                 self._remove_entry(key)\n\u001b[1m\u001b[94m197 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m198 |\u001b[0m             return len(expired_keys)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:203:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m class PersistentCache:\n\u001b[1m\u001b[94m202 |\u001b[0m     \"\"\"File-based persistent cache.\"\"\"\n\u001b[1m\u001b[94m203 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m     def __init__(self, cache_dir: Union[str, Path], max_file_size_mb: int = 50):\n\u001b[1m\u001b[94m205 |\u001b[0m         self.cache_dir = Path(cache_dir)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:209:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m         self.max_file_size_mb = max_file_size_mb\n\u001b[1m\u001b[94m208 |\u001b[0m         self.lock = threading.RLock()\n\u001b[1m\u001b[94m209 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m     def get(self, key: str) -> Optional[Any]:\n\u001b[1m\u001b[94m211 |\u001b[0m         \"\"\"Get value from persistent cache.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:213:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m211 |\u001b[0m         \"\"\"Get value from persistent cache.\"\"\"\n\u001b[1m\u001b[94m212 |\u001b[0m         cache_file = self._get_cache_file(key)\n\u001b[1m\u001b[94m213 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m         if not cache_file.exists():\n\u001b[1m\u001b[94m215 |\u001b[0m             return None\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:216:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m         if not cache_file.exists():\n\u001b[1m\u001b[94m215 |\u001b[0m             return None\n\u001b[1m\u001b[94m216 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m217 |\u001b[0m         try:\n\u001b[1m\u001b[94m218 |\u001b[0m             with open(cache_file, 'rb') as f:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:220:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m218 |\u001b[0m             with open(cache_file, 'rb') as f:\n\u001b[1m\u001b[94m219 |\u001b[0m                 entry = pickle.load(f)\n\u001b[1m\u001b[94m220 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m221 |\u001b[0m             # Check if expired\n\u001b[1m\u001b[94m222 |\u001b[0m             if entry.ttl_seconds and self._is_expired(entry):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:225:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m                 cache_file.unlink()\n\u001b[1m\u001b[94m224 |\u001b[0m                 return None\n\u001b[1m\u001b[94m225 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m226 |\u001b[0m             return entry.value\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:227:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m226 |\u001b[0m             return entry.value\n\u001b[1m\u001b[94m227 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m228 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m229 |\u001b[0m             # Remove corrupted cache file\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:233:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m                 cache_file.unlink()\n\u001b[1m\u001b[94m232 |\u001b[0m             return None\n\u001b[1m\u001b[94m233 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m234 |\u001b[0m     def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:\n\u001b[1m\u001b[94m235 |\u001b[0m         \"\"\"Set value in persistent cache.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:237:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m         \"\"\"Set value in persistent cache.\"\"\"\n\u001b[1m\u001b[94m236 |\u001b[0m         cache_file = self._get_cache_file(key)\n\u001b[1m\u001b[94m237 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m238 |\u001b[0m         try:\n\u001b[1m\u001b[94m239 |\u001b[0m             entry = CacheEntry(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:248:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m246 |\u001b[0m                 ttl_seconds=ttl_seconds\n\u001b[1m\u001b[94m247 |\u001b[0m             )\n\u001b[1m\u001b[94m248 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m249 |\u001b[0m             # Check file size limit\n\u001b[1m\u001b[94m250 |\u001b[0m             estimated_size = self._estimate_size(entry) / 1024 / 1024\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:253:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m251 |\u001b[0m             if estimated_size > self.max_file_size_mb:\n\u001b[1m\u001b[94m252 |\u001b[0m                 return  # Skip caching large objects\n\u001b[1m\u001b[94m253 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m254 |\u001b[0m             with open(cache_file, 'wb') as f:\n\u001b[1m\u001b[94m255 |\u001b[0m                 pickle.dump(entry, f)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:256:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m254 |\u001b[0m             with open(cache_file, 'wb') as f:\n\u001b[1m\u001b[94m255 |\u001b[0m                 pickle.dump(entry, f)\n\u001b[1m\u001b[94m256 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m258 |\u001b[0m             # Skip caching if there's an error\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:260:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m258 |\u001b[0m             # Skip caching if there's an error\n\u001b[1m\u001b[94m259 |\u001b[0m             pass\n\u001b[1m\u001b[94m260 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m261 |\u001b[0m     def delete(self, key: str) -> bool:\n\u001b[1m\u001b[94m262 |\u001b[0m         \"\"\"Delete entry from persistent cache.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:264:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m262 |\u001b[0m         \"\"\"Delete entry from persistent cache.\"\"\"\n\u001b[1m\u001b[94m263 |\u001b[0m         cache_file = self._get_cache_file(key)\n\u001b[1m\u001b[94m264 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m265 |\u001b[0m         if cache_file.exists():\n\u001b[1m\u001b[94m266 |\u001b[0m             cache_file.unlink()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:269:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m267 |\u001b[0m             return True\n\u001b[1m\u001b[94m268 |\u001b[0m         return False\n\u001b[1m\u001b[94m269 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m270 |\u001b[0m     def clear(self) -> None:\n\u001b[1m\u001b[94m271 |\u001b[0m         \"\"\"Clear all cache entries.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:274:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m272 |\u001b[0m         for cache_file in self.cache_dir.glob(\"*.cache\"):\n\u001b[1m\u001b[94m273 |\u001b[0m             cache_file.unlink()\n\u001b[1m\u001b[94m274 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m275 |\u001b[0m     def _get_cache_file(self, key: str) -> Path:\n\u001b[1m\u001b[94m276 |\u001b[0m         \"\"\"Get cache file path for key.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:280:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m278 |\u001b[0m         key_hash = hashlib.md5(key.encode()).hexdigest()\n\u001b[1m\u001b[94m279 |\u001b[0m         return self.cache_dir / f\"{key_hash}.cache\"\n\u001b[1m\u001b[94m280 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m     def _is_expired(self, entry: CacheEntry) -> bool:\n\u001b[1m\u001b[94m282 |\u001b[0m         \"\"\"Check if entry is expired.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:285:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m283 |\u001b[0m         if not entry.ttl_seconds:\n\u001b[1m\u001b[94m284 |\u001b[0m             return False\n\u001b[1m\u001b[94m285 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m         elapsed = (datetime.now() - entry.created_at).total_seconds()\n\u001b[1m\u001b[94m287 |\u001b[0m         return elapsed > entry.ttl_seconds\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:288:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m         elapsed = (datetime.now() - entry.created_at).total_seconds()\n\u001b[1m\u001b[94m287 |\u001b[0m         return elapsed > entry.ttl_seconds\n\u001b[1m\u001b[94m288 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m289 |\u001b[0m     def _estimate_size(self, obj: Any) -> int:\n\u001b[1m\u001b[94m290 |\u001b[0m         \"\"\"Estimate size of object in bytes.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:299:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m297 |\u001b[0m class HybridCache:\n\u001b[1m\u001b[94m298 |\u001b[0m     \"\"\"Hybrid cache combining memory and persistent storage.\"\"\"\n\u001b[1m\u001b[94m299 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m300 |\u001b[0m     def __init__(self, memory_cache: MemoryCache, persistent_cache: PersistentCache):\n\u001b[1m\u001b[94m301 |\u001b[0m         self.memory_cache = memory_cache\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:304:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m302 |\u001b[0m         self.persistent_cache = persistent_cache\n\u001b[1m\u001b[94m303 |\u001b[0m         self.lock = threading.RLock()\n\u001b[1m\u001b[94m304 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m305 |\u001b[0m     def get(self, key: str) -> Optional[Any]:\n\u001b[1m\u001b[94m306 |\u001b[0m         \"\"\"Get value from hybrid cache (memory first, then persistent).\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:311:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m309 |\u001b[0m         if value is not None:\n\u001b[1m\u001b[94m310 |\u001b[0m             return value\n\u001b[1m\u001b[94m311 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m312 |\u001b[0m         # Try persistent cache\n\u001b[1m\u001b[94m313 |\u001b[0m         value = self.persistent_cache.get(key)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:318:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m316 |\u001b[0m             self.memory_cache.set(key, value)\n\u001b[1m\u001b[94m317 |\u001b[0m             return value\n\u001b[1m\u001b[94m318 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m319 |\u001b[0m         return None\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:320:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m319 |\u001b[0m         return None\n\u001b[1m\u001b[94m320 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m321 |\u001b[0m     def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:\n\u001b[1m\u001b[94m322 |\u001b[0m         \"\"\"Set value in both caches.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:325:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m323 |\u001b[0m         # Set in memory cache\n\u001b[1m\u001b[94m324 |\u001b[0m         self.memory_cache.set(key, value, ttl_seconds)\n\u001b[1m\u001b[94m325 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m326 |\u001b[0m         # Set in persistent cache (with longer TTL if specified)\n\u001b[1m\u001b[94m327 |\u001b[0m         persistent_ttl = ttl_seconds * 2 if ttl_seconds else None\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:329:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m327 |\u001b[0m         persistent_ttl = ttl_seconds * 2 if ttl_seconds else None\n\u001b[1m\u001b[94m328 |\u001b[0m         self.persistent_cache.set(key, value, persistent_ttl)\n\u001b[1m\u001b[94m329 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m330 |\u001b[0m     def delete(self, key: str) -> bool:\n\u001b[1m\u001b[94m331 |\u001b[0m         \"\"\"Delete from both caches.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:335:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m333 |\u001b[0m         persistent_deleted = self.persistent_cache.delete(key)\n\u001b[1m\u001b[94m334 |\u001b[0m         return memory_deleted or persistent_deleted\n\u001b[1m\u001b[94m335 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m336 |\u001b[0m     def clear(self) -> None:\n\u001b[1m\u001b[94m337 |\u001b[0m         \"\"\"Clear both caches.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:355:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m353 |\u001b[0m             # Create cache key\n\u001b[1m\u001b[94m354 |\u001b[0m             cache_key = _create_cache_key(func.__name__, args, kwargs)\n\u001b[1m\u001b[94m355 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m356 |\u001b[0m             # Get appropriate cache\n\u001b[1m\u001b[94m357 |\u001b[0m             cache = _get_cache(cache_type)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:358:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m356 |\u001b[0m             # Get appropriate cache\n\u001b[1m\u001b[94m357 |\u001b[0m             cache = _get_cache(cache_type)\n\u001b[1m\u001b[94m358 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m359 |\u001b[0m             # Try to get from cache\n\u001b[1m\u001b[94m360 |\u001b[0m             result = cache.get(cache_key)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:363:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m361 |\u001b[0m             if result is not None:\n\u001b[1m\u001b[94m362 |\u001b[0m                 return result\n\u001b[1m\u001b[94m363 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m364 |\u001b[0m             # Execute function and cache result\n\u001b[1m\u001b[94m365 |\u001b[0m             result = func(*args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:367:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m365 |\u001b[0m             result = func(*args, **kwargs)\n\u001b[1m\u001b[94m366 |\u001b[0m             cache.set(cache_key, result, ttl_seconds)\n\u001b[1m\u001b[94m367 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m             return result\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:369:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m             return result\n\u001b[1m\u001b[94m369 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m370 |\u001b[0m         return wrapper\n\u001b[1m\u001b[94m371 |\u001b[0m     return decorator\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:381:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m379 |\u001b[0m             # Create cache key including DataFrame schema\n\u001b[1m\u001b[94m380 |\u001b[0m             cache_key = _create_dataframe_cache_key(func.__name__, args, kwargs)\n\u001b[1m\u001b[94m381 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m382 |\u001b[0m             # Get appropriate cache\n\u001b[1m\u001b[94m383 |\u001b[0m             cache = _get_cache(cache_type)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:384:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m382 |\u001b[0m             # Get appropriate cache\n\u001b[1m\u001b[94m383 |\u001b[0m             cache = _get_cache(cache_type)\n\u001b[1m\u001b[94m384 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m385 |\u001b[0m             # Try to get from cache\n\u001b[1m\u001b[94m386 |\u001b[0m             result = cache.get(cache_key)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:389:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m387 |\u001b[0m             if result is not None:\n\u001b[1m\u001b[94m388 |\u001b[0m                 return result\n\u001b[1m\u001b[94m389 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m390 |\u001b[0m             # Execute function and cache result\n\u001b[1m\u001b[94m391 |\u001b[0m             result = func(*args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:392:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m390 |\u001b[0m             # Execute function and cache result\n\u001b[1m\u001b[94m391 |\u001b[0m             result = func(*args, **kwargs)\n\u001b[1m\u001b[94m392 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m393 |\u001b[0m             # Cache result (DataFrames are cached by reference in Spark)\n\u001b[1m\u001b[94m394 |\u001b[0m             cache.set(cache_key, result, ttl_seconds)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:395:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m393 |\u001b[0m             # Cache result (DataFrames are cached by reference in Spark)\n\u001b[1m\u001b[94m394 |\u001b[0m             cache.set(cache_key, result, ttl_seconds)\n\u001b[1m\u001b[94m395 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m396 |\u001b[0m             return result\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:397:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m396 |\u001b[0m             return result\n\u001b[1m\u001b[94m397 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m398 |\u001b[0m         return wrapper\n\u001b[1m\u001b[94m399 |\u001b[0m     return decorator\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:408:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m406 |\u001b[0m         def wrapper(*args, **kwargs):\n\u001b[1m\u001b[94m407 |\u001b[0m             cache_key = _create_cache_key(f\"validation_{func.__name__}\", args, kwargs)\n\u001b[1m\u001b[94m408 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m409 |\u001b[0m             result = _memory_cache.get(cache_key)\n\u001b[1m\u001b[94m410 |\u001b[0m             if result is not None:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:412:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m             if result is not None:\n\u001b[1m\u001b[94m411 |\u001b[0m                 return result\n\u001b[1m\u001b[94m412 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m413 |\u001b[0m             result = func(*args, **kwargs)\n\u001b[1m\u001b[94m414 |\u001b[0m             _memory_cache.set(cache_key, result, ttl_seconds)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:415:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m413 |\u001b[0m             result = func(*args, **kwargs)\n\u001b[1m\u001b[94m414 |\u001b[0m             _memory_cache.set(cache_key, result, ttl_seconds)\n\u001b[1m\u001b[94m415 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m             return result\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:417:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m             return result\n\u001b[1m\u001b[94m417 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m418 |\u001b[0m         return wrapper\n\u001b[1m\u001b[94m419 |\u001b[0m     return decorator\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:428:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m426 |\u001b[0m         def wrapper(*args, **kwargs):\n\u001b[1m\u001b[94m427 |\u001b[0m             cache_key = _create_cache_key(f\"pipeline_{func.__name__}\", args, kwargs)\n\u001b[1m\u001b[94m428 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m429 |\u001b[0m             result = _hybrid_cache.get(cache_key)\n\u001b[1m\u001b[94m430 |\u001b[0m             if result is not None:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:432:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m430 |\u001b[0m             if result is not None:\n\u001b[1m\u001b[94m431 |\u001b[0m                 return result\n\u001b[1m\u001b[94m432 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m433 |\u001b[0m             result = func(*args, **kwargs)\n\u001b[1m\u001b[94m434 |\u001b[0m             _hybrid_cache.set(cache_key, result, ttl_seconds)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:435:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m433 |\u001b[0m             result = func(*args, **kwargs)\n\u001b[1m\u001b[94m434 |\u001b[0m             _hybrid_cache.set(cache_key, result, ttl_seconds)\n\u001b[1m\u001b[94m435 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m436 |\u001b[0m             return result\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:437:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m436 |\u001b[0m             return result\n\u001b[1m\u001b[94m437 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m438 |\u001b[0m         return wrapper\n\u001b[1m\u001b[94m439 |\u001b[0m     return decorator\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:447:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m445 |\u001b[0m     args_str = str(args) + str(sorted(kwargs.items()))\n\u001b[1m\u001b[94m446 |\u001b[0m     args_hash = hashlib.md5(args_str.encode()).hexdigest()\n\u001b[1m\u001b[94m447 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m448 |\u001b[0m     return f\"{func_name}:{args_hash}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:454:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m452 |\u001b[0m     \"\"\"Create cache key for DataFrame operations including schema info.\"\"\"\n\u001b[1m\u001b[94m453 |\u001b[0m     key_parts = [func_name]\n\u001b[1m\u001b[94m454 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m455 |\u001b[0m     for arg in args:\n\u001b[1m\u001b[94m456 |\u001b[0m         if hasattr(arg, 'schema') and hasattr(arg, 'count'):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:463:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m461 |\u001b[0m         else:\n\u001b[1m\u001b[94m462 |\u001b[0m             key_parts.append(str(arg))\n\u001b[1m\u001b[94m463 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m     for key, value in sorted(kwargs.items()):\n\u001b[1m\u001b[94m465 |\u001b[0m         key_parts.append(f\"{key}:{value}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:466:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m     for key, value in sorted(kwargs.items()):\n\u001b[1m\u001b[94m465 |\u001b[0m         key_parts.append(f\"{key}:{value}\")\n\u001b[1m\u001b[94m466 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m467 |\u001b[0m     key_str = \":\".join(key_parts)\n\u001b[1m\u001b[94m468 |\u001b[0m     return hashlib.md5(key_str.encode()).hexdigest()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:485:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m483 |\u001b[0m class CacheManager:\n\u001b[1m\u001b[94m484 |\u001b[0m     \"\"\"Centralized cache management.\"\"\"\n\u001b[1m\u001b[94m485 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m486 |\u001b[0m     def __init__(self):\n\u001b[1m\u001b[94m487 |\u001b[0m         self.caches = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:492:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m490 |\u001b[0m             \"hybrid\": _hybrid_cache\n\u001b[1m\u001b[94m491 |\u001b[0m         }\n\u001b[1m\u001b[94m492 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m493 |\u001b[0m     def get_stats(self) -> Dict[str, CacheStats]:\n\u001b[1m\u001b[94m494 |\u001b[0m         \"\"\"Get statistics for all caches.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:502:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m500 |\u001b[0m                 stats[name] = cache.memory_cache.get_stats()\n\u001b[1m\u001b[94m501 |\u001b[0m         return stats\n\u001b[1m\u001b[94m502 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m503 |\u001b[0m     def clear_all(self) -> None:\n\u001b[1m\u001b[94m504 |\u001b[0m         \"\"\"Clear all caches.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:507:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m505 |\u001b[0m         for cache in self.caches.values():\n\u001b[1m\u001b[94m506 |\u001b[0m             cache.clear()\n\u001b[1m\u001b[94m507 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m508 |\u001b[0m     def cleanup_expired(self) -> Dict[str, int]:\n\u001b[1m\u001b[94m509 |\u001b[0m         \"\"\"Clean up expired entries in all caches.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:517:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m515 |\u001b[0m                 results[name] = cache.memory_cache.cleanup_expired()\n\u001b[1m\u001b[94m516 |\u001b[0m         return results\n\u001b[1m\u001b[94m517 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m518 |\u001b[0m     def optimize_memory(self) -> None:\n\u001b[1m\u001b[94m519 |\u001b[0m         \"\"\"Optimize memory usage by cleaning up and evicting.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:522:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m520 |\u001b[0m         # Clean up expired entries\n\u001b[1m\u001b[94m521 |\u001b[0m         self.cleanup_expired()\n\u001b[1m\u001b[94m522 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m523 |\u001b[0m         # Force eviction in memory cache\n\u001b[1m\u001b[94m524 |\u001b[0m         if hasattr(_memory_cache, '_evict_if_needed'):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:560:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m558 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m559 |\u001b[0m     import argparse\n\u001b[1m\u001b[94m560 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m561 |\u001b[0m     parser = argparse.ArgumentParser(description=\"SparkForge Caching Strategies\")\n\u001b[1m\u001b[94m562 |\u001b[0m     parser.add_argument(\"--test\", action=\"store_true\", help=\"Run cache tests\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:565:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m563 |\u001b[0m     parser.add_argument(\"--stats\", action=\"store_true\", help=\"Show cache statistics\")\n\u001b[1m\u001b[94m564 |\u001b[0m     parser.add_argument(\"--clear\", action=\"store_true\", help=\"Clear all caches\")\n\u001b[1m\u001b[94m565 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m566 |\u001b[0m     args = parser.parse_args()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:567:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m566 |\u001b[0m     args = parser.parse_args()\n\u001b[1m\u001b[94m567 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m568 |\u001b[0m     if args.test:\n\u001b[1m\u001b[94m569 |\u001b[0m         # Run cache tests\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:571:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m569 |\u001b[0m         # Run cache tests\n\u001b[1m\u001b[94m570 |\u001b[0m         print(\"Running cache tests...\")\n\u001b[1m\u001b[94m571 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m572 |\u001b[0m         # Test memory cache\n\u001b[1m\u001b[94m573 |\u001b[0m         start_time = time.time()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:576:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m574 |\u001b[0m         result1 = cached_calculation([1, 2, 3, 4, 5])\n\u001b[1m\u001b[94m575 |\u001b[0m         first_call_time = time.time() - start_time\n\u001b[1m\u001b[94m576 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m577 |\u001b[0m         start_time = time.time()\n\u001b[1m\u001b[94m578 |\u001b[0m         result2 = cached_calculation([1, 2, 3, 4, 5])  # Should be cached\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:580:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m578 |\u001b[0m         result2 = cached_calculation([1, 2, 3, 4, 5])  # Should be cached\n\u001b[1m\u001b[94m579 |\u001b[0m         second_call_time = time.time() - start_time\n\u001b[1m\u001b[94m580 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m581 |\u001b[0m         print(f\"First call: {first_call_time:.3f}s, result: {result1}\")\n\u001b[1m\u001b[94m582 |\u001b[0m         print(f\"Second call: {second_call_time:.3f}s, result: {result2}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:584:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m582 |\u001b[0m         print(f\"Second call: {second_call_time:.3f}s, result: {result2}\")\n\u001b[1m\u001b[94m583 |\u001b[0m         print(f\"Cache hit speedup: {first_call_time / second_call_time:.1f}x\")\n\u001b[1m\u001b[94m584 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m585 |\u001b[0m     if args.stats:\n\u001b[1m\u001b[94m586 |\u001b[0m         # Show cache statistics\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:594:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m592 |\u001b[0m             print(f\"  Memory Usage: {cache_stats.memory_usage_mb:.2f} MB\")\n\u001b[1m\u001b[94m593 |\u001b[0m             print(f\"  Evictions: {cache_stats.eviction_count}\")\n\u001b[1m\u001b[94m594 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m595 |\u001b[0m     if args.clear:\n\u001b[1m\u001b[94m596 |\u001b[0m         # Clear all caches\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/caching_strategies.py:599:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m597 |\u001b[0m         cache_manager.clear_all()\n\u001b[1m\u001b[94m598 |\u001b[0m         print(\"All caches cleared\")\n\u001b[1m\u001b[94m599 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m600 |\u001b[0m     if not any([args.test, args.stats, args.clear]):\n\u001b[1m\u001b[94m601 |\u001b[0m         print(\"Use --test, --stats, or --clear to run cache operations\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import os\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import sys\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|________________________^\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m   # Add the project root to the path\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mE402 \u001b[0m\u001b[1mModule level import not at top of file\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:18:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m sys.path.insert(0, str(project_root))\n\u001b[1m\u001b[94m17 |\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m from tests.performance.performance_monitor import performance_monitor\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:26:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m24 |\u001b[0m     # Ensure we have a clean performance monitor\n\u001b[1m\u001b[94m25 |\u001b[0m     performance_monitor.results.clear()\n\u001b[1m\u001b[94m26 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m27 |\u001b[0m     # Set up environment variables for performance testing\n\u001b[1m\u001b[94m28 |\u001b[0m     os.environ[\"SPARKFORGE_PERFORMANCE_TESTING\"] = \"true\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:29:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m27 |\u001b[0m     # Set up environment variables for performance testing\n\u001b[1m\u001b[94m28 |\u001b[0m     os.environ[\"SPARKFORGE_PERFORMANCE_TESTING\"] = \"true\"\n\u001b[1m\u001b[94m29 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m30 |\u001b[0m     yield\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:31:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m30 |\u001b[0m     yield\n\u001b[1m\u001b[94m31 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m32 |\u001b[0m     # Cleanup after all performance tests\n\u001b[1m\u001b[94m33 |\u001b[0m     if hasattr(performance_monitor, 'results'):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:122:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m         if \"performance\" in str(item.fspath):\n\u001b[1m\u001b[94m121 |\u001b[0m             item.add_marker(pytest.mark.performance)\n\u001b[1m\u001b[94m122 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         # Add slow marker to tests that are expected to be slow\n\u001b[1m\u001b[94m124 |\u001b[0m         if any(keyword in item.name.lower() for keyword in [\"memory\", \"large\", \"stress\"]):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:126:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m         if any(keyword in item.name.lower() for keyword in [\"memory\", \"large\", \"stress\"]):\n\u001b[1m\u001b[94m125 |\u001b[0m             item.add_marker(pytest.mark.slow)\n\u001b[1m\u001b[94m126 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m127 |\u001b[0m         # Add memory marker to tests that measure memory\n\u001b[1m\u001b[94m128 |\u001b[0m         if \"memory\" in item.name.lower():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:130:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m         if \"memory\" in item.name.lower():\n\u001b[1m\u001b[94m129 |\u001b[0m             item.add_marker(pytest.mark.memory)\n\u001b[1m\u001b[94m130 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m131 |\u001b[0m         # Add regression marker to tests that check regressions\n\u001b[1m\u001b[94m132 |\u001b[0m         if \"regression\" in item.name.lower():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:143:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m141 |\u001b[0m         if any(\"performance\" in str(item.fspath) for item in terminalreporter.config.getoption(\"--collect-only\", False) or []):\n\u001b[1m\u001b[94m142 |\u001b[0m             terminalreporter.write_sep(\"=\", \"Performance Test Summary\")\n\u001b[1m\u001b[94m143 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m144 |\u001b[0m             summary = performance_monitor.get_performance_summary()\n\u001b[1m\u001b[94m145 |\u001b[0m             if summary.get(\"total_tests\", 0) > 0:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:151:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m                 terminalreporter.write(f\"Functions tested: {summary['functions_tested']}\\n\")\n\u001b[1m\u001b[94m150 |\u001b[0m                 terminalreporter.write(f\"Total execution time: {summary['total_execution_time']:.2f}s\\n\")\n\u001b[1m\u001b[94m151 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m                 # Check for regressions\n\u001b[1m\u001b[94m153 |\u001b[0m                 regressions = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:159:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m                         if regression[\"status\"] == \"regression_detected\":\n\u001b[1m\u001b[94m158 |\u001b[0m                             regressions.append(result.function_name)\n\u001b[1m\u001b[94m159 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m                 if regressions:\n\u001b[1m\u001b[94m161 |\u001b[0m                     terminalreporter.write(f\"\u26a0\ufe0f  Performance regressions detected in: {', '.join(regressions)}\\n\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:204:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m202 |\u001b[0m         \"SPARKFORGE_PERFORMANCE_TOLERANCE\": \"0.2\",\n\u001b[1m\u001b[94m203 |\u001b[0m     }\n\u001b[1m\u001b[94m204 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m205 |\u001b[0m     original_env = {}\n\u001b[1m\u001b[94m206 |\u001b[0m     for key, value in env_vars.items():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:209:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m         original_env[key] = os.environ.get(key)\n\u001b[1m\u001b[94m208 |\u001b[0m         os.environ[key] = value\n\u001b[1m\u001b[94m209 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m     yield env_vars\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/conftest.py:211:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m     yield env_vars\n\u001b[1m\u001b[94m211 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m212 |\u001b[0m     # Restore original environment\n\u001b[1m\u001b[94m213 |\u001b[0m     for key, original_value in original_env.items():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:12:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import gc\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import sys\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import tracemalloc\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import psutil\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import threading\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import time\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Any, Dict, List, Optional, Tuple, Callable\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from collections import defaultdict\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import weakref\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import functools\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import logging\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|______________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Optional` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:18:37\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m import threading\n\u001b[1m\u001b[94m17 |\u001b[0m import time\n\u001b[1m\u001b[94m18 |\u001b[0m from typing import Any, Dict, List, Optional, Tuple, Callable\n   \u001b[1m\u001b[94m|\u001b[0m                                     \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m20 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Tuple` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:18:47\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m import threading\n\u001b[1m\u001b[94m17 |\u001b[0m import time\n\u001b[1m\u001b[94m18 |\u001b[0m from typing import Any, Dict, List, Optional, Tuple, Callable\n   \u001b[1m\u001b[94m|\u001b[0m                                               \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m20 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`dataclasses.asdict` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:19:36\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m import time\n\u001b[1m\u001b[94m18 |\u001b[0m from typing import Any, Dict, List, Optional, Tuple, Callable\n\u001b[1m\u001b[94m19 |\u001b[0m from dataclasses import dataclass, asdict\n   \u001b[1m\u001b[94m|\u001b[0m                                    \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m21 |\u001b[0m from collections import defaultdict\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `dataclasses.asdict`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`datetime.timedelta` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:20:32\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m from typing import Any, Dict, List, Optional, Tuple, Callable\n\u001b[1m\u001b[94m19 |\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m20 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m from collections import defaultdict\n\u001b[1m\u001b[94m22 |\u001b[0m import weakref\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `datetime.timedelta`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:62:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m60 |\u001b[0m class MemoryProfiler:\n\u001b[1m\u001b[94m61 |\u001b[0m     \"\"\"Memory profiling and analysis.\"\"\"\n\u001b[1m\u001b[94m62 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m     def __init__(self, enable_tracemalloc: bool = True):\n\u001b[1m\u001b[94m64 |\u001b[0m         self.enable_tracemalloc = enable_tracemalloc\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:69:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m67 |\u001b[0m         self.object_sizes: Dict[str, int] = defaultdict(int)\n\u001b[1m\u001b[94m68 |\u001b[0m         self.leak_detector = MemoryLeakDetector()\n\u001b[1m\u001b[94m69 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m70 |\u001b[0m         if self.enable_tracemalloc:\n\u001b[1m\u001b[94m71 |\u001b[0m             tracemalloc.start()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:72:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m70 |\u001b[0m         if self.enable_tracemalloc:\n\u001b[1m\u001b[94m71 |\u001b[0m             tracemalloc.start()\n\u001b[1m\u001b[94m72 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m73 |\u001b[0m         self.logger = logging.getLogger(\"memory_profiler\")\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:74:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m73 |\u001b[0m         self.logger = logging.getLogger(\"memory_profiler\")\n\u001b[1m\u001b[94m74 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m75 |\u001b[0m     def take_snapshot(self) -> MemorySnapshot:\n\u001b[1m\u001b[94m76 |\u001b[0m         \"\"\"Take a memory snapshot.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:81:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m             process = psutil.Process()\n\u001b[1m\u001b[94m80 |\u001b[0m             current_memory = process.memory_info().rss / 1024 / 1024  # MB\n\u001b[1m\u001b[94m81 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m82 |\u001b[0m             # Get peak memory if tracemalloc is enabled\n\u001b[1m\u001b[94m83 |\u001b[0m             if self.enable_tracemalloc:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:88:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m                 snapshot = tracemalloc.take_snapshot()\n\u001b[1m\u001b[94m87 |\u001b[0m                 top_stats = snapshot.statistics('lineno')\n\u001b[1m\u001b[94m88 |\u001b[0m                 \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m89 |\u001b[0m                 top_allocations = []\n\u001b[1m\u001b[94m90 |\u001b[0m                 for stat in top_stats[:10]:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:99:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 97 |\u001b[0m                 peak_memory = current_memory\n\u001b[1m\u001b[94m 98 |\u001b[0m                 top_allocations = []\n\u001b[1m\u001b[94m 99 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m100 |\u001b[0m             # Count objects\n\u001b[1m\u001b[94m101 |\u001b[0m             object_counts = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:107:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m105 |\u001b[0m                 self.object_counts[obj_type] += 1\n\u001b[1m\u001b[94m106 |\u001b[0m                 self.object_sizes[obj_type] += sys.getsizeof(obj)\n\u001b[1m\u001b[94m107 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m108 |\u001b[0m             snapshot = MemorySnapshot(\n\u001b[1m\u001b[94m109 |\u001b[0m                 timestamp=datetime.now(),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:115:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m113 |\u001b[0m                 top_allocations=top_allocations\n\u001b[1m\u001b[94m114 |\u001b[0m             )\n\u001b[1m\u001b[94m115 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m116 |\u001b[0m             self.snapshots.append(snapshot)\n\u001b[1m\u001b[94m117 |\u001b[0m             self.leak_detector.analyze_snapshot(snapshot)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:118:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m116 |\u001b[0m             self.snapshots.append(snapshot)\n\u001b[1m\u001b[94m117 |\u001b[0m             self.leak_detector.analyze_snapshot(snapshot)\n\u001b[1m\u001b[94m118 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m             return snapshot\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:120:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m             return snapshot\n\u001b[1m\u001b[94m120 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m121 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m122 |\u001b[0m             self.logger.error(f\"Error taking memory snapshot: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:130:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m                 top_allocations=[]\n\u001b[1m\u001b[94m129 |\u001b[0m             )\n\u001b[1m\u001b[94m130 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m131 |\u001b[0m     def get_memory_stats(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m132 |\u001b[0m         \"\"\"Get comprehensive memory statistics.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:135:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m133 |\u001b[0m         if not self.snapshots:\n\u001b[1m\u001b[94m134 |\u001b[0m             return {}\n\u001b[1m\u001b[94m135 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m         latest_snapshot = self.snapshots[-1]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:137:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m         latest_snapshot = self.snapshots[-1]\n\u001b[1m\u001b[94m137 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m138 |\u001b[0m         # Calculate memory trends\n\u001b[1m\u001b[94m139 |\u001b[0m         if len(self.snapshots) > 1:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:148:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         else:\n\u001b[1m\u001b[94m147 |\u001b[0m             memory_trend = []\n\u001b[1m\u001b[94m148 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m         return {\n\u001b[1m\u001b[94m150 |\u001b[0m             \"current_memory_mb\": latest_snapshot.current_memory_mb,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:159:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m             \"gc_stats\": self._get_gc_stats()\n\u001b[1m\u001b[94m158 |\u001b[0m         }\n\u001b[1m\u001b[94m159 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m     def _get_gc_stats(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m161 |\u001b[0m         \"\"\"Get garbage collection statistics.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:178:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m176 |\u001b[0m class MemoryLeakDetector:\n\u001b[1m\u001b[94m177 |\u001b[0m     \"\"\"Memory leak detection and analysis.\"\"\"\n\u001b[1m\u001b[94m178 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m179 |\u001b[0m     def __init__(self):\n\u001b[1m\u001b[94m180 |\u001b[0m         self.object_history: Dict[str, List[int]] = defaultdict(list)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:184:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m182 |\u001b[0m         self.analysis_interval = 60  # seconds\n\u001b[1m\u001b[94m183 |\u001b[0m         self.last_analysis = datetime.now()\n\u001b[1m\u001b[94m184 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m185 |\u001b[0m     def analyze_snapshot(self, snapshot: MemorySnapshot) -> None:\n\u001b[1m\u001b[94m186 |\u001b[0m         \"\"\"Analyze memory snapshot for potential leaks.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:188:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         \"\"\"Analyze memory snapshot for potential leaks.\"\"\"\n\u001b[1m\u001b[94m187 |\u001b[0m         current_time = datetime.now()\n\u001b[1m\u001b[94m188 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m189 |\u001b[0m         # Only analyze if enough time has passed\n\u001b[1m\u001b[94m190 |\u001b[0m         if (current_time - self.last_analysis).total_seconds() < self.analysis_interval:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:192:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m         if (current_time - self.last_analysis).total_seconds() < self.analysis_interval:\n\u001b[1m\u001b[94m191 |\u001b[0m             return\n\u001b[1m\u001b[94m192 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m         self.last_analysis = current_time\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:194:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m         self.last_analysis = current_time\n\u001b[1m\u001b[94m194 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m195 |\u001b[0m         # Track object counts over time\n\u001b[1m\u001b[94m196 |\u001b[0m         for allocation in snapshot.top_allocations:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:199:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m             obj_type = allocation[\"filename\"]\n\u001b[1m\u001b[94m198 |\u001b[0m             count = allocation[\"count\"]\n\u001b[1m\u001b[94m199 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m             self.object_history[obj_type].append(count)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:201:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m             self.object_history[obj_type].append(count)\n\u001b[1m\u001b[94m201 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m202 |\u001b[0m             # Keep only last 10 measurements\n\u001b[1m\u001b[94m203 |\u001b[0m             if len(self.object_history[obj_type]) > 10:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:205:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m203 |\u001b[0m             if len(self.object_history[obj_type]) > 10:\n\u001b[1m\u001b[94m204 |\u001b[0m                 self.object_history[obj_type] = self.object_history[obj_type][-10:]\n\u001b[1m\u001b[94m205 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m206 |\u001b[0m             # Detect potential leaks\n\u001b[1m\u001b[94m207 |\u001b[0m             if len(self.object_history[obj_type]) >= 5:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:209:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m             if len(self.object_history[obj_type]) >= 5:\n\u001b[1m\u001b[94m208 |\u001b[0m                 self._detect_leak(obj_type, allocation, current_time)\n\u001b[1m\u001b[94m209 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m     def _detect_leak(self, obj_type: str, allocation: Dict[str, Any], timestamp: datetime) -> None:\n\u001b[1m\u001b[94m211 |\u001b[0m         \"\"\"Detect potential memory leak for specific object type.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:213:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m211 |\u001b[0m         \"\"\"Detect potential memory leak for specific object type.\"\"\"\n\u001b[1m\u001b[94m212 |\u001b[0m         history = self.object_history[obj_type]\n\u001b[1m\u001b[94m213 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m         # Calculate growth rate\n\u001b[1m\u001b[94m215 |\u001b[0m         if len(history) >= 3:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:217:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m215 |\u001b[0m         if len(history) >= 3:\n\u001b[1m\u001b[94m216 |\u001b[0m             recent_growth = (history[-1] - history[-3]) / 2  # Average growth over last 2 intervals\n\u001b[1m\u001b[94m217 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m218 |\u001b[0m             # Consider it a leak if growth rate is positive and significant\n\u001b[1m\u001b[94m219 |\u001b[0m             if recent_growth > 10:  # More than 10 objects per interval\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:229:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m227 |\u001b[0m                     severity=\"high\" if recent_growth > 50 else \"medium\"\n\u001b[1m\u001b[94m228 |\u001b[0m                 )\n\u001b[1m\u001b[94m229 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m230 |\u001b[0m                 # Only add if not already detected\n\u001b[1m\u001b[94m231 |\u001b[0m                 if not any(l.object_type == obj_type and not l.resolved for l in self.detected_leaks):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mE741 \u001b[0m\u001b[1mAmbiguous variable name: `l`\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:231:77\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m230 |\u001b[0m                 # Only add if not already detected\n\u001b[1m\u001b[94m231 |\u001b[0m                 if not any(l.object_type == obj_type and not l.resolved for l in self.detected_leaks):\n    \u001b[1m\u001b[94m|\u001b[0m                                                                             \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m232 |\u001b[0m                     self.detected_leaks.append(leak)\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:233:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m                 if not any(l.object_type == obj_type and not l.resolved for l in self.detected_leaks):\n\u001b[1m\u001b[94m232 |\u001b[0m                     self.detected_leaks.append(leak)\n\u001b[1m\u001b[94m233 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m234 |\u001b[0m     def get_detected_leaks(self) -> List[MemoryLeak]:\n\u001b[1m\u001b[94m235 |\u001b[0m         \"\"\"Get list of detected memory leaks.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:241:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m239 |\u001b[0m class MemoryOptimizer:\n\u001b[1m\u001b[94m240 |\u001b[0m     \"\"\"Memory optimization strategies and utilities.\"\"\"\n\u001b[1m\u001b[94m241 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m242 |\u001b[0m     def __init__(self):\n\u001b[1m\u001b[94m243 |\u001b[0m         self.optimization_strategies = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:251:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m249 |\u001b[0m         }\n\u001b[1m\u001b[94m250 |\u001b[0m         self.logger = logging.getLogger(\"memory_optimizer\")\n\u001b[1m\u001b[94m251 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m252 |\u001b[0m     def optimize_memory_usage(self, target_memory_mb: float) -> Dict[str, Any]:\n\u001b[1m\u001b[94m253 |\u001b[0m         \"\"\"Optimize memory usage to target level.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:255:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m253 |\u001b[0m         \"\"\"Optimize memory usage to target level.\"\"\"\n\u001b[1m\u001b[94m254 |\u001b[0m         results = {}\n\u001b[1m\u001b[94m255 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m256 |\u001b[0m         # Get current memory usage\n\u001b[1m\u001b[94m257 |\u001b[0m         process = psutil.Process()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:259:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         process = psutil.Process()\n\u001b[1m\u001b[94m258 |\u001b[0m         current_memory = process.memory_info().rss / 1024 / 1024\n\u001b[1m\u001b[94m259 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m         if current_memory <= target_memory_mb:\n\u001b[1m\u001b[94m261 |\u001b[0m             return {\"status\": \"already_optimized\", \"current_memory_mb\": current_memory}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:262:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m         if current_memory <= target_memory_mb:\n\u001b[1m\u001b[94m261 |\u001b[0m             return {\"status\": \"already_optimized\", \"current_memory_mb\": current_memory}\n\u001b[1m\u001b[94m262 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m         # Apply optimization strategies\n\u001b[1m\u001b[94m264 |\u001b[0m         for strategy_name, strategy_func in self.optimization_strategies.items():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:268:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m                 result = strategy_func()\n\u001b[1m\u001b[94m267 |\u001b[0m                 results[strategy_name] = result\n\u001b[1m\u001b[94m268 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m                 # Check if we've reached target\n\u001b[1m\u001b[94m270 |\u001b[0m                 new_memory = process.memory_info().rss / 1024 / 1024\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:273:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m                 if new_memory <= target_memory_mb:\n\u001b[1m\u001b[94m272 |\u001b[0m                     break\n\u001b[1m\u001b[94m273 |\u001b[0m                     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m274 |\u001b[0m             except Exception as e:\n\u001b[1m\u001b[94m275 |\u001b[0m                 self.logger.error(f\"Error applying optimization strategy {strategy_name}: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:277:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m275 |\u001b[0m                 self.logger.error(f\"Error applying optimization strategy {strategy_name}: {e}\")\n\u001b[1m\u001b[94m276 |\u001b[0m                 results[strategy_name] = {\"error\": str(e)}\n\u001b[1m\u001b[94m277 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m278 |\u001b[0m         final_memory = process.memory_info().rss / 1024 / 1024\n\u001b[1m\u001b[94m279 |\u001b[0m         results[\"final_memory_mb\"] = final_memory\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:281:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m279 |\u001b[0m         results[\"final_memory_mb\"] = final_memory\n\u001b[1m\u001b[94m280 |\u001b[0m         results[\"memory_reduced_mb\"] = current_memory - final_memory\n\u001b[1m\u001b[94m281 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m282 |\u001b[0m         return results\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:283:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m282 |\u001b[0m         return results\n\u001b[1m\u001b[94m283 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m284 |\u001b[0m     def _optimize_garbage_collection(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m285 |\u001b[0m         \"\"\"Optimize garbage collection settings.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:288:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m         # Get current GC settings\n\u001b[1m\u001b[94m287 |\u001b[0m         old_thresholds = gc.get_threshold()\n\u001b[1m\u001b[94m288 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m289 |\u001b[0m         # Optimize thresholds for better memory management\n\u001b[1m\u001b[94m290 |\u001b[0m         gc.set_threshold(700, 10, 10)  # More aggressive collection\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:291:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m289 |\u001b[0m         # Optimize thresholds for better memory management\n\u001b[1m\u001b[94m290 |\u001b[0m         gc.set_threshold(700, 10, 10)  # More aggressive collection\n\u001b[1m\u001b[94m291 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m292 |\u001b[0m         # Force garbage collection\n\u001b[1m\u001b[94m293 |\u001b[0m         collected = gc.collect()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:294:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m292 |\u001b[0m         # Force garbage collection\n\u001b[1m\u001b[94m293 |\u001b[0m         collected = gc.collect()\n\u001b[1m\u001b[94m294 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m295 |\u001b[0m         return {\n\u001b[1m\u001b[94m296 |\u001b[0m             \"old_thresholds\": old_thresholds,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:300:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m298 |\u001b[0m             \"objects_collected\": collected\n\u001b[1m\u001b[94m299 |\u001b[0m         }\n\u001b[1m\u001b[94m300 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m301 |\u001b[0m     def _implement_object_pooling(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m302 |\u001b[0m         \"\"\"Implement object pooling for frequently created objects.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:309:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m307 |\u001b[0m             \"note\": \"Object pooling should be implemented based on specific application needs\"\n\u001b[1m\u001b[94m308 |\u001b[0m         }\n\u001b[1m\u001b[94m309 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m310 |\u001b[0m     def _implement_lazy_evaluation(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m311 |\u001b[0m         \"\"\"Implement lazy evaluation for memory-intensive operations.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:318:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m316 |\u001b[0m             \"note\": \"Lazy evaluation should be implemented for specific data processing operations\"\n\u001b[1m\u001b[94m317 |\u001b[0m         }\n\u001b[1m\u001b[94m318 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m319 |\u001b[0m     def _implement_memory_mapping(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m320 |\u001b[0m         \"\"\"Implement memory mapping for large data structures.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:327:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m325 |\u001b[0m             \"note\": \"Memory mapping should be implemented for large file-based data structures\"\n\u001b[1m\u001b[94m326 |\u001b[0m         }\n\u001b[1m\u001b[94m327 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m328 |\u001b[0m     def _use_weak_references(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m329 |\u001b[0m         \"\"\"Use weak references to prevent memory leaks.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:336:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m334 |\u001b[0m             \"note\": \"Weak references should be used for caching and observer patterns\"\n\u001b[1m\u001b[94m335 |\u001b[0m         }\n\u001b[1m\u001b[94m336 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m337 |\u001b[0m     def analyze_memory_efficiency(self) -> float:\n\u001b[1m\u001b[94m338 |\u001b[0m         \"\"\"Analyze memory efficiency and return score (0-100).\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:342:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m340 |\u001b[0m             process = psutil.Process()\n\u001b[1m\u001b[94m341 |\u001b[0m             memory_info = process.memory_info()\n\u001b[1m\u001b[94m342 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m343 |\u001b[0m             # Calculate efficiency based on various factors\n\u001b[1m\u001b[94m344 |\u001b[0m             memory_usage_mb = memory_info.rss / 1024 / 1024\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:345:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m343 |\u001b[0m             # Calculate efficiency based on various factors\n\u001b[1m\u001b[94m344 |\u001b[0m             memory_usage_mb = memory_info.rss / 1024 / 1024\n\u001b[1m\u001b[94m345 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m346 |\u001b[0m             # Base score\n\u001b[1m\u001b[94m347 |\u001b[0m             efficiency_score = 100.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:348:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m346 |\u001b[0m             # Base score\n\u001b[1m\u001b[94m347 |\u001b[0m             efficiency_score = 100.0\n\u001b[1m\u001b[94m348 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m349 |\u001b[0m             # Penalize high memory usage\n\u001b[1m\u001b[94m350 |\u001b[0m             if memory_usage_mb > 1000:  # > 1GB\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:352:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m350 |\u001b[0m             if memory_usage_mb > 1000:  # > 1GB\n\u001b[1m\u001b[94m351 |\u001b[0m                 efficiency_score -= min(50, (memory_usage_mb - 1000) / 100 * 10)\n\u001b[1m\u001b[94m352 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m353 |\u001b[0m             # Check for memory fragmentation\n\u001b[1m\u001b[94m354 |\u001b[0m             if hasattr(memory_info, 'vms') and memory_info.vms > memory_info.rss * 2:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:356:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m354 |\u001b[0m             if hasattr(memory_info, 'vms') and memory_info.vms > memory_info.rss * 2:\n\u001b[1m\u001b[94m355 |\u001b[0m                 efficiency_score -= 20  # High virtual memory usage\n\u001b[1m\u001b[94m356 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m357 |\u001b[0m             # Check GC efficiency\n\u001b[1m\u001b[94m358 |\u001b[0m             gc_stats = gc.get_stats()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:362:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m360 |\u001b[0m             if total_collections > 1000:  # Too many collections\n\u001b[1m\u001b[94m361 |\u001b[0m                 efficiency_score -= 15\n\u001b[1m\u001b[94m362 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m363 |\u001b[0m             return max(0, min(100, efficiency_score))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:364:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m363 |\u001b[0m             return max(0, min(100, efficiency_score))\n\u001b[1m\u001b[94m364 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m365 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m366 |\u001b[0m             self.logger.error(f\"Error analyzing memory efficiency: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:372:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m370 |\u001b[0m class MemoryEfficientDataStructures:\n\u001b[1m\u001b[94m371 |\u001b[0m     \"\"\"Memory-efficient data structures and utilities.\"\"\"\n\u001b[1m\u001b[94m372 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m373 |\u001b[0m     @staticmethod\n\u001b[1m\u001b[94m374 |\u001b[0m     def create_weak_cache(max_size: int = 1000) -> weakref.WeakValueDictionary:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:377:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m375 |\u001b[0m         \"\"\"Create a memory-efficient cache using weak references.\"\"\"\n\u001b[1m\u001b[94m376 |\u001b[0m         return weakref.WeakValueDictionary()\n\u001b[1m\u001b[94m377 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m     @staticmethod\n\u001b[1m\u001b[94m379 |\u001b[0m     def create_object_pool(pool_size: int = 100, factory_func: Callable = None):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:382:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m380 |\u001b[0m         \"\"\"Create an object pool to reuse expensive objects.\"\"\"\n\u001b[1m\u001b[94m381 |\u001b[0m         pool = []\n\u001b[1m\u001b[94m382 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m383 |\u001b[0m         def get_object():\n\u001b[1m\u001b[94m384 |\u001b[0m             if pool:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:390:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m388 |\u001b[0m             else:\n\u001b[1m\u001b[94m389 |\u001b[0m                 return None\n\u001b[1m\u001b[94m390 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m         def return_object(obj):\n\u001b[1m\u001b[94m392 |\u001b[0m             if len(pool) < pool_size:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:394:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m392 |\u001b[0m             if len(pool) < pool_size:\n\u001b[1m\u001b[94m393 |\u001b[0m                 pool.append(obj)\n\u001b[1m\u001b[94m394 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m395 |\u001b[0m         return get_object, return_object\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:396:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m395 |\u001b[0m         return get_object, return_object\n\u001b[1m\u001b[94m396 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m397 |\u001b[0m     @staticmethod\n\u001b[1m\u001b[94m398 |\u001b[0m     def create_memory_efficient_list(max_size: int = 10000):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:404:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m402 |\u001b[0m                 self.max_size = max_size\n\u001b[1m\u001b[94m403 |\u001b[0m                 self.items = []\n\u001b[1m\u001b[94m404 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m405 |\u001b[0m             def append(self, item):\n\u001b[1m\u001b[94m406 |\u001b[0m                 self.items.append(item)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:409:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m                 if len(self.items) > self.max_size:\n\u001b[1m\u001b[94m408 |\u001b[0m                     self.items = self.items[-self.max_size:]  # Keep only recent items\n\u001b[1m\u001b[94m409 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m             def __getitem__(self, index):\n\u001b[1m\u001b[94m411 |\u001b[0m                 return self.items[index]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:412:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m             def __getitem__(self, index):\n\u001b[1m\u001b[94m411 |\u001b[0m                 return self.items[index]\n\u001b[1m\u001b[94m412 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m413 |\u001b[0m             def __len__(self):\n\u001b[1m\u001b[94m414 |\u001b[0m                 return len(self.items)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:415:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m413 |\u001b[0m             def __len__(self):\n\u001b[1m\u001b[94m414 |\u001b[0m                 return len(self.items)\n\u001b[1m\u001b[94m415 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m             def __iter__(self):\n\u001b[1m\u001b[94m417 |\u001b[0m                 return iter(self.items)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:418:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m             def __iter__(self):\n\u001b[1m\u001b[94m417 |\u001b[0m                 return iter(self.items)\n\u001b[1m\u001b[94m418 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m419 |\u001b[0m         return MemoryEfficientList(max_size)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:420:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m419 |\u001b[0m         return MemoryEfficientList(max_size)\n\u001b[1m\u001b[94m420 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m421 |\u001b[0m     @staticmethod\n\u001b[1m\u001b[94m422 |\u001b[0m     def create_lazy_dict(factory_func: Callable):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:428:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m426 |\u001b[0m                 super().__init__()\n\u001b[1m\u001b[94m427 |\u001b[0m                 self.factory_func = factory_func\n\u001b[1m\u001b[94m428 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m429 |\u001b[0m             def __missing__(self, key):\n\u001b[1m\u001b[94m430 |\u001b[0m                 value = self.factory_func(key)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:433:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m431 |\u001b[0m                 self[key] = value\n\u001b[1m\u001b[94m432 |\u001b[0m                 return value\n\u001b[1m\u001b[94m433 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m434 |\u001b[0m         return LazyDict(factory_func)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:445:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m443 |\u001b[0m             process = psutil.Process()\n\u001b[1m\u001b[94m444 |\u001b[0m             initial_memory = process.memory_info().rss / 1024 / 1024\n\u001b[1m\u001b[94m445 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m446 |\u001b[0m             # Start memory monitoring in background\n\u001b[1m\u001b[94m447 |\u001b[0m             monitor_thread = threading.Thread(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:453:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m451 |\u001b[0m             )\n\u001b[1m\u001b[94m452 |\u001b[0m             monitor_thread.start()\n\u001b[1m\u001b[94m453 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m454 |\u001b[0m             try:\n\u001b[1m\u001b[94m455 |\u001b[0m                 result = func(*args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:461:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m459 |\u001b[0m                 final_memory = process.memory_info().rss / 1024 / 1024\n\u001b[1m\u001b[94m460 |\u001b[0m                 memory_delta = final_memory - initial_memory\n\u001b[1m\u001b[94m461 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m462 |\u001b[0m                 if memory_delta > 100:  # More than 100MB increase\n\u001b[1m\u001b[94m463 |\u001b[0m                     logging.warning(f\"Function {func.__name__} used {memory_delta:.1f}MB of memory\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:464:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m462 |\u001b[0m                 if memory_delta > 100:  # More than 100MB increase\n\u001b[1m\u001b[94m463 |\u001b[0m                     logging.warning(f\"Function {func.__name__} used {memory_delta:.1f}MB of memory\")\n\u001b[1m\u001b[94m464 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m465 |\u001b[0m         return wrapper\n\u001b[1m\u001b[94m466 |\u001b[0m     return decorator\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:473:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m471 |\u001b[0m     process = psutil.Process()\n\u001b[1m\u001b[94m472 |\u001b[0m     initial_memory = process.memory_info().rss / 1024 / 1024\n\u001b[1m\u001b[94m473 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m474 |\u001b[0m     while True:\n\u001b[1m\u001b[94m475 |\u001b[0m         time.sleep(interval_seconds)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:478:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m476 |\u001b[0m         current_memory = process.memory_info().rss / 1024 / 1024\n\u001b[1m\u001b[94m477 |\u001b[0m         memory_delta = current_memory - initial_memory\n\u001b[1m\u001b[94m478 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m479 |\u001b[0m         if memory_delta > 200:  # More than 200MB increase\n\u001b[1m\u001b[94m480 |\u001b[0m             logging.warning(f\"Memory usage in {function_name} increased by {memory_delta:.1f}MB\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:508:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m506 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m507 |\u001b[0m     import argparse\n\u001b[1m\u001b[94m508 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m509 |\u001b[0m     parser = argparse.ArgumentParser(description=\"SparkForge Memory Optimization\")\n\u001b[1m\u001b[94m510 |\u001b[0m     parser.add_argument(\"--profile\", action=\"store_true\", help=\"Run memory profiling\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:514:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m512 |\u001b[0m     parser.add_argument(\"--target-memory\", type=float, default=500, help=\"Target memory in MB\")\n\u001b[1m\u001b[94m513 |\u001b[0m     parser.add_argument(\"--duration\", type=int, default=60, help=\"Profiling duration in seconds\")\n\u001b[1m\u001b[94m514 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m515 |\u001b[0m     args = parser.parse_args()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:516:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m515 |\u001b[0m     args = parser.parse_args()\n\u001b[1m\u001b[94m516 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m517 |\u001b[0m     if args.profile:\n\u001b[1m\u001b[94m518 |\u001b[0m         profiler = MemoryProfiler()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:520:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m518 |\u001b[0m         profiler = MemoryProfiler()\n\u001b[1m\u001b[94m519 |\u001b[0m         print(\"Starting memory profiling...\")\n\u001b[1m\u001b[94m520 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m521 |\u001b[0m         # Take initial snapshot\n\u001b[1m\u001b[94m522 |\u001b[0m         initial_snapshot = profiler.take_snapshot()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:524:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m522 |\u001b[0m         initial_snapshot = profiler.take_snapshot()\n\u001b[1m\u001b[94m523 |\u001b[0m         print(f\"Initial memory: {initial_snapshot.current_memory_mb:.1f}MB\")\n\u001b[1m\u001b[94m524 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m525 |\u001b[0m         # Monitor for specified duration\n\u001b[1m\u001b[94m526 |\u001b[0m         try:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:530:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m528 |\u001b[0m         except KeyboardInterrupt:\n\u001b[1m\u001b[94m529 |\u001b[0m             print(\"\\nProfiling stopped by user\")\n\u001b[1m\u001b[94m530 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m531 |\u001b[0m         # Take final snapshot\n\u001b[1m\u001b[94m532 |\u001b[0m         final_snapshot = profiler.take_snapshot()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:534:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m532 |\u001b[0m         final_snapshot = profiler.take_snapshot()\n\u001b[1m\u001b[94m533 |\u001b[0m         print(f\"Final memory: {final_snapshot.current_memory_mb:.1f}MB\")\n\u001b[1m\u001b[94m534 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m535 |\u001b[0m         # Show stats\n\u001b[1m\u001b[94m536 |\u001b[0m         stats = profiler.get_memory_stats()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:537:15\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m535 |\u001b[0m         # Show stats\n\u001b[1m\u001b[94m536 |\u001b[0m         stats = profiler.get_memory_stats()\n\u001b[1m\u001b[94m537 |\u001b[0m         print(f\"\\nMemory Statistics:\")\n    \u001b[1m\u001b[94m|\u001b[0m               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m538 |\u001b[0m         print(f\"Peak memory: {stats.get('peak_memory_mb', 0):.1f}MB\")\n\u001b[1m\u001b[94m539 |\u001b[0m         print(f\"Allocated blocks: {stats.get('allocated_blocks', 0)}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:540:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m538 |\u001b[0m         print(f\"Peak memory: {stats.get('peak_memory_mb', 0):.1f}MB\")\n\u001b[1m\u001b[94m539 |\u001b[0m         print(f\"Allocated blocks: {stats.get('allocated_blocks', 0)}\")\n\u001b[1m\u001b[94m540 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m541 |\u001b[0m         # Show detected leaks\n\u001b[1m\u001b[94m542 |\u001b[0m         leaks = profiler.leak_detector.get_detected_leaks()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:549:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m547 |\u001b[0m         else:\n\u001b[1m\u001b[94m548 |\u001b[0m             print(\"\\nNo memory leaks detected\")\n\u001b[1m\u001b[94m549 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m550 |\u001b[0m     if args.optimize:\n\u001b[1m\u001b[94m551 |\u001b[0m         optimizer = MemoryOptimizer()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:553:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m551 |\u001b[0m         optimizer = MemoryOptimizer()\n\u001b[1m\u001b[94m552 |\u001b[0m         print(f\"Optimizing memory to target: {args.target_memory}MB\")\n\u001b[1m\u001b[94m553 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m554 |\u001b[0m         results = optimizer.optimize_memory_usage(args.target_memory)\n\u001b[1m\u001b[94m555 |\u001b[0m         print(f\"\\nOptimization Results:\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:555:15\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m554 |\u001b[0m         results = optimizer.optimize_memory_usage(args.target_memory)\n\u001b[1m\u001b[94m555 |\u001b[0m         print(f\"\\nOptimization Results:\")\n    \u001b[1m\u001b[94m|\u001b[0m               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m556 |\u001b[0m         print(f\"Final memory: {results.get('final_memory_mb', 0):.1f}MB\")\n\u001b[1m\u001b[94m557 |\u001b[0m         print(f\"Memory reduced: {results.get('memory_reduced_mb', 0):.1f}MB\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:558:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m556 |\u001b[0m         print(f\"Final memory: {results.get('final_memory_mb', 0):.1f}MB\")\n\u001b[1m\u001b[94m557 |\u001b[0m         print(f\"Memory reduced: {results.get('memory_reduced_mb', 0):.1f}MB\")\n\u001b[1m\u001b[94m558 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m559 |\u001b[0m         # Show efficiency score\n\u001b[1m\u001b[94m560 |\u001b[0m         efficiency_score = optimizer.analyze_memory_efficiency()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/memory_optimization.py:562:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m560 |\u001b[0m         efficiency_score = optimizer.analyze_memory_efficiency()\n\u001b[1m\u001b[94m561 |\u001b[0m         print(f\"Memory efficiency score: {efficiency_score:.1f}/100\")\n\u001b[1m\u001b[94m562 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m563 |\u001b[0m     if not args.profile and not args.optimize:\n\u001b[1m\u001b[94m564 |\u001b[0m         print(\"Use --profile or --optimize to run memory operations\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:12:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import time\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import statistics\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import threading\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import concurrent.futures\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import logging\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import psutil\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import gc\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`threading` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:14:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import time\n\u001b[1m\u001b[94m13 |\u001b[0m import statistics\n\u001b[1m\u001b[94m14 |\u001b[0m import threading\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m import concurrent.futures\n\u001b[1m\u001b[94m16 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `threading`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Union` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:16:64\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m import threading\n\u001b[1m\u001b[94m15 |\u001b[0m import concurrent.futures\n\u001b[1m\u001b[94m16 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n   \u001b[1m\u001b[94m|\u001b[0m                                                                \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m18 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `typing.Union`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`datetime.timedelta` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:18:32\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\u001b[1m\u001b[94m17 |\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m18 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m import json\n\u001b[1m\u001b[94m20 |\u001b[0m import logging\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `datetime.timedelta`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:86:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m84 |\u001b[0m class PerformanceBenchmark:\n\u001b[1m\u001b[94m85 |\u001b[0m     \"\"\"Comprehensive performance benchmarking system.\"\"\"\n\u001b[1m\u001b[94m86 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m87 |\u001b[0m     def __init__(self, config: Optional[Dict[str, Any]] = None):\n\u001b[1m\u001b[94m88 |\u001b[0m         self.config = config or self._default_config()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:93:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m91 |\u001b[0m         self.regression_threshold = 0.15  # 15% regression threshold\n\u001b[1m\u001b[94m92 |\u001b[0m         self.logger = logging.getLogger(\"performance_benchmark\")\n\u001b[1m\u001b[94m93 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m94 |\u001b[0m     def _default_config(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m95 |\u001b[0m         \"\"\"Get default configuration.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:105:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m103 |\u001b[0m             \"regression_detection\": True\n\u001b[1m\u001b[94m104 |\u001b[0m         }\n\u001b[1m\u001b[94m105 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m106 |\u001b[0m     def benchmark_function(self, func: Callable, *args, iterations: Optional[int] = None,\n\u001b[1m\u001b[94m107 |\u001b[0m                           warmup_iterations: Optional[int] = None, **kwargs) -> BenchmarkStats:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:111:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m109 |\u001b[0m         iterations = iterations or self.config.get(\"default_iterations\", 100)\n\u001b[1m\u001b[94m110 |\u001b[0m         warmup_iterations = warmup_iterations or self.config.get(\"warmup_iterations\", 10)\n\u001b[1m\u001b[94m111 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m112 |\u001b[0m         function_name = f\"{func.__module__}.{func.__name__}\"\n\u001b[1m\u001b[94m113 |\u001b[0m         results = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:116:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m         memory_usage = []\n\u001b[1m\u001b[94m115 |\u001b[0m         cpu_usage = []\n\u001b[1m\u001b[94m116 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m117 |\u001b[0m         # Warmup iterations\n\u001b[1m\u001b[94m118 |\u001b[0m         self.logger.info(f\"Warming up {function_name} with {warmup_iterations} iterations\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:124:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m122 |\u001b[0m             except Exception as e:\n\u001b[1m\u001b[94m123 |\u001b[0m                 self.logger.warning(f\"Warmup iteration failed: {e}\")\n\u001b[1m\u001b[94m124 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m125 |\u001b[0m         # Force garbage collection before benchmarking\n\u001b[1m\u001b[94m126 |\u001b[0m         gc.collect()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:127:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m125 |\u001b[0m         # Force garbage collection before benchmarking\n\u001b[1m\u001b[94m126 |\u001b[0m         gc.collect()\n\u001b[1m\u001b[94m127 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m         # Benchmark iterations\n\u001b[1m\u001b[94m129 |\u001b[0m         self.logger.info(f\"Benchmarking {function_name} with {iterations} iterations\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:136:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m134 |\u001b[0m                 start_memory = self._get_memory_usage()\n\u001b[1m\u001b[94m135 |\u001b[0m                 start_cpu = self._get_cpu_usage()\n\u001b[1m\u001b[94m136 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m137 |\u001b[0m                 result = func(*args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `result` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:137:17\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m135 |\u001b[0m                 start_cpu = self._get_cpu_usage()\n\u001b[1m\u001b[94m136 |\u001b[0m                 \n\u001b[1m\u001b[94m137 |\u001b[0m                 result = func(*args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m                 \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m138 |\u001b[0m                 \n\u001b[1m\u001b[94m139 |\u001b[0m                 end_time = time.perf_counter()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `result`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:138:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m137 |\u001b[0m                 result = func(*args, **kwargs)\n\u001b[1m\u001b[94m138 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m139 |\u001b[0m                 end_time = time.perf_counter()\n\u001b[1m\u001b[94m140 |\u001b[0m                 end_memory = self._get_memory_usage()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:142:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m140 |\u001b[0m                 end_memory = self._get_memory_usage()\n\u001b[1m\u001b[94m141 |\u001b[0m                 end_cpu = self._get_cpu_usage()\n\u001b[1m\u001b[94m142 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m143 |\u001b[0m                 execution_time = (end_time - start_time) * 1000  # Convert to milliseconds\n\u001b[1m\u001b[94m144 |\u001b[0m                 memory_delta = end_memory - start_memory\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:146:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m144 |\u001b[0m                 memory_delta = end_memory - start_memory\n\u001b[1m\u001b[94m145 |\u001b[0m                 cpu_delta = end_cpu - start_cpu\n\u001b[1m\u001b[94m146 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m                 # Store result\n\u001b[1m\u001b[94m148 |\u001b[0m                 benchmark_result = BenchmarkResult(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:157:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m155 |\u001b[0m                     success=True\n\u001b[1m\u001b[94m156 |\u001b[0m                 )\n\u001b[1m\u001b[94m157 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m158 |\u001b[0m                 self.benchmark_results.append(benchmark_result)\n\u001b[1m\u001b[94m159 |\u001b[0m                 results.append(execution_time)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:162:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m                 memory_usage.append(memory_delta)\n\u001b[1m\u001b[94m161 |\u001b[0m                 cpu_usage.append(cpu_delta)\n\u001b[1m\u001b[94m162 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m163 |\u001b[0m             except Exception as e:\n\u001b[1m\u001b[94m164 |\u001b[0m                 error_result = BenchmarkResult(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:174:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m172 |\u001b[0m                     error_message=str(e)\n\u001b[1m\u001b[94m173 |\u001b[0m                 )\n\u001b[1m\u001b[94m174 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m175 |\u001b[0m                 self.benchmark_results.append(error_result)\n\u001b[1m\u001b[94m176 |\u001b[0m                 self.logger.error(f\"Benchmark iteration {i} failed: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:177:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m175 |\u001b[0m                 self.benchmark_results.append(error_result)\n\u001b[1m\u001b[94m176 |\u001b[0m                 self.logger.error(f\"Benchmark iteration {i} failed: {e}\")\n\u001b[1m\u001b[94m177 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m178 |\u001b[0m         # Calculate statistics\n\u001b[1m\u001b[94m179 |\u001b[0m         stats = self._calculate_benchmark_stats(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:182:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m180 |\u001b[0m             function_name, results, memory_usage, cpu_usage\n\u001b[1m\u001b[94m181 |\u001b[0m         )\n\u001b[1m\u001b[94m182 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m         return stats\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:184:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m         return stats\n\u001b[1m\u001b[94m184 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m185 |\u001b[0m     def _calculate_benchmark_stats(self, function_name: str, execution_times: List[float],\n\u001b[1m\u001b[94m186 |\u001b[0m                                  memory_usage: List[float], cpu_usage: List[float]) -> BenchmarkStats:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:195:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m                 memory_stats={}, cpu_stats={}\n\u001b[1m\u001b[94m194 |\u001b[0m             )\n\u001b[1m\u001b[94m195 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m196 |\u001b[0m         # Execution time statistics\n\u001b[1m\u001b[94m197 |\u001b[0m         min_time = min(execution_times)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:204:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m202 |\u001b[0m         p95_time = self._percentile(execution_times, 95)\n\u001b[1m\u001b[94m203 |\u001b[0m         p99_time = self._percentile(execution_times, 99)\n\u001b[1m\u001b[94m204 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m205 |\u001b[0m         # Memory statistics\n\u001b[1m\u001b[94m206 |\u001b[0m         memory_stats = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:214:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m212 |\u001b[0m                 \"median_mb\": statistics.median(memory_usage)\n\u001b[1m\u001b[94m213 |\u001b[0m             }\n\u001b[1m\u001b[94m214 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m215 |\u001b[0m         # CPU statistics\n\u001b[1m\u001b[94m216 |\u001b[0m         cpu_stats = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:224:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m222 |\u001b[0m                 \"median_percent\": statistics.median(cpu_usage)\n\u001b[1m\u001b[94m223 |\u001b[0m             }\n\u001b[1m\u001b[94m224 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m225 |\u001b[0m         # Success rate\n\u001b[1m\u001b[94m226 |\u001b[0m         total_results = len(self.benchmark_results)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:229:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m227 |\u001b[0m         successful_results = len([r for r in self.benchmark_results if r.success])\n\u001b[1m\u001b[94m228 |\u001b[0m         success_rate = (successful_results / total_results * 100) if total_results > 0 else 0\n\u001b[1m\u001b[94m229 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m230 |\u001b[0m         return BenchmarkStats(\n\u001b[1m\u001b[94m231 |\u001b[0m             function_name=function_name,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:244:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m242 |\u001b[0m             cpu_stats=cpu_stats\n\u001b[1m\u001b[94m243 |\u001b[0m         )\n\u001b[1m\u001b[94m244 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m245 |\u001b[0m     def _percentile(self, values: List[float], percentile: int) -> float:\n\u001b[1m\u001b[94m246 |\u001b[0m         \"\"\"Calculate percentile of values.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:249:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m247 |\u001b[0m         if not values:\n\u001b[1m\u001b[94m248 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m249 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m250 |\u001b[0m         sorted_values = sorted(values)\n\u001b[1m\u001b[94m251 |\u001b[0m         index = (percentile / 100.0) * (len(sorted_values) - 1)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:252:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m250 |\u001b[0m         sorted_values = sorted(values)\n\u001b[1m\u001b[94m251 |\u001b[0m         index = (percentile / 100.0) * (len(sorted_values) - 1)\n\u001b[1m\u001b[94m252 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m253 |\u001b[0m         if index.is_integer():\n\u001b[1m\u001b[94m254 |\u001b[0m             return sorted_values[int(index)]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:259:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m             upper = sorted_values[int(index) + 1]\n\u001b[1m\u001b[94m258 |\u001b[0m             return lower + (upper - lower) * (index - int(index))\n\u001b[1m\u001b[94m259 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m     def _get_memory_usage(self) -> float:\n\u001b[1m\u001b[94m261 |\u001b[0m         \"\"\"Get current memory usage in MB.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:267:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m265 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m266 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m267 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m268 |\u001b[0m     def _get_cpu_usage(self) -> float:\n\u001b[1m\u001b[94m269 |\u001b[0m         \"\"\"Get current CPU usage percentage.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:274:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m272 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m273 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m274 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m275 |\u001b[0m     def compare_benchmarks(self, baseline_stats: BenchmarkStats, \n\u001b[1m\u001b[94m276 |\u001b[0m                           current_stats: BenchmarkStats) -> Dict[str, Any]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:275:65\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m273 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m274 |\u001b[0m     \n\u001b[1m\u001b[94m275 |\u001b[0m     def compare_benchmarks(self, baseline_stats: BenchmarkStats, \n    \u001b[1m\u001b[94m|\u001b[0m                                                                 \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m276 |\u001b[0m                           current_stats: BenchmarkStats) -> Dict[str, Any]:\n\u001b[1m\u001b[94m277 |\u001b[0m         \"\"\"Compare two benchmark results.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:278:82\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m276 |\u001b[0m                           current_stats: BenchmarkStats) -> Dict[str, Any]:\n\u001b[1m\u001b[94m277 |\u001b[0m         \"\"\"Compare two benchmark results.\"\"\"\n\u001b[1m\u001b[94m278 |\u001b[0m         time_regression = ((current_stats.mean_time - baseline_stats.mean_time) / \n    \u001b[1m\u001b[94m|\u001b[0m                                                                                  \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m279 |\u001b[0m                           baseline_stats.mean_time * 100) if baseline_stats.mean_time > 0 else 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:280:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m278 |\u001b[0m         time_regression = ((current_stats.mean_time - baseline_stats.mean_time) / \n\u001b[1m\u001b[94m279 |\u001b[0m                           baseline_stats.mean_time * 100) if baseline_stats.mean_time > 0 else 0\n\u001b[1m\u001b[94m280 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m         memory_regression = 0\n\u001b[1m\u001b[94m282 |\u001b[0m         if baseline_stats.memory_stats and current_stats.memory_stats:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:285:70\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m283 |\u001b[0m             baseline_memory = baseline_stats.memory_stats.get(\"mean_mb\", 0)\n\u001b[1m\u001b[94m284 |\u001b[0m             current_memory = current_stats.memory_stats.get(\"mean_mb\", 0)\n\u001b[1m\u001b[94m285 |\u001b[0m             memory_regression = ((current_memory - baseline_memory) / \n    \u001b[1m\u001b[94m|\u001b[0m                                                                      \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m                                baseline_memory * 100) if baseline_memory > 0 else 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:287:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m285 |\u001b[0m             memory_regression = ((current_memory - baseline_memory) / \n\u001b[1m\u001b[94m286 |\u001b[0m                                baseline_memory * 100) if baseline_memory > 0 else 0\n\u001b[1m\u001b[94m287 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m288 |\u001b[0m         return {\n\u001b[1m\u001b[94m289 |\u001b[0m             \"function_name\": current_stats.function_name,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:299:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m297 |\u001b[0m             \"regression_severity\": self._calculate_regression_severity(time_regression)\n\u001b[1m\u001b[94m298 |\u001b[0m         }\n\u001b[1m\u001b[94m299 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m300 |\u001b[0m     def _calculate_regression_severity(self, regression_percent: float) -> str:\n\u001b[1m\u001b[94m301 |\u001b[0m         \"\"\"Calculate regression severity based on percentage.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:312:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m310 |\u001b[0m         else:\n\u001b[1m\u001b[94m311 |\u001b[0m             return \"negligible\"\n\u001b[1m\u001b[94m312 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m313 |\u001b[0m     def detect_performance_regressions(self) -> List[PerformanceRegression]:\n\u001b[1m\u001b[94m314 |\u001b[0m         \"\"\"Detect performance regressions compared to baseline.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:316:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m314 |\u001b[0m         \"\"\"Detect performance regressions compared to baseline.\"\"\"\n\u001b[1m\u001b[94m315 |\u001b[0m         regressions = []\n\u001b[1m\u001b[94m316 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m317 |\u001b[0m         # Group results by function name\n\u001b[1m\u001b[94m318 |\u001b[0m         function_results = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:323:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m321 |\u001b[0m                 function_results[result.function_name] = []\n\u001b[1m\u001b[94m322 |\u001b[0m             function_results[result.function_name].append(result)\n\u001b[1m\u001b[94m323 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m324 |\u001b[0m         # Check each function for regressions\n\u001b[1m\u001b[94m325 |\u001b[0m         for function_name, results in function_results.items():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:334:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m332 |\u001b[0m                     [r.cpu_usage for r in results if r.success]\n\u001b[1m\u001b[94m333 |\u001b[0m                 )\n\u001b[1m\u001b[94m334 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m335 |\u001b[0m                 # Check for regression\n\u001b[1m\u001b[94m336 |\u001b[0m                 time_regression = ((current_stats.mean_time - baseline_stats.mean_time) / \n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:336:90\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m335 |\u001b[0m                 # Check for regression\n\u001b[1m\u001b[94m336 |\u001b[0m                 time_regression = ((current_stats.mean_time - baseline_stats.mean_time) / \n    \u001b[1m\u001b[94m|\u001b[0m                                                                                          \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m337 |\u001b[0m                                  baseline_stats.mean_time * 100) if baseline_stats.mean_time > 0 else 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:338:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m336 |\u001b[0m                 time_regression = ((current_stats.mean_time - baseline_stats.mean_time) / \n\u001b[1m\u001b[94m337 |\u001b[0m                                  baseline_stats.mean_time * 100) if baseline_stats.mean_time > 0 else 0\n\u001b[1m\u001b[94m338 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m339 |\u001b[0m                 if time_regression > (self.regression_threshold * 100):\n\u001b[1m\u001b[94m340 |\u001b[0m                     regression = PerformanceRegression(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:350:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m348 |\u001b[0m                     )\n\u001b[1m\u001b[94m349 |\u001b[0m                     regressions.append(regression)\n\u001b[1m\u001b[94m350 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m351 |\u001b[0m         return regressions\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:352:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m351 |\u001b[0m         return regressions\n\u001b[1m\u001b[94m352 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m353 |\u001b[0m     def set_baseline(self, stats: BenchmarkStats) -> None:\n\u001b[1m\u001b[94m354 |\u001b[0m         \"\"\"Set baseline benchmark statistics.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:357:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m355 |\u001b[0m         self.baseline_results[stats.function_name] = stats\n\u001b[1m\u001b[94m356 |\u001b[0m         self.logger.info(f\"Set baseline for {stats.function_name}: {stats.mean_time:.2f}ms\")\n\u001b[1m\u001b[94m357 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m358 |\u001b[0m     def load_test(self, func: Callable, concurrent_users: int, total_requests: int,\n\u001b[1m\u001b[94m359 |\u001b[0m                   *args, **kwargs) -> LoadTestResult:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:363:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m361 |\u001b[0m         test_name = f\"{func.__module__}.{func.__name__}\"\n\u001b[1m\u001b[94m362 |\u001b[0m         self.logger.info(f\"Starting load test: {test_name} with {concurrent_users} concurrent users\")\n\u001b[1m\u001b[94m363 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m364 |\u001b[0m         results = []\n\u001b[1m\u001b[94m365 |\u001b[0m         errors = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:367:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m365 |\u001b[0m         errors = []\n\u001b[1m\u001b[94m366 |\u001b[0m         start_time = time.time()\n\u001b[1m\u001b[94m367 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m         # Create thread pool for concurrent execution\n\u001b[1m\u001b[94m369 |\u001b[0m         with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mB007 \u001b[0m\u001b[1mLoop control variable `i` not used within loop body\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:372:17\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m370 |\u001b[0m             # Submit all requests\n\u001b[1m\u001b[94m371 |\u001b[0m             futures = []\n\u001b[1m\u001b[94m372 |\u001b[0m             for i in range(total_requests):\n    \u001b[1m\u001b[94m|\u001b[0m                 \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m373 |\u001b[0m                 future = executor.submit(self._execute_request, func, *args, **kwargs)\n\u001b[1m\u001b[94m374 |\u001b[0m                 futures.append(future)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRename unused `i` to `_i`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:375:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m373 |\u001b[0m                 future = executor.submit(self._execute_request, func, *args, **kwargs)\n\u001b[1m\u001b[94m374 |\u001b[0m                 futures.append(future)\n\u001b[1m\u001b[94m375 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m376 |\u001b[0m             # Collect results\n\u001b[1m\u001b[94m377 |\u001b[0m             for future in concurrent.futures.as_completed(futures):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:383:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m                 except Exception as e:\n\u001b[1m\u001b[94m382 |\u001b[0m                     errors.append(str(e))\n\u001b[1m\u001b[94m383 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m384 |\u001b[0m         end_time = time.time()\n\u001b[1m\u001b[94m385 |\u001b[0m         duration = end_time - start_time\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:386:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m384 |\u001b[0m         end_time = time.time()\n\u001b[1m\u001b[94m385 |\u001b[0m         duration = end_time - start_time\n\u001b[1m\u001b[94m386 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m387 |\u001b[0m         # Calculate load test metrics\n\u001b[1m\u001b[94m388 |\u001b[0m         successful_requests = len(results)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:390:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m388 |\u001b[0m         successful_requests = len(results)\n\u001b[1m\u001b[94m389 |\u001b[0m         failed_requests = len(errors)\n\u001b[1m\u001b[94m390 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m         if results:\n\u001b[1m\u001b[94m392 |\u001b[0m             avg_response_time = statistics.mean(results)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:399:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m397 |\u001b[0m             p95_response_time = 0\n\u001b[1m\u001b[94m398 |\u001b[0m             p99_response_time = 0\n\u001b[1m\u001b[94m399 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m400 |\u001b[0m         throughput_rps = successful_requests / duration if duration > 0 else 0\n\u001b[1m\u001b[94m401 |\u001b[0m         error_rate = (failed_requests / total_requests * 100) if total_requests > 0 else 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:402:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m400 |\u001b[0m         throughput_rps = successful_requests / duration if duration > 0 else 0\n\u001b[1m\u001b[94m401 |\u001b[0m         error_rate = (failed_requests / total_requests * 100) if total_requests > 0 else 0\n\u001b[1m\u001b[94m402 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m403 |\u001b[0m         return LoadTestResult(\n\u001b[1m\u001b[94m404 |\u001b[0m             test_name=test_name,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:416:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m414 |\u001b[0m             duration=duration\n\u001b[1m\u001b[94m415 |\u001b[0m         )\n\u001b[1m\u001b[94m416 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m417 |\u001b[0m     def _execute_request(self, func: Callable, *args, **kwargs) -> float:\n\u001b[1m\u001b[94m418 |\u001b[0m         \"\"\"Execute a single request and return execution time.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:423:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m421 |\u001b[0m         end_time = time.perf_counter()\n\u001b[1m\u001b[94m422 |\u001b[0m         return (end_time - start_time) * 1000  # Convert to milliseconds\n\u001b[1m\u001b[94m423 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m424 |\u001b[0m     def stress_test(self, func: Callable, max_concurrent_users: int, \n\u001b[1m\u001b[94m425 |\u001b[0m                    duration_seconds: int, *args, **kwargs) -> List[LoadTestResult]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:424:69\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m422 |\u001b[0m         return (end_time - start_time) * 1000  # Convert to milliseconds\n\u001b[1m\u001b[94m423 |\u001b[0m     \n\u001b[1m\u001b[94m424 |\u001b[0m     def stress_test(self, func: Callable, max_concurrent_users: int, \n    \u001b[1m\u001b[94m|\u001b[0m                                                                     \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m425 |\u001b[0m                    duration_seconds: int, *args, **kwargs) -> List[LoadTestResult]:\n\u001b[1m\u001b[94m426 |\u001b[0m         \"\"\"Perform stress testing with increasing concurrent users.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:429:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m427 |\u001b[0m         test_name = f\"{func.__module__}.{func.__name__}\"\n\u001b[1m\u001b[94m428 |\u001b[0m         self.logger.info(f\"Starting stress test: {test_name} for {duration_seconds} seconds\")\n\u001b[1m\u001b[94m429 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m430 |\u001b[0m         results = []\n\u001b[1m\u001b[94m431 |\u001b[0m         concurrent_users = 1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:432:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m430 |\u001b[0m         results = []\n\u001b[1m\u001b[94m431 |\u001b[0m         concurrent_users = 1\n\u001b[1m\u001b[94m432 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m433 |\u001b[0m         while concurrent_users <= max_concurrent_users:\n\u001b[1m\u001b[94m434 |\u001b[0m             # Run load test for this level\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:438:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m436 |\u001b[0m             result = self.load_test(func, concurrent_users, requests_per_level, *args, **kwargs)\n\u001b[1m\u001b[94m437 |\u001b[0m             results.append(result)\n\u001b[1m\u001b[94m438 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m439 |\u001b[0m             # Check if we should continue\n\u001b[1m\u001b[94m440 |\u001b[0m             if result.error_rate > 50:  # More than 50% errors\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:443:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m441 |\u001b[0m                 self.logger.warning(f\"High error rate at {concurrent_users} users: {result.error_rate:.1f}%\")\n\u001b[1m\u001b[94m442 |\u001b[0m                 break\n\u001b[1m\u001b[94m443 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m444 |\u001b[0m             if result.avg_response_time > 10000:  # More than 10 seconds\n\u001b[1m\u001b[94m445 |\u001b[0m                 self.logger.warning(f\"High response time at {concurrent_users} users: {result.avg_response_time:.1f}ms\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:447:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m445 |\u001b[0m                 self.logger.warning(f\"High response time at {concurrent_users} users: {result.avg_response_time:.1f}ms\")\n\u001b[1m\u001b[94m446 |\u001b[0m                 break\n\u001b[1m\u001b[94m447 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m448 |\u001b[0m             concurrent_users *= 2  # Double concurrent users\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:449:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m448 |\u001b[0m             concurrent_users *= 2  # Double concurrent users\n\u001b[1m\u001b[94m449 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m450 |\u001b[0m         return results\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:451:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m450 |\u001b[0m         return results\n\u001b[1m\u001b[94m451 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m452 |\u001b[0m     def benchmark_suite(self, benchmarks: List[Tuple[Callable, tuple, dict]]) -> Dict[str, BenchmarkStats]:\n\u001b[1m\u001b[94m453 |\u001b[0m         \"\"\"Run a suite of benchmarks.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:455:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m453 |\u001b[0m         \"\"\"Run a suite of benchmarks.\"\"\"\n\u001b[1m\u001b[94m454 |\u001b[0m         results = {}\n\u001b[1m\u001b[94m455 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m456 |\u001b[0m         for func, args, kwargs in benchmarks:\n\u001b[1m\u001b[94m457 |\u001b[0m             function_name = f\"{func.__module__}.{func.__name__}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:459:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m457 |\u001b[0m             function_name = f\"{func.__module__}.{func.__name__}\"\n\u001b[1m\u001b[94m458 |\u001b[0m             self.logger.info(f\"Running benchmark: {function_name}\")\n\u001b[1m\u001b[94m459 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m460 |\u001b[0m             try:\n\u001b[1m\u001b[94m461 |\u001b[0m                 stats = self.benchmark_function(func, *args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:465:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m463 |\u001b[0m             except Exception as e:\n\u001b[1m\u001b[94m464 |\u001b[0m                 self.logger.error(f\"Benchmark failed for {function_name}: {e}\")\n\u001b[1m\u001b[94m465 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m466 |\u001b[0m         return results\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:467:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m466 |\u001b[0m         return results\n\u001b[1m\u001b[94m467 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m468 |\u001b[0m     def export_benchmark_report(self, output_file: Optional[Path] = None) -> Path:\n\u001b[1m\u001b[94m469 |\u001b[0m         \"\"\"Export comprehensive benchmark report.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:473:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m471 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\u001b[1m\u001b[94m472 |\u001b[0m             output_file = Path(f\"benchmark_report_{timestamp}.json\")\n\u001b[1m\u001b[94m473 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m474 |\u001b[0m         # Group results by function\n\u001b[1m\u001b[94m475 |\u001b[0m         function_results = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:480:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m478 |\u001b[0m                 function_results[result.function_name] = []\n\u001b[1m\u001b[94m479 |\u001b[0m             function_results[result.function_name].append(result)\n\u001b[1m\u001b[94m480 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m481 |\u001b[0m         # Calculate statistics for each function\n\u001b[1m\u001b[94m482 |\u001b[0m         function_stats = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:489:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m487 |\u001b[0m                 memory_usage = [r.memory_usage for r in successful_results]\n\u001b[1m\u001b[94m488 |\u001b[0m                 cpu_usage = [r.cpu_usage for r in successful_results]\n\u001b[1m\u001b[94m489 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m490 |\u001b[0m                 stats = self._calculate_benchmark_stats(\n\u001b[1m\u001b[94m491 |\u001b[0m                     function_name, execution_times, memory_usage, cpu_usage\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:494:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m492 |\u001b[0m                 )\n\u001b[1m\u001b[94m493 |\u001b[0m                 function_stats[function_name] = asdict(stats)\n\u001b[1m\u001b[94m494 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m495 |\u001b[0m         # Detect regressions\n\u001b[1m\u001b[94m496 |\u001b[0m         regressions = self.detect_performance_regressions()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:497:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m495 |\u001b[0m         # Detect regressions\n\u001b[1m\u001b[94m496 |\u001b[0m         regressions = self.detect_performance_regressions()\n\u001b[1m\u001b[94m497 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m498 |\u001b[0m         # Create report\n\u001b[1m\u001b[94m499 |\u001b[0m         report_data = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:510:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m508 |\u001b[0m             \"baseline_results\": {name: asdict(stats) for name, stats in self.baseline_results.items()}\n\u001b[1m\u001b[94m509 |\u001b[0m         }\n\u001b[1m\u001b[94m510 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m511 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m512 |\u001b[0m             json.dump(report_data, f, indent=2, default=str)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:513:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m511 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m512 |\u001b[0m             json.dump(report_data, f, indent=2, default=str)\n\u001b[1m\u001b[94m513 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m514 |\u001b[0m         return output_file\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:527:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m525 |\u001b[0m     \"\"\"Compare two functions with benchmarking.\"\"\"\n\u001b[1m\u001b[94m526 |\u001b[0m     benchmarker = PerformanceBenchmark()\n\u001b[1m\u001b[94m527 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m528 |\u001b[0m     stats1 = benchmarker.benchmark_function(func1, *args, iterations=iterations, **kwargs)\n\u001b[1m\u001b[94m529 |\u001b[0m     stats2 = benchmarker.benchmark_function(func2, *args, iterations=iterations, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:530:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m528 |\u001b[0m     stats1 = benchmarker.benchmark_function(func1, *args, iterations=iterations, **kwargs)\n\u001b[1m\u001b[94m529 |\u001b[0m     stats2 = benchmarker.benchmark_function(func2, *args, iterations=iterations, **kwargs)\n\u001b[1m\u001b[94m530 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m531 |\u001b[0m     return benchmarker.compare_benchmarks(stats1, stats2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:543:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m541 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m542 |\u001b[0m     import argparse\n\u001b[1m\u001b[94m543 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m544 |\u001b[0m     parser = argparse.ArgumentParser(description=\"SparkForge Performance Benchmarking\")\n\u001b[1m\u001b[94m545 |\u001b[0m     parser.add_argument(\"--benchmark\", help=\"Function to benchmark\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:551:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m549 |\u001b[0m     parser.add_argument(\"--total-requests\", type=int, default=100, help=\"Total requests for load test\")\n\u001b[1m\u001b[94m550 |\u001b[0m     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n\u001b[1m\u001b[94m551 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m552 |\u001b[0m     args = parser.parse_args()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:553:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m552 |\u001b[0m     args = parser.parse_args()\n\u001b[1m\u001b[94m553 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m554 |\u001b[0m     benchmarker = PerformanceBenchmark()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:555:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m554 |\u001b[0m     benchmarker = PerformanceBenchmark()\n\u001b[1m\u001b[94m555 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m556 |\u001b[0m     if args.benchmark:\n\u001b[1m\u001b[94m557 |\u001b[0m         # This would need to be implemented based on the specific function\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:564:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m562 |\u001b[0m         report_file = benchmarker.export_benchmark_report(args.output)\n\u001b[1m\u001b[94m563 |\u001b[0m         print(f\"Benchmark report saved to: {report_file}\")\n\u001b[1m\u001b[94m564 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m565 |\u001b[0m         # Print summary\n\u001b[1m\u001b[94m566 |\u001b[0m         print(f\"\\nBenchmark Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:566:15\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m565 |\u001b[0m         # Print summary\n\u001b[1m\u001b[94m566 |\u001b[0m         print(f\"\\nBenchmark Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m567 |\u001b[0m         print(f\"Total benchmarks: {len(benchmarker.benchmark_results)}\")\n\u001b[1m\u001b[94m568 |\u001b[0m         print(f\"Functions benchmarked: {len(set(r.function_name for r in benchmarker.benchmark_results))}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mC401 \u001b[0m\u001b[1mUnnecessary generator (rewrite as a set comprehension)\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:568:45\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m566 |\u001b[0m         print(f\"\\nBenchmark Summary:\")\n\u001b[1m\u001b[94m567 |\u001b[0m         print(f\"Total benchmarks: {len(benchmarker.benchmark_results)}\")\n\u001b[1m\u001b[94m568 |\u001b[0m         print(f\"Functions benchmarked: {len(set(r.function_name for r in benchmarker.benchmark_results))}\")\n    \u001b[1m\u001b[94m|\u001b[0m                                             \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m569 |\u001b[0m         \n\u001b[1m\u001b[94m570 |\u001b[0m         # Show regressions\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRewrite as a set comprehension\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_benchmarking.py:569:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m567 |\u001b[0m         print(f\"Total benchmarks: {len(benchmarker.benchmark_results)}\")\n\u001b[1m\u001b[94m568 |\u001b[0m         print(f\"Functions benchmarked: {len(set(r.function_name for r in benchmarker.benchmark_results))}\")\n\u001b[1m\u001b[94m569 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m570 |\u001b[0m         # Show regressions\n\u001b[1m\u001b[94m571 |\u001b[0m         regressions = benchmarker.detect_performance_regressions()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.Mock` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:16:27\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from dataclasses import dataclass\n\u001b[1m\u001b[94m15 |\u001b[0m from typing import Any, Callable, Dict, List, Optional, Tuple\n\u001b[1m\u001b[94m16 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m import psutil\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `unittest.mock.Mock`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:24:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m class PerformanceResult:\n\u001b[1m\u001b[94m23 |\u001b[0m     \"\"\"Container for performance measurement results.\"\"\"\n\u001b[1m\u001b[94m24 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m     function_name: str\n\u001b[1m\u001b[94m26 |\u001b[0m     execution_time: float\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:33:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m31 |\u001b[0m     success: bool\n\u001b[1m\u001b[94m32 |\u001b[0m     error_message: Optional[str] = None\n\u001b[1m\u001b[94m33 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m34 |\u001b[0m     @property\n\u001b[1m\u001b[94m35 |\u001b[0m     def throughput(self) -> float:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:40:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m38 |\u001b[0m             return self.iterations / self.execution_time\n\u001b[1m\u001b[94m39 |\u001b[0m         return 0.0\n\u001b[1m\u001b[94m40 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m41 |\u001b[0m     @property\n\u001b[1m\u001b[94m42 |\u001b[0m     def avg_time_per_iteration(self) -> float:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:51:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m49 |\u001b[0m class PerformanceMonitor:\n\u001b[1m\u001b[94m50 |\u001b[0m     \"\"\"Performance monitoring and regression detection.\"\"\"\n\u001b[1m\u001b[94m51 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m52 |\u001b[0m     def __init__(self, baseline_file: str = \"performance_baseline.json\"):\n\u001b[1m\u001b[94m53 |\u001b[0m         \"\"\"Initialize performance monitor with baseline storage.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:58:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m56 |\u001b[0m         self.results: List[PerformanceResult] = []\n\u001b[1m\u001b[94m57 |\u001b[0m         self.load_baselines()\n\u001b[1m\u001b[94m58 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m59 |\u001b[0m     def load_baselines(self) -> None:\n\u001b[1m\u001b[94m60 |\u001b[0m         \"\"\"Load performance baselines from file.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:62:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m60 |\u001b[0m         \"\"\"Load performance baselines from file.\"\"\"\n\u001b[1m\u001b[94m61 |\u001b[0m         import json\n\u001b[1m\u001b[94m62 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         if os.path.exists(self.baseline_file):\n\u001b[1m\u001b[94m64 |\u001b[0m             try:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:65:47\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         if os.path.exists(self.baseline_file):\n\u001b[1m\u001b[94m64 |\u001b[0m             try:\n\u001b[1m\u001b[94m65 |\u001b[0m                 with open(self.baseline_file, 'r') as f:\n   \u001b[1m\u001b[94m|\u001b[0m                                               \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m                     baseline_data = json.load(f)\n\u001b[1m\u001b[94m67 |\u001b[0m                     for name, data in baseline_data.items():\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:72:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m70 |\u001b[0m                 # If baseline file is corrupted, start fresh\n\u001b[1m\u001b[94m71 |\u001b[0m                 self.baselines = {}\n\u001b[1m\u001b[94m72 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m73 |\u001b[0m     def save_baselines(self) -> None:\n\u001b[1m\u001b[94m74 |\u001b[0m         \"\"\"Save current baselines to file.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:76:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m         \"\"\"Save current baselines to file.\"\"\"\n\u001b[1m\u001b[94m75 |\u001b[0m         import json\n\u001b[1m\u001b[94m76 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m         baseline_data = {}\n\u001b[1m\u001b[94m78 |\u001b[0m         for name, result in self.baselines.items():\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:89:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m87 |\u001b[0m                 'error_message': result.error_message\n\u001b[1m\u001b[94m88 |\u001b[0m             }\n\u001b[1m\u001b[94m89 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m90 |\u001b[0m         with open(self.baseline_file, 'w') as f:\n\u001b[1m\u001b[94m91 |\u001b[0m             json.dump(baseline_data, f, indent=2)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:92:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m90 |\u001b[0m         with open(self.baseline_file, 'w') as f:\n\u001b[1m\u001b[94m91 |\u001b[0m             json.dump(baseline_data, f, indent=2)\n\u001b[1m\u001b[94m92 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m93 |\u001b[0m     @contextmanager\n\u001b[1m\u001b[94m94 |\u001b[0m     def measure_performance(self, function_name: str, iterations: int = 1):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:100:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 98 |\u001b[0m         process = psutil.Process()\n\u001b[1m\u001b[94m 99 |\u001b[0m         initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n\u001b[1m\u001b[94m100 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m101 |\u001b[0m         start_time = time.time()\n\u001b[1m\u001b[94m102 |\u001b[0m         start_timestamp = time.time()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:103:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m101 |\u001b[0m         start_time = time.time()\n\u001b[1m\u001b[94m102 |\u001b[0m         start_timestamp = time.time()\n\u001b[1m\u001b[94m103 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m104 |\u001b[0m         result = PerformanceResult(\n\u001b[1m\u001b[94m105 |\u001b[0m             function_name=function_name,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:113:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m             success=False\n\u001b[1m\u001b[94m112 |\u001b[0m         )\n\u001b[1m\u001b[94m113 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m         try:\n\u001b[1m\u001b[94m115 |\u001b[0m             yield result\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:116:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m         try:\n\u001b[1m\u001b[94m115 |\u001b[0m             yield result\n\u001b[1m\u001b[94m116 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m117 |\u001b[0m             # Measure final metrics\n\u001b[1m\u001b[94m118 |\u001b[0m             end_time = time.time()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `final_memory` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:120:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m118 |\u001b[0m             end_time = time.time()\n\u001b[1m\u001b[94m119 |\u001b[0m             current, peak = tracemalloc.get_traced_memory()\n\u001b[1m\u001b[94m120 |\u001b[0m             final_memory = process.memory_info().rss / 1024 / 1024  # MB\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m121 |\u001b[0m             \n\u001b[1m\u001b[94m122 |\u001b[0m             result.execution_time = end_time - start_time\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `final_memory`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:121:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m             current, peak = tracemalloc.get_traced_memory()\n\u001b[1m\u001b[94m120 |\u001b[0m             final_memory = process.memory_info().rss / 1024 / 1024  # MB\n\u001b[1m\u001b[94m121 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m122 |\u001b[0m             result.execution_time = end_time - start_time\n\u001b[1m\u001b[94m123 |\u001b[0m             result.memory_usage_mb = (current / 1024 / 1024) - initial_memory\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:126:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m             result.peak_memory_mb = (peak / 1024 / 1024) - initial_memory\n\u001b[1m\u001b[94m125 |\u001b[0m             result.success = True\n\u001b[1m\u001b[94m126 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m127 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m128 |\u001b[0m             result.error_message = str(e)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:130:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m             result.error_message = str(e)\n\u001b[1m\u001b[94m129 |\u001b[0m             result.success = False\n\u001b[1m\u001b[94m130 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m131 |\u001b[0m         finally:\n\u001b[1m\u001b[94m132 |\u001b[0m             tracemalloc.stop()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:134:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m132 |\u001b[0m             tracemalloc.stop()\n\u001b[1m\u001b[94m133 |\u001b[0m             self.results.append(result)\n\u001b[1m\u001b[94m134 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m135 |\u001b[0m     def run_performance_test(\n\u001b[1m\u001b[94m136 |\u001b[0m         self,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:146:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m144 |\u001b[0m         if kwargs is None:\n\u001b[1m\u001b[94m145 |\u001b[0m             kwargs = {}\n\u001b[1m\u001b[94m146 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m         with self.measure_performance(function_name, iterations) as result:\n\u001b[1m\u001b[94m148 |\u001b[0m             for _ in range(iterations):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:150:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m148 |\u001b[0m             for _ in range(iterations):\n\u001b[1m\u001b[94m149 |\u001b[0m                 function(*args, **kwargs)\n\u001b[1m\u001b[94m150 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m151 |\u001b[0m         return result\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:152:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m151 |\u001b[0m         return result\n\u001b[1m\u001b[94m152 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m153 |\u001b[0m     def check_regression(self, function_name: str, tolerance: float = 0.2) -> Dict[str, Any]:\n\u001b[1m\u001b[94m154 |\u001b[0m         \"\"\"Check for performance regression against baseline.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:159:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m         if not recent_results:\n\u001b[1m\u001b[94m158 |\u001b[0m             return {\"status\": \"no_data\", \"message\": \"No recent results found\"}\n\u001b[1m\u001b[94m159 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         latest_result = recent_results[-1]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:161:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         latest_result = recent_results[-1]\n\u001b[1m\u001b[94m161 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m162 |\u001b[0m         if function_name not in self.baselines:\n\u001b[1m\u001b[94m163 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:168:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m166 |\u001b[0m                 \"current\": latest_result\n\u001b[1m\u001b[94m167 |\u001b[0m             }\n\u001b[1m\u001b[94m168 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m169 |\u001b[0m         baseline = self.baselines[function_name]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:170:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m169 |\u001b[0m         baseline = self.baselines[function_name]\n\u001b[1m\u001b[94m170 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m171 |\u001b[0m         # Check for regressions\n\u001b[1m\u001b[94m172 |\u001b[0m         time_regression = latest_result.execution_time > baseline.execution_time * (1 + tolerance)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:174:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m172 |\u001b[0m         time_regression = latest_result.execution_time > baseline.execution_time * (1 + tolerance)\n\u001b[1m\u001b[94m173 |\u001b[0m         memory_regression = latest_result.memory_usage_mb > baseline.memory_usage_mb * (1 + tolerance)\n\u001b[1m\u001b[94m174 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m175 |\u001b[0m         regression_info = {\n\u001b[1m\u001b[94m176 |\u001b[0m             \"status\": \"ok\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:185:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m             \"memory_change_percent\": ((latest_result.memory_usage_mb - baseline.memory_usage_mb) / baseline.memory_usage_mb) * 100\n\u001b[1m\u001b[94m184 |\u001b[0m         }\n\u001b[1m\u001b[94m185 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         if time_regression or memory_regression:\n\u001b[1m\u001b[94m187 |\u001b[0m             regression_info[\"status\"] = \"regression_detected\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:188:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         if time_regression or memory_regression:\n\u001b[1m\u001b[94m187 |\u001b[0m             regression_info[\"status\"] = \"regression_detected\"\n\u001b[1m\u001b[94m188 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m189 |\u001b[0m         return regression_info\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:190:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m189 |\u001b[0m         return regression_info\n\u001b[1m\u001b[94m190 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m191 |\u001b[0m     def update_baseline(self, function_name: str) -> None:\n\u001b[1m\u001b[94m192 |\u001b[0m         \"\"\"Update baseline for a function with the most recent result.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:197:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m195 |\u001b[0m             self.baselines[function_name] = recent_results[-1]\n\u001b[1m\u001b[94m196 |\u001b[0m             self.save_baselines()\n\u001b[1m\u001b[94m197 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m198 |\u001b[0m     def get_performance_summary(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m199 |\u001b[0m         \"\"\"Get summary of all performance results.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:202:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m         if not self.results:\n\u001b[1m\u001b[94m201 |\u001b[0m             return {\"message\": \"No performance data available\"}\n\u001b[1m\u001b[94m202 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m203 |\u001b[0m         successful_results = [r for r in self.results if r.success]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:204:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m203 |\u001b[0m         successful_results = [r for r in self.results if r.success]\n\u001b[1m\u001b[94m204 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m205 |\u001b[0m         summary = {\n\u001b[1m\u001b[94m206 |\u001b[0m             \"total_tests\": len(self.results),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mC401 \u001b[0m\u001b[1mUnnecessary generator (rewrite as a set comprehension)\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:209:37\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m     \"successful_tests\": len(successful_results),\n\u001b[1m\u001b[94m208 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m     \"failed_tests\": len(self.results) - len(successful_results),\n\u001b[1m\u001b[94m209 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m     \"functions_tested\": len(set(r.function_name for r in self.results)),\n    \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m     \"total_execution_time\": sum(r.execution_time for r in successful_results),\n\u001b[1m\u001b[94m211 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m     \"avg_execution_time\": sum(r.execution_time for r in successful_results) / len(successful_results) if successful_results else 0,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRewrite as a set comprehension\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:215:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m213 |\u001b[0m             \"results\": [r.__dict__ for r in self.results]\n\u001b[1m\u001b[94m214 |\u001b[0m         }\n\u001b[1m\u001b[94m215 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m216 |\u001b[0m         return summary\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:217:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m216 |\u001b[0m         return summary\n\u001b[1m\u001b[94m217 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m218 |\u001b[0m     def benchmark_function(\n\u001b[1m\u001b[94m219 |\u001b[0m         self,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:230:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m228 |\u001b[0m         if kwargs is None:\n\u001b[1m\u001b[94m229 |\u001b[0m             kwargs = {}\n\u001b[1m\u001b[94m230 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m         # Warmup runs\n\u001b[1m\u001b[94m232 |\u001b[0m         for _ in range(warmup_iterations):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:234:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m232 |\u001b[0m         for _ in range(warmup_iterations):\n\u001b[1m\u001b[94m233 |\u001b[0m             function(*args, **kwargs)\n\u001b[1m\u001b[94m234 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m         # Force garbage collection\n\u001b[1m\u001b[94m236 |\u001b[0m         gc.collect()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitor.py:237:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m         # Force garbage collection\n\u001b[1m\u001b[94m236 |\u001b[0m         gc.collect()\n\u001b[1m\u001b[94m237 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m238 |\u001b[0m         # Actual benchmark\n\u001b[1m\u001b[94m239 |\u001b[0m         return self.run_performance_test(function, function_name, iterations, args, kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:12:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import time\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import psutil\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import threading\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import statistics\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Dict, List, Any, Optional, Callable\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from collections import deque\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import logging\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|________________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:33:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m31 |\u001b[0m     timestamp: datetime\n\u001b[1m\u001b[94m32 |\u001b[0m     tags: Dict[str, str] = None\n\u001b[1m\u001b[94m33 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m34 |\u001b[0m     def __post_init__(self):\n\u001b[1m\u001b[94m35 |\u001b[0m         if self.tags is None:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:79:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m class PerformanceMonitor:\n\u001b[1m\u001b[94m78 |\u001b[0m     \"\"\"Real-time performance monitoring system.\"\"\"\n\u001b[1m\u001b[94m79 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m80 |\u001b[0m     def __init__(self, config: Optional[Dict[str, Any]] = None):\n\u001b[1m\u001b[94m81 |\u001b[0m         self.config = config or self._default_config()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:86:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m84 |\u001b[0m         self.alerts: List[PerformanceAlert] = []\n\u001b[1m\u001b[94m85 |\u001b[0m         self.alert_callbacks: List[Callable[[PerformanceAlert], None]] = []\n\u001b[1m\u001b[94m86 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m87 |\u001b[0m         # Monitoring state\n\u001b[1m\u001b[94m88 |\u001b[0m         self.monitoring_active = False\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:91:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m89 |\u001b[0m         self.monitor_thread = None\n\u001b[1m\u001b[94m90 |\u001b[0m         self.last_network_stats = None\n\u001b[1m\u001b[94m91 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m         # Performance thresholds\n\u001b[1m\u001b[94m93 |\u001b[0m         self.thresholds = {\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:101:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 99 |\u001b[0m             \"throughput_rps\": 10.0\n\u001b[1m\u001b[94m100 |\u001b[0m         }\n\u001b[1m\u001b[94m101 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m102 |\u001b[0m         # Setup logging\n\u001b[1m\u001b[94m103 |\u001b[0m         self.logger = logging.getLogger(\"performance_monitor\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:105:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m103 |\u001b[0m         self.logger = logging.getLogger(\"performance_monitor\")\n\u001b[1m\u001b[94m104 |\u001b[0m         self.logger.setLevel(logging.INFO)\n\u001b[1m\u001b[94m105 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m106 |\u001b[0m     def _default_config(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m107 |\u001b[0m         \"\"\"Get default configuration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:117:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m115 |\u001b[0m             \"enable_metrics_collection\": True\n\u001b[1m\u001b[94m116 |\u001b[0m         }\n\u001b[1m\u001b[94m117 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m118 |\u001b[0m     def start_monitoring(self) -> None:\n\u001b[1m\u001b[94m119 |\u001b[0m         \"\"\"Start performance monitoring.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:123:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m121 |\u001b[0m             self.logger.warning(\"Performance monitoring is already active\")\n\u001b[1m\u001b[94m122 |\u001b[0m             return\n\u001b[1m\u001b[94m123 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m         self.monitoring_active = True\n\u001b[1m\u001b[94m125 |\u001b[0m         self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:127:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m125 |\u001b[0m         self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n\u001b[1m\u001b[94m126 |\u001b[0m         self.monitor_thread.start()\n\u001b[1m\u001b[94m127 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m         self.logger.info(\"Performance monitoring started\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:129:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m         self.logger.info(\"Performance monitoring started\")\n\u001b[1m\u001b[94m129 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m130 |\u001b[0m     def stop_monitoring(self) -> None:\n\u001b[1m\u001b[94m131 |\u001b[0m         \"\"\"Stop performance monitoring.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:135:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m133 |\u001b[0m             self.logger.warning(\"Performance monitoring is not active\")\n\u001b[1m\u001b[94m134 |\u001b[0m             return\n\u001b[1m\u001b[94m135 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m         self.monitoring_active = False\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:137:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m         self.monitoring_active = False\n\u001b[1m\u001b[94m137 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m138 |\u001b[0m         if self.monitor_thread and self.monitor_thread.is_alive():\n\u001b[1m\u001b[94m139 |\u001b[0m             self.monitor_thread.join(timeout=5)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:140:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m138 |\u001b[0m         if self.monitor_thread and self.monitor_thread.is_alive():\n\u001b[1m\u001b[94m139 |\u001b[0m             self.monitor_thread.join(timeout=5)\n\u001b[1m\u001b[94m140 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m141 |\u001b[0m         self.logger.info(\"Performance monitoring stopped\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:142:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m141 |\u001b[0m         self.logger.info(\"Performance monitoring stopped\")\n\u001b[1m\u001b[94m142 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m143 |\u001b[0m     def _monitoring_loop(self) -> None:\n\u001b[1m\u001b[94m144 |\u001b[0m         \"\"\"Main monitoring loop.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:151:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m                     resource_usage = self._collect_resource_usage()\n\u001b[1m\u001b[94m150 |\u001b[0m                     self.resource_history.append(resource_usage)\n\u001b[1m\u001b[94m151 |\u001b[0m                     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m                     # Check for resource alerts\n\u001b[1m\u001b[94m153 |\u001b[0m                     if self.config.get(\"enable_alerts\", True):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:155:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m153 |\u001b[0m                     if self.config.get(\"enable_alerts\", True):\n\u001b[1m\u001b[94m154 |\u001b[0m                         self._check_resource_alerts(resource_usage)\n\u001b[1m\u001b[94m155 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m156 |\u001b[0m                 # Clean up old data\n\u001b[1m\u001b[94m157 |\u001b[0m                 self._cleanup_old_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:158:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m156 |\u001b[0m                 # Clean up old data\n\u001b[1m\u001b[94m157 |\u001b[0m                 self._cleanup_old_data()\n\u001b[1m\u001b[94m158 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m159 |\u001b[0m                 # Sleep until next monitoring cycle\n\u001b[1m\u001b[94m160 |\u001b[0m                 time.sleep(self.config.get(\"monitoring_interval\", 10))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:161:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m159 |\u001b[0m                 # Sleep until next monitoring cycle\n\u001b[1m\u001b[94m160 |\u001b[0m                 time.sleep(self.config.get(\"monitoring_interval\", 10))\n\u001b[1m\u001b[94m161 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m162 |\u001b[0m             except Exception as e:\n\u001b[1m\u001b[94m163 |\u001b[0m                 self.logger.error(f\"Error in monitoring loop: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:165:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m163 |\u001b[0m                 self.logger.error(f\"Error in monitoring loop: {e}\")\n\u001b[1m\u001b[94m164 |\u001b[0m                 time.sleep(5)  # Brief pause before retrying\n\u001b[1m\u001b[94m165 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m166 |\u001b[0m     def _collect_resource_usage(self) -> ResourceUsage:\n\u001b[1m\u001b[94m167 |\u001b[0m         \"\"\"Collect current resource usage.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:171:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m169 |\u001b[0m             # CPU usage\n\u001b[1m\u001b[94m170 |\u001b[0m             cpu_percent = psutil.cpu_percent(interval=1)\n\u001b[1m\u001b[94m171 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m172 |\u001b[0m             # Memory usage\n\u001b[1m\u001b[94m173 |\u001b[0m             memory = psutil.virtual_memory()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:177:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m175 |\u001b[0m             memory_used_mb = memory.used / 1024 / 1024\n\u001b[1m\u001b[94m176 |\u001b[0m             memory_available_mb = memory.available / 1024 / 1024\n\u001b[1m\u001b[94m177 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m178 |\u001b[0m             # Disk usage\n\u001b[1m\u001b[94m179 |\u001b[0m             disk = psutil.disk_usage('/')\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:181:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m179 |\u001b[0m             disk = psutil.disk_usage('/')\n\u001b[1m\u001b[94m180 |\u001b[0m             disk_usage_percent = (disk.used / disk.total) * 100\n\u001b[1m\u001b[94m181 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m182 |\u001b[0m             # Network usage\n\u001b[1m\u001b[94m183 |\u001b[0m             network_stats = psutil.net_io_counters()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:190:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m188 |\u001b[0m                 network_bytes_sent = 0\n\u001b[1m\u001b[94m189 |\u001b[0m                 network_bytes_received = 0\n\u001b[1m\u001b[94m190 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m191 |\u001b[0m             self.last_network_stats = network_stats\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:192:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m191 |\u001b[0m             self.last_network_stats = network_stats\n\u001b[1m\u001b[94m192 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m             return ResourceUsage(\n\u001b[1m\u001b[94m194 |\u001b[0m                 cpu_percent=cpu_percent,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:203:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m                 timestamp=datetime.now()\n\u001b[1m\u001b[94m202 |\u001b[0m             )\n\u001b[1m\u001b[94m203 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m205 |\u001b[0m             self.logger.error(f\"Error collecting resource usage: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:216:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m                 timestamp=datetime.now()\n\u001b[1m\u001b[94m215 |\u001b[0m             )\n\u001b[1m\u001b[94m216 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m217 |\u001b[0m     def _check_resource_alerts(self, resource_usage: ResourceUsage) -> None:\n\u001b[1m\u001b[94m218 |\u001b[0m         \"\"\"Check for resource-based alerts.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:220:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m218 |\u001b[0m         \"\"\"Check for resource-based alerts.\"\"\"\n\u001b[1m\u001b[94m219 |\u001b[0m         alerts_to_create = []\n\u001b[1m\u001b[94m220 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m221 |\u001b[0m         # CPU alert\n\u001b[1m\u001b[94m222 |\u001b[0m         if resource_usage.cpu_percent > self.thresholds[\"cpu_percent\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:232:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m230 |\u001b[0m                 timestamp=datetime.now()\n\u001b[1m\u001b[94m231 |\u001b[0m             ))\n\u001b[1m\u001b[94m232 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m         # Memory alert\n\u001b[1m\u001b[94m234 |\u001b[0m         if resource_usage.memory_percent > self.thresholds[\"memory_percent\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:244:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m242 |\u001b[0m                 timestamp=datetime.now()\n\u001b[1m\u001b[94m243 |\u001b[0m             ))\n\u001b[1m\u001b[94m244 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m245 |\u001b[0m         # Disk alert\n\u001b[1m\u001b[94m246 |\u001b[0m         if resource_usage.disk_usage_percent > self.thresholds[\"disk_usage_percent\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:256:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m254 |\u001b[0m                 timestamp=datetime.now()\n\u001b[1m\u001b[94m255 |\u001b[0m             ))\n\u001b[1m\u001b[94m256 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         # Create alerts\n\u001b[1m\u001b[94m258 |\u001b[0m         for alert in alerts_to_create:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:260:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m258 |\u001b[0m         for alert in alerts_to_create:\n\u001b[1m\u001b[94m259 |\u001b[0m             self._create_alert(alert)\n\u001b[1m\u001b[94m260 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m261 |\u001b[0m     def _create_alert(self, alert: PerformanceAlert) -> None:\n\u001b[1m\u001b[94m262 |\u001b[0m         \"\"\"Create a performance alert.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:264:50\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m262 |\u001b[0m         \"\"\"Create a performance alert.\"\"\"\n\u001b[1m\u001b[94m263 |\u001b[0m         # Check if similar alert already exists\n\u001b[1m\u001b[94m264 |\u001b[0m         existing_alerts = [a for a in self.alerts \n    \u001b[1m\u001b[94m|\u001b[0m                                                  \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m265 |\u001b[0m                           if a.metric_name == alert.metric_name and \n\u001b[1m\u001b[94m266 |\u001b[0m                           not a.acknowledged and\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:265:68\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m         # Check if similar alert already exists\n\u001b[1m\u001b[94m264 |\u001b[0m         existing_alerts = [a for a in self.alerts \n\u001b[1m\u001b[94m265 |\u001b[0m                           if a.metric_name == alert.metric_name and \n    \u001b[1m\u001b[94m|\u001b[0m                                                                    \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m                           not a.acknowledged and\n\u001b[1m\u001b[94m267 |\u001b[0m                           (datetime.now() - a.timestamp).total_seconds() < 300]  # 5 minutes\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:268:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m                           not a.acknowledged and\n\u001b[1m\u001b[94m267 |\u001b[0m                           (datetime.now() - a.timestamp).total_seconds() < 300]  # 5 minutes\n\u001b[1m\u001b[94m268 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m         if not existing_alerts:\n\u001b[1m\u001b[94m270 |\u001b[0m             self.alerts.append(alert)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:272:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m270 |\u001b[0m             self.alerts.append(alert)\n\u001b[1m\u001b[94m271 |\u001b[0m             self.logger.warning(f\"Performance Alert: {alert.message}\")\n\u001b[1m\u001b[94m272 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m273 |\u001b[0m             # Call alert callbacks\n\u001b[1m\u001b[94m274 |\u001b[0m             for callback in self.alert_callbacks:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:279:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m277 |\u001b[0m                 except Exception as e:\n\u001b[1m\u001b[94m278 |\u001b[0m                     self.logger.error(f\"Error in alert callback: {e}\")\n\u001b[1m\u001b[94m279 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m280 |\u001b[0m     def record_metric(self, name: str, value: float, unit: str = \"\", \n\u001b[1m\u001b[94m281 |\u001b[0m                      tags: Optional[Dict[str, str]] = None) -> None:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:280:69\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m278 |\u001b[0m                     self.logger.error(f\"Error in alert callback: {e}\")\n\u001b[1m\u001b[94m279 |\u001b[0m     \n\u001b[1m\u001b[94m280 |\u001b[0m     def record_metric(self, name: str, value: float, unit: str = \"\", \n    \u001b[1m\u001b[94m|\u001b[0m                                                                     \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m                      tags: Optional[Dict[str, str]] = None) -> None:\n\u001b[1m\u001b[94m282 |\u001b[0m         \"\"\"Record a performance metric.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:285:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m283 |\u001b[0m         if not self.config.get(\"enable_metrics_collection\", True):\n\u001b[1m\u001b[94m284 |\u001b[0m             return\n\u001b[1m\u001b[94m285 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m         # Initialize metric queue if not exists\n\u001b[1m\u001b[94m287 |\u001b[0m         if name not in self.metrics:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:290:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m288 |\u001b[0m             maxlen = self.config.get(\"metrics_retention\", 1000)\n\u001b[1m\u001b[94m289 |\u001b[0m             self.metrics[name] = deque(maxlen=maxlen)\n\u001b[1m\u001b[94m290 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m291 |\u001b[0m         # Create metric\n\u001b[1m\u001b[94m292 |\u001b[0m         metric = PerformanceMetric(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:299:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m297 |\u001b[0m             tags=tags or {}\n\u001b[1m\u001b[94m298 |\u001b[0m         )\n\u001b[1m\u001b[94m299 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m300 |\u001b[0m         # Add to queue\n\u001b[1m\u001b[94m301 |\u001b[0m         self.metrics[name].append(metric)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:302:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m300 |\u001b[0m         # Add to queue\n\u001b[1m\u001b[94m301 |\u001b[0m         self.metrics[name].append(metric)\n\u001b[1m\u001b[94m302 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m303 |\u001b[0m         # Check for metric-based alerts\n\u001b[1m\u001b[94m304 |\u001b[0m         self._check_metric_alerts(metric)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:305:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m303 |\u001b[0m         # Check for metric-based alerts\n\u001b[1m\u001b[94m304 |\u001b[0m         self._check_metric_alerts(metric)\n\u001b[1m\u001b[94m305 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m306 |\u001b[0m     def _check_metric_alerts(self, metric: PerformanceMetric) -> None:\n\u001b[1m\u001b[94m307 |\u001b[0m         \"\"\"Check for metric-based alerts.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:310:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m308 |\u001b[0m         if not self.config.get(\"enable_alerts\", True):\n\u001b[1m\u001b[94m309 |\u001b[0m             return\n\u001b[1m\u001b[94m310 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m311 |\u001b[0m         # Check response time\n\u001b[1m\u001b[94m312 |\u001b[0m         if metric.name == \"response_time\" and metric.value > self.thresholds[\"response_time_ms\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:323:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m321 |\u001b[0m             )\n\u001b[1m\u001b[94m322 |\u001b[0m             self._create_alert(alert)\n\u001b[1m\u001b[94m323 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m324 |\u001b[0m         # Check throughput\n\u001b[1m\u001b[94m325 |\u001b[0m         if metric.name == \"throughput\" and metric.value < self.thresholds[\"throughput_rps\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:336:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m334 |\u001b[0m             )\n\u001b[1m\u001b[94m335 |\u001b[0m             self._create_alert(alert)\n\u001b[1m\u001b[94m336 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m337 |\u001b[0m     def get_metric_stats(self, name: str, window_minutes: int = 60) -> Dict[str, float]:\n\u001b[1m\u001b[94m338 |\u001b[0m         \"\"\"Get statistics for a specific metric.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:341:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m339 |\u001b[0m         if name not in self.metrics:\n\u001b[1m\u001b[94m340 |\u001b[0m             return {}\n\u001b[1m\u001b[94m341 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m342 |\u001b[0m         # Filter metrics by time window\n\u001b[1m\u001b[94m343 |\u001b[0m         cutoff_time = datetime.now() - timedelta(minutes=window_minutes)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:348:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m346 |\u001b[0m             if m.timestamp > cutoff_time\n\u001b[1m\u001b[94m347 |\u001b[0m         ]\n\u001b[1m\u001b[94m348 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m349 |\u001b[0m         if not recent_metrics:\n\u001b[1m\u001b[94m350 |\u001b[0m             return {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:351:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m349 |\u001b[0m         if not recent_metrics:\n\u001b[1m\u001b[94m350 |\u001b[0m             return {}\n\u001b[1m\u001b[94m351 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m352 |\u001b[0m         values = [m.value for m in recent_metrics]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:353:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m352 |\u001b[0m         values = [m.value for m in recent_metrics]\n\u001b[1m\u001b[94m353 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m354 |\u001b[0m         return {\n\u001b[1m\u001b[94m355 |\u001b[0m             \"count\": len(values),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:364:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m362 |\u001b[0m             \"p99\": self._percentile(values, 99)\n\u001b[1m\u001b[94m363 |\u001b[0m         }\n\u001b[1m\u001b[94m364 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m365 |\u001b[0m     def _percentile(self, values: List[float], percentile: int) -> float:\n\u001b[1m\u001b[94m366 |\u001b[0m         \"\"\"Calculate percentile of values.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:369:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m367 |\u001b[0m         if not values:\n\u001b[1m\u001b[94m368 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m369 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m370 |\u001b[0m         sorted_values = sorted(values)\n\u001b[1m\u001b[94m371 |\u001b[0m         index = (percentile / 100.0) * (len(sorted_values) - 1)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:372:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m370 |\u001b[0m         sorted_values = sorted(values)\n\u001b[1m\u001b[94m371 |\u001b[0m         index = (percentile / 100.0) * (len(sorted_values) - 1)\n\u001b[1m\u001b[94m372 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m373 |\u001b[0m         if index.is_integer():\n\u001b[1m\u001b[94m374 |\u001b[0m             return sorted_values[int(index)]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:379:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m377 |\u001b[0m             upper = sorted_values[int(index) + 1]\n\u001b[1m\u001b[94m378 |\u001b[0m             return lower + (upper - lower) * (index - int(index))\n\u001b[1m\u001b[94m379 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m380 |\u001b[0m     def get_resource_summary(self, window_minutes: int = 60) -> Dict[str, float]:\n\u001b[1m\u001b[94m381 |\u001b[0m         \"\"\"Get resource usage summary.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:384:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m382 |\u001b[0m         if not self.resource_history:\n\u001b[1m\u001b[94m383 |\u001b[0m             return {}\n\u001b[1m\u001b[94m384 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m385 |\u001b[0m         # Filter by time window\n\u001b[1m\u001b[94m386 |\u001b[0m         cutoff_time = datetime.now() - timedelta(minutes=window_minutes)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:391:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m389 |\u001b[0m             if r.timestamp > cutoff_time\n\u001b[1m\u001b[94m390 |\u001b[0m         ]\n\u001b[1m\u001b[94m391 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m392 |\u001b[0m         if not recent_resources:\n\u001b[1m\u001b[94m393 |\u001b[0m             return {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:394:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m392 |\u001b[0m         if not recent_resources:\n\u001b[1m\u001b[94m393 |\u001b[0m             return {}\n\u001b[1m\u001b[94m394 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m395 |\u001b[0m         return {\n\u001b[1m\u001b[94m396 |\u001b[0m             \"avg_cpu_percent\": statistics.mean([r.cpu_percent for r in recent_resources]),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:405:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m403 |\u001b[0m             \"max_disk_usage_percent\": max([r.disk_usage_percent for r in recent_resources])\n\u001b[1m\u001b[94m404 |\u001b[0m         }\n\u001b[1m\u001b[94m405 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m406 |\u001b[0m     def get_active_alerts(self) -> List[PerformanceAlert]:\n\u001b[1m\u001b[94m407 |\u001b[0m         \"\"\"Get active (unacknowledged) alerts.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:409:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m         \"\"\"Get active (unacknowledged) alerts.\"\"\"\n\u001b[1m\u001b[94m408 |\u001b[0m         return [alert for alert in self.alerts if not alert.acknowledged]\n\u001b[1m\u001b[94m409 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m     def acknowledge_alert(self, alert_id: str) -> bool:\n\u001b[1m\u001b[94m411 |\u001b[0m         \"\"\"Acknowledge an alert.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:418:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m                 return True\n\u001b[1m\u001b[94m417 |\u001b[0m         return False\n\u001b[1m\u001b[94m418 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m419 |\u001b[0m     def add_alert_callback(self, callback: Callable[[PerformanceAlert], None]) -> None:\n\u001b[1m\u001b[94m420 |\u001b[0m         \"\"\"Add alert callback.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:422:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m420 |\u001b[0m         \"\"\"Add alert callback.\"\"\"\n\u001b[1m\u001b[94m421 |\u001b[0m         self.alert_callbacks.append(callback)\n\u001b[1m\u001b[94m422 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m423 |\u001b[0m     def _cleanup_old_data(self) -> None:\n\u001b[1m\u001b[94m424 |\u001b[0m         \"\"\"Clean up old data based on retention policies.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:429:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m427 |\u001b[0m             retention_days = self.config.get(\"alert_retention_days\", 7)\n\u001b[1m\u001b[94m428 |\u001b[0m             cutoff_time = datetime.now() - timedelta(days=retention_days)\n\u001b[1m\u001b[94m429 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m430 |\u001b[0m             self.alerts = [\n\u001b[1m\u001b[94m431 |\u001b[0m                 alert for alert in self.alerts\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:434:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m432 |\u001b[0m                 if alert.timestamp > cutoff_time\n\u001b[1m\u001b[94m433 |\u001b[0m             ]\n\u001b[1m\u001b[94m434 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m435 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m436 |\u001b[0m             self.logger.error(f\"Error cleaning up old data: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:437:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m435 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m436 |\u001b[0m             self.logger.error(f\"Error cleaning up old data: {e}\")\n\u001b[1m\u001b[94m437 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m438 |\u001b[0m     def generate_performance_report(self, start_time: Optional[datetime] = None, \n\u001b[1m\u001b[94m439 |\u001b[0m                                   end_time: Optional[datetime] = None) -> PerformanceReport:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:438:81\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m436 |\u001b[0m             self.logger.error(f\"Error cleaning up old data: {e}\")\n\u001b[1m\u001b[94m437 |\u001b[0m     \n\u001b[1m\u001b[94m438 |\u001b[0m     def generate_performance_report(self, start_time: Optional[datetime] = None, \n    \u001b[1m\u001b[94m|\u001b[0m                                                                                 \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m439 |\u001b[0m                                   end_time: Optional[datetime] = None) -> PerformanceReport:\n\u001b[1m\u001b[94m440 |\u001b[0m         \"\"\"Generate comprehensive performance report.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:445:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m443 |\u001b[0m         if not end_time:\n\u001b[1m\u001b[94m444 |\u001b[0m             end_time = datetime.now()\n\u001b[1m\u001b[94m445 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m446 |\u001b[0m         report_period = end_time - start_time\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:447:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m446 |\u001b[0m         report_period = end_time - start_time\n\u001b[1m\u001b[94m447 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m448 |\u001b[0m         # Calculate metrics summary\n\u001b[1m\u001b[94m449 |\u001b[0m         metrics_summary = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:454:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m452 |\u001b[0m             if stats:\n\u001b[1m\u001b[94m453 |\u001b[0m                 metrics_summary[metric_name] = stats\n\u001b[1m\u001b[94m454 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m455 |\u001b[0m         # Calculate resource usage summary\n\u001b[1m\u001b[94m456 |\u001b[0m         resource_summary = self.get_resource_summary(window_minutes=int(report_period.total_seconds() / 60))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:457:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m455 |\u001b[0m         # Calculate resource usage summary\n\u001b[1m\u001b[94m456 |\u001b[0m         resource_summary = self.get_resource_summary(window_minutes=int(report_period.total_seconds() / 60))\n\u001b[1m\u001b[94m457 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m458 |\u001b[0m         # Get alerts in time period\n\u001b[1m\u001b[94m459 |\u001b[0m         period_alerts = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:463:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m461 |\u001b[0m             if start_time <= alert.timestamp <= end_time\n\u001b[1m\u001b[94m462 |\u001b[0m         ]\n\u001b[1m\u001b[94m463 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m         # Generate recommendations\n\u001b[1m\u001b[94m465 |\u001b[0m         recommendations = self._generate_recommendations(metrics_summary, resource_summary, period_alerts)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:466:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m         # Generate recommendations\n\u001b[1m\u001b[94m465 |\u001b[0m         recommendations = self._generate_recommendations(metrics_summary, resource_summary, period_alerts)\n\u001b[1m\u001b[94m466 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m467 |\u001b[0m         return PerformanceReport(\n\u001b[1m\u001b[94m468 |\u001b[0m             report_period=report_period,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:476:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m474 |\u001b[0m             recommendations=recommendations\n\u001b[1m\u001b[94m475 |\u001b[0m         )\n\u001b[1m\u001b[94m476 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m477 |\u001b[0m     def _generate_recommendations(self, metrics_summary: Dict[str, Dict[str, float]], \n\u001b[1m\u001b[94m478 |\u001b[0m                                 resource_summary: Dict[str, float],\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:477:86\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m475 |\u001b[0m         )\n\u001b[1m\u001b[94m476 |\u001b[0m     \n\u001b[1m\u001b[94m477 |\u001b[0m     def _generate_recommendations(self, metrics_summary: Dict[str, Dict[str, float]], \n    \u001b[1m\u001b[94m|\u001b[0m                                                                                      \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m478 |\u001b[0m                                 resource_summary: Dict[str, float],\n\u001b[1m\u001b[94m479 |\u001b[0m                                 alerts: List[PerformanceAlert]) -> List[str]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:482:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m480 |\u001b[0m         \"\"\"Generate performance recommendations.\"\"\"\n\u001b[1m\u001b[94m481 |\u001b[0m         recommendations = []\n\u001b[1m\u001b[94m482 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m483 |\u001b[0m         # Resource-based recommendations\n\u001b[1m\u001b[94m484 |\u001b[0m         if resource_summary:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:487:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m485 |\u001b[0m             if resource_summary.get(\"avg_cpu_percent\", 0) > 70:\n\u001b[1m\u001b[94m486 |\u001b[0m                 recommendations.append(\"Consider optimizing CPU-intensive operations or scaling horizontally\")\n\u001b[1m\u001b[94m487 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m488 |\u001b[0m             if resource_summary.get(\"avg_memory_percent\", 0) > 80:\n\u001b[1m\u001b[94m489 |\u001b[0m                 recommendations.append(\"Consider optimizing memory usage or increasing available memory\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:490:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m488 |\u001b[0m             if resource_summary.get(\"avg_memory_percent\", 0) > 80:\n\u001b[1m\u001b[94m489 |\u001b[0m                 recommendations.append(\"Consider optimizing memory usage or increasing available memory\")\n\u001b[1m\u001b[94m490 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m491 |\u001b[0m             if resource_summary.get(\"max_memory_used_mb\", 0) > 2048:\n\u001b[1m\u001b[94m492 |\u001b[0m                 recommendations.append(\"Memory usage is high - review data structures and caching strategies\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:493:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m491 |\u001b[0m             if resource_summary.get(\"max_memory_used_mb\", 0) > 2048:\n\u001b[1m\u001b[94m492 |\u001b[0m                 recommendations.append(\"Memory usage is high - review data structures and caching strategies\")\n\u001b[1m\u001b[94m493 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m494 |\u001b[0m         # Metrics-based recommendations\n\u001b[1m\u001b[94m495 |\u001b[0m         if \"response_time\" in metrics_summary:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:499:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m497 |\u001b[0m             if avg_response > 1000:\n\u001b[1m\u001b[94m498 |\u001b[0m                 recommendations.append(\"Response times are high - consider optimizing slow operations\")\n\u001b[1m\u001b[94m499 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m500 |\u001b[0m         if \"throughput\" in metrics_summary:\n\u001b[1m\u001b[94m501 |\u001b[0m             avg_throughput = metrics_summary[\"throughput\"].get(\"mean\", 0)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:504:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m502 |\u001b[0m             if avg_throughput < 50:\n\u001b[1m\u001b[94m503 |\u001b[0m                 recommendations.append(\"Throughput is low - consider parallelization or caching\")\n\u001b[1m\u001b[94m504 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m505 |\u001b[0m         # Alert-based recommendations\n\u001b[1m\u001b[94m506 |\u001b[0m         critical_alerts = [a for a in alerts if a.severity == \"critical\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:509:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m507 |\u001b[0m         if critical_alerts:\n\u001b[1m\u001b[94m508 |\u001b[0m             recommendations.append(f\"Address {len(critical_alerts)} critical performance alerts immediately\")\n\u001b[1m\u001b[94m509 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m510 |\u001b[0m         warning_alerts = [a for a in alerts if a.severity == \"warning\"]\n\u001b[1m\u001b[94m511 |\u001b[0m         if warning_alerts:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:513:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m511 |\u001b[0m         if warning_alerts:\n\u001b[1m\u001b[94m512 |\u001b[0m             recommendations.append(f\"Review {len(warning_alerts)} warning-level performance alerts\")\n\u001b[1m\u001b[94m513 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m514 |\u001b[0m         # General recommendations\n\u001b[1m\u001b[94m515 |\u001b[0m         recommendations.extend([\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:521:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m519 |\u001b[0m             \"Implement automated alerting for critical performance metrics\"\n\u001b[1m\u001b[94m520 |\u001b[0m         ])\n\u001b[1m\u001b[94m521 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m522 |\u001b[0m         return recommendations\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:523:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m522 |\u001b[0m         return recommendations\n\u001b[1m\u001b[94m523 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m524 |\u001b[0m     def export_report(self, report: PerformanceReport, output_file: Optional[Path] = None) -> Path:\n\u001b[1m\u001b[94m525 |\u001b[0m         \"\"\"Export performance report to file.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:529:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m527 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\u001b[1m\u001b[94m528 |\u001b[0m             output_file = Path(f\"performance_report_{timestamp}.json\")\n\u001b[1m\u001b[94m529 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m530 |\u001b[0m         # Convert to serializable format\n\u001b[1m\u001b[94m531 |\u001b[0m         report_data = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:543:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m541 |\u001b[0m             \"recommendations\": report.recommendations\n\u001b[1m\u001b[94m542 |\u001b[0m         }\n\u001b[1m\u001b[94m543 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m544 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m545 |\u001b[0m             json.dump(report_data, f, indent=2, default=str)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:546:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m544 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m545 |\u001b[0m             json.dump(report_data, f, indent=2, default=str)\n\u001b[1m\u001b[94m546 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m547 |\u001b[0m         return output_file\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:548:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m547 |\u001b[0m         return output_file\n\u001b[1m\u001b[94m548 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m549 |\u001b[0m     def get_dashboard_data(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m550 |\u001b[0m         \"\"\"Get data for performance dashboard.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:561:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m559 |\u001b[0m                         \"stats\": stats\n\u001b[1m\u001b[94m560 |\u001b[0m                     }\n\u001b[1m\u001b[94m561 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m562 |\u001b[0m             # Get resource summary\n\u001b[1m\u001b[94m563 |\u001b[0m             resource_summary = self.get_resource_summary(window_minutes=60)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:564:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m562 |\u001b[0m             # Get resource summary\n\u001b[1m\u001b[94m563 |\u001b[0m             resource_summary = self.get_resource_summary(window_minutes=60)\n\u001b[1m\u001b[94m564 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m565 |\u001b[0m             # Get active alerts\n\u001b[1m\u001b[94m566 |\u001b[0m             active_alerts = self.get_active_alerts()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:567:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m565 |\u001b[0m             # Get active alerts\n\u001b[1m\u001b[94m566 |\u001b[0m             active_alerts = self.get_active_alerts()\n\u001b[1m\u001b[94m567 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m568 |\u001b[0m             # Get current resource usage\n\u001b[1m\u001b[94m569 |\u001b[0m             current_resources = self.resource_history[-1] if self.resource_history else None\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:570:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m568 |\u001b[0m             # Get current resource usage\n\u001b[1m\u001b[94m569 |\u001b[0m             current_resources = self.resource_history[-1] if self.resource_history else None\n\u001b[1m\u001b[94m570 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m571 |\u001b[0m             return {\n\u001b[1m\u001b[94m572 |\u001b[0m                 \"current_time\": datetime.now().isoformat(),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:585:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m583 |\u001b[0m                 \"thresholds\": self.thresholds\n\u001b[1m\u001b[94m584 |\u001b[0m             }\n\u001b[1m\u001b[94m585 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m586 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m587 |\u001b[0m             self.logger.error(f\"Error getting dashboard data: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:618:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m616 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m617 |\u001b[0m     import argparse\n\u001b[1m\u001b[94m618 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m619 |\u001b[0m     parser = argparse.ArgumentParser(description=\"SparkForge Performance Monitor\")\n\u001b[1m\u001b[94m620 |\u001b[0m     parser.add_argument(\"--start\", action=\"store_true\", help=\"Start performance monitoring\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:625:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m623 |\u001b[0m     parser.add_argument(\"--dashboard\", action=\"store_true\", help=\"Show dashboard data\")\n\u001b[1m\u001b[94m624 |\u001b[0m     parser.add_argument(\"--duration\", type=int, default=60, help=\"Monitoring duration in seconds\")\n\u001b[1m\u001b[94m625 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m626 |\u001b[0m     args = parser.parse_args()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:627:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m626 |\u001b[0m     args = parser.parse_args()\n\u001b[1m\u001b[94m627 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m628 |\u001b[0m     monitor = PerformanceMonitor()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:629:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m628 |\u001b[0m     monitor = PerformanceMonitor()\n\u001b[1m\u001b[94m629 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m630 |\u001b[0m     if args.start:\n\u001b[1m\u001b[94m631 |\u001b[0m         monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:633:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m631 |\u001b[0m         monitor.start_monitoring()\n\u001b[1m\u001b[94m632 |\u001b[0m         print(\"Performance monitoring started\")\n\u001b[1m\u001b[94m633 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m634 |\u001b[0m         try:\n\u001b[1m\u001b[94m635 |\u001b[0m             time.sleep(args.duration)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:640:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m638 |\u001b[0m         finally:\n\u001b[1m\u001b[94m639 |\u001b[0m             monitor.stop_monitoring()\n\u001b[1m\u001b[94m640 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m641 |\u001b[0m     if args.stop:\n\u001b[1m\u001b[94m642 |\u001b[0m         monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:644:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m642 |\u001b[0m         monitor.stop_monitoring()\n\u001b[1m\u001b[94m643 |\u001b[0m         print(\"Performance monitoring stopped\")\n\u001b[1m\u001b[94m644 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m645 |\u001b[0m     if args.report:\n\u001b[1m\u001b[94m646 |\u001b[0m         report = monitor.generate_performance_report()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:649:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m647 |\u001b[0m         report_file = monitor.export_report(report)\n\u001b[1m\u001b[94m648 |\u001b[0m         print(f\"Performance report saved to: {report_file}\")\n\u001b[1m\u001b[94m649 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m650 |\u001b[0m         print(f\"\\nPerformance Summary:\")\n\u001b[1m\u001b[94m651 |\u001b[0m         print(f\"Report Period: {report.report_period}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:650:15\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m648 |\u001b[0m         print(f\"Performance report saved to: {report_file}\")\n\u001b[1m\u001b[94m649 |\u001b[0m         \n\u001b[1m\u001b[94m650 |\u001b[0m         print(f\"\\nPerformance Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m651 |\u001b[0m         print(f\"Report Period: {report.report_period}\")\n\u001b[1m\u001b[94m652 |\u001b[0m         print(f\"Metrics Tracked: {len(report.metrics_summary)}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_monitoring.py:655:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m653 |\u001b[0m         print(f\"Active Alerts: {len(report.alerts)}\")\n\u001b[1m\u001b[94m654 |\u001b[0m         print(f\"Recommendations: {len(report.recommendations)}\")\n\u001b[1m\u001b[94m655 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m656 |\u001b[0m     if args.dashboard:\n\u001b[1m\u001b[94m657 |\u001b[0m         dashboard_data = monitor.get_dashboard_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:12:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import time\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import psutil\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import tracemalloc\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import cProfile\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pstats\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import io\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import threading\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from functools import wraps\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Dict, List, Any, Optional, Callable, Tuple\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import sys\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|________________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`datetime.timedelta` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:22:32\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m from typing import Dict, List, Any, Optional, Callable, Tuple\n\u001b[1m\u001b[94m21 |\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m22 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m23 |\u001b[0m import json\n\u001b[1m\u001b[94m24 |\u001b[0m import sys\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `datetime.timedelta`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sys` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:24:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m23 |\u001b[0m import json\n\u001b[1m\u001b[94m24 |\u001b[0m import sys\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m from pathlib import Path\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `sys`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:56:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m54 |\u001b[0m class PerformanceProfiler:\n\u001b[1m\u001b[94m55 |\u001b[0m     \"\"\"Comprehensive performance profiler for SparkForge.\"\"\"\n\u001b[1m\u001b[94m56 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m57 |\u001b[0m     def __init__(self, enable_memory_tracking: bool = True, enable_cpu_tracking: bool = True):\n\u001b[1m\u001b[94m58 |\u001b[0m         self.enable_memory_tracking = enable_memory_tracking\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:63:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         self.active_profiles: Dict[str, Dict[str, Any]] = {}\n\u001b[1m\u001b[94m62 |\u001b[0m         self.thread_local = threading.local()\n\u001b[1m\u001b[94m63 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m         # Performance thresholds\n\u001b[1m\u001b[94m65 |\u001b[0m         self.thresholds = {\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:72:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m70 |\u001b[0m             \"memory_leak_threshold\": 50    # Memory increase threshold in MB\n\u001b[1m\u001b[94m71 |\u001b[0m         }\n\u001b[1m\u001b[94m72 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m73 |\u001b[0m         # Start memory tracking if enabled\n\u001b[1m\u001b[94m74 |\u001b[0m         if self.enable_memory_tracking:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:76:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m         if self.enable_memory_tracking:\n\u001b[1m\u001b[94m75 |\u001b[0m             tracemalloc.start()\n\u001b[1m\u001b[94m76 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m     def profile_function(self, func: Callable) -> Callable:\n\u001b[1m\u001b[94m78 |\u001b[0m         \"\"\"Decorator to profile a function.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:85:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m             start_memory = self._get_memory_usage()\n\u001b[1m\u001b[94m84 |\u001b[0m             start_cpu = self._get_cpu_usage()\n\u001b[1m\u001b[94m85 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m             # Track exceptions\n\u001b[1m\u001b[94m87 |\u001b[0m             exception_count = 0\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:89:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m87 |\u001b[0m             exception_count = 0\n\u001b[1m\u001b[94m88 |\u001b[0m             result = None\n\u001b[1m\u001b[94m89 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m90 |\u001b[0m             try:\n\u001b[1m\u001b[94m91 |\u001b[0m                 result = func(*args, **kwargs)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mLocal variable `e` is assigned to but never used\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:92:33\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m90 |\u001b[0m             try:\n\u001b[1m\u001b[94m91 |\u001b[0m                 result = func(*args, **kwargs)\n\u001b[1m\u001b[94m92 |\u001b[0m             except Exception as e:\n   \u001b[1m\u001b[94m|\u001b[0m                                 \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m93 |\u001b[0m                 exception_count += 1\n\u001b[1m\u001b[94m94 |\u001b[0m                 raise\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `e`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:99:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 97 |\u001b[0m                 end_memory = self._get_memory_usage()\n\u001b[1m\u001b[94m 98 |\u001b[0m                 end_cpu = self._get_cpu_usage()\n\u001b[1m\u001b[94m 99 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m100 |\u001b[0m                 # Calculate metrics\n\u001b[1m\u001b[94m101 |\u001b[0m                 execution_time = (end_time - start_time) * 1000  # Convert to milliseconds\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:104:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m102 |\u001b[0m                 memory_usage = end_memory - start_memory\n\u001b[1m\u001b[94m103 |\u001b[0m                 cpu_usage = end_cpu - start_cpu\n\u001b[1m\u001b[94m104 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m105 |\u001b[0m                 # Estimate argument and return sizes\n\u001b[1m\u001b[94m106 |\u001b[0m                 args_size = self._estimate_size(args) + self._estimate_size(kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:108:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m106 |\u001b[0m                 args_size = self._estimate_size(args) + self._estimate_size(kwargs)\n\u001b[1m\u001b[94m107 |\u001b[0m                 return_size = self._estimate_size(result) if result is not None else 0\n\u001b[1m\u001b[94m108 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m109 |\u001b[0m                 # Create metrics\n\u001b[1m\u001b[94m110 |\u001b[0m                 metrics = PerformanceMetrics(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:121:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m                     exception_count=exception_count\n\u001b[1m\u001b[94m120 |\u001b[0m                 )\n\u001b[1m\u001b[94m121 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m122 |\u001b[0m                 self.metrics.append(metrics)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:123:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m122 |\u001b[0m                 self.metrics.append(metrics)\n\u001b[1m\u001b[94m123 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m             return result\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:125:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m             return result\n\u001b[1m\u001b[94m125 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m         return wrapper\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:127:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m         return wrapper\n\u001b[1m\u001b[94m127 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m     def profile_class(self, cls: type) -> type:\n\u001b[1m\u001b[94m129 |\u001b[0m         \"\"\"Decorator to profile all methods of a class.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:135:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m133 |\u001b[0m                 setattr(cls, attr_name, self.profile_function(attr))\n\u001b[1m\u001b[94m134 |\u001b[0m         return cls\n\u001b[1m\u001b[94m135 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m     def start_profile(self, profile_name: str) -> None:\n\u001b[1m\u001b[94m137 |\u001b[0m         \"\"\"Start profiling with a specific name.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:144:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m142 |\u001b[0m             \"metrics_count\": len(self.metrics)\n\u001b[1m\u001b[94m143 |\u001b[0m         }\n\u001b[1m\u001b[94m144 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m145 |\u001b[0m     def end_profile(self, profile_name: str) -> Dict[str, Any]:\n\u001b[1m\u001b[94m146 |\u001b[0m         \"\"\"End profiling and return results.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:149:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m         if profile_name not in self.active_profiles:\n\u001b[1m\u001b[94m148 |\u001b[0m             raise ValueError(f\"Profile '{profile_name}' not found\")\n\u001b[1m\u001b[94m149 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m         profile_data = self.active_profiles[profile_name]\n\u001b[1m\u001b[94m151 |\u001b[0m         end_time = time.time()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:154:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m         end_memory = self._get_memory_usage()\n\u001b[1m\u001b[94m153 |\u001b[0m         end_cpu = self._get_cpu_usage()\n\u001b[1m\u001b[94m154 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m155 |\u001b[0m         # Calculate profile metrics\n\u001b[1m\u001b[94m156 |\u001b[0m         total_time = (end_time - profile_data[\"start_time\"]) * 1000\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:159:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m         total_memory = end_memory - profile_data[\"start_memory\"]\n\u001b[1m\u001b[94m158 |\u001b[0m         total_cpu = end_cpu - profile_data[\"start_cpu\"]\n\u001b[1m\u001b[94m159 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         # Get metrics collected during this profile\n\u001b[1m\u001b[94m161 |\u001b[0m         profile_metrics = self.metrics[profile_data[\"metrics_count\"]:]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:162:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         # Get metrics collected during this profile\n\u001b[1m\u001b[94m161 |\u001b[0m         profile_metrics = self.metrics[profile_data[\"metrics_count\"]:]\n\u001b[1m\u001b[94m162 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m163 |\u001b[0m         profile_result = {\n\u001b[1m\u001b[94m164 |\u001b[0m             \"profile_name\": profile_name,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:172:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m170 |\u001b[0m             \"timestamp\": datetime.now().isoformat()\n\u001b[1m\u001b[94m171 |\u001b[0m         }\n\u001b[1m\u001b[94m172 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m173 |\u001b[0m         # Clean up\n\u001b[1m\u001b[94m174 |\u001b[0m         del self.active_profiles[profile_name]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:175:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m173 |\u001b[0m         # Clean up\n\u001b[1m\u001b[94m174 |\u001b[0m         del self.active_profiles[profile_name]\n\u001b[1m\u001b[94m175 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m176 |\u001b[0m         return profile_result\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:177:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m176 |\u001b[0m         return profile_result\n\u001b[1m\u001b[94m177 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m178 |\u001b[0m     def profile_pipeline(self, pipeline_func: Callable, *args, **kwargs) -> Tuple[Any, ProfilerReport]:\n\u001b[1m\u001b[94m179 |\u001b[0m         \"\"\"Profile a complete pipeline execution.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:181:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m179 |\u001b[0m         \"\"\"Profile a complete pipeline execution.\"\"\"\n\u001b[1m\u001b[94m180 |\u001b[0m         self.start_profile(\"pipeline_execution\")\n\u001b[1m\u001b[94m181 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m182 |\u001b[0m         try:\n\u001b[1m\u001b[94m183 |\u001b[0m             result = pipeline_func(*args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `profile_result` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:185:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m             result = pipeline_func(*args, **kwargs)\n\u001b[1m\u001b[94m184 |\u001b[0m         finally:\n\u001b[1m\u001b[94m185 |\u001b[0m             profile_result = self.end_profile(\"pipeline_execution\")\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         \n\u001b[1m\u001b[94m187 |\u001b[0m         # Generate comprehensive report\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `profile_result`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:186:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m         finally:\n\u001b[1m\u001b[94m185 |\u001b[0m             profile_result = self.end_profile(\"pipeline_execution\")\n\u001b[1m\u001b[94m186 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m187 |\u001b[0m         # Generate comprehensive report\n\u001b[1m\u001b[94m188 |\u001b[0m         report = self.generate_report()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:189:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m187 |\u001b[0m         # Generate comprehensive report\n\u001b[1m\u001b[94m188 |\u001b[0m         report = self.generate_report()\n\u001b[1m\u001b[94m189 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m         return result, report\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:191:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m         return result, report\n\u001b[1m\u001b[94m191 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m192 |\u001b[0m     def profile_spark_operations(self, spark_func: Callable, *args, **kwargs) -> Tuple[Any, ProfilerReport]:\n\u001b[1m\u001b[94m193 |\u001b[0m         \"\"\"Profile Spark-specific operations.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:195:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m         \"\"\"Profile Spark-specific operations.\"\"\"\n\u001b[1m\u001b[94m194 |\u001b[0m         self.start_profile(\"spark_operations\")\n\u001b[1m\u001b[94m195 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m196 |\u001b[0m         try:\n\u001b[1m\u001b[94m197 |\u001b[0m             result = spark_func(*args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `profile_result` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:199:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m             result = spark_func(*args, **kwargs)\n\u001b[1m\u001b[94m198 |\u001b[0m         finally:\n\u001b[1m\u001b[94m199 |\u001b[0m             profile_result = self.end_profile(\"spark_operations\")\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m         \n\u001b[1m\u001b[94m201 |\u001b[0m         # Generate Spark-specific report\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `profile_result`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:200:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m198 |\u001b[0m         finally:\n\u001b[1m\u001b[94m199 |\u001b[0m             profile_result = self.end_profile(\"spark_operations\")\n\u001b[1m\u001b[94m200 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m         # Generate Spark-specific report\n\u001b[1m\u001b[94m202 |\u001b[0m         report = self.generate_spark_report()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:203:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m         # Generate Spark-specific report\n\u001b[1m\u001b[94m202 |\u001b[0m         report = self.generate_spark_report()\n\u001b[1m\u001b[94m203 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m         return result, report\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:205:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m         return result, report\n\u001b[1m\u001b[94m205 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m206 |\u001b[0m     def generate_report(self) -> ProfilerReport:\n\u001b[1m\u001b[94m207 |\u001b[0m         \"\"\"Generate comprehensive performance report.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:218:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m216 |\u001b[0m                 timestamp=datetime.now()\n\u001b[1m\u001b[94m217 |\u001b[0m             )\n\u001b[1m\u001b[94m218 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m219 |\u001b[0m         # Aggregate metrics by function\n\u001b[1m\u001b[94m220 |\u001b[0m         function_aggregates = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:233:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m                     \"return_size\": 0\n\u001b[1m\u001b[94m232 |\u001b[0m                 }\n\u001b[1m\u001b[94m233 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m234 |\u001b[0m             agg = function_aggregates[func_name]\n\u001b[1m\u001b[94m235 |\u001b[0m             agg[\"execution_time\"] += metric.execution_time\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:242:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m240 |\u001b[0m             agg[\"args_size\"] += metric.args_size\n\u001b[1m\u001b[94m241 |\u001b[0m             agg[\"return_size\"] += metric.return_size\n\u001b[1m\u001b[94m242 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m243 |\u001b[0m         # Create aggregated metrics\n\u001b[1m\u001b[94m244 |\u001b[0m         aggregated_metrics = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:257:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m255 |\u001b[0m                 exception_count=agg[\"exception_count\"]\n\u001b[1m\u001b[94m256 |\u001b[0m             ))\n\u001b[1m\u001b[94m257 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m258 |\u001b[0m         # Sort by execution time (descending)\n\u001b[1m\u001b[94m259 |\u001b[0m         aggregated_metrics.sort(key=lambda x: x.execution_time, reverse=True)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:260:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m258 |\u001b[0m         # Sort by execution time (descending)\n\u001b[1m\u001b[94m259 |\u001b[0m         aggregated_metrics.sort(key=lambda x: x.execution_time, reverse=True)\n\u001b[1m\u001b[94m260 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m261 |\u001b[0m         # Calculate totals\n\u001b[1m\u001b[94m262 |\u001b[0m         total_execution_time = sum(m.execution_time for m in aggregated_metrics)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:265:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m         total_memory_usage = sum(m.memory_usage for m in aggregated_metrics)\n\u001b[1m\u001b[94m264 |\u001b[0m         total_cpu_usage = sum(m.cpu_usage for m in aggregated_metrics)\n\u001b[1m\u001b[94m265 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m         # Identify bottlenecks\n\u001b[1m\u001b[94m267 |\u001b[0m         bottlenecks = self._identify_bottlenecks(aggregated_metrics)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:268:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m         # Identify bottlenecks\n\u001b[1m\u001b[94m267 |\u001b[0m         bottlenecks = self._identify_bottlenecks(aggregated_metrics)\n\u001b[1m\u001b[94m268 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m         # Generate recommendations\n\u001b[1m\u001b[94m270 |\u001b[0m         recommendations = self._generate_recommendations(aggregated_metrics, bottlenecks)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:271:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m         # Generate recommendations\n\u001b[1m\u001b[94m270 |\u001b[0m         recommendations = self._generate_recommendations(aggregated_metrics, bottlenecks)\n\u001b[1m\u001b[94m271 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m272 |\u001b[0m         return ProfilerReport(\n\u001b[1m\u001b[94m273 |\u001b[0m             total_execution_time=total_execution_time,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:281:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m279 |\u001b[0m             timestamp=datetime.now()\n\u001b[1m\u001b[94m280 |\u001b[0m         )\n\u001b[1m\u001b[94m281 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m282 |\u001b[0m     def generate_spark_report(self) -> ProfilerReport:\n\u001b[1m\u001b[94m283 |\u001b[0m         \"\"\"Generate Spark-specific performance report.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:285:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m283 |\u001b[0m         \"\"\"Generate Spark-specific performance report.\"\"\"\n\u001b[1m\u001b[94m284 |\u001b[0m         report = self.generate_report()\n\u001b[1m\u001b[94m285 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m         # Add Spark-specific analysis\n\u001b[1m\u001b[94m287 |\u001b[0m         spark_metrics = [m for m in report.function_metrics \n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:287:60\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m         # Add Spark-specific analysis\n\u001b[1m\u001b[94m287 |\u001b[0m         spark_metrics = [m for m in report.function_metrics \n    \u001b[1m\u001b[94m|\u001b[0m                                                            \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m288 |\u001b[0m                         if any(spark_indicator in m.function_name.lower() \n\u001b[1m\u001b[94m289 |\u001b[0m                               for spark_indicator in ['spark', 'dataframe', 'rdd', 'sql'])]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:288:74\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m         # Add Spark-specific analysis\n\u001b[1m\u001b[94m287 |\u001b[0m         spark_metrics = [m for m in report.function_metrics \n\u001b[1m\u001b[94m288 |\u001b[0m                         if any(spark_indicator in m.function_name.lower() \n    \u001b[1m\u001b[94m|\u001b[0m                                                                          \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m289 |\u001b[0m                               for spark_indicator in ['spark', 'dataframe', 'rdd', 'sql'])]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:290:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m288 |\u001b[0m                         if any(spark_indicator in m.function_name.lower() \n\u001b[1m\u001b[94m289 |\u001b[0m                               for spark_indicator in ['spark', 'dataframe', 'rdd', 'sql'])]\n\u001b[1m\u001b[94m290 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m291 |\u001b[0m         # Add Spark-specific recommendations\n\u001b[1m\u001b[94m292 |\u001b[0m         spark_recommendations = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:293:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m291 |\u001b[0m         # Add Spark-specific recommendations\n\u001b[1m\u001b[94m292 |\u001b[0m         spark_recommendations = []\n\u001b[1m\u001b[94m293 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m294 |\u001b[0m         # Check for DataFrame operations\n\u001b[1m\u001b[94m295 |\u001b[0m         df_ops = [m for m in spark_metrics if 'dataframe' in m.function_name.lower()]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:302:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m300 |\u001b[0m                     \"Consider optimizing DataFrame operations - they are taking significant time\"\n\u001b[1m\u001b[94m301 |\u001b[0m                 )\n\u001b[1m\u001b[94m302 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m303 |\u001b[0m         # Check for SQL operations\n\u001b[1m\u001b[94m304 |\u001b[0m         sql_ops = [m for m in spark_metrics if 'sql' in m.function_name.lower()]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:309:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m307 |\u001b[0m                 \"Review SQL queries for optimization opportunities\"\n\u001b[1m\u001b[94m308 |\u001b[0m             )\n\u001b[1m\u001b[94m309 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m310 |\u001b[0m         # Check for caching opportunities\n\u001b[1m\u001b[94m311 |\u001b[0m         frequent_ops = [m for m in spark_metrics if m.call_count > 10]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:316:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m314 |\u001b[0m                 \"Consider caching frequently accessed DataFrames or RDDs\"\n\u001b[1m\u001b[94m315 |\u001b[0m             )\n\u001b[1m\u001b[94m316 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m317 |\u001b[0m         # Update recommendations\n\u001b[1m\u001b[94m318 |\u001b[0m         report.recommendations.extend(spark_recommendations)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:319:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m317 |\u001b[0m         # Update recommendations\n\u001b[1m\u001b[94m318 |\u001b[0m         report.recommendations.extend(spark_recommendations)\n\u001b[1m\u001b[94m319 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m320 |\u001b[0m         return report\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:321:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m320 |\u001b[0m         return report\n\u001b[1m\u001b[94m321 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m322 |\u001b[0m     def _identify_bottlenecks(self, metrics: List[PerformanceMetrics]) -> List[Dict[str, Any]]:\n\u001b[1m\u001b[94m323 |\u001b[0m         \"\"\"Identify performance bottlenecks.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:325:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m323 |\u001b[0m         \"\"\"Identify performance bottlenecks.\"\"\"\n\u001b[1m\u001b[94m324 |\u001b[0m         bottlenecks = []\n\u001b[1m\u001b[94m325 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m326 |\u001b[0m         for metric in metrics:\n\u001b[1m\u001b[94m327 |\u001b[0m             bottleneck_reasons = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:328:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m326 |\u001b[0m         for metric in metrics:\n\u001b[1m\u001b[94m327 |\u001b[0m             bottleneck_reasons = []\n\u001b[1m\u001b[94m328 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m329 |\u001b[0m             # Check execution time\n\u001b[1m\u001b[94m330 |\u001b[0m             if metric.execution_time > self.thresholds[\"slow_function_ms\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:332:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m330 |\u001b[0m             if metric.execution_time > self.thresholds[\"slow_function_ms\"]:\n\u001b[1m\u001b[94m331 |\u001b[0m                 bottleneck_reasons.append(f\"Slow execution: {metric.execution_time:.2f}ms\")\n\u001b[1m\u001b[94m332 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m333 |\u001b[0m             # Check memory usage\n\u001b[1m\u001b[94m334 |\u001b[0m             if metric.memory_usage > self.thresholds[\"high_memory_mb\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:336:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m334 |\u001b[0m             if metric.memory_usage > self.thresholds[\"high_memory_mb\"]:\n\u001b[1m\u001b[94m335 |\u001b[0m                 bottleneck_reasons.append(f\"High memory usage: {metric.memory_usage:.2f}MB\")\n\u001b[1m\u001b[94m336 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m337 |\u001b[0m             # Check CPU usage\n\u001b[1m\u001b[94m338 |\u001b[0m             if metric.cpu_usage > self.thresholds[\"high_cpu_percent\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:340:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m338 |\u001b[0m             if metric.cpu_usage > self.thresholds[\"high_cpu_percent\"]:\n\u001b[1m\u001b[94m339 |\u001b[0m                 bottleneck_reasons.append(f\"High CPU usage: {metric.cpu_usage:.2f}%\")\n\u001b[1m\u001b[94m340 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m341 |\u001b[0m             # Check call frequency\n\u001b[1m\u001b[94m342 |\u001b[0m             if metric.call_count > self.thresholds[\"frequent_calls\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:344:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m342 |\u001b[0m             if metric.call_count > self.thresholds[\"frequent_calls\"]:\n\u001b[1m\u001b[94m343 |\u001b[0m                 bottleneck_reasons.append(f\"Frequently called: {metric.call_count} times\")\n\u001b[1m\u001b[94m344 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m345 |\u001b[0m             # Check for memory leaks (simplified)\n\u001b[1m\u001b[94m346 |\u001b[0m             if metric.memory_usage > self.thresholds[\"memory_leak_threshold\"]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:348:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m346 |\u001b[0m             if metric.memory_usage > self.thresholds[\"memory_leak_threshold\"]:\n\u001b[1m\u001b[94m347 |\u001b[0m                 bottleneck_reasons.append(f\"Potential memory leak: {metric.memory_usage:.2f}MB\")\n\u001b[1m\u001b[94m348 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m349 |\u001b[0m             if bottleneck_reasons:\n\u001b[1m\u001b[94m350 |\u001b[0m                 bottlenecks.append({\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:356:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m354 |\u001b[0m                     \"impact_score\": self._calculate_impact_score(metric)\n\u001b[1m\u001b[94m355 |\u001b[0m                 })\n\u001b[1m\u001b[94m356 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m357 |\u001b[0m         # Sort by impact score\n\u001b[1m\u001b[94m358 |\u001b[0m         bottlenecks.sort(key=lambda x: x[\"impact_score\"], reverse=True)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:359:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m357 |\u001b[0m         # Sort by impact score\n\u001b[1m\u001b[94m358 |\u001b[0m         bottlenecks.sort(key=lambda x: x[\"impact_score\"], reverse=True)\n\u001b[1m\u001b[94m359 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m360 |\u001b[0m         return bottlenecks\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:361:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m360 |\u001b[0m         return bottlenecks\n\u001b[1m\u001b[94m361 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m362 |\u001b[0m     def _calculate_bottleneck_severity(self, metric: PerformanceMetrics) -> str:\n\u001b[1m\u001b[94m363 |\u001b[0m         \"\"\"Calculate bottleneck severity.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:365:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m363 |\u001b[0m         \"\"\"Calculate bottleneck severity.\"\"\"\n\u001b[1m\u001b[94m364 |\u001b[0m         score = 0\n\u001b[1m\u001b[94m365 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m366 |\u001b[0m         if metric.execution_time > self.thresholds[\"slow_function_ms\"] * 2:\n\u001b[1m\u001b[94m367 |\u001b[0m             score += 3\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:370:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m         elif metric.execution_time > self.thresholds[\"slow_function_ms\"]:\n\u001b[1m\u001b[94m369 |\u001b[0m             score += 2\n\u001b[1m\u001b[94m370 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m371 |\u001b[0m         if metric.memory_usage > self.thresholds[\"high_memory_mb\"] * 2:\n\u001b[1m\u001b[94m372 |\u001b[0m             score += 3\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:375:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m373 |\u001b[0m         elif metric.memory_usage > self.thresholds[\"high_memory_mb\"]:\n\u001b[1m\u001b[94m374 |\u001b[0m             score += 2\n\u001b[1m\u001b[94m375 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m376 |\u001b[0m         if metric.cpu_usage > self.thresholds[\"high_cpu_percent\"] * 1.5:\n\u001b[1m\u001b[94m377 |\u001b[0m             score += 3\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:380:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m         elif metric.cpu_usage > self.thresholds[\"high_cpu_percent\"]:\n\u001b[1m\u001b[94m379 |\u001b[0m             score += 2\n\u001b[1m\u001b[94m380 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m         if score >= 6:\n\u001b[1m\u001b[94m382 |\u001b[0m             return \"critical\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:389:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m387 |\u001b[0m         else:\n\u001b[1m\u001b[94m388 |\u001b[0m             return \"low\"\n\u001b[1m\u001b[94m389 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m390 |\u001b[0m     def _calculate_impact_score(self, metric: PerformanceMetrics) -> float:\n\u001b[1m\u001b[94m391 |\u001b[0m         \"\"\"Calculate impact score for prioritization.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:397:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m395 |\u001b[0m         cpu_weight = 0.2\n\u001b[1m\u001b[94m396 |\u001b[0m         frequency_weight = 0.1\n\u001b[1m\u001b[94m397 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m398 |\u001b[0m         # Normalize values (simplified)\n\u001b[1m\u001b[94m399 |\u001b[0m         time_score = min(metric.execution_time / (self.thresholds[\"slow_function_ms\"] * 2), 1.0)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:403:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m401 |\u001b[0m         cpu_score = min(metric.cpu_usage / (self.thresholds[\"high_cpu_percent\"] * 1.5), 1.0)\n\u001b[1m\u001b[94m402 |\u001b[0m         frequency_score = min(metric.call_count / (self.thresholds[\"frequent_calls\"] * 2), 1.0)\n\u001b[1m\u001b[94m403 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m404 |\u001b[0m         return (time_score * time_weight + \n\u001b[1m\u001b[94m405 |\u001b[0m                 memory_score * memory_weight + \n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:404:43\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m402 |\u001b[0m         frequency_score = min(metric.call_count / (self.thresholds[\"frequent_calls\"] * 2), 1.0)\n\u001b[1m\u001b[94m403 |\u001b[0m         \n\u001b[1m\u001b[94m404 |\u001b[0m         return (time_score * time_weight + \n    \u001b[1m\u001b[94m|\u001b[0m                                           \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m405 |\u001b[0m                 memory_score * memory_weight + \n\u001b[1m\u001b[94m406 |\u001b[0m                 cpu_score * cpu_weight + \n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:405:47\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m404 |\u001b[0m         return (time_score * time_weight + \n\u001b[1m\u001b[94m405 |\u001b[0m                 memory_score * memory_weight + \n    \u001b[1m\u001b[94m|\u001b[0m                                               \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m406 |\u001b[0m                 cpu_score * cpu_weight + \n\u001b[1m\u001b[94m407 |\u001b[0m                 frequency_score * frequency_weight)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:406:41\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m404 |\u001b[0m         return (time_score * time_weight + \n\u001b[1m\u001b[94m405 |\u001b[0m                 memory_score * memory_weight + \n\u001b[1m\u001b[94m406 |\u001b[0m                 cpu_score * cpu_weight + \n    \u001b[1m\u001b[94m|\u001b[0m                                         \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m                 frequency_score * frequency_weight)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:408:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m406 |\u001b[0m                 cpu_score * cpu_weight + \n\u001b[1m\u001b[94m407 |\u001b[0m                 frequency_score * frequency_weight)\n\u001b[1m\u001b[94m408 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m409 |\u001b[0m     def _generate_recommendations(self, metrics: List[PerformanceMetrics], \n\u001b[1m\u001b[94m410 |\u001b[0m                                 bottlenecks: List[Dict[str, Any]]) -> List[str]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:409:75\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m                 frequency_score * frequency_weight)\n\u001b[1m\u001b[94m408 |\u001b[0m     \n\u001b[1m\u001b[94m409 |\u001b[0m     def _generate_recommendations(self, metrics: List[PerformanceMetrics], \n    \u001b[1m\u001b[94m|\u001b[0m                                                                           \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m                                 bottlenecks: List[Dict[str, Any]]) -> List[str]:\n\u001b[1m\u001b[94m411 |\u001b[0m         \"\"\"Generate performance optimization recommendations.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:413:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m411 |\u001b[0m         \"\"\"Generate performance optimization recommendations.\"\"\"\n\u001b[1m\u001b[94m412 |\u001b[0m         recommendations = []\n\u001b[1m\u001b[94m413 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m414 |\u001b[0m         # General recommendations based on bottlenecks\n\u001b[1m\u001b[94m415 |\u001b[0m         if bottlenecks:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:421:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m419 |\u001b[0m                     f\"Address {len(critical_bottlenecks)} critical performance bottlenecks immediately\"\n\u001b[1m\u001b[94m420 |\u001b[0m                 )\n\u001b[1m\u001b[94m421 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m422 |\u001b[0m             high_bottlenecks = [b for b in bottlenecks if b[\"severity\"] == \"high\"]\n\u001b[1m\u001b[94m423 |\u001b[0m             if high_bottlenecks:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:427:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m425 |\u001b[0m                     f\"Review and optimize {len(high_bottlenecks)} high-priority bottlenecks\"\n\u001b[1m\u001b[94m426 |\u001b[0m                 )\n\u001b[1m\u001b[94m427 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m428 |\u001b[0m         # Specific recommendations\n\u001b[1m\u001b[94m429 |\u001b[0m         slow_functions = [m for m in metrics if m.execution_time > self.thresholds[\"slow_function_ms\"]]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:434:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m432 |\u001b[0m                 f\"Optimize {len(slow_functions)} slow functions (>{self.thresholds['slow_function_ms']}ms)\"\n\u001b[1m\u001b[94m433 |\u001b[0m             )\n\u001b[1m\u001b[94m434 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m435 |\u001b[0m         memory_intensive = [m for m in metrics if m.memory_usage > self.thresholds[\"high_memory_mb\"]]\n\u001b[1m\u001b[94m436 |\u001b[0m         if memory_intensive:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:440:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m438 |\u001b[0m                 f\"Optimize memory usage in {len(memory_intensive)} memory-intensive functions\"\n\u001b[1m\u001b[94m439 |\u001b[0m             )\n\u001b[1m\u001b[94m440 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m441 |\u001b[0m         frequent_calls = [m for m in metrics if m.call_count > self.thresholds[\"frequent_calls\"]]\n\u001b[1m\u001b[94m442 |\u001b[0m         if frequent_calls:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:446:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m444 |\u001b[0m                 f\"Consider caching or optimization for {len(frequent_calls)} frequently called functions\"\n\u001b[1m\u001b[94m445 |\u001b[0m             )\n\u001b[1m\u001b[94m446 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m447 |\u001b[0m         # Top function recommendation\n\u001b[1m\u001b[94m448 |\u001b[0m         if metrics:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:454:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m452 |\u001b[0m                 f\"(takes {top_function.execution_time:.2f}ms, {top_function.call_count} calls)\"\n\u001b[1m\u001b[94m453 |\u001b[0m             )\n\u001b[1m\u001b[94m454 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m455 |\u001b[0m         # Add general recommendations\n\u001b[1m\u001b[94m456 |\u001b[0m         recommendations.extend([\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:463:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m461 |\u001b[0m             \"Implement lazy evaluation where appropriate\"\n\u001b[1m\u001b[94m462 |\u001b[0m         ])\n\u001b[1m\u001b[94m463 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m         return recommendations\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:465:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m         return recommendations\n\u001b[1m\u001b[94m465 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m466 |\u001b[0m     def _get_memory_usage(self) -> float:\n\u001b[1m\u001b[94m467 |\u001b[0m         \"\"\"Get current memory usage in MB.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:470:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m468 |\u001b[0m         if not self.enable_memory_tracking:\n\u001b[1m\u001b[94m469 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m470 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m471 |\u001b[0m         try:\n\u001b[1m\u001b[94m472 |\u001b[0m             process = psutil.Process()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:476:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m474 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m475 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m476 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m477 |\u001b[0m     def _get_cpu_usage(self) -> float:\n\u001b[1m\u001b[94m478 |\u001b[0m         \"\"\"Get current CPU usage percentage.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:481:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m479 |\u001b[0m         if not self.enable_cpu_tracking:\n\u001b[1m\u001b[94m480 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m481 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m482 |\u001b[0m         try:\n\u001b[1m\u001b[94m483 |\u001b[0m             process = psutil.Process()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:487:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m485 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m486 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m487 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m488 |\u001b[0m     def _estimate_size(self, obj: Any) -> int:\n\u001b[1m\u001b[94m489 |\u001b[0m         \"\"\"Estimate the size of an object in bytes.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:495:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m493 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m494 |\u001b[0m             return 0\n\u001b[1m\u001b[94m495 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m496 |\u001b[0m     def get_memory_snapshot(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m497 |\u001b[0m         \"\"\"Get detailed memory snapshot.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:500:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m498 |\u001b[0m         if not self.enable_memory_tracking:\n\u001b[1m\u001b[94m499 |\u001b[0m             return {}\n\u001b[1m\u001b[94m500 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m501 |\u001b[0m         try:\n\u001b[1m\u001b[94m502 |\u001b[0m             snapshot = tracemalloc.take_snapshot()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:504:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m502 |\u001b[0m             snapshot = tracemalloc.take_snapshot()\n\u001b[1m\u001b[94m503 |\u001b[0m             top_stats = snapshot.statistics('lineno')\n\u001b[1m\u001b[94m504 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m505 |\u001b[0m             return {\n\u001b[1m\u001b[94m506 |\u001b[0m                 \"current_memory_mb\": self._get_memory_usage(),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:519:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m517 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m518 |\u001b[0m             return {\"error\": str(e)}\n\u001b[1m\u001b[94m519 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m520 |\u001b[0m     def reset_metrics(self) -> None:\n\u001b[1m\u001b[94m521 |\u001b[0m         \"\"\"Reset all performance metrics.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:524:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m522 |\u001b[0m         self.metrics.clear()\n\u001b[1m\u001b[94m523 |\u001b[0m         self.active_profiles.clear()\n\u001b[1m\u001b[94m524 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m525 |\u001b[0m         # Restart memory tracking if enabled\n\u001b[1m\u001b[94m526 |\u001b[0m         if self.enable_memory_tracking:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:529:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m527 |\u001b[0m             tracemalloc.stop()\n\u001b[1m\u001b[94m528 |\u001b[0m             tracemalloc.start()\n\u001b[1m\u001b[94m529 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m530 |\u001b[0m     def export_report(self, output_file: Optional[Path] = None) -> Path:\n\u001b[1m\u001b[94m531 |\u001b[0m         \"\"\"Export performance report to file.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:533:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m531 |\u001b[0m         \"\"\"Export performance report to file.\"\"\"\n\u001b[1m\u001b[94m532 |\u001b[0m         report = self.generate_report()\n\u001b[1m\u001b[94m533 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m534 |\u001b[0m         if output_file is None:\n\u001b[1m\u001b[94m535 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:537:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m535 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\u001b[1m\u001b[94m536 |\u001b[0m             output_file = Path(f\"performance_report_{timestamp}.json\")\n\u001b[1m\u001b[94m537 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m538 |\u001b[0m         # Convert to serializable format\n\u001b[1m\u001b[94m539 |\u001b[0m         report_data = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:561:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m559 |\u001b[0m             \"memory_snapshot\": self.get_memory_snapshot()\n\u001b[1m\u001b[94m560 |\u001b[0m         }\n\u001b[1m\u001b[94m561 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m562 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m563 |\u001b[0m             json.dump(report_data, f, indent=2, default=str)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:564:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m562 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m563 |\u001b[0m             json.dump(report_data, f, indent=2, default=str)\n\u001b[1m\u001b[94m564 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m565 |\u001b[0m         return output_file\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:566:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m565 |\u001b[0m         return output_file\n\u001b[1m\u001b[94m566 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m567 |\u001b[0m     def profile_with_cprofile(self, func: Callable, *args, **kwargs) -> Tuple[Any, str]:\n\u001b[1m\u001b[94m568 |\u001b[0m         \"\"\"Profile function using cProfile for detailed analysis.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:570:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m568 |\u001b[0m         \"\"\"Profile function using cProfile for detailed analysis.\"\"\"\n\u001b[1m\u001b[94m569 |\u001b[0m         profiler = cProfile.Profile()\n\u001b[1m\u001b[94m570 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m571 |\u001b[0m         try:\n\u001b[1m\u001b[94m572 |\u001b[0m             result = profiler.runcall(func, *args, **kwargs)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:575:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m573 |\u001b[0m         finally:\n\u001b[1m\u001b[94m574 |\u001b[0m             profiler.disable()\n\u001b[1m\u001b[94m575 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m576 |\u001b[0m         # Generate stats\n\u001b[1m\u001b[94m577 |\u001b[0m         s = io.StringIO()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:581:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m579 |\u001b[0m         ps.sort_stats('cumulative')\n\u001b[1m\u001b[94m580 |\u001b[0m         ps.print_stats(20)  # Top 20 functions\n\u001b[1m\u001b[94m581 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m582 |\u001b[0m         return result, s.getvalue()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:607:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m605 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m606 |\u001b[0m     import argparse\n\u001b[1m\u001b[94m607 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m608 |\u001b[0m     parser = argparse.ArgumentParser(description=\"SparkForge Performance Profiler\")\n\u001b[1m\u001b[94m609 |\u001b[0m     parser.add_argument(\"--function\", help=\"Function to profile\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:613:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m611 |\u001b[0m     parser.add_argument(\"--memory-tracking\", action=\"store_true\", help=\"Enable memory tracking\")\n\u001b[1m\u001b[94m612 |\u001b[0m     parser.add_argument(\"--cpu-tracking\", action=\"store_true\", help=\"Enable CPU tracking\")\n\u001b[1m\u001b[94m613 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m614 |\u001b[0m     args = parser.parse_args()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:615:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m614 |\u001b[0m     args = parser.parse_args()\n\u001b[1m\u001b[94m615 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m616 |\u001b[0m     profiler = PerformanceProfiler(\n\u001b[1m\u001b[94m617 |\u001b[0m         enable_memory_tracking=args.memory_tracking,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:620:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m618 |\u001b[0m         enable_cpu_tracking=args.cpu_tracking\n\u001b[1m\u001b[94m619 |\u001b[0m     )\n\u001b[1m\u001b[94m620 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m621 |\u001b[0m     if args.function:\n\u001b[1m\u001b[94m622 |\u001b[0m         # Profile specific function\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:633:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m631 |\u001b[0m         report_file = profiler.export_report(args.output)\n\u001b[1m\u001b[94m632 |\u001b[0m         print(f\"Performance report saved to: {report_file}\")\n\u001b[1m\u001b[94m633 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m634 |\u001b[0m         # Print summary\n\u001b[1m\u001b[94m635 |\u001b[0m         print(f\"\\nPerformance Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_profiler.py:635:15\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m634 |\u001b[0m         # Print summary\n\u001b[1m\u001b[94m635 |\u001b[0m         print(f\"\\nPerformance Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m636 |\u001b[0m         print(f\"Total execution time: {report.total_execution_time:.2f}ms\")\n\u001b[1m\u001b[94m637 |\u001b[0m         print(f\"Total memory usage: {report.total_memory_usage:.2f}MB\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import pytest\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ParallelConfig,\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     safe_divide,\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     validate_dataframe_schema,\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     assess_data_quality,\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     get_dataframe_info,\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m27 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from .performance_monitor import performance_monitor\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|____________________________________________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:32:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m30 |\u001b[0m class TestValidationPerformance:\n\u001b[1m\u001b[94m31 |\u001b[0m     \"\"\"Performance tests for validation functions.\"\"\"\n\u001b[1m\u001b[94m32 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m33 |\u001b[0m     @pytest.mark.performance\n\u001b[1m\u001b[94m34 |\u001b[0m     @pytest.mark.benchmark\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:38:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m36 |\u001b[0m         \"\"\"Test performance of safe_divide function.\"\"\"\n\u001b[1m\u001b[94m37 |\u001b[0m         iterations = 10000\n\u001b[1m\u001b[94m38 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m39 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m40 |\u001b[0m             safe_divide,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:46:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m44 |\u001b[0m             warmup_iterations=100\n\u001b[1m\u001b[94m45 |\u001b[0m         )\n\u001b[1m\u001b[94m46 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m47 |\u001b[0m         # Performance assertions\n\u001b[1m\u001b[94m48 |\u001b[0m         assert result.success\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:52:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m50 |\u001b[0m         assert result.avg_time_per_iteration < 0.1  # Each call should be < 0.1ms\n\u001b[1m\u001b[94m51 |\u001b[0m         assert result.throughput > 1000  # Should handle > 1000 calls/second\n\u001b[1m\u001b[94m52 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m53 |\u001b[0m         # Check for regression\n\u001b[1m\u001b[94m54 |\u001b[0m         regression = performance_monitor_clean.check_regression(\"safe_divide\")\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:57:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m55 |\u001b[0m         if regression[\"status\"] == \"regression_detected\":\n\u001b[1m\u001b[94m56 |\u001b[0m             pytest.fail(f\"Performance regression detected: {regression}\")\n\u001b[1m\u001b[94m57 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m     @pytest.mark.performance\n\u001b[1m\u001b[94m59 |\u001b[0m     def test_safe_divide_zero_denominator_performance(self) -> None:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:62:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m60 |\u001b[0m         \"\"\"Test performance of safe_divide with zero denominator.\"\"\"\n\u001b[1m\u001b[94m61 |\u001b[0m         iterations = 5000\n\u001b[1m\u001b[94m62 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m64 |\u001b[0m             safe_divide,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:70:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m68 |\u001b[0m             warmup_iterations=50\n\u001b[1m\u001b[94m69 |\u001b[0m         )\n\u001b[1m\u001b[94m70 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m72 |\u001b[0m         assert result.execution_time < 0.5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:74:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m72 |\u001b[0m         assert result.execution_time < 0.5\n\u001b[1m\u001b[94m73 |\u001b[0m         assert result.avg_time_per_iteration < 0.1\n\u001b[1m\u001b[94m74 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m75 |\u001b[0m     def test_validate_dataframe_schema_performance(self) -> None:\n\u001b[1m\u001b[94m76 |\u001b[0m         \"\"\"Test performance of validate_dataframe_schema function.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:78:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m76 |\u001b[0m         \"\"\"Test performance of validate_dataframe_schema function.\"\"\"\n\u001b[1m\u001b[94m77 |\u001b[0m         iterations = 1000\n\u001b[1m\u001b[94m78 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m         # Create mock DataFrame with many columns\n\u001b[1m\u001b[94m80 |\u001b[0m         mock_df = Mock()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:82:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m80 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m81 |\u001b[0m         mock_df.columns = [f\"col{i}\" for i in range(100)]\n\u001b[1m\u001b[94m82 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m         expected_columns = [f\"col{i}\" for i in range(50)]\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:84:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m         expected_columns = [f\"col{i}\" for i in range(50)]\n\u001b[1m\u001b[94m84 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m85 |\u001b[0m         def test_function():\n\u001b[1m\u001b[94m86 |\u001b[0m             return validate_dataframe_schema(mock_df, expected_columns)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:87:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m85 |\u001b[0m         def test_function():\n\u001b[1m\u001b[94m86 |\u001b[0m             return validate_dataframe_schema(mock_df, expected_columns)\n\u001b[1m\u001b[94m87 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m88 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m89 |\u001b[0m             test_function,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:94:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m             warmup_iterations=10\n\u001b[1m\u001b[94m93 |\u001b[0m         )\n\u001b[1m\u001b[94m94 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m95 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m96 |\u001b[0m         assert result.execution_time < 0.5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:98:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 96 |\u001b[0m         assert result.execution_time < 0.5\n\u001b[1m\u001b[94m 97 |\u001b[0m         assert result.avg_time_per_iteration < 1.0\n\u001b[1m\u001b[94m 98 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m 99 |\u001b[0m     def test_assess_data_quality_performance(self) -> None:\n\u001b[1m\u001b[94m100 |\u001b[0m         \"\"\"Test performance of assess_data_quality function.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:102:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m100 |\u001b[0m         \"\"\"Test performance of assess_data_quality function.\"\"\"\n\u001b[1m\u001b[94m101 |\u001b[0m         iterations = 100\n\u001b[1m\u001b[94m102 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m103 |\u001b[0m         # Create mock DataFrame\n\u001b[1m\u001b[94m104 |\u001b[0m         mock_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:106:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m104 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m105 |\u001b[0m         mock_df.count.return_value = 1000\n\u001b[1m\u001b[94m106 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m107 |\u001b[0m         rules = {\n\u001b[1m\u001b[94m108 |\u001b[0m             f\"col{i}\": [f\"col{i} > 0\", f\"col{i} IS NOT NULL\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:111:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m109 |\u001b[0m             for i in range(10)\n\u001b[1m\u001b[94m110 |\u001b[0m         }\n\u001b[1m\u001b[94m111 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m112 |\u001b[0m         def test_function():\n\u001b[1m\u001b[94m113 |\u001b[0m             return assess_data_quality(mock_df, rules)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:114:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m112 |\u001b[0m         def test_function():\n\u001b[1m\u001b[94m113 |\u001b[0m             return assess_data_quality(mock_df, rules)\n\u001b[1m\u001b[94m114 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m115 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m116 |\u001b[0m             test_function,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:121:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m             warmup_iterations=5\n\u001b[1m\u001b[94m120 |\u001b[0m         )\n\u001b[1m\u001b[94m121 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m122 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m123 |\u001b[0m         assert result.execution_time < 2.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:125:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         assert result.execution_time < 2.0\n\u001b[1m\u001b[94m124 |\u001b[0m         assert result.avg_time_per_iteration < 20.0\n\u001b[1m\u001b[94m125 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m     def test_get_dataframe_info_performance(self) -> None:\n\u001b[1m\u001b[94m127 |\u001b[0m         \"\"\"Test performance of get_dataframe_info function.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:129:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m127 |\u001b[0m         \"\"\"Test performance of get_dataframe_info function.\"\"\"\n\u001b[1m\u001b[94m128 |\u001b[0m         iterations = 500\n\u001b[1m\u001b[94m129 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m130 |\u001b[0m         # Create mock DataFrame\n\u001b[1m\u001b[94m131 |\u001b[0m         mock_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:134:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m132 |\u001b[0m         mock_df.count.return_value = 10000\n\u001b[1m\u001b[94m133 |\u001b[0m         mock_df.columns = [f\"col{i}\" for i in range(50)]\n\u001b[1m\u001b[94m134 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m135 |\u001b[0m         # Create mock schema\n\u001b[1m\u001b[94m136 |\u001b[0m         mock_schema = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:139:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m137 |\u001b[0m         mock_schema.__str__ = Mock(return_value=\"struct<col1:string,col2:string>\")\n\u001b[1m\u001b[94m138 |\u001b[0m         mock_df.schema = mock_schema\n\u001b[1m\u001b[94m139 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m140 |\u001b[0m         def test_function():\n\u001b[1m\u001b[94m141 |\u001b[0m             return get_dataframe_info(mock_df)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:142:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m140 |\u001b[0m         def test_function():\n\u001b[1m\u001b[94m141 |\u001b[0m             return get_dataframe_info(mock_df)\n\u001b[1m\u001b[94m142 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m143 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m144 |\u001b[0m             test_function,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:149:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m             warmup_iterations=5\n\u001b[1m\u001b[94m148 |\u001b[0m         )\n\u001b[1m\u001b[94m149 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m151 |\u001b[0m         assert result.execution_time < 1.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:157:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m155 |\u001b[0m class TestModelCreationPerformance:\n\u001b[1m\u001b[94m156 |\u001b[0m     \"\"\"Performance tests for model creation operations.\"\"\"\n\u001b[1m\u001b[94m157 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m158 |\u001b[0m     @pytest.mark.performance\n\u001b[1m\u001b[94m159 |\u001b[0m     @pytest.mark.benchmark\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:163:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m161 |\u001b[0m         \"\"\"Test performance of ValidationThresholds creation.\"\"\"\n\u001b[1m\u001b[94m162 |\u001b[0m         iterations = 10000\n\u001b[1m\u001b[94m163 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m164 |\u001b[0m         def create_thresholds():\n\u001b[1m\u001b[94m165 |\u001b[0m             return ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:166:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m164 |\u001b[0m         def create_thresholds():\n\u001b[1m\u001b[94m165 |\u001b[0m             return ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n\u001b[1m\u001b[94m166 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m167 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m168 |\u001b[0m             create_thresholds,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:173:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m171 |\u001b[0m             warmup_iterations=100\n\u001b[1m\u001b[94m172 |\u001b[0m         )\n\u001b[1m\u001b[94m173 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m174 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m175 |\u001b[0m         assert result.execution_time < 1.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:178:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m176 |\u001b[0m         assert result.avg_time_per_iteration < 0.1\n\u001b[1m\u001b[94m177 |\u001b[0m         assert result.throughput > 5000\n\u001b[1m\u001b[94m178 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m179 |\u001b[0m     def test_parallel_config_creation_performance(self) -> None:\n\u001b[1m\u001b[94m180 |\u001b[0m         \"\"\"Test performance of ParallelConfig creation.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:182:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m180 |\u001b[0m         \"\"\"Test performance of ParallelConfig creation.\"\"\"\n\u001b[1m\u001b[94m181 |\u001b[0m         iterations = 10000\n\u001b[1m\u001b[94m182 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m         def create_parallel_config():\n\u001b[1m\u001b[94m184 |\u001b[0m             return ParallelConfig(enabled=True, max_workers=4)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:185:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m         def create_parallel_config():\n\u001b[1m\u001b[94m184 |\u001b[0m             return ParallelConfig(enabled=True, max_workers=4)\n\u001b[1m\u001b[94m185 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m187 |\u001b[0m             create_parallel_config,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:192:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m             warmup_iterations=100\n\u001b[1m\u001b[94m191 |\u001b[0m         )\n\u001b[1m\u001b[94m192 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m194 |\u001b[0m         assert result.execution_time < 1.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:196:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m194 |\u001b[0m         assert result.execution_time < 1.0\n\u001b[1m\u001b[94m195 |\u001b[0m         assert result.avg_time_per_iteration < 0.1\n\u001b[1m\u001b[94m196 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m     def test_pipeline_config_creation_performance(self) -> None:\n\u001b[1m\u001b[94m198 |\u001b[0m         \"\"\"Test performance of PipelineConfig creation.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:200:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m198 |\u001b[0m         \"\"\"Test performance of PipelineConfig creation.\"\"\"\n\u001b[1m\u001b[94m199 |\u001b[0m         iterations = 1000\n\u001b[1m\u001b[94m200 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m         def create_pipeline_config():\n\u001b[1m\u001b[94m202 |\u001b[0m             return PipelineConfig.create_default(\"test_schema\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:203:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m         def create_pipeline_config():\n\u001b[1m\u001b[94m202 |\u001b[0m             return PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m203 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m205 |\u001b[0m             create_pipeline_config,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:210:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m208 |\u001b[0m             warmup_iterations=10\n\u001b[1m\u001b[94m209 |\u001b[0m         )\n\u001b[1m\u001b[94m210 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m211 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m212 |\u001b[0m         assert result.execution_time < 0.5\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:214:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m212 |\u001b[0m         assert result.execution_time < 0.5\n\u001b[1m\u001b[94m213 |\u001b[0m         assert result.avg_time_per_iteration < 0.5\n\u001b[1m\u001b[94m214 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m215 |\u001b[0m     def test_bronze_step_creation_performance(self) -> None:\n\u001b[1m\u001b[94m216 |\u001b[0m         \"\"\"Test performance of BronzeStep creation.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:218:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m216 |\u001b[0m         \"\"\"Test performance of BronzeStep creation.\"\"\"\n\u001b[1m\u001b[94m217 |\u001b[0m         iterations = 5000\n\u001b[1m\u001b[94m218 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m219 |\u001b[0m         rules = {\n\u001b[1m\u001b[94m220 |\u001b[0m             f\"col{i}\": [f\"col{i} > 0\", f\"col{i} IS NOT NULL\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:223:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m221 |\u001b[0m             for i in range(5)\n\u001b[1m\u001b[94m222 |\u001b[0m         }\n\u001b[1m\u001b[94m223 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m224 |\u001b[0m         def create_bronze_step():\n\u001b[1m\u001b[94m225 |\u001b[0m             return BronzeStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:230:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m228 |\u001b[0m                 incremental_col=\"updated_at\"\n\u001b[1m\u001b[94m229 |\u001b[0m             )\n\u001b[1m\u001b[94m230 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m232 |\u001b[0m             create_bronze_step,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:237:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m             warmup_iterations=50\n\u001b[1m\u001b[94m236 |\u001b[0m         )\n\u001b[1m\u001b[94m237 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m238 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m239 |\u001b[0m         assert result.execution_time < 1.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:241:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m239 |\u001b[0m         assert result.execution_time < 1.0\n\u001b[1m\u001b[94m240 |\u001b[0m         assert result.avg_time_per_iteration < 0.2\n\u001b[1m\u001b[94m241 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m242 |\u001b[0m     def test_silver_step_creation_performance(self) -> None:\n\u001b[1m\u001b[94m243 |\u001b[0m         \"\"\"Test performance of SilverStep creation.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:245:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m243 |\u001b[0m         \"\"\"Test performance of SilverStep creation.\"\"\"\n\u001b[1m\u001b[94m244 |\u001b[0m         iterations = 2000\n\u001b[1m\u001b[94m245 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m246 |\u001b[0m         def mock_transform(spark, bronze_df, prior_silvers):\n\u001b[1m\u001b[94m247 |\u001b[0m             return bronze_df\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:248:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m246 |\u001b[0m         def mock_transform(spark, bronze_df, prior_silvers):\n\u001b[1m\u001b[94m247 |\u001b[0m             return bronze_df\n\u001b[1m\u001b[94m248 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m249 |\u001b[0m         rules = {\n\u001b[1m\u001b[94m250 |\u001b[0m             f\"col{i}\": [f\"col{i} > 0\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:253:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m251 |\u001b[0m             for i in range(3)\n\u001b[1m\u001b[94m252 |\u001b[0m         }\n\u001b[1m\u001b[94m253 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m254 |\u001b[0m         def create_silver_step():\n\u001b[1m\u001b[94m255 |\u001b[0m             return SilverStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:262:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m                 table_name=\"silver_table\"\n\u001b[1m\u001b[94m261 |\u001b[0m             )\n\u001b[1m\u001b[94m262 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m264 |\u001b[0m             create_silver_step,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:269:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m267 |\u001b[0m             warmup_iterations=20\n\u001b[1m\u001b[94m268 |\u001b[0m         )\n\u001b[1m\u001b[94m269 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m270 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m271 |\u001b[0m         assert result.execution_time < 1.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:273:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m         assert result.execution_time < 1.0\n\u001b[1m\u001b[94m272 |\u001b[0m         assert result.avg_time_per_iteration < 0.5\n\u001b[1m\u001b[94m273 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m274 |\u001b[0m     def test_gold_step_creation_performance(self) -> None:\n\u001b[1m\u001b[94m275 |\u001b[0m         \"\"\"Test performance of GoldStep creation.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:277:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m275 |\u001b[0m         \"\"\"Test performance of GoldStep creation.\"\"\"\n\u001b[1m\u001b[94m276 |\u001b[0m         iterations = 2000\n\u001b[1m\u001b[94m277 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m278 |\u001b[0m         def mock_transform(spark, silver_dfs):\n\u001b[1m\u001b[94m279 |\u001b[0m             return silver_dfs\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:280:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m278 |\u001b[0m         def mock_transform(spark, silver_dfs):\n\u001b[1m\u001b[94m279 |\u001b[0m             return silver_dfs\n\u001b[1m\u001b[94m280 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m         rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m282 |\u001b[0m         source_silvers = [\"silver1\", \"silver2\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:283:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m         rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m282 |\u001b[0m         source_silvers = [\"silver1\", \"silver2\"]\n\u001b[1m\u001b[94m283 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m284 |\u001b[0m         def create_gold_step():\n\u001b[1m\u001b[94m285 |\u001b[0m             return GoldStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:292:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m290 |\u001b[0m                 source_silvers=source_silvers\n\u001b[1m\u001b[94m291 |\u001b[0m             )\n\u001b[1m\u001b[94m292 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m293 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m294 |\u001b[0m             create_gold_step,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:299:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m297 |\u001b[0m             warmup_iterations=20\n\u001b[1m\u001b[94m298 |\u001b[0m         )\n\u001b[1m\u001b[94m299 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m300 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m301 |\u001b[0m         assert result.execution_time < 1.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:307:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m305 |\u001b[0m class TestSerializationPerformance:\n\u001b[1m\u001b[94m306 |\u001b[0m     \"\"\"Performance tests for serialization operations.\"\"\"\n\u001b[1m\u001b[94m307 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m308 |\u001b[0m     def test_model_to_dict_performance(self) -> None:\n\u001b[1m\u001b[94m309 |\u001b[0m         \"\"\"Test performance of model.to_dict() serialization.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:311:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m309 |\u001b[0m         \"\"\"Test performance of model.to_dict() serialization.\"\"\"\n\u001b[1m\u001b[94m310 |\u001b[0m         iterations = 1000\n\u001b[1m\u001b[94m311 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m312 |\u001b[0m         # Create a complex model\n\u001b[1m\u001b[94m313 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:314:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m312 |\u001b[0m         # Create a complex model\n\u001b[1m\u001b[94m313 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m314 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m315 |\u001b[0m         def serialize_model():\n\u001b[1m\u001b[94m316 |\u001b[0m             return config.to_dict()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:317:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m315 |\u001b[0m         def serialize_model():\n\u001b[1m\u001b[94m316 |\u001b[0m             return config.to_dict()\n\u001b[1m\u001b[94m317 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m318 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m319 |\u001b[0m             serialize_model,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:324:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m322 |\u001b[0m             warmup_iterations=10\n\u001b[1m\u001b[94m323 |\u001b[0m         )\n\u001b[1m\u001b[94m324 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m325 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m326 |\u001b[0m         assert result.execution_time < 0.5\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:328:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m326 |\u001b[0m         assert result.execution_time < 0.5\n\u001b[1m\u001b[94m327 |\u001b[0m         assert result.avg_time_per_iteration < 0.5\n\u001b[1m\u001b[94m328 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m329 |\u001b[0m     def test_model_to_json_performance(self) -> None:\n\u001b[1m\u001b[94m330 |\u001b[0m         \"\"\"Test performance of model.to_json() serialization.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:332:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m330 |\u001b[0m         \"\"\"Test performance of model.to_json() serialization.\"\"\"\n\u001b[1m\u001b[94m331 |\u001b[0m         iterations = 1000\n\u001b[1m\u001b[94m332 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m333 |\u001b[0m         # Create a complex model\n\u001b[1m\u001b[94m334 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:335:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m333 |\u001b[0m         # Create a complex model\n\u001b[1m\u001b[94m334 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m335 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m336 |\u001b[0m         def serialize_model():\n\u001b[1m\u001b[94m337 |\u001b[0m             return config.to_json()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:338:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m336 |\u001b[0m         def serialize_model():\n\u001b[1m\u001b[94m337 |\u001b[0m             return config.to_json()\n\u001b[1m\u001b[94m338 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m339 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m340 |\u001b[0m             serialize_model,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:345:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m343 |\u001b[0m             warmup_iterations=10\n\u001b[1m\u001b[94m344 |\u001b[0m         )\n\u001b[1m\u001b[94m345 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m346 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m347 |\u001b[0m         assert result.execution_time < 1.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:349:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m347 |\u001b[0m         assert result.execution_time < 1.0\n\u001b[1m\u001b[94m348 |\u001b[0m         assert result.avg_time_per_iteration < 1.0\n\u001b[1m\u001b[94m349 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m350 |\u001b[0m     def test_model_validation_performance(self) -> None:\n\u001b[1m\u001b[94m351 |\u001b[0m         \"\"\"Test performance of model validation.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:353:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m351 |\u001b[0m         \"\"\"Test performance of model validation.\"\"\"\n\u001b[1m\u001b[94m352 |\u001b[0m         iterations = 500\n\u001b[1m\u001b[94m353 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m354 |\u001b[0m         # Create a model\n\u001b[1m\u001b[94m355 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:356:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m354 |\u001b[0m         # Create a model\n\u001b[1m\u001b[94m355 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m356 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m357 |\u001b[0m         def validate_model():\n\u001b[1m\u001b[94m358 |\u001b[0m             config.validate()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:359:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m357 |\u001b[0m         def validate_model():\n\u001b[1m\u001b[94m358 |\u001b[0m             config.validate()\n\u001b[1m\u001b[94m359 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m360 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m361 |\u001b[0m             validate_model,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:366:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m364 |\u001b[0m             warmup_iterations=5\n\u001b[1m\u001b[94m365 |\u001b[0m         )\n\u001b[1m\u001b[94m366 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m367 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m368 |\u001b[0m         assert result.execution_time < 0.5\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:374:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m372 |\u001b[0m class TestMemoryUsagePerformance:\n\u001b[1m\u001b[94m373 |\u001b[0m     \"\"\"Performance tests focusing on memory usage.\"\"\"\n\u001b[1m\u001b[94m374 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m375 |\u001b[0m     @pytest.mark.performance\n\u001b[1m\u001b[94m376 |\u001b[0m     @pytest.mark.memory\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:380:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m         \"\"\"Test memory usage of model creation.\"\"\"\n\u001b[1m\u001b[94m379 |\u001b[0m         iterations = 1000\n\u001b[1m\u001b[94m380 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m         def create_models():\n\u001b[1m\u001b[94m382 |\u001b[0m             models = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:386:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m384 |\u001b[0m                 models.append(PipelineConfig.create_default(f\"schema_{i}\"))\n\u001b[1m\u001b[94m385 |\u001b[0m             return models\n\u001b[1m\u001b[94m386 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m387 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m388 |\u001b[0m             create_models,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:393:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m             warmup_iterations=5\n\u001b[1m\u001b[94m392 |\u001b[0m         )\n\u001b[1m\u001b[94m393 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m394 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m395 |\u001b[0m         assert result.memory_usage_mb < memory_limit_mb  # Should use < limit\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:397:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m395 |\u001b[0m         assert result.memory_usage_mb < memory_limit_mb  # Should use < limit\n\u001b[1m\u001b[94m396 |\u001b[0m         assert result.peak_memory_mb < memory_limit_mb * 2   # Peak should be < 2x limit\n\u001b[1m\u001b[94m397 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m398 |\u001b[0m     def test_serialization_memory_usage(self) -> None:\n\u001b[1m\u001b[94m399 |\u001b[0m         \"\"\"Test memory usage of serialization operations.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:401:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m399 |\u001b[0m         \"\"\"Test memory usage of serialization operations.\"\"\"\n\u001b[1m\u001b[94m400 |\u001b[0m         iterations = 500\n\u001b[1m\u001b[94m401 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m402 |\u001b[0m         # Create a complex model\n\u001b[1m\u001b[94m403 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:404:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m402 |\u001b[0m         # Create a complex model\n\u001b[1m\u001b[94m403 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m404 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m405 |\u001b[0m         def serialize_and_deserialize():\n\u001b[1m\u001b[94m406 |\u001b[0m             data = config.to_dict()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:409:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m             json_str = config.to_json()\n\u001b[1m\u001b[94m408 |\u001b[0m             return data, json_str\n\u001b[1m\u001b[94m409 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m         result = performance_monitor.benchmark_function(\n\u001b[1m\u001b[94m411 |\u001b[0m             serialize_and_deserialize,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:416:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m414 |\u001b[0m             warmup_iterations=5\n\u001b[1m\u001b[94m415 |\u001b[0m         )\n\u001b[1m\u001b[94m416 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m417 |\u001b[0m         assert result.success\n\u001b[1m\u001b[94m418 |\u001b[0m         assert result.memory_usage_mb < 50   # Should use < 50MB\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:425:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m423 |\u001b[0m     \"\"\"Test that generates a performance summary.\"\"\"\n\u001b[1m\u001b[94m424 |\u001b[0m     summary = performance_monitor.get_performance_summary()\n\u001b[1m\u001b[94m425 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m426 |\u001b[0m     assert \"total_tests\" in summary\n\u001b[1m\u001b[94m427 |\u001b[0m     assert \"successful_tests\" in summary\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:430:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m428 |\u001b[0m     assert \"functions_tested\" in summary\n\u001b[1m\u001b[94m429 |\u001b[0m     assert summary[\"total_tests\"] > 0\n\u001b[1m\u001b[94m430 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m431 |\u001b[0m     # Print summary for manual review\n\u001b[1m\u001b[94m432 |\u001b[0m     print(f\"\\nPerformance Test Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:432:11\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m431 |\u001b[0m     # Print summary for manual review\n\u001b[1m\u001b[94m432 |\u001b[0m     print(f\"\\nPerformance Test Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m           \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m433 |\u001b[0m     print(f\"Total tests: {summary['total_tests']}\")\n\u001b[1m\u001b[94m434 |\u001b[0m     print(f\"Successful tests: {summary['successful_tests']}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:445:28\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m443 |\u001b[0m     test_functions = [\n\u001b[1m\u001b[94m444 |\u001b[0m         \"safe_divide\",\n\u001b[1m\u001b[94m445 |\u001b[0m         \"safe_divide_zero\", \n    \u001b[1m\u001b[94m|\u001b[0m                            \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m446 |\u001b[0m         \"validate_dataframe_schema\",\n\u001b[1m\u001b[94m447 |\u001b[0m         \"assess_data_quality\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:461:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m459 |\u001b[0m         \"serialization_memory\"\n\u001b[1m\u001b[94m460 |\u001b[0m     ]\n\u001b[1m\u001b[94m461 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m462 |\u001b[0m     for func_name in test_functions:\n\u001b[1m\u001b[94m463 |\u001b[0m         performance_monitor.update_baseline(func_name)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/performance_tests.py:464:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m462 |\u001b[0m     for func_name in test_functions:\n\u001b[1m\u001b[94m463 |\u001b[0m         performance_monitor.update_baseline(func_name)\n\u001b[1m\u001b[94m464 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m465 |\u001b[0m     print(\"Performance baselines updated successfully\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:12:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import time\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import tempfile\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m # Import performance components\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.performance.performance_profiler import PerformanceProfiler, profile_function, quick_profile\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.performance.caching_strategies import (\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     MemoryCache, PersistentCache, HybridCache, cache_result, cache_dataframe,\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     cache_manager\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.performance.performance_monitoring import (\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PerformanceMonitor, get_performance_monitor, record_metric,\n\u001b[1m\u001b[94m27 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     start_performance_monitoring, stop_performance_monitoring\n\u001b[1m\u001b[94m28 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n\u001b[1m\u001b[94m29 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.performance.memory_optimization import (\n\u001b[1m\u001b[94m30 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     MemoryProfiler, MemoryOptimizer, memory_monitor, optimize_spark_memory\n\u001b[1m\u001b[94m31 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n\u001b[1m\u001b[94m32 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.performance.performance_benchmarking import (\n\u001b[1m\u001b[94m33 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PerformanceBenchmark, benchmark, compare_benchmarks, load_test\n\u001b[1m\u001b[94m34 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.Mock` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:16:27\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m import tempfile\n\u001b[1m\u001b[94m15 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m from unittest.mock import Mock, patch\n   \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.patch` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:16:33\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m import tempfile\n\u001b[1m\u001b[94m15 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m from unittest.mock import Mock, patch\n   \u001b[1m\u001b[94m|\u001b[0m                                 \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`datetime.datetime` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:17:22\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m17 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m                      \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m # Import performance components\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`datetime.timedelta` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:17:32\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m17 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m # Import performance components\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.performance_profiler.quick_profile` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:20:91\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m # Import performance components\n\u001b[1m\u001b[94m20 |\u001b[0m from tests.performance.performance_profiler import PerformanceProfiler, profile_function, quick_profile\n   \u001b[1m\u001b[94m|\u001b[0m                                                                                           \u001b[1m\u001b[91m^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m from tests.performance.caching_strategies import (\n\u001b[1m\u001b[94m22 |\u001b[0m     MemoryCache, PersistentCache, HybridCache, cache_result, cache_dataframe,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `tests.performance.performance_profiler.quick_profile`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.caching_strategies.cache_dataframe` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:22:62\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m from tests.performance.performance_profiler import PerformanceProfiler, profile_function, quick_profile\n\u001b[1m\u001b[94m21 |\u001b[0m from tests.performance.caching_strategies import (\n\u001b[1m\u001b[94m22 |\u001b[0m     MemoryCache, PersistentCache, HybridCache, cache_result, cache_dataframe,\n   \u001b[1m\u001b[94m|\u001b[0m                                                              \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m23 |\u001b[0m     cache_manager\n\u001b[1m\u001b[94m24 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `tests.performance.caching_strategies.cache_dataframe`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.performance_monitoring.get_performance_monitor` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:26:25\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m24 |\u001b[0m )\n\u001b[1m\u001b[94m25 |\u001b[0m from tests.performance.performance_monitoring import (\n\u001b[1m\u001b[94m26 |\u001b[0m     PerformanceMonitor, get_performance_monitor, record_metric,\n   \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m27 |\u001b[0m     start_performance_monitoring, stop_performance_monitoring\n\u001b[1m\u001b[94m28 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.performance_monitoring.record_metric` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:26:50\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m24 |\u001b[0m )\n\u001b[1m\u001b[94m25 |\u001b[0m from tests.performance.performance_monitoring import (\n\u001b[1m\u001b[94m26 |\u001b[0m     PerformanceMonitor, get_performance_monitor, record_metric,\n   \u001b[1m\u001b[94m|\u001b[0m                                                  \u001b[1m\u001b[91m^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m27 |\u001b[0m     start_performance_monitoring, stop_performance_monitoring\n\u001b[1m\u001b[94m28 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.performance_monitoring.start_performance_monitoring` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:27:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m from tests.performance.performance_monitoring import (\n\u001b[1m\u001b[94m26 |\u001b[0m     PerformanceMonitor, get_performance_monitor, record_metric,\n\u001b[1m\u001b[94m27 |\u001b[0m     start_performance_monitoring, stop_performance_monitoring\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m28 |\u001b[0m )\n\u001b[1m\u001b[94m29 |\u001b[0m from tests.performance.memory_optimization import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.performance_monitoring.stop_performance_monitoring` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:27:35\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m from tests.performance.performance_monitoring import (\n\u001b[1m\u001b[94m26 |\u001b[0m     PerformanceMonitor, get_performance_monitor, record_metric,\n\u001b[1m\u001b[94m27 |\u001b[0m     start_performance_monitoring, stop_performance_monitoring\n   \u001b[1m\u001b[94m|\u001b[0m                                   \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m28 |\u001b[0m )\n\u001b[1m\u001b[94m29 |\u001b[0m from tests.performance.memory_optimization import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.memory_optimization.memory_monitor` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:30:38\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m28 |\u001b[0m )\n\u001b[1m\u001b[94m29 |\u001b[0m from tests.performance.memory_optimization import (\n\u001b[1m\u001b[94m30 |\u001b[0m     MemoryProfiler, MemoryOptimizer, memory_monitor, optimize_spark_memory\n   \u001b[1m\u001b[94m|\u001b[0m                                      \u001b[1m\u001b[91m^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m31 |\u001b[0m )\n\u001b[1m\u001b[94m32 |\u001b[0m from tests.performance.performance_benchmarking import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `tests.performance.memory_optimization.memory_monitor`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.performance_benchmarking.benchmark` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:33:27\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m31 |\u001b[0m )\n\u001b[1m\u001b[94m32 |\u001b[0m from tests.performance.performance_benchmarking import (\n\u001b[1m\u001b[94m33 |\u001b[0m     PerformanceBenchmark, benchmark, compare_benchmarks, load_test\n   \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m34 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.performance_benchmarking.compare_benchmarks` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:33:38\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m31 |\u001b[0m )\n\u001b[1m\u001b[94m32 |\u001b[0m from tests.performance.performance_benchmarking import (\n\u001b[1m\u001b[94m33 |\u001b[0m     PerformanceBenchmark, benchmark, compare_benchmarks, load_test\n   \u001b[1m\u001b[94m|\u001b[0m                                      \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m34 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.performance.performance_benchmarking.load_test` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:33:58\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m31 |\u001b[0m )\n\u001b[1m\u001b[94m32 |\u001b[0m from tests.performance.performance_benchmarking import (\n\u001b[1m\u001b[94m33 |\u001b[0m     PerformanceBenchmark, benchmark, compare_benchmarks, load_test\n   \u001b[1m\u001b[94m|\u001b[0m                                                          \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m34 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:39:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m37 |\u001b[0m class TestPerformanceIntegration:\n\u001b[1m\u001b[94m38 |\u001b[0m     \"\"\"Integration tests for performance components.\"\"\"\n\u001b[1m\u001b[94m39 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m40 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m41 |\u001b[0m     def temp_dir(self):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:45:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m43 |\u001b[0m         with tempfile.TemporaryDirectory() as temp_dir:\n\u001b[1m\u001b[94m44 |\u001b[0m             yield Path(temp_dir)\n\u001b[1m\u001b[94m45 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m46 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m47 |\u001b[0m     def performance_profiler(self):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:50:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m48 |\u001b[0m         \"\"\"Create performance profiler instance.\"\"\"\n\u001b[1m\u001b[94m49 |\u001b[0m         return PerformanceProfiler(enable_memory_tracking=True, enable_cpu_tracking=True)\n\u001b[1m\u001b[94m50 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m51 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m52 |\u001b[0m     def performance_monitor(self):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:60:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m         }\n\u001b[1m\u001b[94m59 |\u001b[0m         return PerformanceMonitor(config)\n\u001b[1m\u001b[94m60 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m62 |\u001b[0m     def memory_profiler(self):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:65:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         \"\"\"Create memory profiler instance.\"\"\"\n\u001b[1m\u001b[94m64 |\u001b[0m         return MemoryProfiler(enable_tracemalloc=True)\n\u001b[1m\u001b[94m65 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m67 |\u001b[0m     def performance_benchmark(self):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:70:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m68 |\u001b[0m         \"\"\"Create performance benchmark instance.\"\"\"\n\u001b[1m\u001b[94m69 |\u001b[0m         return PerformanceBenchmark()\n\u001b[1m\u001b[94m70 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m     def test_performance_profiler_integration(self, performance_profiler):\n\u001b[1m\u001b[94m72 |\u001b[0m         \"\"\"Test performance profiler integration.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:78:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m76 |\u001b[0m             time.sleep(0.01)  # Simulate work\n\u001b[1m\u001b[94m77 |\u001b[0m             return x * 2\n\u001b[1m\u001b[94m78 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m         # Profile function\n\u001b[1m\u001b[94m80 |\u001b[0m         result, report = performance_profiler.profile_pipeline(test_function, 5)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:81:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m         # Profile function\n\u001b[1m\u001b[94m80 |\u001b[0m         result, report = performance_profiler.profile_pipeline(test_function, 5)\n\u001b[1m\u001b[94m81 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m82 |\u001b[0m         assert result == 10\n\u001b[1m\u001b[94m83 |\u001b[0m         assert report.total_execution_time > 0\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:86:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m84 |\u001b[0m         assert len(report.function_metrics) > 0\n\u001b[1m\u001b[94m85 |\u001b[0m         assert len(report.recommendations) > 0\n\u001b[1m\u001b[94m86 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m87 |\u001b[0m     def test_caching_strategies_integration(self, temp_dir):\n\u001b[1m\u001b[94m88 |\u001b[0m         \"\"\"Test caching strategies integration.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:91:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m89 |\u001b[0m         # Test memory cache\n\u001b[1m\u001b[94m90 |\u001b[0m         memory_cache = MemoryCache(max_size_mb=10, default_ttl=60)\n\u001b[1m\u001b[94m91 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m         # Test basic caching\n\u001b[1m\u001b[94m93 |\u001b[0m         memory_cache.set(\"test_key\", \"test_value\")\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:96:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m94 |\u001b[0m         value = memory_cache.get(\"test_key\")\n\u001b[1m\u001b[94m95 |\u001b[0m         assert value == \"test_value\"\n\u001b[1m\u001b[94m96 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m97 |\u001b[0m         # Test cache statistics\n\u001b[1m\u001b[94m98 |\u001b[0m         stats = memory_cache.get_stats()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:101:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 99 |\u001b[0m         assert stats.total_entries == 1\n\u001b[1m\u001b[94m100 |\u001b[0m         assert stats.hit_count == 1\n\u001b[1m\u001b[94m101 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m102 |\u001b[0m         # Test persistent cache\n\u001b[1m\u001b[94m103 |\u001b[0m         persistent_cache = PersistentCache(temp_dir, max_file_size_mb=5)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:107:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m105 |\u001b[0m         persistent_value = persistent_cache.get(\"persistent_key\")\n\u001b[1m\u001b[94m106 |\u001b[0m         assert persistent_value == \"persistent_value\"\n\u001b[1m\u001b[94m107 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m108 |\u001b[0m         # Test hybrid cache\n\u001b[1m\u001b[94m109 |\u001b[0m         hybrid_cache = HybridCache(memory_cache, persistent_cache)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:113:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m         hybrid_value = hybrid_cache.get(\"hybrid_key\")\n\u001b[1m\u001b[94m112 |\u001b[0m         assert hybrid_value == \"hybrid_value\"\n\u001b[1m\u001b[94m113 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m     def test_performance_monitoring_integration(self, performance_monitor):\n\u001b[1m\u001b[94m115 |\u001b[0m         \"\"\"Test performance monitoring integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:118:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m116 |\u001b[0m         # Start monitoring\n\u001b[1m\u001b[94m117 |\u001b[0m         performance_monitor.start_monitoring()\n\u001b[1m\u001b[94m118 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m         # Record some metrics\n\u001b[1m\u001b[94m120 |\u001b[0m         performance_monitor.record_metric(\"test_metric\", 100.0, \"ms\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:122:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m         performance_monitor.record_metric(\"test_metric\", 100.0, \"ms\")\n\u001b[1m\u001b[94m121 |\u001b[0m         performance_monitor.record_metric(\"throughput\", 50.0, \"rps\")\n\u001b[1m\u001b[94m122 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         # Wait for monitoring to collect data\n\u001b[1m\u001b[94m124 |\u001b[0m         time.sleep(2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:125:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         # Wait for monitoring to collect data\n\u001b[1m\u001b[94m124 |\u001b[0m         time.sleep(2)\n\u001b[1m\u001b[94m125 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m         # Get dashboard data\n\u001b[1m\u001b[94m127 |\u001b[0m         dashboard_data = performance_monitor.get_dashboard_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:131:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m129 |\u001b[0m         assert \"resources\" in dashboard_data\n\u001b[1m\u001b[94m130 |\u001b[0m         assert \"alerts\" in dashboard_data\n\u001b[1m\u001b[94m131 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m132 |\u001b[0m         # Stop monitoring\n\u001b[1m\u001b[94m133 |\u001b[0m         performance_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:135:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m133 |\u001b[0m         performance_monitor.stop_monitoring()\n\u001b[1m\u001b[94m134 |\u001b[0m         assert not performance_monitor.monitoring_active\n\u001b[1m\u001b[94m135 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m     def test_memory_optimization_integration(self, memory_profiler):\n\u001b[1m\u001b[94m137 |\u001b[0m         \"\"\"Test memory optimization integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:141:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m139 |\u001b[0m         initial_snapshot = memory_profiler.take_snapshot()\n\u001b[1m\u001b[94m140 |\u001b[0m         assert initial_snapshot.current_memory_mb >= 0\n\u001b[1m\u001b[94m141 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m142 |\u001b[0m         # Create some objects to test memory tracking\n\u001b[1m\u001b[94m143 |\u001b[0m         test_objects = [f\"test_object_{i}\" for i in range(1000)]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `test_objects` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:143:9\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m142 |\u001b[0m         # Create some objects to test memory tracking\n\u001b[1m\u001b[94m143 |\u001b[0m         test_objects = [f\"test_object_{i}\" for i in range(1000)]\n    \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m144 |\u001b[0m         \n\u001b[1m\u001b[94m145 |\u001b[0m         # Take another snapshot\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `test_objects`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:144:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m142 |\u001b[0m         # Create some objects to test memory tracking\n\u001b[1m\u001b[94m143 |\u001b[0m         test_objects = [f\"test_object_{i}\" for i in range(1000)]\n\u001b[1m\u001b[94m144 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m145 |\u001b[0m         # Take another snapshot\n\u001b[1m\u001b[94m146 |\u001b[0m         final_snapshot = memory_profiler.take_snapshot()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:148:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         final_snapshot = memory_profiler.take_snapshot()\n\u001b[1m\u001b[94m147 |\u001b[0m         assert final_snapshot.current_memory_mb >= initial_snapshot.current_memory_mb\n\u001b[1m\u001b[94m148 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m         # Get memory stats\n\u001b[1m\u001b[94m150 |\u001b[0m         stats = memory_profiler.get_memory_stats()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:153:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m151 |\u001b[0m         assert \"current_memory_mb\" in stats\n\u001b[1m\u001b[94m152 |\u001b[0m         assert \"object_counts\" in stats\n\u001b[1m\u001b[94m153 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m154 |\u001b[0m         # Test memory optimizer\n\u001b[1m\u001b[94m155 |\u001b[0m         optimizer = MemoryOptimizer()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:158:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m156 |\u001b[0m         efficiency_score = optimizer.analyze_memory_efficiency()\n\u001b[1m\u001b[94m157 |\u001b[0m         assert 0 <= efficiency_score <= 100\n\u001b[1m\u001b[94m158 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m159 |\u001b[0m     def test_performance_benchmarking_integration(self, performance_benchmark):\n\u001b[1m\u001b[94m160 |\u001b[0m         \"\"\"Test performance benchmarking integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:165:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m163 |\u001b[0m             time.sleep(0.001)  # Simulate work\n\u001b[1m\u001b[94m164 |\u001b[0m             return x * x\n\u001b[1m\u001b[94m165 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m166 |\u001b[0m         # Benchmark function\n\u001b[1m\u001b[94m167 |\u001b[0m         stats = performance_benchmark.benchmark_function(benchmark_function, 5, iterations=50)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:168:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m166 |\u001b[0m         # Benchmark function\n\u001b[1m\u001b[94m167 |\u001b[0m         stats = performance_benchmark.benchmark_function(benchmark_function, 5, iterations=50)\n\u001b[1m\u001b[94m168 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m169 |\u001b[0m         assert stats.function_name.endswith(\"benchmark_function\")\n\u001b[1m\u001b[94m170 |\u001b[0m         assert stats.mean_time > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:173:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m171 |\u001b[0m         assert stats.total_iterations == 50\n\u001b[1m\u001b[94m172 |\u001b[0m         assert stats.success_rate == 100.0\n\u001b[1m\u001b[94m173 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m174 |\u001b[0m         # Set baseline\n\u001b[1m\u001b[94m175 |\u001b[0m         performance_benchmark.set_baseline(stats)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:176:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m174 |\u001b[0m         # Set baseline\n\u001b[1m\u001b[94m175 |\u001b[0m         performance_benchmark.set_baseline(stats)\n\u001b[1m\u001b[94m176 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m         # Test regression detection\n\u001b[1m\u001b[94m178 |\u001b[0m         regressions = performance_benchmark.detect_performance_regressions()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:180:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m178 |\u001b[0m         regressions = performance_benchmark.detect_performance_regressions()\n\u001b[1m\u001b[94m179 |\u001b[0m         assert isinstance(regressions, list)\n\u001b[1m\u001b[94m180 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m181 |\u001b[0m     def test_load_testing_integration(self, performance_benchmark):\n\u001b[1m\u001b[94m182 |\u001b[0m         \"\"\"Test load testing integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:186:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m             time.sleep(0.001)  # Simulate work\n\u001b[1m\u001b[94m185 |\u001b[0m             return x + 1\n\u001b[1m\u001b[94m186 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m187 |\u001b[0m         # Run load test\n\u001b[1m\u001b[94m188 |\u001b[0m         load_result = performance_benchmark.load_test(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:191:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m189 |\u001b[0m             load_test_function, 5, 50, 10\n\u001b[1m\u001b[94m190 |\u001b[0m         )\n\u001b[1m\u001b[94m191 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m192 |\u001b[0m         assert load_result.concurrent_users == 5\n\u001b[1m\u001b[94m193 |\u001b[0m         assert load_result.total_requests == 50\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:197:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m195 |\u001b[0m         assert load_result.avg_response_time >= 0\n\u001b[1m\u001b[94m196 |\u001b[0m         assert load_result.throughput_rps >= 0\n\u001b[1m\u001b[94m197 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m198 |\u001b[0m     def test_caching_decorators_integration(self):\n\u001b[1m\u001b[94m199 |\u001b[0m         \"\"\"Test caching decorators integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:201:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m199 |\u001b[0m         \"\"\"Test caching decorators integration.\"\"\"\n\u001b[1m\u001b[94m200 |\u001b[0m         call_count = 0\n\u001b[1m\u001b[94m201 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m202 |\u001b[0m         @cache_result(ttl_seconds=60)\n\u001b[1m\u001b[94m203 |\u001b[0m         def cached_function(x):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:208:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m206 |\u001b[0m             time.sleep(0.01)  # Simulate work\n\u001b[1m\u001b[94m207 |\u001b[0m             return x * 2\n\u001b[1m\u001b[94m208 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m209 |\u001b[0m         # First call\n\u001b[1m\u001b[94m210 |\u001b[0m         result1 = cached_function(5)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:213:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m211 |\u001b[0m         assert result1 == 10\n\u001b[1m\u001b[94m212 |\u001b[0m         assert call_count == 1\n\u001b[1m\u001b[94m213 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m         # Second call (should be cached)\n\u001b[1m\u001b[94m215 |\u001b[0m         result2 = cached_function(5)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:218:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m216 |\u001b[0m         assert result2 == 10\n\u001b[1m\u001b[94m217 |\u001b[0m         assert call_count == 1  # Should not increment\n\u001b[1m\u001b[94m218 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m219 |\u001b[0m         # Different argument (should not be cached)\n\u001b[1m\u001b[94m220 |\u001b[0m         result3 = cached_function(6)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:223:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m221 |\u001b[0m         assert result3 == 12\n\u001b[1m\u001b[94m222 |\u001b[0m         assert call_count == 2\n\u001b[1m\u001b[94m223 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m224 |\u001b[0m     def test_memory_monitoring_decorator_integration(self):\n\u001b[1m\u001b[94m225 |\u001b[0m         \"\"\"Test memory monitoring decorator integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF811 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mRedefinition of unused `memory_monitor` from line 30\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:226:59\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m224 |\u001b[0m     def test_memory_monitoring_decorator_integration(self):\n\u001b[1m\u001b[94m225 |\u001b[0m         \"\"\"Test memory monitoring decorator integration.\"\"\"\n\u001b[1m\u001b[94m226 |\u001b[0m         from tests.performance.memory_optimization import memory_monitor\n    \u001b[1m\u001b[94m|\u001b[0m                                                           \u001b[1m\u001b[91m^^^^^^^^^^^^^^\u001b[0m \u001b[1m\u001b[91m`memory_monitor` redefined here\u001b[0m\n\u001b[1m\u001b[94m227 |\u001b[0m         \n\u001b[1m\u001b[94m228 |\u001b[0m         @memory_monitor(interval_seconds=1)\n    \u001b[1m\u001b[94m|\u001b[0m\n   \u001b[1m\u001b[94m:::\u001b[0m tests/performance/test_performance_integration.py:30:38\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 28 |\u001b[0m )\n\u001b[1m\u001b[94m 29 |\u001b[0m from tests.performance.memory_optimization import (\n\u001b[1m\u001b[94m 30 |\u001b[0m     MemoryProfiler, MemoryOptimizer, memory_monitor, optimize_spark_memory\n    \u001b[1m\u001b[94m|\u001b[0m                                      \u001b[1m\u001b[33m--------------\u001b[0m \u001b[1m\u001b[33mprevious definition of `memory_monitor` here\u001b[0m\n\u001b[1m\u001b[94m 31 |\u001b[0m )\n\u001b[1m\u001b[94m 32 |\u001b[0m from tests.performance.performance_benchmarking import (\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove definition: `memory_monitor`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:227:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m225 |\u001b[0m         \"\"\"Test memory monitoring decorator integration.\"\"\"\n\u001b[1m\u001b[94m226 |\u001b[0m         from tests.performance.memory_optimization import memory_monitor\n\u001b[1m\u001b[94m227 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m228 |\u001b[0m         @memory_monitor(interval_seconds=1)\n\u001b[1m\u001b[94m229 |\u001b[0m         def memory_intensive_function():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:234:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m232 |\u001b[0m             time.sleep(0.1)\n\u001b[1m\u001b[94m233 |\u001b[0m             return len(objects)\n\u001b[1m\u001b[94m234 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m         # Execute function\n\u001b[1m\u001b[94m236 |\u001b[0m         result = memory_intensive_function()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:238:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m236 |\u001b[0m         result = memory_intensive_function()\n\u001b[1m\u001b[94m237 |\u001b[0m         assert result == 10000\n\u001b[1m\u001b[94m238 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m239 |\u001b[0m     def test_performance_profiler_decorator_integration(self):\n\u001b[1m\u001b[94m240 |\u001b[0m         \"\"\"Test performance profiler decorator integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:245:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m243 |\u001b[0m             time.sleep(0.01)\n\u001b[1m\u001b[94m244 |\u001b[0m             return x * 3\n\u001b[1m\u001b[94m245 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m246 |\u001b[0m         # Execute function\n\u001b[1m\u001b[94m247 |\u001b[0m         result = profiled_function(4)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:249:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m247 |\u001b[0m         result = profiled_function(4)\n\u001b[1m\u001b[94m248 |\u001b[0m         assert result == 12\n\u001b[1m\u001b[94m249 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m250 |\u001b[0m     def test_comprehensive_performance_workflow(self, performance_profiler, \n\u001b[1m\u001b[94m251 |\u001b[0m                                              performance_monitor, memory_profiler,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:250:76\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m248 |\u001b[0m         assert result == 12\n\u001b[1m\u001b[94m249 |\u001b[0m     \n\u001b[1m\u001b[94m250 |\u001b[0m     def test_comprehensive_performance_workflow(self, performance_profiler, \n    \u001b[1m\u001b[94m|\u001b[0m                                                                            \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m251 |\u001b[0m                                              performance_monitor, memory_profiler,\n\u001b[1m\u001b[94m252 |\u001b[0m                                              performance_benchmark):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:256:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m254 |\u001b[0m         # Step 1: Start monitoring\n\u001b[1m\u001b[94m255 |\u001b[0m         performance_monitor.start_monitoring()\n\u001b[1m\u001b[94m256 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         # Step 2: Take memory snapshot\n\u001b[1m\u001b[94m258 |\u001b[0m         memory_snapshot = memory_profiler.take_snapshot()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `memory_snapshot` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:258:9\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         # Step 2: Take memory snapshot\n\u001b[1m\u001b[94m258 |\u001b[0m         memory_snapshot = memory_profiler.take_snapshot()\n    \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m259 |\u001b[0m         \n\u001b[1m\u001b[94m260 |\u001b[0m         # Step 3: Profile a function\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `memory_snapshot`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:259:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         # Step 2: Take memory snapshot\n\u001b[1m\u001b[94m258 |\u001b[0m         memory_snapshot = memory_profiler.take_snapshot()\n\u001b[1m\u001b[94m259 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m         # Step 3: Profile a function\n\u001b[1m\u001b[94m261 |\u001b[0m         @performance_profiler.profile_function\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:265:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m             time.sleep(0.01)\n\u001b[1m\u001b[94m264 |\u001b[0m             return [x * 2 for x in data]\n\u001b[1m\u001b[94m265 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m         # Step 4: Benchmark the function\n\u001b[1m\u001b[94m267 |\u001b[0m         stats = performance_benchmark.benchmark_function(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:270:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m268 |\u001b[0m             workflow_function, [1, 2, 3, 4, 5], iterations=20\n\u001b[1m\u001b[94m269 |\u001b[0m         )\n\u001b[1m\u001b[94m270 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m         # Step 5: Record performance metrics\n\u001b[1m\u001b[94m272 |\u001b[0m         performance_monitor.record_metric(\"workflow_execution_time\", stats.mean_time, \"ms\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:273:67\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m         # Step 5: Record performance metrics\n\u001b[1m\u001b[94m272 |\u001b[0m         performance_monitor.record_metric(\"workflow_execution_time\", stats.mean_time, \"ms\")\n\u001b[1m\u001b[94m273 |\u001b[0m         performance_monitor.record_metric(\"workflow_memory_usage\", \n    \u001b[1m\u001b[94m|\u001b[0m                                                                   \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m274 |\u001b[0m                                         memory_profiler.get_memory_stats().get(\"current_memory_mb\", 0), \"mb\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:275:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m273 |\u001b[0m         performance_monitor.record_metric(\"workflow_memory_usage\", \n\u001b[1m\u001b[94m274 |\u001b[0m                                         memory_profiler.get_memory_stats().get(\"current_memory_mb\", 0), \"mb\")\n\u001b[1m\u001b[94m275 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m276 |\u001b[0m         # Step 6: Wait for monitoring\n\u001b[1m\u001b[94m277 |\u001b[0m         time.sleep(2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:278:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m276 |\u001b[0m         # Step 6: Wait for monitoring\n\u001b[1m\u001b[94m277 |\u001b[0m         time.sleep(2)\n\u001b[1m\u001b[94m278 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m279 |\u001b[0m         # Step 7: Get comprehensive results\n\u001b[1m\u001b[94m280 |\u001b[0m         dashboard_data = performance_monitor.get_dashboard_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:283:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m         memory_stats = memory_profiler.get_memory_stats()\n\u001b[1m\u001b[94m282 |\u001b[0m         profiler_report = performance_profiler.generate_report()\n\u001b[1m\u001b[94m283 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m284 |\u001b[0m         # Step 8: Verify integration\n\u001b[1m\u001b[94m285 |\u001b[0m         assert dashboard_data[\"monitoring_active\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:289:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m287 |\u001b[0m         assert memory_stats[\"current_memory_mb\"] >= 0\n\u001b[1m\u001b[94m288 |\u001b[0m         assert profiler_report.total_execution_time > 0\n\u001b[1m\u001b[94m289 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m290 |\u001b[0m         # Step 9: Stop monitoring\n\u001b[1m\u001b[94m291 |\u001b[0m         performance_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:292:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m290 |\u001b[0m         # Step 9: Stop monitoring\n\u001b[1m\u001b[94m291 |\u001b[0m         performance_monitor.stop_monitoring()\n\u001b[1m\u001b[94m292 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m293 |\u001b[0m     def test_performance_optimization_workflow(self, temp_dir):\n\u001b[1m\u001b[94m294 |\u001b[0m         \"\"\"Test performance optimization workflow.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:297:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m295 |\u001b[0m         # Create cache manager\n\u001b[1m\u001b[94m296 |\u001b[0m         cache_manager.clear_all()\n\u001b[1m\u001b[94m297 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m298 |\u001b[0m         # Test optimization workflow\n\u001b[1m\u001b[94m299 |\u001b[0m         optimizer = MemoryOptimizer()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:300:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m298 |\u001b[0m         # Test optimization workflow\n\u001b[1m\u001b[94m299 |\u001b[0m         optimizer = MemoryOptimizer()\n\u001b[1m\u001b[94m300 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m301 |\u001b[0m         # Test Spark memory optimization\n\u001b[1m\u001b[94m302 |\u001b[0m         spark_config = optimize_spark_memory()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:305:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m303 |\u001b[0m         assert \"spark_config\" in spark_config\n\u001b[1m\u001b[94m304 |\u001b[0m         assert \"memory_settings\" in spark_config\n\u001b[1m\u001b[94m305 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m306 |\u001b[0m         # Test memory efficiency analysis\n\u001b[1m\u001b[94m307 |\u001b[0m         efficiency_score = optimizer.analyze_memory_efficiency()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:309:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m307 |\u001b[0m         efficiency_score = optimizer.analyze_memory_efficiency()\n\u001b[1m\u001b[94m308 |\u001b[0m         assert 0 <= efficiency_score <= 100\n\u001b[1m\u001b[94m309 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m310 |\u001b[0m     def test_performance_reporting_integration(self, performance_profiler,\n\u001b[1m\u001b[94m311 |\u001b[0m                                             performance_monitor, memory_profiler,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:319:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m317 |\u001b[0m             time.sleep(0.01)\n\u001b[1m\u001b[94m318 |\u001b[0m             return x ** 2\n\u001b[1m\u001b[94m319 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m320 |\u001b[0m         # Profile function\n\u001b[1m\u001b[94m321 |\u001b[0m         result, profiler_report = performance_profiler.profile_pipeline(report_function, 5)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:322:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m320 |\u001b[0m         # Profile function\n\u001b[1m\u001b[94m321 |\u001b[0m         result, profiler_report = performance_profiler.profile_pipeline(report_function, 5)\n\u001b[1m\u001b[94m322 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m323 |\u001b[0m         # Benchmark function\n\u001b[1m\u001b[94m324 |\u001b[0m         benchmark_stats = performance_benchmark.benchmark_function(report_function, 5, iterations=10)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `benchmark_stats` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:324:9\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m323 |\u001b[0m         # Benchmark function\n\u001b[1m\u001b[94m324 |\u001b[0m         benchmark_stats = performance_benchmark.benchmark_function(report_function, 5, iterations=10)\n    \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m325 |\u001b[0m         \n\u001b[1m\u001b[94m326 |\u001b[0m         # Start monitoring and record metrics\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `benchmark_stats`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:325:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m323 |\u001b[0m         # Benchmark function\n\u001b[1m\u001b[94m324 |\u001b[0m         benchmark_stats = performance_benchmark.benchmark_function(report_function, 5, iterations=10)\n\u001b[1m\u001b[94m325 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m326 |\u001b[0m         # Start monitoring and record metrics\n\u001b[1m\u001b[94m327 |\u001b[0m         performance_monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:330:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m328 |\u001b[0m         performance_monitor.record_metric(\"report_metric\", 100.0, \"ms\")\n\u001b[1m\u001b[94m329 |\u001b[0m         time.sleep(1)\n\u001b[1m\u001b[94m330 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m331 |\u001b[0m         # Take memory snapshot\n\u001b[1m\u001b[94m332 |\u001b[0m         memory_snapshot = memory_profiler.take_snapshot()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `memory_snapshot` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:332:9\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m331 |\u001b[0m         # Take memory snapshot\n\u001b[1m\u001b[94m332 |\u001b[0m         memory_snapshot = memory_profiler.take_snapshot()\n    \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m333 |\u001b[0m         \n\u001b[1m\u001b[94m334 |\u001b[0m         # Generate reports\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `memory_snapshot`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:333:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m331 |\u001b[0m         # Take memory snapshot\n\u001b[1m\u001b[94m332 |\u001b[0m         memory_snapshot = memory_profiler.take_snapshot()\n\u001b[1m\u001b[94m333 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m334 |\u001b[0m         # Generate reports\n\u001b[1m\u001b[94m335 |\u001b[0m         profiler_report_file = performance_profiler.export_report(temp_dir / \"profiler_report.json\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:339:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m337 |\u001b[0m         performance_report = performance_monitor.generate_performance_report()\n\u001b[1m\u001b[94m338 |\u001b[0m         performance_report_file = performance_monitor.export_report(performance_report, temp_dir / \"performance_report.json\")\n\u001b[1m\u001b[94m339 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m340 |\u001b[0m         # Verify reports were created\n\u001b[1m\u001b[94m341 |\u001b[0m         assert profiler_report_file.exists()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:344:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m342 |\u001b[0m         assert benchmark_report_file.exists()\n\u001b[1m\u001b[94m343 |\u001b[0m         assert performance_report_file.exists()\n\u001b[1m\u001b[94m344 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m345 |\u001b[0m         # Stop monitoring\n\u001b[1m\u001b[94m346 |\u001b[0m         performance_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:347:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m345 |\u001b[0m         # Stop monitoring\n\u001b[1m\u001b[94m346 |\u001b[0m         performance_monitor.stop_monitoring()\n\u001b[1m\u001b[94m347 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m348 |\u001b[0m     def test_error_handling_integration(self, performance_profiler, performance_monitor):\n\u001b[1m\u001b[94m349 |\u001b[0m         \"\"\"Test error handling in performance components.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:354:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m352 |\u001b[0m         def failing_function():\n\u001b[1m\u001b[94m353 |\u001b[0m             raise ValueError(\"Test error\")\n\u001b[1m\u001b[94m354 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m355 |\u001b[0m         # Should handle error gracefully\n\u001b[1m\u001b[94m356 |\u001b[0m         try:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:360:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m358 |\u001b[0m         except ValueError:\n\u001b[1m\u001b[94m359 |\u001b[0m             pass  # Expected\n\u001b[1m\u001b[94m360 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m361 |\u001b[0m         # Check that error was recorded\n\u001b[1m\u001b[94m362 |\u001b[0m         error_results = [r for r in performance_profiler.metrics if not r.success]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:364:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m362 |\u001b[0m         error_results = [r for r in performance_profiler.metrics if not r.success]\n\u001b[1m\u001b[94m363 |\u001b[0m         assert len(error_results) > 0\n\u001b[1m\u001b[94m364 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m365 |\u001b[0m         # Test monitor with invalid metrics\n\u001b[1m\u001b[94m366 |\u001b[0m         performance_monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:367:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m365 |\u001b[0m         # Test monitor with invalid metrics\n\u001b[1m\u001b[94m366 |\u001b[0m         performance_monitor.start_monitoring()\n\u001b[1m\u001b[94m367 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m         # Record invalid metric\n\u001b[1m\u001b[94m369 |\u001b[0m         performance_monitor.record_metric(\"\", -1.0, \"\")  # Invalid name and value\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:370:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m         # Record invalid metric\n\u001b[1m\u001b[94m369 |\u001b[0m         performance_monitor.record_metric(\"\", -1.0, \"\")  # Invalid name and value\n\u001b[1m\u001b[94m370 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m371 |\u001b[0m         # Should handle gracefully\n\u001b[1m\u001b[94m372 |\u001b[0m         dashboard_data = performance_monitor.get_dashboard_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:374:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m372 |\u001b[0m         dashboard_data = performance_monitor.get_dashboard_data()\n\u001b[1m\u001b[94m373 |\u001b[0m         assert \"metrics\" in dashboard_data\n\u001b[1m\u001b[94m374 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m375 |\u001b[0m         performance_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:376:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m375 |\u001b[0m         performance_monitor.stop_monitoring()\n\u001b[1m\u001b[94m376 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m377 |\u001b[0m     def test_concurrent_performance_operations(self, performance_profiler, performance_monitor):\n\u001b[1m\u001b[94m378 |\u001b[0m         \"\"\"Test concurrent performance operations.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:380:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m         \"\"\"Test concurrent performance operations.\"\"\"\n\u001b[1m\u001b[94m379 |\u001b[0m         import threading\n\u001b[1m\u001b[94m380 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m         results = []\n\u001b[1m\u001b[94m382 |\u001b[0m         errors = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:383:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m         results = []\n\u001b[1m\u001b[94m382 |\u001b[0m         errors = []\n\u001b[1m\u001b[94m383 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m384 |\u001b[0m         def concurrent_operation(operation_id):\n\u001b[1m\u001b[94m385 |\u001b[0m             try:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:390:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m388 |\u001b[0m                     time.sleep(0.01)\n\u001b[1m\u001b[94m389 |\u001b[0m                     return x + operation_id\n\u001b[1m\u001b[94m390 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m                 result = concurrent_function(10)\n\u001b[1m\u001b[94m392 |\u001b[0m                 results.append(result)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:393:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m                 result = concurrent_function(10)\n\u001b[1m\u001b[94m392 |\u001b[0m                 results.append(result)\n\u001b[1m\u001b[94m393 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m394 |\u001b[0m                 # Record metric\n\u001b[1m\u001b[94m395 |\u001b[0m                 performance_monitor.record_metric(f\"concurrent_metric_{operation_id}\", result, \"count\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:396:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m394 |\u001b[0m                 # Record metric\n\u001b[1m\u001b[94m395 |\u001b[0m                 performance_monitor.record_metric(f\"concurrent_metric_{operation_id}\", result, \"count\")\n\u001b[1m\u001b[94m396 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m397 |\u001b[0m             except Exception as e:\n\u001b[1m\u001b[94m398 |\u001b[0m                 errors.append(str(e))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:399:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m397 |\u001b[0m             except Exception as e:\n\u001b[1m\u001b[94m398 |\u001b[0m                 errors.append(str(e))\n\u001b[1m\u001b[94m399 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m400 |\u001b[0m         # Start monitoring\n\u001b[1m\u001b[94m401 |\u001b[0m         performance_monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:402:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m400 |\u001b[0m         # Start monitoring\n\u001b[1m\u001b[94m401 |\u001b[0m         performance_monitor.start_monitoring()\n\u001b[1m\u001b[94m402 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m403 |\u001b[0m         # Run concurrent operations\n\u001b[1m\u001b[94m404 |\u001b[0m         threads = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:409:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m             threads.append(thread)\n\u001b[1m\u001b[94m408 |\u001b[0m             thread.start()\n\u001b[1m\u001b[94m409 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m         # Wait for completion\n\u001b[1m\u001b[94m411 |\u001b[0m         for thread in threads:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:413:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m411 |\u001b[0m         for thread in threads:\n\u001b[1m\u001b[94m412 |\u001b[0m             thread.join()\n\u001b[1m\u001b[94m413 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m414 |\u001b[0m         # Verify results\n\u001b[1m\u001b[94m415 |\u001b[0m         assert len(results) == 5\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:417:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m415 |\u001b[0m         assert len(results) == 5\n\u001b[1m\u001b[94m416 |\u001b[0m         assert len(errors) == 0\n\u001b[1m\u001b[94m417 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m418 |\u001b[0m         # Check monitoring data\n\u001b[1m\u001b[94m419 |\u001b[0m         time.sleep(1)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:422:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m420 |\u001b[0m         dashboard_data = performance_monitor.get_dashboard_data()\n\u001b[1m\u001b[94m421 |\u001b[0m         assert dashboard_data[\"monitoring_active\"]\n\u001b[1m\u001b[94m422 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m423 |\u001b[0m         performance_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:430:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m428 |\u001b[0m class TestPerformanceMarkers:\n\u001b[1m\u001b[94m429 |\u001b[0m     \"\"\"Test performance-specific pytest markers.\"\"\"\n\u001b[1m\u001b[94m430 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m431 |\u001b[0m     def test_performance_marker_works(self):\n\u001b[1m\u001b[94m432 |\u001b[0m         \"\"\"Test that performance marker works.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:434:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m432 |\u001b[0m         \"\"\"Test that performance marker works.\"\"\"\n\u001b[1m\u001b[94m433 |\u001b[0m         assert True\n\u001b[1m\u001b[94m434 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m435 |\u001b[0m     @pytest.mark.slow\n\u001b[1m\u001b[94m436 |\u001b[0m     def test_slow_performance_test(self):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF811 \u001b[0m\u001b[1mRedefinition of unused `benchmark` from line 33\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:448:5\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m446 |\u001b[0m     profiler = PerformanceProfiler()\n\u001b[1m\u001b[94m447 |\u001b[0m     monitor = PerformanceMonitor({\"monitoring_interval\": 1})\n\u001b[1m\u001b[94m448 |\u001b[0m     benchmark = PerformanceBenchmark()\n    \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m \u001b[1m\u001b[91m`benchmark` redefined here\u001b[0m\n\u001b[1m\u001b[94m449 |\u001b[0m     \n\u001b[1m\u001b[94m450 |\u001b[0m     # Test function\n    \u001b[1m\u001b[94m|\u001b[0m\n   \u001b[1m\u001b[94m:::\u001b[0m tests/performance/test_performance_integration.py:33:27\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 31 |\u001b[0m )\n\u001b[1m\u001b[94m 32 |\u001b[0m from tests.performance.performance_benchmarking import (\n\u001b[1m\u001b[94m 33 |\u001b[0m     PerformanceBenchmark, benchmark, compare_benchmarks, load_test\n    \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[33m---------\u001b[0m \u001b[1m\u001b[33mprevious definition of `benchmark` here\u001b[0m\n\u001b[1m\u001b[94m 34 |\u001b[0m )\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove definition: `benchmark`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:449:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m447 |\u001b[0m     monitor = PerformanceMonitor({\"monitoring_interval\": 1})\n\u001b[1m\u001b[94m448 |\u001b[0m     benchmark = PerformanceBenchmark()\n\u001b[1m\u001b[94m449 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m450 |\u001b[0m     # Test function\n\u001b[1m\u001b[94m451 |\u001b[0m     def cicd_test_function(data):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:453:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m451 |\u001b[0m     def cicd_test_function(data):\n\u001b[1m\u001b[94m452 |\u001b[0m         return sum(data)\n\u001b[1m\u001b[94m453 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m454 |\u001b[0m     # Profile function\n\u001b[1m\u001b[94m455 |\u001b[0m     result, profiler_report = profiler.profile_pipeline(cicd_test_function, [1, 2, 3, 4, 5])\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:457:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m455 |\u001b[0m     result, profiler_report = profiler.profile_pipeline(cicd_test_function, [1, 2, 3, 4, 5])\n\u001b[1m\u001b[94m456 |\u001b[0m     assert result == 15\n\u001b[1m\u001b[94m457 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m458 |\u001b[0m     # Benchmark function\n\u001b[1m\u001b[94m459 |\u001b[0m     benchmark_stats = benchmark.benchmark_function(cicd_test_function, [1, 2, 3, 4, 5], iterations=10)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:462:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m460 |\u001b[0m     assert benchmark_stats.mean_time >= 0\n\u001b[1m\u001b[94m461 |\u001b[0m     assert benchmark_stats.success_rate == 100.0\n\u001b[1m\u001b[94m462 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m463 |\u001b[0m     # Start monitoring\n\u001b[1m\u001b[94m464 |\u001b[0m     monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:467:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m465 |\u001b[0m     monitor.record_metric(\"cicd_metric\", benchmark_stats.mean_time, \"ms\")\n\u001b[1m\u001b[94m466 |\u001b[0m     time.sleep(1)\n\u001b[1m\u001b[94m467 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m468 |\u001b[0m     # Get results\n\u001b[1m\u001b[94m469 |\u001b[0m     dashboard_data = monitor.get_dashboard_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/performance/test_performance_integration.py:471:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m469 |\u001b[0m     dashboard_data = monitor.get_dashboard_data()\n\u001b[1m\u001b[94m470 |\u001b[0m     assert dashboard_data[\"monitoring_active\"]\n\u001b[1m\u001b[94m471 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m472 |\u001b[0m     monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mE722 \u001b[0m\u001b[1mDo not use bare `except`\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/run_tests.py:91:9\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m89 |\u001b[0m             spark.stop()\n\u001b[1m\u001b[94m90 |\u001b[0m             print(\"\\n\u2705 Spark session stopped\")\n\u001b[1m\u001b[94m91 |\u001b[0m         except:\n   \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m             pass\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/__init__.py:11:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m10 |\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from .security_tests import SecurityTestSuite\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from .vulnerability_scanner import VulnerabilityScanner\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from .compliance_checker import ComplianceChecker\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_________________________________________________^\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m   __all__ = [\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/__init__.py:17:28\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m __all__ = [\n\u001b[1m\u001b[94m16 |\u001b[0m     'SecurityTestSuite',\n\u001b[1m\u001b[94m17 |\u001b[0m     'VulnerabilityScanner', \n   \u001b[1m\u001b[94m|\u001b[0m                            \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m     'ComplianceChecker'\n\u001b[1m\u001b[94m19 |\u001b[0m ]\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:12:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import os\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import subprocess\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import sys\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Dict, List, Any, Optional\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from dataclasses import dataclass\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from enum import Enum\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_____________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`os` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:12:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import os\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m import json\n\u001b[1m\u001b[94m14 |\u001b[0m import subprocess\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `os`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Any` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:17:32\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m import sys\n\u001b[1m\u001b[94m16 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m17 |\u001b[0m from typing import Dict, List, Any, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m from dataclasses import dataclass\n\u001b[1m\u001b[94m19 |\u001b[0m from datetime import datetime\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `typing.Any`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:60:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m class ComplianceChecker:\n\u001b[1m\u001b[94m59 |\u001b[0m     \"\"\"Comprehensive compliance checker for SparkForge.\"\"\"\n\u001b[1m\u001b[94m60 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m     def __init__(self, project_root: Optional[Path] = None):\n\u001b[1m\u001b[94m62 |\u001b[0m         self.project_root = project_root or Path.cwd()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:64:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m62 |\u001b[0m         self.project_root = project_root or Path.cwd()\n\u001b[1m\u001b[94m63 |\u001b[0m         self.compliance_reports = {}\n\u001b[1m\u001b[94m64 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m65 |\u001b[0m     def check_all_standards(self) -> Dict[str, ComplianceReport]:\n\u001b[1m\u001b[94m66 |\u001b[0m         \"\"\"Check compliance against all supported standards.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:73:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m             ComplianceStandard.SECURITY_BEST_PRACTICES\n\u001b[1m\u001b[94m72 |\u001b[0m         ]\n\u001b[1m\u001b[94m73 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m         for standard in standards:\n\u001b[1m\u001b[94m75 |\u001b[0m             self.compliance_reports[standard.value] = self.check_standard(standard)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:76:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m         for standard in standards:\n\u001b[1m\u001b[94m75 |\u001b[0m             self.compliance_reports[standard.value] = self.check_standard(standard)\n\u001b[1m\u001b[94m76 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m         return self.compliance_reports\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:78:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m         return self.compliance_reports\n\u001b[1m\u001b[94m78 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m     def check_standard(self, standard: ComplianceStandard) -> ComplianceReport:\n\u001b[1m\u001b[94m80 |\u001b[0m         \"\"\"Check compliance against a specific standard.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:95:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m93 |\u001b[0m         else:\n\u001b[1m\u001b[94m94 |\u001b[0m             raise ValueError(f\"Unsupported compliance standard: {standard}\")\n\u001b[1m\u001b[94m95 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m96 |\u001b[0m     def _check_owasp_top_10(self) -> ComplianceReport:\n\u001b[1m\u001b[94m97 |\u001b[0m         \"\"\"Check OWASP Top 10 compliance.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:110:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m108 |\u001b[0m             self._check_insufficient_logging()\n\u001b[1m\u001b[94m109 |\u001b[0m         ]\n\u001b[1m\u001b[94m110 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m112 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:113:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m112 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n\u001b[1m\u001b[94m113 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m         return ComplianceReport(\n\u001b[1m\u001b[94m115 |\u001b[0m             standard=ComplianceStandard.OWASP_TOP_10,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:122:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m             recommendations=self._generate_owasp_recommendations(checks)\n\u001b[1m\u001b[94m121 |\u001b[0m         )\n\u001b[1m\u001b[94m122 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m     def _check_injection_prevention(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m124 |\u001b[0m         \"\"\"Check A01:2021 - Broken Access Control.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:126:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m         \"\"\"Check A01:2021 - Broken Access Control.\"\"\"\n\u001b[1m\u001b[94m125 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m126 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m127 |\u001b[0m         # Check for SQL injection prevention\n\u001b[1m\u001b[94m128 |\u001b[0m         if self._has_sql_injection_prevention():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:130:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m         if self._has_sql_injection_prevention():\n\u001b[1m\u001b[94m129 |\u001b[0m             evidence.append(\"SQL injection prevention mechanisms in place\")\n\u001b[1m\u001b[94m130 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m131 |\u001b[0m         # Check for NoSQL injection prevention\n\u001b[1m\u001b[94m132 |\u001b[0m         if self._has_nosql_injection_prevention():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:134:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m132 |\u001b[0m         if self._has_nosql_injection_prevention():\n\u001b[1m\u001b[94m133 |\u001b[0m             evidence.append(\"NoSQL injection prevention mechanisms in place\")\n\u001b[1m\u001b[94m134 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m135 |\u001b[0m         # Check for command injection prevention\n\u001b[1m\u001b[94m136 |\u001b[0m         if self._has_command_injection_prevention():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:138:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m         if self._has_command_injection_prevention():\n\u001b[1m\u001b[94m137 |\u001b[0m             evidence.append(\"Command injection prevention mechanisms in place\")\n\u001b[1m\u001b[94m138 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m139 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:140:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m139 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m140 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m141 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m142 |\u001b[0m             check_id=\"owasp_a01_injection\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:151:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m150 |\u001b[0m         )\n\u001b[1m\u001b[94m151 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m     def _check_broken_authentication(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m153 |\u001b[0m         \"\"\"Check A02:2021 - Cryptographic Failures.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:155:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m153 |\u001b[0m         \"\"\"Check A02:2021 - Cryptographic Failures.\"\"\"\n\u001b[1m\u001b[94m154 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m155 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m156 |\u001b[0m         # SparkForge doesn't implement authentication, so this is N/A\n\u001b[1m\u001b[94m157 |\u001b[0m         evidence.append(\"No authentication system implemented (framework level)\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:159:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m         evidence.append(\"No authentication system implemented (framework level)\")\n\u001b[1m\u001b[94m158 |\u001b[0m         evidence.append(\"Authentication should be implemented at application level\")\n\u001b[1m\u001b[94m159 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m161 |\u001b[0m             check_id=\"owasp_a02_authentication\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:171:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m169 |\u001b[0m             remediation=\"Implement authentication at application level using secure frameworks\"\n\u001b[1m\u001b[94m170 |\u001b[0m         )\n\u001b[1m\u001b[94m171 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m172 |\u001b[0m     def _check_sensitive_data_exposure(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m173 |\u001b[0m         \"\"\"Check A03:2021 - Injection.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:175:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m173 |\u001b[0m         \"\"\"Check A03:2021 - Injection.\"\"\"\n\u001b[1m\u001b[94m174 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m175 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m176 |\u001b[0m         # Check for data encryption in transit\n\u001b[1m\u001b[94m177 |\u001b[0m         if self._has_transit_encryption():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:179:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m         if self._has_transit_encryption():\n\u001b[1m\u001b[94m178 |\u001b[0m             evidence.append(\"Data encryption in transit implemented\")\n\u001b[1m\u001b[94m179 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m180 |\u001b[0m         # Check for data encryption at rest\n\u001b[1m\u001b[94m181 |\u001b[0m         if self._has_at_rest_encryption():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:183:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m181 |\u001b[0m         if self._has_at_rest_encryption():\n\u001b[1m\u001b[94m182 |\u001b[0m             evidence.append(\"Data encryption at rest implemented\")\n\u001b[1m\u001b[94m183 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m         # Check for sensitive data handling\n\u001b[1m\u001b[94m185 |\u001b[0m         if self._has_sensitive_data_protection():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:187:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m185 |\u001b[0m         if self._has_sensitive_data_protection():\n\u001b[1m\u001b[94m186 |\u001b[0m             evidence.append(\"Sensitive data protection mechanisms in place\")\n\u001b[1m\u001b[94m187 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m188 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:189:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m188 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m189 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m191 |\u001b[0m             check_id=\"owasp_a03_data_exposure\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:200:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m198 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m199 |\u001b[0m         )\n\u001b[1m\u001b[94m200 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m     def _check_xml_external_entities(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m202 |\u001b[0m         \"\"\"Check A04:2021 - Insecure Design.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:204:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m202 |\u001b[0m         \"\"\"Check A04:2021 - Insecure Design.\"\"\"\n\u001b[1m\u001b[94m203 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m204 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m205 |\u001b[0m         # SparkForge doesn't process XML, so this is N/A\n\u001b[1m\u001b[94m206 |\u001b[0m         evidence.append(\"No XML processing in framework\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:207:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m205 |\u001b[0m         # SparkForge doesn't process XML, so this is N/A\n\u001b[1m\u001b[94m206 |\u001b[0m         evidence.append(\"No XML processing in framework\")\n\u001b[1m\u001b[94m207 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m208 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m209 |\u001b[0m             check_id=\"owasp_a04_xxe\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:218:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m216 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m217 |\u001b[0m         )\n\u001b[1m\u001b[94m218 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m219 |\u001b[0m     def _check_broken_access_control(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m220 |\u001b[0m         \"\"\"Check A05:2021 - Security Misconfiguration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:222:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m220 |\u001b[0m         \"\"\"Check A05:2021 - Security Misconfiguration.\"\"\"\n\u001b[1m\u001b[94m221 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m222 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m         # Check for access control mechanisms\n\u001b[1m\u001b[94m224 |\u001b[0m         if self._has_access_control():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:226:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m224 |\u001b[0m         if self._has_access_control():\n\u001b[1m\u001b[94m225 |\u001b[0m             evidence.append(\"Access control mechanisms in place\")\n\u001b[1m\u001b[94m226 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m227 |\u001b[0m         # Check for privilege escalation prevention\n\u001b[1m\u001b[94m228 |\u001b[0m         if self._has_privilege_escalation_prevention():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:230:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m228 |\u001b[0m         if self._has_privilege_escalation_prevention():\n\u001b[1m\u001b[94m229 |\u001b[0m             evidence.append(\"Privilege escalation prevention mechanisms in place\")\n\u001b[1m\u001b[94m230 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:232:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m232 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m234 |\u001b[0m             check_id=\"owasp_a05_access_control\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:243:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m241 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m242 |\u001b[0m         )\n\u001b[1m\u001b[94m243 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m244 |\u001b[0m     def _check_security_misconfiguration(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m245 |\u001b[0m         \"\"\"Check A06:2021 - Vulnerable and Outdated Components.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:247:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m245 |\u001b[0m         \"\"\"Check A06:2021 - Vulnerable and Outdated Components.\"\"\"\n\u001b[1m\u001b[94m246 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m247 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m248 |\u001b[0m         # Check for secure defaults\n\u001b[1m\u001b[94m249 |\u001b[0m         if self._has_secure_defaults():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:251:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m249 |\u001b[0m         if self._has_secure_defaults():\n\u001b[1m\u001b[94m250 |\u001b[0m             evidence.append(\"Secure default configurations\")\n\u001b[1m\u001b[94m251 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m252 |\u001b[0m         # Check for security headers\n\u001b[1m\u001b[94m253 |\u001b[0m         if self._has_security_headers():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:255:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m253 |\u001b[0m         if self._has_security_headers():\n\u001b[1m\u001b[94m254 |\u001b[0m             evidence.append(\"Security headers configured\")\n\u001b[1m\u001b[94m255 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m256 |\u001b[0m         # Check for error handling\n\u001b[1m\u001b[94m257 |\u001b[0m         if self._has_secure_error_handling():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:259:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         if self._has_secure_error_handling():\n\u001b[1m\u001b[94m258 |\u001b[0m             evidence.append(\"Secure error handling implemented\")\n\u001b[1m\u001b[94m259 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:261:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m261 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m262 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m263 |\u001b[0m             check_id=\"owasp_a06_misconfiguration\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:272:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m270 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m271 |\u001b[0m         )\n\u001b[1m\u001b[94m272 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m273 |\u001b[0m     def _check_cross_site_scripting(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m274 |\u001b[0m         \"\"\"Check A07:2021 - Identification and Authentication Failures.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:276:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m274 |\u001b[0m         \"\"\"Check A07:2021 - Identification and Authentication Failures.\"\"\"\n\u001b[1m\u001b[94m275 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m276 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m277 |\u001b[0m         # Check for XSS prevention\n\u001b[1m\u001b[94m278 |\u001b[0m         if self._has_xss_prevention():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:280:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m278 |\u001b[0m         if self._has_xss_prevention():\n\u001b[1m\u001b[94m279 |\u001b[0m             evidence.append(\"XSS prevention mechanisms in place\")\n\u001b[1m\u001b[94m280 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m         # Check for input sanitization\n\u001b[1m\u001b[94m282 |\u001b[0m         if self._has_input_sanitization():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:284:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m282 |\u001b[0m         if self._has_input_sanitization():\n\u001b[1m\u001b[94m283 |\u001b[0m             evidence.append(\"Input sanitization implemented\")\n\u001b[1m\u001b[94m284 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m285 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:286:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m285 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m286 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m287 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m288 |\u001b[0m             check_id=\"owasp_a07_xss\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:297:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m295 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m296 |\u001b[0m         )\n\u001b[1m\u001b[94m297 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m298 |\u001b[0m     def _check_insecure_deserialization(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m299 |\u001b[0m         \"\"\"Check A08:2021 - Software and Data Integrity Failures.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:301:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m299 |\u001b[0m         \"\"\"Check A08:2021 - Software and Data Integrity Failures.\"\"\"\n\u001b[1m\u001b[94m300 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m301 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m302 |\u001b[0m         # Check for secure serialization\n\u001b[1m\u001b[94m303 |\u001b[0m         if self._has_secure_serialization():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:305:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m303 |\u001b[0m         if self._has_secure_serialization():\n\u001b[1m\u001b[94m304 |\u001b[0m             evidence.append(\"Secure serialization mechanisms in place\")\n\u001b[1m\u001b[94m305 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m306 |\u001b[0m         # Check for deserialization security\n\u001b[1m\u001b[94m307 |\u001b[0m         if self._has_secure_deserialization():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:309:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m307 |\u001b[0m         if self._has_secure_deserialization():\n\u001b[1m\u001b[94m308 |\u001b[0m             evidence.append(\"Secure deserialization implemented\")\n\u001b[1m\u001b[94m309 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m310 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:311:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m310 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m311 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m312 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m313 |\u001b[0m             check_id=\"owasp_a08_deserialization\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:322:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m320 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m321 |\u001b[0m         )\n\u001b[1m\u001b[94m322 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m323 |\u001b[0m     def _check_known_vulnerabilities(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m324 |\u001b[0m         \"\"\"Check A09:2021 - Security Logging and Monitoring Failures.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:326:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m324 |\u001b[0m         \"\"\"Check A09:2021 - Security Logging and Monitoring Failures.\"\"\"\n\u001b[1m\u001b[94m325 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m326 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m327 |\u001b[0m         # Check for dependency vulnerability scanning\n\u001b[1m\u001b[94m328 |\u001b[0m         if self._has_dependency_scanning():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:330:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m328 |\u001b[0m         if self._has_dependency_scanning():\n\u001b[1m\u001b[94m329 |\u001b[0m             evidence.append(\"Dependency vulnerability scanning implemented\")\n\u001b[1m\u001b[94m330 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m331 |\u001b[0m         # Check for known CVE scanning\n\u001b[1m\u001b[94m332 |\u001b[0m         if self._has_cve_scanning():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:334:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m332 |\u001b[0m         if self._has_cve_scanning():\n\u001b[1m\u001b[94m333 |\u001b[0m             evidence.append(\"CVE scanning implemented\")\n\u001b[1m\u001b[94m334 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m335 |\u001b[0m         # Check for outdated dependencies\n\u001b[1m\u001b[94m336 |\u001b[0m         if self._has_updated_dependencies():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:338:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m336 |\u001b[0m         if self._has_updated_dependencies():\n\u001b[1m\u001b[94m337 |\u001b[0m             evidence.append(\"Dependencies are up to date\")\n\u001b[1m\u001b[94m338 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m339 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:340:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m339 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m340 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m341 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m342 |\u001b[0m             check_id=\"owasp_a09_vulnerabilities\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:351:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m349 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m350 |\u001b[0m         )\n\u001b[1m\u001b[94m351 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m352 |\u001b[0m     def _check_insufficient_logging(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m353 |\u001b[0m         \"\"\"Check A10:2021 - Server-Side Request Forgery.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:355:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m353 |\u001b[0m         \"\"\"Check A10:2021 - Server-Side Request Forgery.\"\"\"\n\u001b[1m\u001b[94m354 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m355 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m356 |\u001b[0m         # Check for security logging\n\u001b[1m\u001b[94m357 |\u001b[0m         if self._has_security_logging():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:359:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m357 |\u001b[0m         if self._has_security_logging():\n\u001b[1m\u001b[94m358 |\u001b[0m             evidence.append(\"Security logging implemented\")\n\u001b[1m\u001b[94m359 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m360 |\u001b[0m         # Check for audit trails\n\u001b[1m\u001b[94m361 |\u001b[0m         if self._has_audit_trails():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:363:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m361 |\u001b[0m         if self._has_audit_trails():\n\u001b[1m\u001b[94m362 |\u001b[0m             evidence.append(\"Audit trails implemented\")\n\u001b[1m\u001b[94m363 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m364 |\u001b[0m         # Check for monitoring\n\u001b[1m\u001b[94m365 |\u001b[0m         if self._has_security_monitoring():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:367:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m365 |\u001b[0m         if self._has_security_monitoring():\n\u001b[1m\u001b[94m366 |\u001b[0m             evidence.append(\"Security monitoring implemented\")\n\u001b[1m\u001b[94m367 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:369:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m369 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m370 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m371 |\u001b[0m             check_id=\"owasp_a10_logging\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:380:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m379 |\u001b[0m         )\n\u001b[1m\u001b[94m380 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m     def _check_cve_compliance(self) -> ComplianceReport:\n\u001b[1m\u001b[94m382 |\u001b[0m         \"\"\"Check CVE compliance.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:388:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m386 |\u001b[0m             self._check_runtime_cves()\n\u001b[1m\u001b[94m387 |\u001b[0m         ]\n\u001b[1m\u001b[94m388 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m389 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m390 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:391:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m389 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m390 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n\u001b[1m\u001b[94m391 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m392 |\u001b[0m         return ComplianceReport(\n\u001b[1m\u001b[94m393 |\u001b[0m             standard=ComplianceStandard.CVE_COMPLIANCE,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:400:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m398 |\u001b[0m             recommendations=self._generate_cve_recommendations(checks)\n\u001b[1m\u001b[94m399 |\u001b[0m         )\n\u001b[1m\u001b[94m400 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m401 |\u001b[0m     def _check_no_known_cves(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m402 |\u001b[0m         \"\"\"Check for known CVE vulnerabilities.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:408:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m406 |\u001b[0m                 sys.executable, \"-m\", \"safety\", \"check\", \"--json\"\n\u001b[1m\u001b[94m407 |\u001b[0m             ], capture_output=True, text=True, cwd=self.project_root)\n\u001b[1m\u001b[94m408 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m409 |\u001b[0m             if result.returncode == 0:\n\u001b[1m\u001b[94m410 |\u001b[0m                 evidence = [\"No known CVE vulnerabilities found\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:415:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m413 |\u001b[0m                 evidence = [f\"Known CVE vulnerabilities found: {result.stdout}\"]\n\u001b[1m\u001b[94m414 |\u001b[0m                 passed = False\n\u001b[1m\u001b[94m415 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m             return ComplianceCheck(\n\u001b[1m\u001b[94m417 |\u001b[0m                 check_id=\"cve_known_vulnerabilities\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:426:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m424 |\u001b[0m                 score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m425 |\u001b[0m             )\n\u001b[1m\u001b[94m426 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m427 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m428 |\u001b[0m             return ComplianceCheck(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:438:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m436 |\u001b[0m                 score=0.0\n\u001b[1m\u001b[94m437 |\u001b[0m             )\n\u001b[1m\u001b[94m438 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m439 |\u001b[0m     def _check_dependency_cves(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m440 |\u001b[0m         \"\"\"Check for dependency CVE vulnerabilities.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:442:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m440 |\u001b[0m         \"\"\"Check for dependency CVE vulnerabilities.\"\"\"\n\u001b[1m\u001b[94m441 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m442 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m443 |\u001b[0m         # Check for dependency vulnerability scanning\n\u001b[1m\u001b[94m444 |\u001b[0m         if self._has_dependency_scanning():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:446:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m444 |\u001b[0m         if self._has_dependency_scanning():\n\u001b[1m\u001b[94m445 |\u001b[0m             evidence.append(\"Dependency vulnerability scanning implemented\")\n\u001b[1m\u001b[94m446 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m447 |\u001b[0m         # Check for automated updates\n\u001b[1m\u001b[94m448 |\u001b[0m         if self._has_automated_updates():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:450:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m448 |\u001b[0m         if self._has_automated_updates():\n\u001b[1m\u001b[94m449 |\u001b[0m             evidence.append(\"Automated dependency updates configured\")\n\u001b[1m\u001b[94m450 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m451 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:452:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m451 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m452 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m453 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m454 |\u001b[0m             check_id=\"cve_dependency_scanning\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:463:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m461 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m462 |\u001b[0m         )\n\u001b[1m\u001b[94m463 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m     def _check_runtime_cves(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m465 |\u001b[0m         \"\"\"Check for runtime CVE vulnerabilities.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:467:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m465 |\u001b[0m         \"\"\"Check for runtime CVE vulnerabilities.\"\"\"\n\u001b[1m\u001b[94m466 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m467 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m468 |\u001b[0m         # Check for runtime security monitoring\n\u001b[1m\u001b[94m469 |\u001b[0m         if self._has_runtime_monitoring():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:471:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m469 |\u001b[0m         if self._has_runtime_monitoring():\n\u001b[1m\u001b[94m470 |\u001b[0m             evidence.append(\"Runtime security monitoring implemented\")\n\u001b[1m\u001b[94m471 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m472 |\u001b[0m         # Check for intrusion detection\n\u001b[1m\u001b[94m473 |\u001b[0m         if self._has_intrusion_detection():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:475:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m473 |\u001b[0m         if self._has_intrusion_detection():\n\u001b[1m\u001b[94m474 |\u001b[0m             evidence.append(\"Intrusion detection implemented\")\n\u001b[1m\u001b[94m475 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m476 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:477:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m476 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m477 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m478 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m479 |\u001b[0m             check_id=\"cve_runtime_monitoring\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:488:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m486 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m487 |\u001b[0m         )\n\u001b[1m\u001b[94m488 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m489 |\u001b[0m     def _check_license_compliance(self) -> ComplianceReport:\n\u001b[1m\u001b[94m490 |\u001b[0m         \"\"\"Check license compliance.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:496:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m494 |\u001b[0m             self._check_copyleft_licenses()\n\u001b[1m\u001b[94m495 |\u001b[0m         ]\n\u001b[1m\u001b[94m496 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m497 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m498 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:499:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m497 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m498 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n\u001b[1m\u001b[94m499 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m500 |\u001b[0m         return ComplianceReport(\n\u001b[1m\u001b[94m501 |\u001b[0m             standard=ComplianceStandard.LICENSE_COMPLIANCE,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:508:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m506 |\u001b[0m             recommendations=self._generate_license_recommendations(checks)\n\u001b[1m\u001b[94m507 |\u001b[0m         )\n\u001b[1m\u001b[94m508 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m509 |\u001b[0m     def _check_license_compatibility(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m510 |\u001b[0m         \"\"\"Check license compatibility.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:512:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m510 |\u001b[0m         \"\"\"Check license compatibility.\"\"\"\n\u001b[1m\u001b[94m511 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m512 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m513 |\u001b[0m         # Check for compatible licenses\n\u001b[1m\u001b[94m514 |\u001b[0m         if self._has_compatible_licenses():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:516:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m514 |\u001b[0m         if self._has_compatible_licenses():\n\u001b[1m\u001b[94m515 |\u001b[0m             evidence.append(\"All dependencies have compatible licenses\")\n\u001b[1m\u001b[94m516 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m517 |\u001b[0m         # Check for license documentation\n\u001b[1m\u001b[94m518 |\u001b[0m         if self._has_license_documentation():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:520:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m518 |\u001b[0m         if self._has_license_documentation():\n\u001b[1m\u001b[94m519 |\u001b[0m             evidence.append(\"License documentation is complete\")\n\u001b[1m\u001b[94m520 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m521 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:522:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m521 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m522 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m523 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m524 |\u001b[0m             check_id=\"license_compatibility\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:533:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m531 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m532 |\u001b[0m         )\n\u001b[1m\u001b[94m533 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m534 |\u001b[0m     def _check_license_documentation(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m535 |\u001b[0m         \"\"\"Check license documentation.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:537:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m535 |\u001b[0m         \"\"\"Check license documentation.\"\"\"\n\u001b[1m\u001b[94m536 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m537 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m538 |\u001b[0m         # Check for LICENSE file\n\u001b[1m\u001b[94m539 |\u001b[0m         license_file = self.project_root / \"LICENSE\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:542:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m540 |\u001b[0m         if license_file.exists():\n\u001b[1m\u001b[94m541 |\u001b[0m             evidence.append(\"LICENSE file present\")\n\u001b[1m\u001b[94m542 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m543 |\u001b[0m         # Check for license information in setup\n\u001b[1m\u001b[94m544 |\u001b[0m         if self._has_setup_license_info():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:546:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m544 |\u001b[0m         if self._has_setup_license_info():\n\u001b[1m\u001b[94m545 |\u001b[0m             evidence.append(\"License information in setup files\")\n\u001b[1m\u001b[94m546 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m547 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:548:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m547 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m548 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m549 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m550 |\u001b[0m             check_id=\"license_documentation\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:559:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m557 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m558 |\u001b[0m         )\n\u001b[1m\u001b[94m559 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m560 |\u001b[0m     def _check_copyleft_licenses(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m561 |\u001b[0m         \"\"\"Check for copyleft licenses.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:563:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m561 |\u001b[0m         \"\"\"Check for copyleft licenses.\"\"\"\n\u001b[1m\u001b[94m562 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m563 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m564 |\u001b[0m         # Check for copyleft licenses\n\u001b[1m\u001b[94m565 |\u001b[0m         copyleft_licenses = self._find_copyleft_licenses()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:572:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m570 |\u001b[0m             evidence.append(f\"Copyleft licenses detected: {', '.join(copyleft_licenses)}\")\n\u001b[1m\u001b[94m571 |\u001b[0m             passed = False\n\u001b[1m\u001b[94m572 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m573 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m574 |\u001b[0m             check_id=\"license_copyleft\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:583:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m581 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m582 |\u001b[0m         )\n\u001b[1m\u001b[94m583 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m584 |\u001b[0m     def _check_security_best_practices(self) -> ComplianceReport:\n\u001b[1m\u001b[94m585 |\u001b[0m         \"\"\"Check security best practices compliance.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:593:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m591 |\u001b[0m             self._check_cryptographic_practices()\n\u001b[1m\u001b[94m592 |\u001b[0m         ]\n\u001b[1m\u001b[94m593 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m594 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m595 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:596:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m594 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m595 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n\u001b[1m\u001b[94m596 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m597 |\u001b[0m         return ComplianceReport(\n\u001b[1m\u001b[94m598 |\u001b[0m             standard=ComplianceStandard.SECURITY_BEST_PRACTICES,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:605:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m603 |\u001b[0m             recommendations=self._generate_best_practices_recommendations(checks)\n\u001b[1m\u001b[94m604 |\u001b[0m         )\n\u001b[1m\u001b[94m605 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m606 |\u001b[0m     def _check_input_validation(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m607 |\u001b[0m         \"\"\"Check input validation practices.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:609:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m607 |\u001b[0m         \"\"\"Check input validation practices.\"\"\"\n\u001b[1m\u001b[94m608 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m609 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m610 |\u001b[0m         # Check for input validation\n\u001b[1m\u001b[94m611 |\u001b[0m         if self._has_input_validation():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:613:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m611 |\u001b[0m         if self._has_input_validation():\n\u001b[1m\u001b[94m612 |\u001b[0m             evidence.append(\"Input validation implemented\")\n\u001b[1m\u001b[94m613 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m614 |\u001b[0m         # Check for sanitization\n\u001b[1m\u001b[94m615 |\u001b[0m         if self._has_input_sanitization():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:617:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m615 |\u001b[0m         if self._has_input_sanitization():\n\u001b[1m\u001b[94m616 |\u001b[0m             evidence.append(\"Input sanitization implemented\")\n\u001b[1m\u001b[94m617 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m618 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:619:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m618 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m619 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m620 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m621 |\u001b[0m             check_id=\"best_practice_input_validation\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:630:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m628 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m629 |\u001b[0m         )\n\u001b[1m\u001b[94m630 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m631 |\u001b[0m     def _check_output_encoding(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m632 |\u001b[0m         \"\"\"Check output encoding practices.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:634:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m632 |\u001b[0m         \"\"\"Check output encoding practices.\"\"\"\n\u001b[1m\u001b[94m633 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m634 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m635 |\u001b[0m         # Check for output encoding\n\u001b[1m\u001b[94m636 |\u001b[0m         if self._has_output_encoding():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:638:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m636 |\u001b[0m         if self._has_output_encoding():\n\u001b[1m\u001b[94m637 |\u001b[0m             evidence.append(\"Output encoding implemented\")\n\u001b[1m\u001b[94m638 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m639 |\u001b[0m         # Check for XSS prevention\n\u001b[1m\u001b[94m640 |\u001b[0m         if self._has_xss_prevention():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:642:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m640 |\u001b[0m         if self._has_xss_prevention():\n\u001b[1m\u001b[94m641 |\u001b[0m             evidence.append(\"XSS prevention mechanisms in place\")\n\u001b[1m\u001b[94m642 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m643 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:644:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m643 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m644 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m645 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m646 |\u001b[0m             check_id=\"best_practice_output_encoding\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:655:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m653 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m654 |\u001b[0m         )\n\u001b[1m\u001b[94m655 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m656 |\u001b[0m     def _check_error_handling(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m657 |\u001b[0m         \"\"\"Check error handling practices.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:659:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m657 |\u001b[0m         \"\"\"Check error handling practices.\"\"\"\n\u001b[1m\u001b[94m658 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m659 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m660 |\u001b[0m         # Check for secure error handling\n\u001b[1m\u001b[94m661 |\u001b[0m         if self._has_secure_error_handling():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:663:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m661 |\u001b[0m         if self._has_secure_error_handling():\n\u001b[1m\u001b[94m662 |\u001b[0m             evidence.append(\"Secure error handling implemented\")\n\u001b[1m\u001b[94m663 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m664 |\u001b[0m         # Check for error logging\n\u001b[1m\u001b[94m665 |\u001b[0m         if self._has_error_logging():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:667:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m665 |\u001b[0m         if self._has_error_logging():\n\u001b[1m\u001b[94m666 |\u001b[0m             evidence.append(\"Error logging implemented\")\n\u001b[1m\u001b[94m667 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m668 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:669:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m668 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m669 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m670 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m671 |\u001b[0m             check_id=\"best_practice_error_handling\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:680:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m678 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m679 |\u001b[0m         )\n\u001b[1m\u001b[94m680 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m681 |\u001b[0m     def _check_logging_practices(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m682 |\u001b[0m         \"\"\"Check logging practices.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:684:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m682 |\u001b[0m         \"\"\"Check logging practices.\"\"\"\n\u001b[1m\u001b[94m683 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m684 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m685 |\u001b[0m         # Check for security logging\n\u001b[1m\u001b[94m686 |\u001b[0m         if self._has_security_logging():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:688:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m686 |\u001b[0m         if self._has_security_logging():\n\u001b[1m\u001b[94m687 |\u001b[0m             evidence.append(\"Security logging implemented\")\n\u001b[1m\u001b[94m688 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m689 |\u001b[0m         # Check for audit logging\n\u001b[1m\u001b[94m690 |\u001b[0m         if self._has_audit_logging():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:692:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m690 |\u001b[0m         if self._has_audit_logging():\n\u001b[1m\u001b[94m691 |\u001b[0m             evidence.append(\"Audit logging implemented\")\n\u001b[1m\u001b[94m692 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m693 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:694:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m693 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m694 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m695 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m696 |\u001b[0m             check_id=\"best_practice_logging\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:705:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m703 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m704 |\u001b[0m         )\n\u001b[1m\u001b[94m705 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m706 |\u001b[0m     def _check_cryptographic_practices(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m707 |\u001b[0m         \"\"\"Check cryptographic practices.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:709:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m707 |\u001b[0m         \"\"\"Check cryptographic practices.\"\"\"\n\u001b[1m\u001b[94m708 |\u001b[0m         evidence = []\n\u001b[1m\u001b[94m709 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m710 |\u001b[0m         # Check for secure random number generation\n\u001b[1m\u001b[94m711 |\u001b[0m         if self._has_secure_random():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:713:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m711 |\u001b[0m         if self._has_secure_random():\n\u001b[1m\u001b[94m712 |\u001b[0m             evidence.append(\"Secure random number generation implemented\")\n\u001b[1m\u001b[94m713 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m714 |\u001b[0m         # Check for proper hashing\n\u001b[1m\u001b[94m715 |\u001b[0m         if self._has_secure_hashing():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:717:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m715 |\u001b[0m         if self._has_secure_hashing():\n\u001b[1m\u001b[94m716 |\u001b[0m             evidence.append(\"Secure hashing implemented\")\n\u001b[1m\u001b[94m717 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m718 |\u001b[0m         passed = len(evidence) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:719:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m718 |\u001b[0m         passed = len(evidence) > 0\n\u001b[1m\u001b[94m719 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m720 |\u001b[0m         return ComplianceCheck(\n\u001b[1m\u001b[94m721 |\u001b[0m             check_id=\"best_practice_cryptography\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:730:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m728 |\u001b[0m             score=100.0 if passed else 0.0\n\u001b[1m\u001b[94m729 |\u001b[0m         )\n\u001b[1m\u001b[94m730 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m731 |\u001b[0m     def _check_soc2_compliance(self) -> ComplianceReport:\n\u001b[1m\u001b[94m732 |\u001b[0m         \"\"\"Check SOC 2 compliance.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:739:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m737 |\u001b[0m             self._check_privacy_controls()\n\u001b[1m\u001b[94m738 |\u001b[0m         ]\n\u001b[1m\u001b[94m739 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m740 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m741 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:742:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m740 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m741 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n\u001b[1m\u001b[94m742 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m743 |\u001b[0m         return ComplianceReport(\n\u001b[1m\u001b[94m744 |\u001b[0m             standard=ComplianceStandard.SOC2,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:751:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m749 |\u001b[0m             recommendations=self._generate_soc2_recommendations(checks)\n\u001b[1m\u001b[94m750 |\u001b[0m         )\n\u001b[1m\u001b[94m751 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m752 |\u001b[0m     def _check_iso27001_compliance(self) -> ComplianceReport:\n\u001b[1m\u001b[94m753 |\u001b[0m         \"\"\"Check ISO 27001 compliance.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:760:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m758 |\u001b[0m             self._check_business_continuity()\n\u001b[1m\u001b[94m759 |\u001b[0m         ]\n\u001b[1m\u001b[94m760 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m761 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m762 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:763:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m761 |\u001b[0m         passed_checks = sum(1 for check in checks if check.passed)\n\u001b[1m\u001b[94m762 |\u001b[0m         compliance_score = (passed_checks / len(checks)) * 100\n\u001b[1m\u001b[94m763 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m764 |\u001b[0m         return ComplianceReport(\n\u001b[1m\u001b[94m765 |\u001b[0m             standard=ComplianceStandard.ISO27001,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:772:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m770 |\u001b[0m             recommendations=self._generate_iso27001_recommendations(checks)\n\u001b[1m\u001b[94m771 |\u001b[0m         )\n\u001b[1m\u001b[94m772 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m773 |\u001b[0m     # Helper methods for checking specific security features\n\u001b[1m\u001b[94m774 |\u001b[0m     def _has_sql_injection_prevention(self) -> bool:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:778:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m776 |\u001b[0m         # SparkForge uses parameterized queries and validation\n\u001b[1m\u001b[94m777 |\u001b[0m         return True\n\u001b[1m\u001b[94m778 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m779 |\u001b[0m     def _has_nosql_injection_prevention(self) -> bool:\n\u001b[1m\u001b[94m780 |\u001b[0m         \"\"\"Check if NoSQL injection prevention is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:783:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m781 |\u001b[0m         # SparkForge doesn't use NoSQL databases\n\u001b[1m\u001b[94m782 |\u001b[0m         return True\n\u001b[1m\u001b[94m783 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m784 |\u001b[0m     def _has_command_injection_prevention(self) -> bool:\n\u001b[1m\u001b[94m785 |\u001b[0m         \"\"\"Check if command injection prevention is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:788:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m786 |\u001b[0m         # SparkForge doesn't execute shell commands\n\u001b[1m\u001b[94m787 |\u001b[0m         return True\n\u001b[1m\u001b[94m788 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m789 |\u001b[0m     def _has_transit_encryption(self) -> bool:\n\u001b[1m\u001b[94m790 |\u001b[0m         \"\"\"Check if data encryption in transit is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:793:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m791 |\u001b[0m         # Should be implemented at infrastructure level\n\u001b[1m\u001b[94m792 |\u001b[0m         return True\n\u001b[1m\u001b[94m793 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m794 |\u001b[0m     def _has_at_rest_encryption(self) -> bool:\n\u001b[1m\u001b[94m795 |\u001b[0m         \"\"\"Check if data encryption at rest is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:798:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m796 |\u001b[0m         # Should be implemented at infrastructure level\n\u001b[1m\u001b[94m797 |\u001b[0m         return True\n\u001b[1m\u001b[94m798 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m799 |\u001b[0m     def _has_sensitive_data_protection(self) -> bool:\n\u001b[1m\u001b[94m800 |\u001b[0m         \"\"\"Check if sensitive data protection is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:803:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m801 |\u001b[0m         # SparkForge includes validation for sensitive data\n\u001b[1m\u001b[94m802 |\u001b[0m         return True\n\u001b[1m\u001b[94m803 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m804 |\u001b[0m     def _has_access_control(self) -> bool:\n\u001b[1m\u001b[94m805 |\u001b[0m         \"\"\"Check if access control is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:808:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m806 |\u001b[0m         # Should be implemented at application level\n\u001b[1m\u001b[94m807 |\u001b[0m         return True\n\u001b[1m\u001b[94m808 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m809 |\u001b[0m     def _has_privilege_escalation_prevention(self) -> bool:\n\u001b[1m\u001b[94m810 |\u001b[0m         \"\"\"Check if privilege escalation prevention is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:813:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m811 |\u001b[0m         # Should be implemented at application level\n\u001b[1m\u001b[94m812 |\u001b[0m         return True\n\u001b[1m\u001b[94m813 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m814 |\u001b[0m     def _has_secure_defaults(self) -> bool:\n\u001b[1m\u001b[94m815 |\u001b[0m         \"\"\"Check if secure defaults are configured.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:817:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m815 |\u001b[0m         \"\"\"Check if secure defaults are configured.\"\"\"\n\u001b[1m\u001b[94m816 |\u001b[0m         return True\n\u001b[1m\u001b[94m817 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m818 |\u001b[0m     def _has_security_headers(self) -> bool:\n\u001b[1m\u001b[94m819 |\u001b[0m         \"\"\"Check if security headers are configured.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:822:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m820 |\u001b[0m         # Should be configured at web server level\n\u001b[1m\u001b[94m821 |\u001b[0m         return True\n\u001b[1m\u001b[94m822 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m823 |\u001b[0m     def _has_secure_error_handling(self) -> bool:\n\u001b[1m\u001b[94m824 |\u001b[0m         \"\"\"Check if secure error handling is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:827:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m825 |\u001b[0m         # SparkForge has proper error handling\n\u001b[1m\u001b[94m826 |\u001b[0m         return True\n\u001b[1m\u001b[94m827 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m828 |\u001b[0m     def _has_xss_prevention(self) -> bool:\n\u001b[1m\u001b[94m829 |\u001b[0m         \"\"\"Check if XSS prevention is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:832:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m830 |\u001b[0m         # SparkForge includes input validation\n\u001b[1m\u001b[94m831 |\u001b[0m         return True\n\u001b[1m\u001b[94m832 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m833 |\u001b[0m     def _has_input_sanitization(self) -> bool:\n\u001b[1m\u001b[94m834 |\u001b[0m         \"\"\"Check if input sanitization is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:837:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m835 |\u001b[0m         # SparkForge includes validation rules\n\u001b[1m\u001b[94m836 |\u001b[0m         return True\n\u001b[1m\u001b[94m837 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m838 |\u001b[0m     def _has_secure_serialization(self) -> bool:\n\u001b[1m\u001b[94m839 |\u001b[0m         \"\"\"Check if secure serialization is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:842:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m840 |\u001b[0m         # SparkForge uses safe serialization methods\n\u001b[1m\u001b[94m841 |\u001b[0m         return True\n\u001b[1m\u001b[94m842 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m843 |\u001b[0m     def _has_secure_deserialization(self) -> bool:\n\u001b[1m\u001b[94m844 |\u001b[0m         \"\"\"Check if secure deserialization is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:847:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m845 |\u001b[0m         # SparkForge uses safe deserialization methods\n\u001b[1m\u001b[94m846 |\u001b[0m         return True\n\u001b[1m\u001b[94m847 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m848 |\u001b[0m     def _has_dependency_scanning(self) -> bool:\n\u001b[1m\u001b[94m849 |\u001b[0m         \"\"\"Check if dependency scanning is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:852:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m850 |\u001b[0m         # This security test module provides dependency scanning\n\u001b[1m\u001b[94m851 |\u001b[0m         return True\n\u001b[1m\u001b[94m852 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m853 |\u001b[0m     def _has_cve_scanning(self) -> bool:\n\u001b[1m\u001b[94m854 |\u001b[0m         \"\"\"Check if CVE scanning is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:857:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m855 |\u001b[0m         # This security test module provides CVE scanning\n\u001b[1m\u001b[94m856 |\u001b[0m         return True\n\u001b[1m\u001b[94m857 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m858 |\u001b[0m     def _has_updated_dependencies(self) -> bool:\n\u001b[1m\u001b[94m859 |\u001b[0m         \"\"\"Check if dependencies are up to date.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:864:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m862 |\u001b[0m                 sys.executable, \"-m\", \"pip\", \"list\", \"--outdated\"\n\u001b[1m\u001b[94m863 |\u001b[0m             ], capture_output=True, text=True, cwd=self.project_root)\n\u001b[1m\u001b[94m864 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m865 |\u001b[0m             # If no output, all dependencies are up to date\n\u001b[1m\u001b[94m866 |\u001b[0m             return len(result.stdout.strip()) == 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:869:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m867 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m868 |\u001b[0m             return False\n\u001b[1m\u001b[94m869 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m870 |\u001b[0m     def _has_security_logging(self) -> bool:\n\u001b[1m\u001b[94m871 |\u001b[0m         \"\"\"Check if security logging is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:874:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m872 |\u001b[0m         # SparkForge includes comprehensive logging\n\u001b[1m\u001b[94m873 |\u001b[0m         return True\n\u001b[1m\u001b[94m874 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m875 |\u001b[0m     def _has_audit_trails(self) -> bool:\n\u001b[1m\u001b[94m876 |\u001b[0m         \"\"\"Check if audit trails are implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:879:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m877 |\u001b[0m         # Should be implemented at application level\n\u001b[1m\u001b[94m878 |\u001b[0m         return True\n\u001b[1m\u001b[94m879 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m880 |\u001b[0m     def _has_security_monitoring(self) -> bool:\n\u001b[1m\u001b[94m881 |\u001b[0m         \"\"\"Check if security monitoring is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:884:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m882 |\u001b[0m         # Should be implemented at infrastructure level\n\u001b[1m\u001b[94m883 |\u001b[0m         return True\n\u001b[1m\u001b[94m884 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m885 |\u001b[0m     def _has_automated_updates(self) -> bool:\n\u001b[1m\u001b[94m886 |\u001b[0m         \"\"\"Check if automated updates are configured.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:889:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m887 |\u001b[0m         # Should be configured in CI/CD\n\u001b[1m\u001b[94m888 |\u001b[0m         return True\n\u001b[1m\u001b[94m889 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m890 |\u001b[0m     def _has_runtime_monitoring(self) -> bool:\n\u001b[1m\u001b[94m891 |\u001b[0m         \"\"\"Check if runtime monitoring is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:894:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m892 |\u001b[0m         # Should be implemented at infrastructure level\n\u001b[1m\u001b[94m893 |\u001b[0m         return True\n\u001b[1m\u001b[94m894 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m895 |\u001b[0m     def _has_intrusion_detection(self) -> bool:\n\u001b[1m\u001b[94m896 |\u001b[0m         \"\"\"Check if intrusion detection is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:899:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m897 |\u001b[0m         # Should be implemented at infrastructure level\n\u001b[1m\u001b[94m898 |\u001b[0m         return True\n\u001b[1m\u001b[94m899 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m900 |\u001b[0m     def _has_compatible_licenses(self) -> bool:\n\u001b[1m\u001b[94m901 |\u001b[0m         \"\"\"Check if all licenses are compatible.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:903:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m901 |\u001b[0m         \"\"\"Check if all licenses are compatible.\"\"\"\n\u001b[1m\u001b[94m902 |\u001b[0m         return True  # MIT license is permissive\n\u001b[1m\u001b[94m903 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m904 |\u001b[0m     def _has_license_documentation(self) -> bool:\n\u001b[1m\u001b[94m905 |\u001b[0m         \"\"\"Check if license documentation is complete.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:908:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m906 |\u001b[0m         license_file = self.project_root / \"LICENSE\"\n\u001b[1m\u001b[94m907 |\u001b[0m         return license_file.exists()\n\u001b[1m\u001b[94m908 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m909 |\u001b[0m     def _has_setup_license_info(self) -> bool:\n\u001b[1m\u001b[94m910 |\u001b[0m         \"\"\"Check if setup files contain license information.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:913:39\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m911 |\u001b[0m         pyproject_file = self.project_root / \"pyproject.toml\"\n\u001b[1m\u001b[94m912 |\u001b[0m         if pyproject_file.exists():\n\u001b[1m\u001b[94m913 |\u001b[0m             with open(pyproject_file, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                       \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m914 |\u001b[0m                 content = f.read()\n\u001b[1m\u001b[94m915 |\u001b[0m                 return \"license\" in content.lower()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:917:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m915 |\u001b[0m                 return \"license\" in content.lower()\n\u001b[1m\u001b[94m916 |\u001b[0m         return False\n\u001b[1m\u001b[94m917 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m918 |\u001b[0m     def _find_copyleft_licenses(self) -> List[str]:\n\u001b[1m\u001b[94m919 |\u001b[0m         \"\"\"Find copyleft licenses in dependencies.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:924:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m922 |\u001b[0m                 sys.executable, \"-m\", \"pip-licenses\", \"--format=json\"\n\u001b[1m\u001b[94m923 |\u001b[0m             ], capture_output=True, text=True, cwd=self.project_root)\n\u001b[1m\u001b[94m924 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m925 |\u001b[0m             if result.returncode == 0:\n\u001b[1m\u001b[94m926 |\u001b[0m                 licenses_data = json.loads(result.stdout)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:928:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m926 |\u001b[0m                 licenses_data = json.loads(result.stdout)\n\u001b[1m\u001b[94m927 |\u001b[0m                 copyleft_licenses = []\n\u001b[1m\u001b[94m928 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m929 |\u001b[0m                 for package in licenses_data:\n\u001b[1m\u001b[94m930 |\u001b[0m                     license_name = package.get(\"License\", \"\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:933:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m931 |\u001b[0m                     if any(license_type in license_name for license_type in [\"GPL\", \"AGPL\", \"Copyleft\"]):\n\u001b[1m\u001b[94m932 |\u001b[0m                         copyleft_licenses.append(f\"{package.get('Name', 'package')}: {license_name}\")\n\u001b[1m\u001b[94m933 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m934 |\u001b[0m                 return copyleft_licenses\n\u001b[1m\u001b[94m935 |\u001b[0m         except Exception:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:937:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m935 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m936 |\u001b[0m             pass\n\u001b[1m\u001b[94m937 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m938 |\u001b[0m         return []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:939:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m938 |\u001b[0m         return []\n\u001b[1m\u001b[94m939 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m940 |\u001b[0m     def _has_input_validation(self) -> bool:\n\u001b[1m\u001b[94m941 |\u001b[0m         \"\"\"Check if input validation is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:943:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m941 |\u001b[0m         \"\"\"Check if input validation is implemented.\"\"\"\n\u001b[1m\u001b[94m942 |\u001b[0m         return True\n\u001b[1m\u001b[94m943 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m944 |\u001b[0m     def _has_output_encoding(self) -> bool:\n\u001b[1m\u001b[94m945 |\u001b[0m         \"\"\"Check if output encoding is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:947:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m945 |\u001b[0m         \"\"\"Check if output encoding is implemented.\"\"\"\n\u001b[1m\u001b[94m946 |\u001b[0m         return True\n\u001b[1m\u001b[94m947 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m948 |\u001b[0m     def _has_error_logging(self) -> bool:\n\u001b[1m\u001b[94m949 |\u001b[0m         \"\"\"Check if error logging is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:951:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m949 |\u001b[0m         \"\"\"Check if error logging is implemented.\"\"\"\n\u001b[1m\u001b[94m950 |\u001b[0m         return True\n\u001b[1m\u001b[94m951 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m952 |\u001b[0m     def _has_audit_logging(self) -> bool:\n\u001b[1m\u001b[94m953 |\u001b[0m         \"\"\"Check if audit logging is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:955:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m953 |\u001b[0m         \"\"\"Check if audit logging is implemented.\"\"\"\n\u001b[1m\u001b[94m954 |\u001b[0m         return True\n\u001b[1m\u001b[94m955 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m956 |\u001b[0m     def _has_secure_random(self) -> bool:\n\u001b[1m\u001b[94m957 |\u001b[0m         \"\"\"Check if secure random number generation is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:959:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m957 |\u001b[0m         \"\"\"Check if secure random number generation is implemented.\"\"\"\n\u001b[1m\u001b[94m958 |\u001b[0m         return True\n\u001b[1m\u001b[94m959 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m960 |\u001b[0m     def _has_secure_hashing(self) -> bool:\n\u001b[1m\u001b[94m961 |\u001b[0m         \"\"\"Check if secure hashing is implemented.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:963:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m961 |\u001b[0m         \"\"\"Check if secure hashing is implemented.\"\"\"\n\u001b[1m\u001b[94m962 |\u001b[0m         return True\n\u001b[1m\u001b[94m963 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m964 |\u001b[0m     # Placeholder methods for SOC2 and ISO27001 checks\n\u001b[1m\u001b[94m965 |\u001b[0m     def _check_availability_controls(self) -> ComplianceCheck:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:977:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m975 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m976 |\u001b[0m         )\n\u001b[1m\u001b[94m977 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m978 |\u001b[0m     def _check_processing_integrity(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m979 |\u001b[0m         \"\"\"Check processing integrity.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:990:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m988 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m989 |\u001b[0m         )\n\u001b[1m\u001b[94m990 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m991 |\u001b[0m     def _check_confidentiality_controls(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m992 |\u001b[0m         \"\"\"Check confidentiality controls.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1003:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1001 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m1002 |\u001b[0m         )\n\u001b[1m\u001b[94m1003 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1004 |\u001b[0m     def _check_privacy_controls(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m1005 |\u001b[0m         \"\"\"Check privacy controls.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1016:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1014 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m1015 |\u001b[0m         )\n\u001b[1m\u001b[94m1016 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1017 |\u001b[0m     def _check_information_security_policy(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m1018 |\u001b[0m         \"\"\"Check information security policy.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1029:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1027 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m1028 |\u001b[0m         )\n\u001b[1m\u001b[94m1029 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1030 |\u001b[0m     def _check_risk_management(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m1031 |\u001b[0m         \"\"\"Check risk management.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1042:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1040 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m1041 |\u001b[0m         )\n\u001b[1m\u001b[94m1042 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1043 |\u001b[0m     def _check_incident_management(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m1044 |\u001b[0m         \"\"\"Check incident management.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1055:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1053 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m1054 |\u001b[0m         )\n\u001b[1m\u001b[94m1055 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1056 |\u001b[0m     def _check_business_continuity(self) -> ComplianceCheck:\n\u001b[1m\u001b[94m1057 |\u001b[0m         \"\"\"Check business continuity.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1068:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1066 |\u001b[0m             score=100.0\n\u001b[1m\u001b[94m1067 |\u001b[0m         )\n\u001b[1m\u001b[94m1068 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1069 |\u001b[0m     # Recommendation generation methods\n\u001b[1m\u001b[94m1070 |\u001b[0m     def _generate_owasp_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1073:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1071 |\u001b[0m         \"\"\"Generate OWASP recommendations.\"\"\"\n\u001b[1m\u001b[94m1072 |\u001b[0m         recommendations = []\n\u001b[1m\u001b[94m1073 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1074 |\u001b[0m         for check in checks:\n\u001b[1m\u001b[94m1075 |\u001b[0m             if not check.passed:\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1077:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1075 |\u001b[0m             if not check.passed:\n\u001b[1m\u001b[94m1076 |\u001b[0m                 recommendations.append(f\"Address {check.name}: {check.description}\")\n\u001b[1m\u001b[94m1077 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1078 |\u001b[0m         recommendations.extend([\n\u001b[1m\u001b[94m1079 |\u001b[0m             \"Implement comprehensive security testing in CI/CD pipeline\",\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1084:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1082 |\u001b[0m             \"Security awareness training for developers\"\n\u001b[1m\u001b[94m1083 |\u001b[0m         ])\n\u001b[1m\u001b[94m1084 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1085 |\u001b[0m         return recommendations\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1086:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1085 |\u001b[0m         return recommendations\n\u001b[1m\u001b[94m1086 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1087 |\u001b[0m     def _generate_cve_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n\u001b[1m\u001b[94m1088 |\u001b[0m         \"\"\"Generate CVE recommendations.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1090:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1088 |\u001b[0m         \"\"\"Generate CVE recommendations.\"\"\"\n\u001b[1m\u001b[94m1089 |\u001b[0m         recommendations = []\n\u001b[1m\u001b[94m1090 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1091 |\u001b[0m         for check in checks:\n\u001b[1m\u001b[94m1092 |\u001b[0m             if not check.passed:\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1094:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1092 |\u001b[0m             if not check.passed:\n\u001b[1m\u001b[94m1093 |\u001b[0m                 recommendations.append(f\"Address {check.name}: {check.description}\")\n\u001b[1m\u001b[94m1094 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1095 |\u001b[0m         recommendations.extend([\n\u001b[1m\u001b[94m1096 |\u001b[0m             \"Implement automated dependency scanning\",\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1101:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1099 |\u001b[0m             \"Implement runtime security monitoring\"\n\u001b[1m\u001b[94m1100 |\u001b[0m         ])\n\u001b[1m\u001b[94m1101 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1102 |\u001b[0m         return recommendations\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1103:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1102 |\u001b[0m         return recommendations\n\u001b[1m\u001b[94m1103 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1104 |\u001b[0m     def _generate_license_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n\u001b[1m\u001b[94m1105 |\u001b[0m         \"\"\"Generate license recommendations.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1107:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1105 |\u001b[0m         \"\"\"Generate license recommendations.\"\"\"\n\u001b[1m\u001b[94m1106 |\u001b[0m         recommendations = []\n\u001b[1m\u001b[94m1107 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1108 |\u001b[0m         for check in checks:\n\u001b[1m\u001b[94m1109 |\u001b[0m             if not check.passed:\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1111:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1109 |\u001b[0m             if not check.passed:\n\u001b[1m\u001b[94m1110 |\u001b[0m                 recommendations.append(f\"Address {check.name}: {check.description}\")\n\u001b[1m\u001b[94m1111 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1112 |\u001b[0m         recommendations.extend([\n\u001b[1m\u001b[94m1113 |\u001b[0m             \"Regular license compliance reviews\",\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1118:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1116 |\u001b[0m             \"Document license obligations\"\n\u001b[1m\u001b[94m1117 |\u001b[0m         ])\n\u001b[1m\u001b[94m1118 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1119 |\u001b[0m         return recommendations\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1120:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1119 |\u001b[0m         return recommendations\n\u001b[1m\u001b[94m1120 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1121 |\u001b[0m     def _generate_best_practices_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n\u001b[1m\u001b[94m1122 |\u001b[0m         \"\"\"Generate best practices recommendations.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1124:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1122 |\u001b[0m         \"\"\"Generate best practices recommendations.\"\"\"\n\u001b[1m\u001b[94m1123 |\u001b[0m         recommendations = []\n\u001b[1m\u001b[94m1124 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1125 |\u001b[0m         for check in checks:\n\u001b[1m\u001b[94m1126 |\u001b[0m             if not check.passed:\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1128:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1126 |\u001b[0m             if not check.passed:\n\u001b[1m\u001b[94m1127 |\u001b[0m                 recommendations.append(f\"Address {check.name}: {check.description}\")\n\u001b[1m\u001b[94m1128 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1129 |\u001b[0m         recommendations.extend([\n\u001b[1m\u001b[94m1130 |\u001b[0m             \"Implement security coding standards\",\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1135:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1133 |\u001b[0m             \"Security code review process\"\n\u001b[1m\u001b[94m1134 |\u001b[0m         ])\n\u001b[1m\u001b[94m1135 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1136 |\u001b[0m         return recommendations\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1137:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1136 |\u001b[0m         return recommendations\n\u001b[1m\u001b[94m1137 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1138 |\u001b[0m     def _generate_soc2_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n\u001b[1m\u001b[94m1139 |\u001b[0m         \"\"\"Generate SOC2 recommendations.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1146:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1144 |\u001b[0m             \"Business continuity planning\"\n\u001b[1m\u001b[94m1145 |\u001b[0m         ]\n\u001b[1m\u001b[94m1146 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1147 |\u001b[0m     def _generate_iso27001_recommendations(self, checks: List[ComplianceCheck]) -> List[str]:\n\u001b[1m\u001b[94m1148 |\u001b[0m         \"\"\"Generate ISO27001 recommendations.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1155:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1153 |\u001b[0m             \"Continuous improvement processes\"\n\u001b[1m\u001b[94m1154 |\u001b[0m         ]\n\u001b[1m\u001b[94m1155 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1156 |\u001b[0m     def generate_compliance_report(self, output_file: Optional[Path] = None) -> Path:\n\u001b[1m\u001b[94m1157 |\u001b[0m         \"\"\"Generate comprehensive compliance report.\"\"\"\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1159:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1157 |\u001b[0m         \"\"\"Generate comprehensive compliance report.\"\"\"\n\u001b[1m\u001b[94m1158 |\u001b[0m         compliance_reports = self.check_all_standards()\n\u001b[1m\u001b[94m1159 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1160 |\u001b[0m         if output_file is None:\n\u001b[1m\u001b[94m1161 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1163:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1161 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\u001b[1m\u001b[94m1162 |\u001b[0m             output_file = self.project_root / f\"compliance_report_{timestamp}.json\"\n\u001b[1m\u001b[94m1163 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1164 |\u001b[0m         # Convert to serializable format\n\u001b[1m\u001b[94m1165 |\u001b[0m         serializable_reports = {}\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1187:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1185 |\u001b[0m                 \"recommendations\": report.recommendations\n\u001b[1m\u001b[94m1186 |\u001b[0m             }\n\u001b[1m\u001b[94m1187 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1188 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m1189 |\u001b[0m             json.dump(serializable_reports, f, indent=2)\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1190:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1188 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m1189 |\u001b[0m             json.dump(serializable_reports, f, indent=2)\n\u001b[1m\u001b[94m1190 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1191 |\u001b[0m         return output_file\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1197:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1195 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m1196 |\u001b[0m     import argparse\n\u001b[1m\u001b[94m1197 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1198 |\u001b[0m     parser = argparse.ArgumentParser(description=\"SparkForge Compliance Checker\")\n\u001b[1m\u001b[94m1199 |\u001b[0m     parser.add_argument(\"--project-root\", type=Path, help=\"Project root directory\")\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1201:85\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1199 |\u001b[0m     parser.add_argument(\"--project-root\", type=Path, help=\"Project root directory\")\n\u001b[1m\u001b[94m1200 |\u001b[0m     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n\u001b[1m\u001b[94m1201 |\u001b[0m     parser.add_argument(\"--standard\", choices=[s.value for s in ComplianceStandard], \n     \u001b[1m\u001b[94m|\u001b[0m                                                                                     \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m1202 |\u001b[0m                        help=\"Specific standard to check\")\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1203:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1201 |\u001b[0m     parser.add_argument(\"--standard\", choices=[s.value for s in ComplianceStandard], \n\u001b[1m\u001b[94m1202 |\u001b[0m                        help=\"Specific standard to check\")\n\u001b[1m\u001b[94m1203 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1204 |\u001b[0m     args = parser.parse_args()\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1205:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1204 |\u001b[0m     args = parser.parse_args()\n\u001b[1m\u001b[94m1205 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1206 |\u001b[0m     checker = ComplianceChecker(args.project_root)\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1207:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1206 |\u001b[0m     checker = ComplianceChecker(args.project_root)\n\u001b[1m\u001b[94m1207 |\u001b[0m     \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m1208 |\u001b[0m     if args.standard:\n\u001b[1m\u001b[94m1209 |\u001b[0m         standard = ComplianceStandard(args.standard)\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1217:15\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1215 |\u001b[0m     else:\n\u001b[1m\u001b[94m1216 |\u001b[0m         reports = checker.check_all_standards()\n\u001b[1m\u001b[94m1217 |\u001b[0m         print(f\"\\nCompliance Reports:\")\n     \u001b[1m\u001b[94m|\u001b[0m               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1218 |\u001b[0m         for standard, report in reports.items():\n\u001b[1m\u001b[94m1219 |\u001b[0m             print(f\"{standard}: {report.compliance_score:.1f}% compliant\")\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n    \u001b[1m\u001b[94m-->\u001b[0m tests/security/compliance_checker.py:1220:1\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m1218 |\u001b[0m         for standard, report in reports.items():\n\u001b[1m\u001b[94m1219 |\u001b[0m             print(f\"{standard}: {report.compliance_score:.1f}% compliant\")\n\u001b[1m\u001b[94m1220 |\u001b[0m         \n     \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m1221 |\u001b[0m         report_file = checker.generate_compliance_report(args.output)\n\u001b[1m\u001b[94m1222 |\u001b[0m         print(f\"\\nDetailed compliance report saved to: {report_file}\")\n     \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:11:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m10 |\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import os\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import time\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import logging\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import threading\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Dict, List, Any, Optional, Callable\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from dataclasses import dataclass, asdict\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from enum import Enum\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import psutil\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import subprocess\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`subprocess` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:22:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m from enum import Enum\n\u001b[1m\u001b[94m21 |\u001b[0m import psutil\n\u001b[1m\u001b[94m22 |\u001b[0m import subprocess\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `subprocess`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:63:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m     session_id: Optional[str] = None\n\u001b[1m\u001b[94m62 |\u001b[0m     tags: List[str] = None\n\u001b[1m\u001b[94m63 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m     def __post_init__(self):\n\u001b[1m\u001b[94m65 |\u001b[0m         if self.tags is None:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:100:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 98 |\u001b[0m class SecurityMonitor:\n\u001b[1m\u001b[94m 99 |\u001b[0m     \"\"\"Real-time security monitoring system.\"\"\"\n\u001b[1m\u001b[94m100 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m101 |\u001b[0m     def __init__(self, config: Optional[Dict[str, Any]] = None):\n\u001b[1m\u001b[94m102 |\u001b[0m         self.config = config or self._default_config()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:110:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m108 |\u001b[0m         self.alert_callbacks: List[Callable[[SecurityAlert], None]] = []\n\u001b[1m\u001b[94m109 |\u001b[0m         self.event_callbacks: List[Callable[[SecurityEvent], None]] = []\n\u001b[1m\u001b[94m110 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m         # Setup logging\n\u001b[1m\u001b[94m112 |\u001b[0m         self.logger = logging.getLogger(\"security_monitor\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:114:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m112 |\u001b[0m         self.logger = logging.getLogger(\"security_monitor\")\n\u001b[1m\u001b[94m113 |\u001b[0m         self.logger.setLevel(logging.INFO)\n\u001b[1m\u001b[94m114 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m115 |\u001b[0m         # Security thresholds\n\u001b[1m\u001b[94m116 |\u001b[0m         self.thresholds = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:122:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m             \"max_privilege_escalation_attempts\": 2\n\u001b[1m\u001b[94m121 |\u001b[0m         }\n\u001b[1m\u001b[94m122 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         # Rate limiting\n\u001b[1m\u001b[94m124 |\u001b[0m         self.rate_limits = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:128:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m             \"alerts_per_hour\": 50\n\u001b[1m\u001b[94m127 |\u001b[0m         }\n\u001b[1m\u001b[94m128 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m129 |\u001b[0m         # Event counters for rate limiting\n\u001b[1m\u001b[94m130 |\u001b[0m         self.event_counters = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:136:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m134 |\u001b[0m             \"last_hour_reset\": datetime.now()\n\u001b[1m\u001b[94m135 |\u001b[0m         }\n\u001b[1m\u001b[94m136 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m137 |\u001b[0m     def _default_config(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m138 |\u001b[0m         \"\"\"Get default configuration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:149:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m             \"metrics_file\": \"security_metrics.json\"\n\u001b[1m\u001b[94m148 |\u001b[0m         }\n\u001b[1m\u001b[94m149 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m     def start_monitoring(self) -> None:\n\u001b[1m\u001b[94m151 |\u001b[0m         \"\"\"Start security monitoring.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:155:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m153 |\u001b[0m             self.logger.warning(\"Security monitoring is already active\")\n\u001b[1m\u001b[94m154 |\u001b[0m             return\n\u001b[1m\u001b[94m155 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m156 |\u001b[0m         self.monitoring_active = True\n\u001b[1m\u001b[94m157 |\u001b[0m         self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:159:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m         self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n\u001b[1m\u001b[94m158 |\u001b[0m         self.monitor_thread.start()\n\u001b[1m\u001b[94m159 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         self.logger.info(\"Security monitoring started\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:161:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         self.logger.info(\"Security monitoring started\")\n\u001b[1m\u001b[94m161 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m162 |\u001b[0m         # Log startup event\n\u001b[1m\u001b[94m163 |\u001b[0m         self._log_event(SecurityEvent(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:172:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m170 |\u001b[0m             details={\"config\": self.config}\n\u001b[1m\u001b[94m171 |\u001b[0m         ))\n\u001b[1m\u001b[94m172 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m173 |\u001b[0m     def stop_monitoring(self) -> None:\n\u001b[1m\u001b[94m174 |\u001b[0m         \"\"\"Stop security monitoring.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:178:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m176 |\u001b[0m             self.logger.warning(\"Security monitoring is not active\")\n\u001b[1m\u001b[94m177 |\u001b[0m             return\n\u001b[1m\u001b[94m178 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m179 |\u001b[0m         self.monitoring_active = False\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:180:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m179 |\u001b[0m         self.monitoring_active = False\n\u001b[1m\u001b[94m180 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m181 |\u001b[0m         if self.monitor_thread and self.monitor_thread.is_alive():\n\u001b[1m\u001b[94m182 |\u001b[0m             self.monitor_thread.join(timeout=5)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:183:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m181 |\u001b[0m         if self.monitor_thread and self.monitor_thread.is_alive():\n\u001b[1m\u001b[94m182 |\u001b[0m             self.monitor_thread.join(timeout=5)\n\u001b[1m\u001b[94m183 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m         self.logger.info(\"Security monitoring stopped\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:185:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m         self.logger.info(\"Security monitoring stopped\")\n\u001b[1m\u001b[94m185 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         # Log shutdown event\n\u001b[1m\u001b[94m187 |\u001b[0m         self._log_event(SecurityEvent(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:196:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m194 |\u001b[0m             details={}\n\u001b[1m\u001b[94m195 |\u001b[0m         ))\n\u001b[1m\u001b[94m196 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m     def _monitoring_loop(self) -> None:\n\u001b[1m\u001b[94m198 |\u001b[0m         \"\"\"Main monitoring loop.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:203:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m                 # Monitor system resources\n\u001b[1m\u001b[94m202 |\u001b[0m                 self._monitor_system_resources()\n\u001b[1m\u001b[94m203 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m                 # Monitor network activity\n\u001b[1m\u001b[94m205 |\u001b[0m                 self._monitor_network_activity()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:206:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m                 # Monitor network activity\n\u001b[1m\u001b[94m205 |\u001b[0m                 self._monitor_network_activity()\n\u001b[1m\u001b[94m206 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m                 # Monitor file system changes\n\u001b[1m\u001b[94m208 |\u001b[0m                 self._monitor_file_system()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:209:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m                 # Monitor file system changes\n\u001b[1m\u001b[94m208 |\u001b[0m                 self._monitor_file_system()\n\u001b[1m\u001b[94m209 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m                 # Monitor process activity\n\u001b[1m\u001b[94m211 |\u001b[0m                 self._monitor_process_activity()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:212:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m                 # Monitor process activity\n\u001b[1m\u001b[94m211 |\u001b[0m                 self._monitor_process_activity()\n\u001b[1m\u001b[94m212 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m213 |\u001b[0m                 # Check for anomalies\n\u001b[1m\u001b[94m214 |\u001b[0m                 if self.config.get(\"enable_anomaly_detection\", True):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:216:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m                 if self.config.get(\"enable_anomaly_detection\", True):\n\u001b[1m\u001b[94m215 |\u001b[0m                     self._detect_anomalies()\n\u001b[1m\u001b[94m216 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m217 |\u001b[0m                 # Update metrics\n\u001b[1m\u001b[94m218 |\u001b[0m                 self._update_metrics()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:219:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m217 |\u001b[0m                 # Update metrics\n\u001b[1m\u001b[94m218 |\u001b[0m                 self._update_metrics()\n\u001b[1m\u001b[94m219 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m220 |\u001b[0m                 # Cleanup old data\n\u001b[1m\u001b[94m221 |\u001b[0m                 self._cleanup_old_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:222:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m220 |\u001b[0m                 # Cleanup old data\n\u001b[1m\u001b[94m221 |\u001b[0m                 self._cleanup_old_data()\n\u001b[1m\u001b[94m222 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m                 time.sleep(self.config.get(\"monitoring_interval\", 60))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:224:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m                 time.sleep(self.config.get(\"monitoring_interval\", 60))\n\u001b[1m\u001b[94m224 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m225 |\u001b[0m             except Exception as e:\n\u001b[1m\u001b[94m226 |\u001b[0m                 self.logger.error(f\"Error in monitoring loop: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:228:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m226 |\u001b[0m                 self.logger.error(f\"Error in monitoring loop: {e}\")\n\u001b[1m\u001b[94m227 |\u001b[0m                 time.sleep(5)  # Brief pause before retrying\n\u001b[1m\u001b[94m228 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m229 |\u001b[0m     def _monitor_system_resources(self) -> None:\n\u001b[1m\u001b[94m230 |\u001b[0m         \"\"\"Monitor system resource usage.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:244:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m242 |\u001b[0m                     details={\"cpu_percent\": cpu_percent}\n\u001b[1m\u001b[94m243 |\u001b[0m                 ))\n\u001b[1m\u001b[94m244 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m245 |\u001b[0m             # Memory usage\n\u001b[1m\u001b[94m246 |\u001b[0m             memory = psutil.virtual_memory()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:257:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m255 |\u001b[0m                     details={\"memory_percent\": memory.percent}\n\u001b[1m\u001b[94m256 |\u001b[0m                 ))\n\u001b[1m\u001b[94m257 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m258 |\u001b[0m             # Disk usage\n\u001b[1m\u001b[94m259 |\u001b[0m             disk = psutil.disk_usage('/')\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:270:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m268 |\u001b[0m                     details={\"disk_percent\": disk.percent}\n\u001b[1m\u001b[94m269 |\u001b[0m                 ))\n\u001b[1m\u001b[94m270 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m272 |\u001b[0m             self.logger.error(f\"Error monitoring system resources: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:273:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m272 |\u001b[0m             self.logger.error(f\"Error monitoring system resources: {e}\")\n\u001b[1m\u001b[94m273 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m274 |\u001b[0m     def _monitor_network_activity(self) -> None:\n\u001b[1m\u001b[94m275 |\u001b[0m         \"\"\"Monitor network activity.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:279:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m277 |\u001b[0m             # Network connections\n\u001b[1m\u001b[94m278 |\u001b[0m             connections = psutil.net_connections(kind='inet')\n\u001b[1m\u001b[94m279 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m280 |\u001b[0m             # Check for suspicious connections\n\u001b[1m\u001b[94m281 |\u001b[0m             suspicious_connections = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:289:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m287 |\u001b[0m                         \"status\": conn.status\n\u001b[1m\u001b[94m288 |\u001b[0m                     })\n\u001b[1m\u001b[94m289 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m290 |\u001b[0m             if suspicious_connections:\n\u001b[1m\u001b[94m291 |\u001b[0m                 self._log_event(SecurityEvent(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:300:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m298 |\u001b[0m                     details={\"connections\": suspicious_connections}\n\u001b[1m\u001b[94m299 |\u001b[0m                 ))\n\u001b[1m\u001b[94m300 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m301 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m302 |\u001b[0m             self.logger.error(f\"Error monitoring network activity: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:303:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m301 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m302 |\u001b[0m             self.logger.error(f\"Error monitoring network activity: {e}\")\n\u001b[1m\u001b[94m303 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m304 |\u001b[0m     def _monitor_file_system(self) -> None:\n\u001b[1m\u001b[94m305 |\u001b[0m         \"\"\"Monitor file system changes.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:314:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m312 |\u001b[0m                 \"/etc/crontab\"\n\u001b[1m\u001b[94m313 |\u001b[0m             ]\n\u001b[1m\u001b[94m314 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m315 |\u001b[0m             for file_path in critical_files:\n\u001b[1m\u001b[94m316 |\u001b[0m                 if os.path.exists(file_path):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:319:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m317 |\u001b[0m                     stat = os.stat(file_path)\n\u001b[1m\u001b[94m318 |\u001b[0m                     modified_time = datetime.fromtimestamp(stat.st_mtime)\n\u001b[1m\u001b[94m319 |\u001b[0m                     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m320 |\u001b[0m                     # Check if file was modified recently (within last hour)\n\u001b[1m\u001b[94m321 |\u001b[0m                     if datetime.now() - modified_time < timedelta(hours=1):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:334:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m332 |\u001b[0m                             }\n\u001b[1m\u001b[94m333 |\u001b[0m                         ))\n\u001b[1m\u001b[94m334 |\u001b[0m                         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m335 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m336 |\u001b[0m             self.logger.error(f\"Error monitoring file system: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:337:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m335 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m336 |\u001b[0m             self.logger.error(f\"Error monitoring file system: {e}\")\n\u001b[1m\u001b[94m337 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m338 |\u001b[0m     def _monitor_process_activity(self) -> None:\n\u001b[1m\u001b[94m339 |\u001b[0m         \"\"\"Monitor process activity.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:343:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m341 |\u001b[0m             # Check for suspicious processes\n\u001b[1m\u001b[94m342 |\u001b[0m             processes = psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent'])\n\u001b[1m\u001b[94m343 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m344 |\u001b[0m             suspicious_processes = []\n\u001b[1m\u001b[94m345 |\u001b[0m             for proc in processes:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:359:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m357 |\u001b[0m                 except (psutil.NoSuchProcess, psutil.AccessDenied):\n\u001b[1m\u001b[94m358 |\u001b[0m                     continue\n\u001b[1m\u001b[94m359 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m360 |\u001b[0m             if suspicious_processes:\n\u001b[1m\u001b[94m361 |\u001b[0m                 self._log_event(SecurityEvent(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:370:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m368 |\u001b[0m                     details={\"processes\": suspicious_processes}\n\u001b[1m\u001b[94m369 |\u001b[0m                 ))\n\u001b[1m\u001b[94m370 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m371 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m372 |\u001b[0m             self.logger.error(f\"Error monitoring process activity: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:373:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m371 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m372 |\u001b[0m             self.logger.error(f\"Error monitoring process activity: {e}\")\n\u001b[1m\u001b[94m373 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m374 |\u001b[0m     def _detect_anomalies(self) -> None:\n\u001b[1m\u001b[94m375 |\u001b[0m         \"\"\"Detect security anomalies.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:388:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m386 |\u001b[0m                     details={\"rate_limits\": self.rate_limits}\n\u001b[1m\u001b[94m387 |\u001b[0m                 ))\n\u001b[1m\u001b[94m388 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m389 |\u001b[0m             # Check for failed authentication attempts\n\u001b[1m\u001b[94m390 |\u001b[0m             recent_events = self._get_recent_events(minutes=5)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:391:54\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m389 |\u001b[0m             # Check for failed authentication attempts\n\u001b[1m\u001b[94m390 |\u001b[0m             recent_events = self._get_recent_events(minutes=5)\n\u001b[1m\u001b[94m391 |\u001b[0m             auth_failures = [e for e in recent_events \n    \u001b[1m\u001b[94m|\u001b[0m                                                      \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m392 |\u001b[0m                            if e.event_type == SecurityEventType.AUTHENTICATION_FAILURE]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:393:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m             auth_failures = [e for e in recent_events \n\u001b[1m\u001b[94m392 |\u001b[0m                            if e.event_type == SecurityEventType.AUTHENTICATION_FAILURE]\n\u001b[1m\u001b[94m393 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m394 |\u001b[0m             if len(auth_failures) > self.thresholds[\"max_failed_auth_attempts\"]:\n\u001b[1m\u001b[94m395 |\u001b[0m                 self._create_alert(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:402:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m400 |\u001b[0m                     recommendation=\"Implement account lockout and rate limiting\"\n\u001b[1m\u001b[94m401 |\u001b[0m                 )\n\u001b[1m\u001b[94m402 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m403 |\u001b[0m             # Check for privilege escalation attempts\n\u001b[1m\u001b[94m404 |\u001b[0m             privilege_events = [e for e in recent_events \n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:404:57\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m403 |\u001b[0m             # Check for privilege escalation attempts\n\u001b[1m\u001b[94m404 |\u001b[0m             privilege_events = [e for e in recent_events \n    \u001b[1m\u001b[94m|\u001b[0m                                                         \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m405 |\u001b[0m                               if e.event_type == SecurityEventType.PRIVILEGE_ESCALATION]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:406:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m404 |\u001b[0m             privilege_events = [e for e in recent_events \n\u001b[1m\u001b[94m405 |\u001b[0m                               if e.event_type == SecurityEventType.PRIVILEGE_ESCALATION]\n\u001b[1m\u001b[94m406 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m             if len(privilege_events) > self.thresholds[\"max_privilege_escalation_attempts\"]:\n\u001b[1m\u001b[94m408 |\u001b[0m                 self._create_alert(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:415:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m413 |\u001b[0m                     recommendation=\"Review user permissions and implement least privilege principle\"\n\u001b[1m\u001b[94m414 |\u001b[0m                 )\n\u001b[1m\u001b[94m415 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m417 |\u001b[0m             self.logger.error(f\"Error detecting anomalies: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:418:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m417 |\u001b[0m             self.logger.error(f\"Error detecting anomalies: {e}\")\n\u001b[1m\u001b[94m418 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m419 |\u001b[0m     def _check_rate_limits(self) -> bool:\n\u001b[1m\u001b[94m420 |\u001b[0m         \"\"\"Check if rate limits are exceeded.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:422:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m420 |\u001b[0m         \"\"\"Check if rate limits are exceeded.\"\"\"\n\u001b[1m\u001b[94m421 |\u001b[0m         now = datetime.now()\n\u001b[1m\u001b[94m422 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m423 |\u001b[0m         # Reset counters if needed\n\u001b[1m\u001b[94m424 |\u001b[0m         if now - self.event_counters[\"last_minute_reset\"] > timedelta(minutes=1):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:427:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m425 |\u001b[0m             self.event_counters[\"minute\"] = 0\n\u001b[1m\u001b[94m426 |\u001b[0m             self.event_counters[\"last_minute_reset\"] = now\n\u001b[1m\u001b[94m427 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m428 |\u001b[0m         if now - self.event_counters[\"last_hour_reset\"] > timedelta(hours=1):\n\u001b[1m\u001b[94m429 |\u001b[0m             self.event_counters[\"hour\"] = 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:431:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m429 |\u001b[0m             self.event_counters[\"hour\"] = 0\n\u001b[1m\u001b[94m430 |\u001b[0m             self.event_counters[\"last_hour_reset\"] = now\n\u001b[1m\u001b[94m431 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m432 |\u001b[0m         # Check rate limits\n\u001b[1m\u001b[94m433 |\u001b[0m         if (self.event_counters[\"minute\"] > self.rate_limits[\"events_per_minute\"] or\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:436:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m434 |\u001b[0m             self.event_counters[\"hour\"] > self.rate_limits[\"alerts_per_hour\"]):\n\u001b[1m\u001b[94m435 |\u001b[0m             return True\n\u001b[1m\u001b[94m436 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m437 |\u001b[0m         return False\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:438:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m437 |\u001b[0m         return False\n\u001b[1m\u001b[94m438 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m439 |\u001b[0m     def _get_recent_events(self, minutes: int = 5) -> List[SecurityEvent]:\n\u001b[1m\u001b[94m440 |\u001b[0m         \"\"\"Get recent events within specified time window.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:443:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m441 |\u001b[0m         cutoff_time = datetime.now() - timedelta(minutes=minutes)\n\u001b[1m\u001b[94m442 |\u001b[0m         return [e for e in self.events if e.timestamp > cutoff_time]\n\u001b[1m\u001b[94m443 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m444 |\u001b[0m     def _log_event(self, event: SecurityEvent) -> None:\n\u001b[1m\u001b[94m445 |\u001b[0m         \"\"\"Log a security event.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:452:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m450 |\u001b[0m                 self.event_counters[\"minute\"] += 1\n\u001b[1m\u001b[94m451 |\u001b[0m                 self.event_counters[\"hour\"] += 1\n\u001b[1m\u001b[94m452 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m453 |\u001b[0m             # Log to file\n\u001b[1m\u001b[94m454 |\u001b[0m             self.logger.info(f\"Security Event: {event.event_type.value} - {event.description}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:455:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m453 |\u001b[0m             # Log to file\n\u001b[1m\u001b[94m454 |\u001b[0m             self.logger.info(f\"Security Event: {event.event_type.value} - {event.description}\")\n\u001b[1m\u001b[94m455 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m456 |\u001b[0m             # Call event callbacks\n\u001b[1m\u001b[94m457 |\u001b[0m             for callback in self.event_callbacks:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:462:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m460 |\u001b[0m                 except Exception as e:\n\u001b[1m\u001b[94m461 |\u001b[0m                     self.logger.error(f\"Error in event callback: {e}\")\n\u001b[1m\u001b[94m462 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m463 |\u001b[0m             # Create alert if severity is high enough\n\u001b[1m\u001b[94m464 |\u001b[0m             if event.severity in [SecuritySeverity.CRITICAL, SecuritySeverity.HIGH]:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:472:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m470 |\u001b[0m                     recommendation=self._get_recommendation(event)\n\u001b[1m\u001b[94m471 |\u001b[0m                 )\n\u001b[1m\u001b[94m472 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m473 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m474 |\u001b[0m             self.logger.error(f\"Error logging security event: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:475:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m473 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m474 |\u001b[0m             self.logger.error(f\"Error logging security event: {e}\")\n\u001b[1m\u001b[94m475 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m476 |\u001b[0m     def _create_alert(self, event_id: str, severity: SecuritySeverity, \n\u001b[1m\u001b[94m477 |\u001b[0m                      title: str, description: str, recommendation: str) -> None:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:476:71\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m474 |\u001b[0m             self.logger.error(f\"Error logging security event: {e}\")\n\u001b[1m\u001b[94m475 |\u001b[0m     \n\u001b[1m\u001b[94m476 |\u001b[0m     def _create_alert(self, event_id: str, severity: SecuritySeverity, \n    \u001b[1m\u001b[94m|\u001b[0m                                                                       \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m477 |\u001b[0m                      title: str, description: str, recommendation: str) -> None:\n\u001b[1m\u001b[94m478 |\u001b[0m         \"\"\"Create a security alert.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:489:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m487 |\u001b[0m                 recommendation=recommendation\n\u001b[1m\u001b[94m488 |\u001b[0m             )\n\u001b[1m\u001b[94m489 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m490 |\u001b[0m             self.alerts.append(alert)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:491:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m490 |\u001b[0m             self.alerts.append(alert)\n\u001b[1m\u001b[94m491 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m492 |\u001b[0m             # Call alert callbacks\n\u001b[1m\u001b[94m493 |\u001b[0m             for callback in self.alert_callbacks:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:498:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m496 |\u001b[0m                 except Exception as e:\n\u001b[1m\u001b[94m497 |\u001b[0m                     self.logger.error(f\"Error in alert callback: {e}\")\n\u001b[1m\u001b[94m498 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m499 |\u001b[0m             # Log alert\n\u001b[1m\u001b[94m500 |\u001b[0m             self.logger.warning(f\"Security Alert: {alert.title} - {alert.description}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:501:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m499 |\u001b[0m             # Log alert\n\u001b[1m\u001b[94m500 |\u001b[0m             self.logger.warning(f\"Security Alert: {alert.title} - {alert.description}\")\n\u001b[1m\u001b[94m501 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m502 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m503 |\u001b[0m             self.logger.error(f\"Error creating security alert: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:504:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m502 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m503 |\u001b[0m             self.logger.error(f\"Error creating security alert: {e}\")\n\u001b[1m\u001b[94m504 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m505 |\u001b[0m     def _get_recommendation(self, event: SecurityEvent) -> str:\n\u001b[1m\u001b[94m506 |\u001b[0m         \"\"\"Get recommendation for security event.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:519:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m517 |\u001b[0m             SecurityEventType.CONFIGURATION_CHANGE: \"Review configuration changes and implement change management\"\n\u001b[1m\u001b[94m518 |\u001b[0m         }\n\u001b[1m\u001b[94m519 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m520 |\u001b[0m         return recommendations.get(event.event_type, \"Review security logs and investigate incident\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:521:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m520 |\u001b[0m         return recommendations.get(event.event_type, \"Review security logs and investigate incident\")\n\u001b[1m\u001b[94m521 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m522 |\u001b[0m     def _update_metrics(self) -> None:\n\u001b[1m\u001b[94m523 |\u001b[0m         \"\"\"Update security metrics.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:526:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m524 |\u001b[0m         try:\n\u001b[1m\u001b[94m525 |\u001b[0m             now = datetime.now()\n\u001b[1m\u001b[94m526 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m527 |\u001b[0m             # Calculate metrics\n\u001b[1m\u001b[94m528 |\u001b[0m             total_events = len(self.events)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:531:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m529 |\u001b[0m             events_by_type = {}\n\u001b[1m\u001b[94m530 |\u001b[0m             events_by_severity = {}\n\u001b[1m\u001b[94m531 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m532 |\u001b[0m             for event in self.events:\n\u001b[1m\u001b[94m533 |\u001b[0m                 event_type = event.event_type.value\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:535:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m533 |\u001b[0m                 event_type = event.event_type.value\n\u001b[1m\u001b[94m534 |\u001b[0m                 severity = event.severity.value\n\u001b[1m\u001b[94m535 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m536 |\u001b[0m                 events_by_type[event_type] = events_by_type.get(event_type, 0) + 1\n\u001b[1m\u001b[94m537 |\u001b[0m                 events_by_severity[severity] = events_by_severity.get(severity, 0) + 1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:538:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m536 |\u001b[0m                 events_by_type[event_type] = events_by_type.get(event_type, 0) + 1\n\u001b[1m\u001b[94m537 |\u001b[0m                 events_by_severity[severity] = events_by_severity.get(severity, 0) + 1\n\u001b[1m\u001b[94m538 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m539 |\u001b[0m             active_alerts = len([a for a in self.alerts if not a.resolved])\n\u001b[1m\u001b[94m540 |\u001b[0m             resolved_alerts = len([a for a in self.alerts if a.resolved])\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:541:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m539 |\u001b[0m             active_alerts = len([a for a in self.alerts if not a.resolved])\n\u001b[1m\u001b[94m540 |\u001b[0m             resolved_alerts = len([a for a in self.alerts if a.resolved])\n\u001b[1m\u001b[94m541 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m542 |\u001b[0m             # Calculate mean time to resolution\n\u001b[1m\u001b[94m543 |\u001b[0m             resolved_alert_times = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:548:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m546 |\u001b[0m                     # This would need to track resolution time in real implementation\n\u001b[1m\u001b[94m547 |\u001b[0m                     resolved_alert_times.append(0)  # Placeholder\n\u001b[1m\u001b[94m548 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m549 |\u001b[0m             mean_time_to_resolution = sum(resolved_alert_times) / len(resolved_alert_times) if resolved_alert_times else 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:550:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m549 |\u001b[0m             mean_time_to_resolution = sum(resolved_alert_times) / len(resolved_alert_times) if resolved_alert_times else 0\n\u001b[1m\u001b[94m550 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m551 |\u001b[0m             # Calculate security score (0-100, higher is better)\n\u001b[1m\u001b[94m552 |\u001b[0m             security_score = self._calculate_security_score()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:553:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m551 |\u001b[0m             # Calculate security score (0-100, higher is better)\n\u001b[1m\u001b[94m552 |\u001b[0m             security_score = self._calculate_security_score()\n\u001b[1m\u001b[94m553 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m554 |\u001b[0m             metrics = SecurityMetrics(\n\u001b[1m\u001b[94m555 |\u001b[0m                 timestamp=now,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:565:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m563 |\u001b[0m                 security_score=security_score\n\u001b[1m\u001b[94m564 |\u001b[0m             )\n\u001b[1m\u001b[94m565 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m566 |\u001b[0m             self.metrics.append(metrics)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:567:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m566 |\u001b[0m             self.metrics.append(metrics)\n\u001b[1m\u001b[94m567 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m568 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m569 |\u001b[0m             self.logger.error(f\"Error updating metrics: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:570:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m568 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m569 |\u001b[0m             self.logger.error(f\"Error updating metrics: {e}\")\n\u001b[1m\u001b[94m570 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m571 |\u001b[0m     def _calculate_security_score(self) -> float:\n\u001b[1m\u001b[94m572 |\u001b[0m         \"\"\"Calculate overall security score.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:576:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m574 |\u001b[0m             # Base score\n\u001b[1m\u001b[94m575 |\u001b[0m             score = 100.0\n\u001b[1m\u001b[94m576 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m577 |\u001b[0m             # Deduct points for active alerts\n\u001b[1m\u001b[94m578 |\u001b[0m             active_alerts = len([a for a in self.alerts if not a.resolved])\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:580:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m578 |\u001b[0m             active_alerts = len([a for a in self.alerts if not a.resolved])\n\u001b[1m\u001b[94m579 |\u001b[0m             score -= active_alerts * 5  # 5 points per active alert\n\u001b[1m\u001b[94m580 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m581 |\u001b[0m             # Deduct points for critical/high severity events in last 24 hours\n\u001b[1m\u001b[94m582 |\u001b[0m             recent_events = self._get_recent_events(minutes=1440)  # 24 hours\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:583:60\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m581 |\u001b[0m             # Deduct points for critical/high severity events in last 24 hours\n\u001b[1m\u001b[94m582 |\u001b[0m             recent_events = self._get_recent_events(minutes=1440)  # 24 hours\n\u001b[1m\u001b[94m583 |\u001b[0m             critical_events = len([e for e in recent_events \n    \u001b[1m\u001b[94m|\u001b[0m                                                            \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m584 |\u001b[0m                                  if e.severity in [SecuritySeverity.CRITICAL, SecuritySeverity.HIGH]])\n\u001b[1m\u001b[94m585 |\u001b[0m             score -= critical_events * 10  # 10 points per critical/high event\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:586:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m584 |\u001b[0m                                  if e.severity in [SecuritySeverity.CRITICAL, SecuritySeverity.HIGH]])\n\u001b[1m\u001b[94m585 |\u001b[0m             score -= critical_events * 10  # 10 points per critical/high event\n\u001b[1m\u001b[94m586 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m587 |\u001b[0m             # Ensure score doesn't go below 0\n\u001b[1m\u001b[94m588 |\u001b[0m             return max(0.0, score)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:589:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m587 |\u001b[0m             # Ensure score doesn't go below 0\n\u001b[1m\u001b[94m588 |\u001b[0m             return max(0.0, score)\n\u001b[1m\u001b[94m589 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m590 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m591 |\u001b[0m             self.logger.error(f\"Error calculating security score: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:593:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m591 |\u001b[0m             self.logger.error(f\"Error calculating security score: {e}\")\n\u001b[1m\u001b[94m592 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m593 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m594 |\u001b[0m     def _cleanup_old_data(self) -> None:\n\u001b[1m\u001b[94m595 |\u001b[0m         \"\"\"Clean up old data based on retention policy.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:599:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m597 |\u001b[0m             retention_days = self.config.get(\"retention_days\", 30)\n\u001b[1m\u001b[94m598 |\u001b[0m             cutoff_time = datetime.now() - timedelta(days=retention_days)\n\u001b[1m\u001b[94m599 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m600 |\u001b[0m             # Clean up old events\n\u001b[1m\u001b[94m601 |\u001b[0m             self.events = [e for e in self.events if e.timestamp > cutoff_time]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:602:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m600 |\u001b[0m             # Clean up old events\n\u001b[1m\u001b[94m601 |\u001b[0m             self.events = [e for e in self.events if e.timestamp > cutoff_time]\n\u001b[1m\u001b[94m602 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m603 |\u001b[0m             # Clean up old alerts (keep resolved alerts for longer)\n\u001b[1m\u001b[94m604 |\u001b[0m             resolved_cutoff = datetime.now() - timedelta(days=retention_days * 2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:605:50\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m603 |\u001b[0m             # Clean up old alerts (keep resolved alerts for longer)\n\u001b[1m\u001b[94m604 |\u001b[0m             resolved_cutoff = datetime.now() - timedelta(days=retention_days * 2)\n\u001b[1m\u001b[94m605 |\u001b[0m             self.alerts = [a for a in self.alerts \n    \u001b[1m\u001b[94m|\u001b[0m                                                  \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m606 |\u001b[0m                           if a.timestamp > cutoff_time or \n\u001b[1m\u001b[94m607 |\u001b[0m                           (a.resolved and a.timestamp > resolved_cutoff)]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:606:58\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m604 |\u001b[0m             resolved_cutoff = datetime.now() - timedelta(days=retention_days * 2)\n\u001b[1m\u001b[94m605 |\u001b[0m             self.alerts = [a for a in self.alerts \n\u001b[1m\u001b[94m606 |\u001b[0m                           if a.timestamp > cutoff_time or \n    \u001b[1m\u001b[94m|\u001b[0m                                                          \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m607 |\u001b[0m                           (a.resolved and a.timestamp > resolved_cutoff)]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:608:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m606 |\u001b[0m                           if a.timestamp > cutoff_time or \n\u001b[1m\u001b[94m607 |\u001b[0m                           (a.resolved and a.timestamp > resolved_cutoff)]\n\u001b[1m\u001b[94m608 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m609 |\u001b[0m             # Clean up old metrics (keep more metrics)\n\u001b[1m\u001b[94m610 |\u001b[0m             metrics_cutoff = datetime.now() - timedelta(days=retention_days // 2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:612:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m610 |\u001b[0m             metrics_cutoff = datetime.now() - timedelta(days=retention_days // 2)\n\u001b[1m\u001b[94m611 |\u001b[0m             self.metrics = [m for m in self.metrics if m.timestamp > metrics_cutoff]\n\u001b[1m\u001b[94m612 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m613 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m614 |\u001b[0m             self.logger.error(f\"Error cleaning up old data: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:615:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m613 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m614 |\u001b[0m             self.logger.error(f\"Error cleaning up old data: {e}\")\n\u001b[1m\u001b[94m615 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m616 |\u001b[0m     def add_event_callback(self, callback: Callable[[SecurityEvent], None]) -> None:\n\u001b[1m\u001b[94m617 |\u001b[0m         \"\"\"Add event callback.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:619:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m617 |\u001b[0m         \"\"\"Add event callback.\"\"\"\n\u001b[1m\u001b[94m618 |\u001b[0m         self.event_callbacks.append(callback)\n\u001b[1m\u001b[94m619 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m620 |\u001b[0m     def add_alert_callback(self, callback: Callable[[SecurityAlert], None]) -> None:\n\u001b[1m\u001b[94m621 |\u001b[0m         \"\"\"Add alert callback.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:623:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m621 |\u001b[0m         \"\"\"Add alert callback.\"\"\"\n\u001b[1m\u001b[94m622 |\u001b[0m         self.alert_callbacks.append(callback)\n\u001b[1m\u001b[94m623 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m624 |\u001b[0m     def get_security_dashboard_data(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m625 |\u001b[0m         \"\"\"Get data for security dashboard.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `recent_alerts` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:628:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m626 |\u001b[0m         try:\n\u001b[1m\u001b[94m627 |\u001b[0m             recent_events = self._get_recent_events(minutes=60)  # Last hour\n\u001b[1m\u001b[94m628 |\u001b[0m             recent_alerts = [a for a in self.alerts \n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m629 |\u001b[0m                            if a.timestamp > datetime.now() - timedelta(hours=24)]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `recent_alerts`\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:628:52\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m626 |\u001b[0m         try:\n\u001b[1m\u001b[94m627 |\u001b[0m             recent_events = self._get_recent_events(minutes=60)  # Last hour\n\u001b[1m\u001b[94m628 |\u001b[0m             recent_alerts = [a for a in self.alerts \n    \u001b[1m\u001b[94m|\u001b[0m                                                    \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m629 |\u001b[0m                            if a.timestamp > datetime.now() - timedelta(hours=24)]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:630:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m628 |\u001b[0m             recent_alerts = [a for a in self.alerts \n\u001b[1m\u001b[94m629 |\u001b[0m                            if a.timestamp > datetime.now() - timedelta(hours=24)]\n\u001b[1m\u001b[94m630 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m631 |\u001b[0m             # Get latest metrics\n\u001b[1m\u001b[94m632 |\u001b[0m             latest_metrics = self.metrics[-1] if self.metrics else None\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:633:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m631 |\u001b[0m             # Get latest metrics\n\u001b[1m\u001b[94m632 |\u001b[0m             latest_metrics = self.metrics[-1] if self.metrics else None\n\u001b[1m\u001b[94m633 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m634 |\u001b[0m             return {\n\u001b[1m\u001b[94m635 |\u001b[0m                 \"current_security_score\": latest_metrics.security_score if latest_metrics else 0,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:647:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m645 |\u001b[0m                 }\n\u001b[1m\u001b[94m646 |\u001b[0m             }\n\u001b[1m\u001b[94m647 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m648 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m649 |\u001b[0m             self.logger.error(f\"Error getting dashboard data: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:651:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m649 |\u001b[0m             self.logger.error(f\"Error getting dashboard data: {e}\")\n\u001b[1m\u001b[94m650 |\u001b[0m             return {}\n\u001b[1m\u001b[94m651 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m652 |\u001b[0m     def _get_top_threats(self) -> List[Dict[str, Any]]:\n\u001b[1m\u001b[94m653 |\u001b[0m         \"\"\"Get top security threats.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:657:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m655 |\u001b[0m             # Analyze recent events to identify top threats\n\u001b[1m\u001b[94m656 |\u001b[0m             recent_events = self._get_recent_events(minutes=1440)  # Last 24 hours\n\u001b[1m\u001b[94m657 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m658 |\u001b[0m             threat_counts = {}\n\u001b[1m\u001b[94m659 |\u001b[0m             for event in recent_events:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:662:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m660 |\u001b[0m                 threat_type = event.event_type.value\n\u001b[1m\u001b[94m661 |\u001b[0m                 threat_counts[threat_type] = threat_counts.get(threat_type, 0) + 1\n\u001b[1m\u001b[94m662 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m663 |\u001b[0m             # Sort by count and return top 5\n\u001b[1m\u001b[94m664 |\u001b[0m             sorted_threats = sorted(threat_counts.items(), key=lambda x: x[1], reverse=True)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:665:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m663 |\u001b[0m             # Sort by count and return top 5\n\u001b[1m\u001b[94m664 |\u001b[0m             sorted_threats = sorted(threat_counts.items(), key=lambda x: x[1], reverse=True)\n\u001b[1m\u001b[94m665 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m666 |\u001b[0m             return [\n\u001b[1m\u001b[94m667 |\u001b[0m                 {\"threat_type\": threat, \"count\": count}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:670:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m668 |\u001b[0m                 for threat, count in sorted_threats[:5]\n\u001b[1m\u001b[94m669 |\u001b[0m             ]\n\u001b[1m\u001b[94m670 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m671 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m672 |\u001b[0m             self.logger.error(f\"Error getting top threats: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:674:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m672 |\u001b[0m             self.logger.error(f\"Error getting top threats: {e}\")\n\u001b[1m\u001b[94m673 |\u001b[0m             return []\n\u001b[1m\u001b[94m674 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m675 |\u001b[0m     def _get_security_trends(self) -> List[Dict[str, Any]]:\n\u001b[1m\u001b[94m676 |\u001b[0m         \"\"\"Get security trends over time.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:681:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m679 |\u001b[0m             week_ago = datetime.now() - timedelta(days=7)\n\u001b[1m\u001b[94m680 |\u001b[0m             recent_metrics = [m for m in self.metrics if m.timestamp > week_ago]\n\u001b[1m\u001b[94m681 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m682 |\u001b[0m             trends = []\n\u001b[1m\u001b[94m683 |\u001b[0m             for metrics in recent_metrics:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:690:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m688 |\u001b[0m                     \"active_alerts\": metrics.active_alerts\n\u001b[1m\u001b[94m689 |\u001b[0m                 })\n\u001b[1m\u001b[94m690 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m691 |\u001b[0m             return trends\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:692:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m691 |\u001b[0m             return trends\n\u001b[1m\u001b[94m692 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m693 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m694 |\u001b[0m             self.logger.error(f\"Error getting security trends: {e}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:696:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m694 |\u001b[0m             self.logger.error(f\"Error getting security trends: {e}\")\n\u001b[1m\u001b[94m695 |\u001b[0m             return []\n\u001b[1m\u001b[94m696 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m697 |\u001b[0m     def _get_uptime(self) -> str:\n\u001b[1m\u001b[94m698 |\u001b[0m         \"\"\"Get monitoring uptime.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:701:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m699 |\u001b[0m         if not self.monitoring_active:\n\u001b[1m\u001b[94m700 |\u001b[0m             return \"Stopped\"\n\u001b[1m\u001b[94m701 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m702 |\u001b[0m         # This would track actual start time in real implementation\n\u001b[1m\u001b[94m703 |\u001b[0m         return \"Running\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:704:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m702 |\u001b[0m         # This would track actual start time in real implementation\n\u001b[1m\u001b[94m703 |\u001b[0m         return \"Running\"\n\u001b[1m\u001b[94m704 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m705 |\u001b[0m     def acknowledge_alert(self, alert_id: str) -> bool:\n\u001b[1m\u001b[94m706 |\u001b[0m         \"\"\"Acknowledge a security alert.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:717:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m715 |\u001b[0m             self.logger.error(f\"Error acknowledging alert: {e}\")\n\u001b[1m\u001b[94m716 |\u001b[0m             return False\n\u001b[1m\u001b[94m717 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m718 |\u001b[0m     def resolve_alert(self, alert_id: str) -> bool:\n\u001b[1m\u001b[94m719 |\u001b[0m         \"\"\"Resolve a security alert.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:731:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m729 |\u001b[0m             self.logger.error(f\"Error resolving alert: {e}\")\n\u001b[1m\u001b[94m730 |\u001b[0m             return False\n\u001b[1m\u001b[94m731 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m732 |\u001b[0m     def escalate_alert(self, alert_id: str) -> bool:\n\u001b[1m\u001b[94m733 |\u001b[0m         \"\"\"Escalate a security alert.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:744:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m742 |\u001b[0m             self.logger.error(f\"Error escalating alert: {e}\")\n\u001b[1m\u001b[94m743 |\u001b[0m             return False\n\u001b[1m\u001b[94m744 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m745 |\u001b[0m     def export_security_report(self, output_file: Optional[Path] = None) -> Path:\n\u001b[1m\u001b[94m746 |\u001b[0m         \"\"\"Export comprehensive security report.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:750:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m748 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\u001b[1m\u001b[94m749 |\u001b[0m             output_file = Path(f\"security_report_{timestamp}.json\")\n\u001b[1m\u001b[94m750 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m751 |\u001b[0m         report_data = {\n\u001b[1m\u001b[94m752 |\u001b[0m             \"report_metadata\": {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:765:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m763 |\u001b[0m             \"configuration\": self.config\n\u001b[1m\u001b[94m764 |\u001b[0m         }\n\u001b[1m\u001b[94m765 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m766 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m767 |\u001b[0m             json.dump(report_data, f, indent=2, default=str)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:768:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m766 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m767 |\u001b[0m             json.dump(report_data, f, indent=2, default=str)\n\u001b[1m\u001b[94m768 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m769 |\u001b[0m         return output_file\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:775:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m773 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m774 |\u001b[0m     import argparse\n\u001b[1m\u001b[94m775 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m776 |\u001b[0m     parser = argparse.ArgumentParser(description=\"SparkForge Security Monitor\")\n\u001b[1m\u001b[94m777 |\u001b[0m     parser.add_argument(\"--config\", type=Path, help=\"Configuration file\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:780:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m778 |\u001b[0m     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n\u001b[1m\u001b[94m779 |\u001b[0m     parser.add_argument(\"--duration\", type=int, default=300, help=\"Monitoring duration in seconds\")\n\u001b[1m\u001b[94m780 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m781 |\u001b[0m     args = parser.parse_args()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:782:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m781 |\u001b[0m     args = parser.parse_args()\n\u001b[1m\u001b[94m782 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m783 |\u001b[0m     # Load configuration if provided\n\u001b[1m\u001b[94m784 |\u001b[0m     config = {}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:786:32\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m784 |\u001b[0m     config = {}\n\u001b[1m\u001b[94m785 |\u001b[0m     if args.config and args.config.exists():\n\u001b[1m\u001b[94m786 |\u001b[0m         with open(args.config, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m787 |\u001b[0m             config = json.load(f)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:788:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m786 |\u001b[0m         with open(args.config, 'r') as f:\n\u001b[1m\u001b[94m787 |\u001b[0m             config = json.load(f)\n\u001b[1m\u001b[94m788 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m789 |\u001b[0m     # Create security monitor\n\u001b[1m\u001b[94m790 |\u001b[0m     monitor = SecurityMonitor(config)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:791:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m789 |\u001b[0m     # Create security monitor\n\u001b[1m\u001b[94m790 |\u001b[0m     monitor = SecurityMonitor(config)\n\u001b[1m\u001b[94m791 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m792 |\u001b[0m     # Add example callbacks\n\u001b[1m\u001b[94m793 |\u001b[0m     def event_callback(event: SecurityEvent):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:795:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m793 |\u001b[0m     def event_callback(event: SecurityEvent):\n\u001b[1m\u001b[94m794 |\u001b[0m         print(f\"Security Event: {event.event_type.value} - {event.description}\")\n\u001b[1m\u001b[94m795 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m796 |\u001b[0m     def alert_callback(alert: SecurityAlert):\n\u001b[1m\u001b[94m797 |\u001b[0m         print(f\"Security Alert: {alert.title} - {alert.description}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:798:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m796 |\u001b[0m     def alert_callback(alert: SecurityAlert):\n\u001b[1m\u001b[94m797 |\u001b[0m         print(f\"Security Alert: {alert.title} - {alert.description}\")\n\u001b[1m\u001b[94m798 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m799 |\u001b[0m     monitor.add_event_callback(event_callback)\n\u001b[1m\u001b[94m800 |\u001b[0m     monitor.add_alert_callback(alert_callback)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:801:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m799 |\u001b[0m     monitor.add_event_callback(event_callback)\n\u001b[1m\u001b[94m800 |\u001b[0m     monitor.add_alert_callback(alert_callback)\n\u001b[1m\u001b[94m801 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m802 |\u001b[0m     # Start monitoring\n\u001b[1m\u001b[94m803 |\u001b[0m     monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:804:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m802 |\u001b[0m     # Start monitoring\n\u001b[1m\u001b[94m803 |\u001b[0m     monitor.start_monitoring()\n\u001b[1m\u001b[94m804 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m805 |\u001b[0m     try:\n\u001b[1m\u001b[94m806 |\u001b[0m         # Run for specified duration\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:813:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m811 |\u001b[0m         # Stop monitoring\n\u001b[1m\u001b[94m812 |\u001b[0m         monitor.stop_monitoring()\n\u001b[1m\u001b[94m813 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m814 |\u001b[0m         # Generate report\n\u001b[1m\u001b[94m815 |\u001b[0m         report_file = monitor.export_security_report(args.output)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:817:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m815 |\u001b[0m         report_file = monitor.export_security_report(args.output)\n\u001b[1m\u001b[94m816 |\u001b[0m         print(f\"Security report saved to: {report_file}\")\n\u001b[1m\u001b[94m817 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m818 |\u001b[0m         # Print summary\n\u001b[1m\u001b[94m819 |\u001b[0m         dashboard_data = monitor.get_security_dashboard_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_monitoring.py:820:15\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m818 |\u001b[0m         # Print summary\n\u001b[1m\u001b[94m819 |\u001b[0m         dashboard_data = monitor.get_security_dashboard_data()\n\u001b[1m\u001b[94m820 |\u001b[0m         print(f\"\\nSecurity Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m821 |\u001b[0m         print(f\"Security Score: {dashboard_data.get('current_security_score', 0):.1f}/100\")\n\u001b[1m\u001b[94m822 |\u001b[0m         print(f\"Active Alerts: {dashboard_data.get('active_alerts', 0)}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:12:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import os\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import tempfile\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import subprocess\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import sys\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Dict, List, Any\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m # Import SparkForge modules\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.pipeline.builder import PipelineBuilder\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.models import PipelineConfig, ValidationThresholds, ParallelConfig\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.validation import UnifiedValidator\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.errors import ValidationError\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_____________________________________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pytest` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:12:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m import os\n\u001b[1m\u001b[94m14 |\u001b[0m import tempfile\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `pytest`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`os` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:13:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m import os\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m import tempfile\n\u001b[1m\u001b[94m15 |\u001b[0m import json\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `os`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tempfile` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:14:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m import os\n\u001b[1m\u001b[94m14 |\u001b[0m import tempfile\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m import json\n\u001b[1m\u001b[94m16 |\u001b[0m import subprocess\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `tempfile`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.List` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:19:26\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m import sys\n\u001b[1m\u001b[94m18 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m19 |\u001b[0m from typing import Dict, List, Any\n   \u001b[1m\u001b[94m|\u001b[0m                          \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m from unittest.mock import Mock, patch\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `typing.List`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.Mock` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:20:27\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m19 |\u001b[0m from typing import Dict, List, Any\n\u001b[1m\u001b[94m20 |\u001b[0m from unittest.mock import Mock, patch\n   \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m # Import SparkForge modules\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.patch` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:20:33\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m19 |\u001b[0m from typing import Dict, List, Any\n\u001b[1m\u001b[94m20 |\u001b[0m from unittest.mock import Mock, patch\n   \u001b[1m\u001b[94m|\u001b[0m                                 \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m # Import SparkForge modules\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.pipeline.builder.PipelineBuilder` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:23:41\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m # Import SparkForge modules\n\u001b[1m\u001b[94m23 |\u001b[0m from sparkforge.pipeline.builder import PipelineBuilder\n   \u001b[1m\u001b[94m|\u001b[0m                                         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m24 |\u001b[0m from sparkforge.models import PipelineConfig, ValidationThresholds, ParallelConfig\n\u001b[1m\u001b[94m25 |\u001b[0m from sparkforge.validation import UnifiedValidator\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `sparkforge.pipeline.builder.PipelineBuilder`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.validation.UnifiedValidator` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:25:35\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m23 |\u001b[0m from sparkforge.pipeline.builder import PipelineBuilder\n\u001b[1m\u001b[94m24 |\u001b[0m from sparkforge.models import PipelineConfig, ValidationThresholds, ParallelConfig\n\u001b[1m\u001b[94m25 |\u001b[0m from sparkforge.validation import UnifiedValidator\n   \u001b[1m\u001b[94m|\u001b[0m                                   \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m26 |\u001b[0m from sparkforge.errors import ValidationError\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `sparkforge.validation.UnifiedValidator`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:31:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m29 |\u001b[0m class SecurityTestSuite:\n\u001b[1m\u001b[94m30 |\u001b[0m     \"\"\"Comprehensive security test suite for SparkForge.\"\"\"\n\u001b[1m\u001b[94m31 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m32 |\u001b[0m     def __init__(self):\n\u001b[1m\u001b[94m33 |\u001b[0m         self.security_issues = []\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:35:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m33 |\u001b[0m         self.security_issues = []\n\u001b[1m\u001b[94m34 |\u001b[0m         self.compliance_results = {}\n\u001b[1m\u001b[94m35 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m36 |\u001b[0m     def run_security_scan(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m37 |\u001b[0m         \"\"\"Run comprehensive security scan.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:46:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m44 |\u001b[0m             \"compliance_check\": self._check_compliance()\n\u001b[1m\u001b[94m45 |\u001b[0m         }\n\u001b[1m\u001b[94m46 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m47 |\u001b[0m         return results\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:48:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m47 |\u001b[0m         return results\n\u001b[1m\u001b[94m48 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m49 |\u001b[0m     def _scan_vulnerabilities(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m50 |\u001b[0m         \"\"\"Scan for known vulnerabilities.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:54:69\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m52 |\u001b[0m             # Run bandit security scan\n\u001b[1m\u001b[94m53 |\u001b[0m             result = subprocess.run([\n\u001b[1m\u001b[94m54 |\u001b[0m                 sys.executable, \"-m\", \"bandit\", \"-r\", \"sparkforge/\", \n   \u001b[1m\u001b[94m|\u001b[0m                                                                     \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m55 |\u001b[0m                 \"-f\", \"json\", \"-ll\"\n\u001b[1m\u001b[94m56 |\u001b[0m             ], capture_output=True, text=True, cwd=Path.cwd())\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:57:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m55 |\u001b[0m                 \"-f\", \"json\", \"-ll\"\n\u001b[1m\u001b[94m56 |\u001b[0m             ], capture_output=True, text=True, cwd=Path.cwd())\n\u001b[1m\u001b[94m57 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m             bandit_results = json.loads(result.stdout) if result.stdout else {}\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:59:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m             bandit_results = json.loads(result.stdout) if result.stdout else {}\n\u001b[1m\u001b[94m59 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m60 |\u001b[0m             # Run safety check for known vulnerabilities\n\u001b[1m\u001b[94m61 |\u001b[0m             safety_result = subprocess.run([\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:64:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m62 |\u001b[0m                 sys.executable, \"-m\", \"safety\", \"check\", \"--json\"\n\u001b[1m\u001b[94m63 |\u001b[0m             ], capture_output=True, text=True)\n\u001b[1m\u001b[94m64 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m65 |\u001b[0m             safety_results = []\n\u001b[1m\u001b[94m66 |\u001b[0m             if safety_result.stdout:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:71:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m69 |\u001b[0m                 except json.JSONDecodeError:\n\u001b[1m\u001b[94m70 |\u001b[0m                     pass\n\u001b[1m\u001b[94m71 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m72 |\u001b[0m             return {\n\u001b[1m\u001b[94m73 |\u001b[0m                 \"bandit_issues\": bandit_results.get(\"results\", []),\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:78:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m76 |\u001b[0m                 \"success\": result.returncode == 0 and len(safety_results) == 0\n\u001b[1m\u001b[94m77 |\u001b[0m             }\n\u001b[1m\u001b[94m78 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m80 |\u001b[0m             return {\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:84:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m82 |\u001b[0m                 \"success\": False\n\u001b[1m\u001b[94m83 |\u001b[0m             }\n\u001b[1m\u001b[94m84 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m85 |\u001b[0m     def _check_dependencies(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m86 |\u001b[0m         \"\"\"Check for vulnerable dependencies.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:92:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m90 |\u001b[0m                 sys.executable, \"-m\", \"pip\", \"check\"\n\u001b[1m\u001b[94m91 |\u001b[0m             ], capture_output=True, text=True)\n\u001b[1m\u001b[94m92 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m93 |\u001b[0m             # Check for known vulnerabilities in dependencies\n\u001b[1m\u001b[94m94 |\u001b[0m             safety_check = subprocess.run([\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:97:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m95 |\u001b[0m                 sys.executable, \"-m\", \"safety\", \"check\"\n\u001b[1m\u001b[94m96 |\u001b[0m             ], capture_output=True, text=True)\n\u001b[1m\u001b[94m97 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m98 |\u001b[0m             return {\n\u001b[1m\u001b[94m99 |\u001b[0m                 \"dependency_conflicts\": pip_check.stdout,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:103:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m101 |\u001b[0m                 \"success\": pip_check.returncode == 0 and safety_check.returncode == 0\n\u001b[1m\u001b[94m102 |\u001b[0m             }\n\u001b[1m\u001b[94m103 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m104 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m105 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:109:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m107 |\u001b[0m                 \"success\": False\n\u001b[1m\u001b[94m108 |\u001b[0m             }\n\u001b[1m\u001b[94m109 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m110 |\u001b[0m     def _test_code_security(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m111 |\u001b[0m         \"\"\"Test code for security vulnerabilities.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:113:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m         \"\"\"Test code for security vulnerabilities.\"\"\"\n\u001b[1m\u001b[94m112 |\u001b[0m         security_tests = []\n\u001b[1m\u001b[94m113 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m         # Test for SQL injection vulnerabilities\n\u001b[1m\u001b[94m115 |\u001b[0m         security_tests.append(self._test_sql_injection_prevention())\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:116:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m         # Test for SQL injection vulnerabilities\n\u001b[1m\u001b[94m115 |\u001b[0m         security_tests.append(self._test_sql_injection_prevention())\n\u001b[1m\u001b[94m116 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m117 |\u001b[0m         # Test for path traversal vulnerabilities\n\u001b[1m\u001b[94m118 |\u001b[0m         security_tests.append(self._test_path_traversal_prevention())\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:119:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m117 |\u001b[0m         # Test for path traversal vulnerabilities\n\u001b[1m\u001b[94m118 |\u001b[0m         security_tests.append(self._test_path_traversal_prevention())\n\u001b[1m\u001b[94m119 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m         # Test for command injection vulnerabilities\n\u001b[1m\u001b[94m121 |\u001b[0m         security_tests.append(self._test_command_injection_prevention())\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:122:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m         # Test for command injection vulnerabilities\n\u001b[1m\u001b[94m121 |\u001b[0m         security_tests.append(self._test_command_injection_prevention())\n\u001b[1m\u001b[94m122 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         # Test for information disclosure\n\u001b[1m\u001b[94m124 |\u001b[0m         security_tests.append(self._test_information_disclosure())\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:125:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         # Test for information disclosure\n\u001b[1m\u001b[94m124 |\u001b[0m         security_tests.append(self._test_information_disclosure())\n\u001b[1m\u001b[94m125 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m         return {\n\u001b[1m\u001b[94m127 |\u001b[0m             \"sql_injection_test\": security_tests[0],\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:133:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m131 |\u001b[0m             \"overall_success\": all(test[\"success\"] for test in security_tests)\n\u001b[1m\u001b[94m132 |\u001b[0m         }\n\u001b[1m\u001b[94m133 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m134 |\u001b[0m     def _test_sql_injection_prevention(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m135 |\u001b[0m         \"\"\"Test for SQL injection prevention.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:139:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m137 |\u001b[0m             # Test that validation functions properly escape inputs\n\u001b[1m\u001b[94m138 |\u001b[0m             from sparkforge.validation import validate_dataframe_schema\n\u001b[1m\u001b[94m139 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m140 |\u001b[0m             # Test with potentially malicious input\n\u001b[1m\u001b[94m141 |\u001b[0m             malicious_inputs = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:147:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m145 |\u001b[0m                 \"'; INSERT INTO users VALUES ('hacker', 'password'); --\"\n\u001b[1m\u001b[94m146 |\u001b[0m             ]\n\u001b[1m\u001b[94m147 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m148 |\u001b[0m             for malicious_input in malicious_inputs:\n\u001b[1m\u001b[94m149 |\u001b[0m                 # These should not cause SQL injection\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `result` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:150:17\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m148 |\u001b[0m             for malicious_input in malicious_inputs:\n\u001b[1m\u001b[94m149 |\u001b[0m                 # These should not cause SQL injection\n\u001b[1m\u001b[94m150 |\u001b[0m                 result = validate_dataframe_schema(None, [malicious_input])\n    \u001b[1m\u001b[94m|\u001b[0m                 \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m151 |\u001b[0m                 # Should handle gracefully without executing SQL\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `result`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:152:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m                 result = validate_dataframe_schema(None, [malicious_input])\n\u001b[1m\u001b[94m151 |\u001b[0m                 # Should handle gracefully without executing SQL\n\u001b[1m\u001b[94m152 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m153 |\u001b[0m             return {\n\u001b[1m\u001b[94m154 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:157:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m155 |\u001b[0m                 \"message\": \"SQL injection prevention tests passed\"\n\u001b[1m\u001b[94m156 |\u001b[0m             }\n\u001b[1m\u001b[94m157 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m158 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m159 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:164:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m162 |\u001b[0m                 \"message\": \"SQL injection prevention test failed\"\n\u001b[1m\u001b[94m163 |\u001b[0m             }\n\u001b[1m\u001b[94m164 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m165 |\u001b[0m     def _test_path_traversal_prevention(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m166 |\u001b[0m         \"\"\"Test for path traversal prevention.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.validation.get_dataframe_info` imported but unused\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:169:47\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m167 |\u001b[0m         try:\n\u001b[1m\u001b[94m168 |\u001b[0m             # Test that file operations are safe from path traversal\n\u001b[1m\u001b[94m169 |\u001b[0m             from sparkforge.validation import get_dataframe_info\n    \u001b[1m\u001b[94m|\u001b[0m                                               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m170 |\u001b[0m             \n\u001b[1m\u001b[94m171 |\u001b[0m             malicious_paths = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `sparkforge.validation.get_dataframe_info`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:170:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m168 |\u001b[0m             # Test that file operations are safe from path traversal\n\u001b[1m\u001b[94m169 |\u001b[0m             from sparkforge.validation import get_dataframe_info\n\u001b[1m\u001b[94m170 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m171 |\u001b[0m             malicious_paths = [\n\u001b[1m\u001b[94m172 |\u001b[0m                 \"../../../etc/passwd\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:177:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m175 |\u001b[0m                 \"C:\\\\Windows\\\\System32\\\\config\\\\SAM\"\n\u001b[1m\u001b[94m176 |\u001b[0m             ]\n\u001b[1m\u001b[94m177 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m178 |\u001b[0m             # These operations should not allow path traversal\n\u001b[1m\u001b[94m179 |\u001b[0m             for malicious_path in malicious_paths:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mB007 \u001b[0m\u001b[1mLoop control variable `malicious_path` not used within loop body\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:179:17\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m178 |\u001b[0m             # These operations should not allow path traversal\n\u001b[1m\u001b[94m179 |\u001b[0m             for malicious_path in malicious_paths:\n    \u001b[1m\u001b[94m|\u001b[0m                 \u001b[1m\u001b[91m^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m180 |\u001b[0m                 # Should not be able to access files outside allowed directories\n\u001b[1m\u001b[94m181 |\u001b[0m                 try:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRename unused `malicious_path` to `_malicious_path`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:187:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m185 |\u001b[0m                     # Expected to fail safely\n\u001b[1m\u001b[94m186 |\u001b[0m                     pass\n\u001b[1m\u001b[94m187 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m188 |\u001b[0m             return {\n\u001b[1m\u001b[94m189 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:192:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m                 \"message\": \"Path traversal prevention tests passed\"\n\u001b[1m\u001b[94m191 |\u001b[0m             }\n\u001b[1m\u001b[94m192 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m194 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:199:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m                 \"message\": \"Path traversal prevention test failed\"\n\u001b[1m\u001b[94m198 |\u001b[0m             }\n\u001b[1m\u001b[94m199 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m     def _test_command_injection_prevention(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m201 |\u001b[0m         \"\"\"Test for command injection prevention.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:210:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m208 |\u001b[0m                 \"|| echo 'hacked'\"\n\u001b[1m\u001b[94m209 |\u001b[0m             ]\n\u001b[1m\u001b[94m210 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m211 |\u001b[0m             for command in malicious_commands:\n\u001b[1m\u001b[94m212 |\u001b[0m                 # These should not be executed as shell commands\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mB007 \u001b[0m\u001b[1mLoop control variable `command` not used within loop body\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:211:17\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m209 |\u001b[0m             ]\n\u001b[1m\u001b[94m210 |\u001b[0m             \n\u001b[1m\u001b[94m211 |\u001b[0m             for command in malicious_commands:\n    \u001b[1m\u001b[94m|\u001b[0m                 \u001b[1m\u001b[91m^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m212 |\u001b[0m                 # These should not be executed as shell commands\n\u001b[1m\u001b[94m213 |\u001b[0m                 # SparkForge should not execute arbitrary shell commands\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRename unused `command` to `_command`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:215:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m213 |\u001b[0m                 # SparkForge should not execute arbitrary shell commands\n\u001b[1m\u001b[94m214 |\u001b[0m                 pass\n\u001b[1m\u001b[94m215 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m216 |\u001b[0m             return {\n\u001b[1m\u001b[94m217 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:220:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m218 |\u001b[0m                 \"message\": \"Command injection prevention tests passed\"\n\u001b[1m\u001b[94m219 |\u001b[0m             }\n\u001b[1m\u001b[94m220 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m221 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m222 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:227:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m225 |\u001b[0m                 \"message\": \"Command injection prevention test failed\"\n\u001b[1m\u001b[94m226 |\u001b[0m             }\n\u001b[1m\u001b[94m227 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m228 |\u001b[0m     def _test_information_disclosure(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m229 |\u001b[0m         \"\"\"Test for information disclosure vulnerabilities.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:232:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m230 |\u001b[0m         try:\n\u001b[1m\u001b[94m231 |\u001b[0m             # Test that sensitive information is not leaked in error messages\n\u001b[1m\u001b[94m232 |\u001b[0m             from sparkforge.errors import ValidationError, PipelineError, ConfigurationError\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m             \n\u001b[1m\u001b[94m234 |\u001b[0m             # Test error messages don't contain sensitive information\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:233:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m             # Test that sensitive information is not leaked in error messages\n\u001b[1m\u001b[94m232 |\u001b[0m             from sparkforge.errors import ValidationError, PipelineError, ConfigurationError\n\u001b[1m\u001b[94m233 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m234 |\u001b[0m             # Test error messages don't contain sensitive information\n\u001b[1m\u001b[94m235 |\u001b[0m             test_errors = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:240:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m238 |\u001b[0m                 ConfigurationError(\"Test configuration error\")\n\u001b[1m\u001b[94m239 |\u001b[0m             ]\n\u001b[1m\u001b[94m240 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m241 |\u001b[0m             for error in test_errors:\n\u001b[1m\u001b[94m242 |\u001b[0m                 error_message = str(error)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:245:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m243 |\u001b[0m                 # Error messages should not contain sensitive paths, passwords, etc.\n\u001b[1m\u001b[94m244 |\u001b[0m                 sensitive_patterns = [\"password\", \"secret\", \"key\", \"/etc/\", \"C:\\\\\"]\n\u001b[1m\u001b[94m245 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m246 |\u001b[0m                 for pattern in sensitive_patterns:\n\u001b[1m\u001b[94m247 |\u001b[0m                     if pattern.lower() in error_message.lower():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:253:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m251 |\u001b[0m                             \"message\": \"Information disclosure test failed\"\n\u001b[1m\u001b[94m252 |\u001b[0m                         }\n\u001b[1m\u001b[94m253 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m254 |\u001b[0m             return {\n\u001b[1m\u001b[94m255 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:258:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m256 |\u001b[0m                 \"message\": \"Information disclosure prevention tests passed\"\n\u001b[1m\u001b[94m257 |\u001b[0m             }\n\u001b[1m\u001b[94m258 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m259 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m260 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:265:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m                 \"message\": \"Information disclosure prevention test failed\"\n\u001b[1m\u001b[94m264 |\u001b[0m             }\n\u001b[1m\u001b[94m265 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m     def _test_configuration_security(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m267 |\u001b[0m         \"\"\"Test configuration security.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `config` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:270:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m268 |\u001b[0m         try:\n\u001b[1m\u001b[94m269 |\u001b[0m             # Test that default configurations are secure\n\u001b[1m\u001b[94m270 |\u001b[0m             config = PipelineConfig(\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m                 schema=\"test_schema\",\n\u001b[1m\u001b[94m272 |\u001b[0m                 quality_thresholds=ValidationThresholds(80.0, 85.0, 90.0),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `config`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:275:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m273 |\u001b[0m                 parallel=ParallelConfig(enabled=True, max_workers=4)\n\u001b[1m\u001b[94m274 |\u001b[0m             )\n\u001b[1m\u001b[94m275 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m276 |\u001b[0m             # Test that configuration validation prevents insecure settings\n\u001b[1m\u001b[94m277 |\u001b[0m             try:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `insecure_config` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:279:17\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m277 |\u001b[0m             try:\n\u001b[1m\u001b[94m278 |\u001b[0m                 # This should fail validation\n\u001b[1m\u001b[94m279 |\u001b[0m                 insecure_config = PipelineConfig(\n    \u001b[1m\u001b[94m|\u001b[0m                 \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m280 |\u001b[0m                     schema=\"\",  # Empty schema should not be allowed\n\u001b[1m\u001b[94m281 |\u001b[0m                     quality_thresholds=ValidationThresholds(-1.0, 150.0, 200.0),  # Invalid thresholds\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `insecure_config`\u001b[0m\n\n\u001b[1m\u001b[91mF821 \u001b[0m\u001b[1mUndefined name `ConfigurationError`\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:289:38\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m287 |\u001b[0m                     \"message\": \"Configuration security test failed\"\n\u001b[1m\u001b[94m288 |\u001b[0m                 }\n\u001b[1m\u001b[94m289 |\u001b[0m             except (ValidationError, ConfigurationError):\n    \u001b[1m\u001b[94m|\u001b[0m                                      \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m290 |\u001b[0m                 # Expected to fail validation\n\u001b[1m\u001b[94m291 |\u001b[0m                 pass\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:292:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m290 |\u001b[0m                 # Expected to fail validation\n\u001b[1m\u001b[94m291 |\u001b[0m                 pass\n\u001b[1m\u001b[94m292 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m293 |\u001b[0m             return {\n\u001b[1m\u001b[94m294 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:297:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m295 |\u001b[0m                 \"message\": \"Configuration security tests passed\"\n\u001b[1m\u001b[94m296 |\u001b[0m             }\n\u001b[1m\u001b[94m297 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m298 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m299 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:304:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m302 |\u001b[0m                 \"message\": \"Configuration security test failed\"\n\u001b[1m\u001b[94m303 |\u001b[0m             }\n\u001b[1m\u001b[94m304 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m305 |\u001b[0m     def _test_data_security(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m306 |\u001b[0m         \"\"\"Test data security features.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:309:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m307 |\u001b[0m           try:\n\u001b[1m\u001b[94m308 |\u001b[0m               # Test data validation prevents malicious input\n\u001b[1m\u001b[94m309 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m             from pyspark.sql import functions as F\n\u001b[1m\u001b[94m310 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m             from sparkforge.models import BronzeStep\n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|____________________________________________________^\u001b[0m\n\u001b[1m\u001b[94m311 |\u001b[0m               \n\u001b[1m\u001b[94m312 |\u001b[0m               # Test validation rules prevent malicious data\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:311:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m309 |\u001b[0m             from pyspark.sql import functions as F\n\u001b[1m\u001b[94m310 |\u001b[0m             from sparkforge.models import BronzeStep\n\u001b[1m\u001b[94m311 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m312 |\u001b[0m             # Test validation rules prevent malicious data\n\u001b[1m\u001b[94m313 |\u001b[0m             malicious_rules = {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:320:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m318 |\u001b[0m                 ]\n\u001b[1m\u001b[94m319 |\u001b[0m             }\n\u001b[1m\u001b[94m320 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m321 |\u001b[0m             # Test that validation rules are properly applied\n\u001b[1m\u001b[94m322 |\u001b[0m             bronze_step = BronzeStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:327:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m325 |\u001b[0m                 rules=malicious_rules\n\u001b[1m\u001b[94m326 |\u001b[0m             )\n\u001b[1m\u001b[94m327 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m328 |\u001b[0m             bronze_step.validate()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:329:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m328 |\u001b[0m             bronze_step.validate()\n\u001b[1m\u001b[94m329 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m330 |\u001b[0m             return {\n\u001b[1m\u001b[94m331 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:334:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m332 |\u001b[0m                 \"message\": \"Data security tests passed\"\n\u001b[1m\u001b[94m333 |\u001b[0m             }\n\u001b[1m\u001b[94m334 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m335 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m336 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:341:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m339 |\u001b[0m                 \"message\": \"Data security test failed\"\n\u001b[1m\u001b[94m340 |\u001b[0m             }\n\u001b[1m\u001b[94m341 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m342 |\u001b[0m     def _check_compliance(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m343 |\u001b[0m         \"\"\"Check compliance with security standards.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:349:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m347 |\u001b[0m             \"dependency_compliance\": self._check_dependency_compliance()\n\u001b[1m\u001b[94m348 |\u001b[0m         }\n\u001b[1m\u001b[94m349 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m350 |\u001b[0m         return compliance_results\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:351:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m350 |\u001b[0m         return compliance_results\n\u001b[1m\u001b[94m351 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m352 |\u001b[0m     def _check_owasp_compliance(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m353 |\u001b[0m         \"\"\"Check compliance with OWASP Top 10.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:366:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m364 |\u001b[0m             \"insufficient_logging\": True  # Comprehensive logging\n\u001b[1m\u001b[94m365 |\u001b[0m         }\n\u001b[1m\u001b[94m366 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m367 |\u001b[0m         return {\n\u001b[1m\u001b[94m368 |\u001b[0m             \"checks\": owasp_checks,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:372:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m370 |\u001b[0m             \"score\": sum(owasp_checks.values()) / len(owasp_checks) * 100\n\u001b[1m\u001b[94m371 |\u001b[0m         }\n\u001b[1m\u001b[94m372 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m373 |\u001b[0m     def _check_cve_compliance(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m374 |\u001b[0m         \"\"\"Check for known CVE vulnerabilities.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:380:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m                 sys.executable, \"-m\", \"safety\", \"check\"\n\u001b[1m\u001b[94m379 |\u001b[0m             ], capture_output=True, text=True)\n\u001b[1m\u001b[94m380 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m             if result.returncode == 0:\n\u001b[1m\u001b[94m382 |\u001b[0m                 return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:394:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m392 |\u001b[0m                     \"details\": result.stdout\n\u001b[1m\u001b[94m393 |\u001b[0m                 }\n\u001b[1m\u001b[94m394 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m395 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m396 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:401:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m399 |\u001b[0m                 \"message\": \"CVE compliance check failed\"\n\u001b[1m\u001b[94m400 |\u001b[0m             }\n\u001b[1m\u001b[94m401 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m402 |\u001b[0m     def _check_dependency_compliance(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m403 |\u001b[0m         \"\"\"Check dependency compliance.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:409:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m                 sys.executable, \"-m\", \"pip\", \"audit\"\n\u001b[1m\u001b[94m408 |\u001b[0m             ], capture_output=True, text=True)\n\u001b[1m\u001b[94m409 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m             if result.returncode == 0:\n\u001b[1m\u001b[94m411 |\u001b[0m                 return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:423:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m421 |\u001b[0m                     \"details\": result.stdout\n\u001b[1m\u001b[94m422 |\u001b[0m                 }\n\u001b[1m\u001b[94m423 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m424 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m425 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:437:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m435 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m436 |\u001b[0m     results = security_suite.run_security_scan()\n\u001b[1m\u001b[94m437 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m438 |\u001b[0m     # Assert overall security\n\u001b[1m\u001b[94m439 |\u001b[0m     assert results[\"vulnerability_scan\"][\"success\"], \"Vulnerability scan failed\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:450:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m448 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m449 |\u001b[0m     result = security_suite._test_sql_injection_prevention()\n\u001b[1m\u001b[94m450 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m451 |\u001b[0m     assert result[\"success\"], f\"SQL injection prevention failed: {result.get('error', 'Unknown error')}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:458:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m456 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m457 |\u001b[0m     result = security_suite._test_path_traversal_prevention()\n\u001b[1m\u001b[94m458 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m459 |\u001b[0m     assert result[\"success\"], f\"Path traversal prevention failed: {result.get('error', 'Unknown error')}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:466:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m464 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m465 |\u001b[0m     result = security_suite._test_command_injection_prevention()\n\u001b[1m\u001b[94m466 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m467 |\u001b[0m     assert result[\"success\"], f\"Command injection prevention failed: {result.get('error', 'Unknown error')}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:474:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m472 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m473 |\u001b[0m     result = security_suite._test_information_disclosure()\n\u001b[1m\u001b[94m474 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m475 |\u001b[0m     assert result[\"success\"], f\"Information disclosure prevention failed: {result.get('error', 'Unknown error')}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:482:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m480 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m481 |\u001b[0m     result = security_suite._test_configuration_security()\n\u001b[1m\u001b[94m482 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m483 |\u001b[0m     assert result[\"success\"], f\"Configuration security failed: {result.get('error', 'Unknown error')}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:490:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m488 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m489 |\u001b[0m     result = security_suite._test_data_security()\n\u001b[1m\u001b[94m490 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m491 |\u001b[0m     assert result[\"success\"], f\"Data security failed: {result.get('error', 'Unknown error')}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:498:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m496 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m497 |\u001b[0m     result = security_suite._check_owasp_compliance()\n\u001b[1m\u001b[94m498 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m499 |\u001b[0m     assert result[\"compliant\"], f\"OWASP compliance failed: {result['score']}% score\"\n\u001b[1m\u001b[94m500 |\u001b[0m     assert result[\"score\"] >= 90, f\"OWASP compliance score too low: {result['score']}%\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:507:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m505 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m506 |\u001b[0m     result = security_suite._check_cve_compliance()\n\u001b[1m\u001b[94m507 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m508 |\u001b[0m     assert result[\"compliant\"], f\"CVE compliance failed: {result.get('message', 'Unknown error')}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:515:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m513 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m514 |\u001b[0m     result = security_suite._check_dependency_compliance()\n\u001b[1m\u001b[94m515 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m516 |\u001b[0m     assert result[\"compliant\"], f\"Dependency compliance failed: {result.get('message', 'Unknown error')}\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/security_tests.py:523:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m521 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m522 |\u001b[0m     results = security_suite.run_security_scan()\n\u001b[1m\u001b[94m523 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m524 |\u001b[0m     print(\"Security Scan Results:\")\n\u001b[1m\u001b[94m525 |\u001b[0m     print(json.dumps(results, indent=2))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:11:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m10 |\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import pytest\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import tempfile\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import time\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime, timedelta\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m # Import security components\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.security.security_tests import SecurityTestSuite\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.security.vulnerability_scanner import VulnerabilityScanner, VulnerabilityReport\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.security.compliance_checker import ComplianceChecker, ComplianceStandard\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from tests.security.security_monitoring import SecurityMonitor, SecurityEvent, SecurityEventType, SecuritySeverity\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|__________________________________________________________________________________________________________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.Mock` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:16:27\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m import time\n\u001b[1m\u001b[94m15 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m from unittest.mock import Mock, patch\n   \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.patch` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:16:33\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m import time\n\u001b[1m\u001b[94m15 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m from unittest.mock import Mock, patch\n   \u001b[1m\u001b[94m|\u001b[0m                                 \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`datetime.timedelta` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:17:32\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m16 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m17 |\u001b[0m from datetime import datetime, timedelta\n   \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m # Import security components\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `datetime.timedelta`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`tests.security.vulnerability_scanner.VulnerabilityReport` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:21:72\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m # Import security components\n\u001b[1m\u001b[94m20 |\u001b[0m from tests.security.security_tests import SecurityTestSuite\n\u001b[1m\u001b[94m21 |\u001b[0m from tests.security.vulnerability_scanner import VulnerabilityScanner, VulnerabilityReport\n   \u001b[1m\u001b[94m|\u001b[0m                                                                        \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m from tests.security.compliance_checker import ComplianceChecker, ComplianceStandard\n\u001b[1m\u001b[94m23 |\u001b[0m from tests.security.security_monitoring import SecurityMonitor, SecurityEvent, SecurityEventType, SecuritySeverity\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `tests.security.vulnerability_scanner.VulnerabilityReport`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:28:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m26 |\u001b[0m class TestSecurityIntegration:\n\u001b[1m\u001b[94m27 |\u001b[0m     \"\"\"Integration tests for security components.\"\"\"\n\u001b[1m\u001b[94m28 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m29 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m30 |\u001b[0m     def temp_project_dir(self):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:34:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m32 |\u001b[0m         with tempfile.TemporaryDirectory() as temp_dir:\n\u001b[1m\u001b[94m33 |\u001b[0m             yield Path(temp_dir)\n\u001b[1m\u001b[94m34 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m35 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m36 |\u001b[0m     def security_test_suite(self):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:39:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m37 |\u001b[0m         \"\"\"Create security test suite instance.\"\"\"\n\u001b[1m\u001b[94m38 |\u001b[0m         return SecurityTestSuite()\n\u001b[1m\u001b[94m39 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m40 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m41 |\u001b[0m     def vulnerability_scanner(self, temp_project_dir):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:44:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m42 |\u001b[0m         \"\"\"Create vulnerability scanner instance.\"\"\"\n\u001b[1m\u001b[94m43 |\u001b[0m         return VulnerabilityScanner(temp_project_dir)\n\u001b[1m\u001b[94m44 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m45 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m46 |\u001b[0m     def compliance_checker(self, temp_project_dir):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:49:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m47 |\u001b[0m         \"\"\"Create compliance checker instance.\"\"\"\n\u001b[1m\u001b[94m48 |\u001b[0m         return ComplianceChecker(temp_project_dir)\n\u001b[1m\u001b[94m49 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m50 |\u001b[0m     @pytest.fixture\n\u001b[1m\u001b[94m51 |\u001b[0m     def security_monitor(self):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:60:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m         }\n\u001b[1m\u001b[94m59 |\u001b[0m         return SecurityMonitor(config)\n\u001b[1m\u001b[94m60 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m     def test_security_test_suite_integration(self, security_test_suite):\n\u001b[1m\u001b[94m62 |\u001b[0m         \"\"\"Test security test suite integration.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:65:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         # Run comprehensive security scan\n\u001b[1m\u001b[94m64 |\u001b[0m         results = security_test_suite.run_security_scan()\n\u001b[1m\u001b[94m65 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m         # Verify all scan components are present\n\u001b[1m\u001b[94m67 |\u001b[0m         assert \"vulnerability_scan\" in results\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:73:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m         assert \"data_security\" in results\n\u001b[1m\u001b[94m72 |\u001b[0m         assert \"compliance_check\" in results\n\u001b[1m\u001b[94m73 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m         # Verify scan results structure\n\u001b[1m\u001b[94m75 |\u001b[0m         for component, result in results.items():\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mB007 \u001b[0m\u001b[1mLoop control variable `component` not used within loop body\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:75:13\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m         # Verify scan results structure\n\u001b[1m\u001b[94m75 |\u001b[0m         for component, result in results.items():\n   \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m76 |\u001b[0m             assert isinstance(result, dict)\n\u001b[1m\u001b[94m77 |\u001b[0m             assert \"success\" in result or \"compliant\" in result\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRename unused `component` to `_component`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:78:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m76 |\u001b[0m             assert isinstance(result, dict)\n\u001b[1m\u001b[94m77 |\u001b[0m             assert \"success\" in result or \"compliant\" in result\n\u001b[1m\u001b[94m78 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m     def test_vulnerability_scanner_integration(self, vulnerability_scanner, temp_project_dir):\n\u001b[1m\u001b[94m80 |\u001b[0m         \"\"\"Test vulnerability scanner integration.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:87:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m85 |\u001b[0m password = \"hardcoded_password\"  # This should trigger a security issue\n\u001b[1m\u001b[94m86 |\u001b[0m \"\"\")\n\u001b[1m\u001b[94m87 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m88 |\u001b[0m         # Run vulnerability scan\n\u001b[1m\u001b[94m89 |\u001b[0m         scan_results = vulnerability_scanner.scan_all()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:90:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m88 |\u001b[0m         # Run vulnerability scan\n\u001b[1m\u001b[94m89 |\u001b[0m         scan_results = vulnerability_scanner.scan_all()\n\u001b[1m\u001b[94m90 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m91 |\u001b[0m         # Verify scan results\n\u001b[1m\u001b[94m92 |\u001b[0m         assert \"scan_results\" in scan_results\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:95:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m93 |\u001b[0m         assert \"security_metrics\" in scan_results\n\u001b[1m\u001b[94m94 |\u001b[0m         assert \"summary\" in scan_results\n\u001b[1m\u001b[94m95 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m96 |\u001b[0m         # Verify security metrics\n\u001b[1m\u001b[94m97 |\u001b[0m         metrics = scan_results[\"security_metrics\"]\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:101:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 99 |\u001b[0m         assert hasattr(metrics, \"total_vulnerabilities\")\n\u001b[1m\u001b[94m100 |\u001b[0m         assert hasattr(metrics, \"scan_timestamp\")\n\u001b[1m\u001b[94m101 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m102 |\u001b[0m     def test_compliance_checker_integration(self, compliance_checker):\n\u001b[1m\u001b[94m103 |\u001b[0m         \"\"\"Test compliance checker integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:106:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m104 |\u001b[0m         # Check all compliance standards\n\u001b[1m\u001b[94m105 |\u001b[0m         compliance_reports = compliance_checker.check_all_standards()\n\u001b[1m\u001b[94m106 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m107 |\u001b[0m         # Verify all standards are checked\n\u001b[1m\u001b[94m108 |\u001b[0m         expected_standards = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:114:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m112 |\u001b[0m             ComplianceStandard.SECURITY_BEST_PRACTICES\n\u001b[1m\u001b[94m113 |\u001b[0m         ]\n\u001b[1m\u001b[94m114 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m115 |\u001b[0m         for standard in expected_standards:\n\u001b[1m\u001b[94m116 |\u001b[0m             assert standard.value in compliance_reports\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:117:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m115 |\u001b[0m         for standard in expected_standards:\n\u001b[1m\u001b[94m116 |\u001b[0m             assert standard.value in compliance_reports\n\u001b[1m\u001b[94m117 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m118 |\u001b[0m             report = compliance_reports[standard.value]\n\u001b[1m\u001b[94m119 |\u001b[0m             assert hasattr(report, \"overall_compliant\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:123:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m121 |\u001b[0m             assert hasattr(report, \"checks\")\n\u001b[1m\u001b[94m122 |\u001b[0m             assert hasattr(report, \"recommendations\")\n\u001b[1m\u001b[94m123 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m     def test_security_monitor_integration(self, security_monitor):\n\u001b[1m\u001b[94m125 |\u001b[0m         \"\"\"Test security monitor integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:128:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m         # Start monitoring\n\u001b[1m\u001b[94m127 |\u001b[0m         security_monitor.start_monitoring()\n\u001b[1m\u001b[94m128 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m129 |\u001b[0m         # Wait for monitoring to start\n\u001b[1m\u001b[94m130 |\u001b[0m         time.sleep(2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:131:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m129 |\u001b[0m         # Wait for monitoring to start\n\u001b[1m\u001b[94m130 |\u001b[0m         time.sleep(2)\n\u001b[1m\u001b[94m131 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m132 |\u001b[0m         # Verify monitoring is active\n\u001b[1m\u001b[94m133 |\u001b[0m         assert security_monitor.monitoring_active\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:134:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m132 |\u001b[0m         # Verify monitoring is active\n\u001b[1m\u001b[94m133 |\u001b[0m         assert security_monitor.monitoring_active\n\u001b[1m\u001b[94m134 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m135 |\u001b[0m         # Create test security event\n\u001b[1m\u001b[94m136 |\u001b[0m         test_event = SecurityEvent(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:145:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m143 |\u001b[0m             details={\"test\": True}\n\u001b[1m\u001b[94m144 |\u001b[0m         )\n\u001b[1m\u001b[94m145 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         # Log the event\n\u001b[1m\u001b[94m147 |\u001b[0m         security_monitor._log_event(test_event)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:148:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         # Log the event\n\u001b[1m\u001b[94m147 |\u001b[0m         security_monitor._log_event(test_event)\n\u001b[1m\u001b[94m148 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m         # Verify event was logged\n\u001b[1m\u001b[94m150 |\u001b[0m         assert len(security_monitor.events) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:151:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m         # Verify event was logged\n\u001b[1m\u001b[94m150 |\u001b[0m         assert len(security_monitor.events) > 0\n\u001b[1m\u001b[94m151 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m         # Get dashboard data\n\u001b[1m\u001b[94m153 |\u001b[0m         dashboard_data = security_monitor.get_security_dashboard_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:157:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m155 |\u001b[0m         assert \"active_alerts\" in dashboard_data\n\u001b[1m\u001b[94m156 |\u001b[0m         assert \"recent_events\" in dashboard_data\n\u001b[1m\u001b[94m157 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m158 |\u001b[0m         # Stop monitoring\n\u001b[1m\u001b[94m159 |\u001b[0m         security_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:161:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m159 |\u001b[0m         security_monitor.stop_monitoring()\n\u001b[1m\u001b[94m160 |\u001b[0m         assert not security_monitor.monitoring_active\n\u001b[1m\u001b[94m161 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m162 |\u001b[0m     def test_security_components_workflow(self, security_test_suite, vulnerability_scanner, \n\u001b[1m\u001b[94m163 |\u001b[0m                                         compliance_checker, security_monitor):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:162:92\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         assert not security_monitor.monitoring_active\n\u001b[1m\u001b[94m161 |\u001b[0m     \n\u001b[1m\u001b[94m162 |\u001b[0m     def test_security_components_workflow(self, security_test_suite, vulnerability_scanner, \n    \u001b[1m\u001b[94m|\u001b[0m                                                                                            \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m163 |\u001b[0m                                         compliance_checker, security_monitor):\n\u001b[1m\u001b[94m164 |\u001b[0m         \"\"\"Test complete security workflow integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:167:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m165 |\u001b[0m         # Step 1: Run security tests\n\u001b[1m\u001b[94m166 |\u001b[0m         security_results = security_test_suite.run_security_scan()\n\u001b[1m\u001b[94m167 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m168 |\u001b[0m         # Step 2: Run vulnerability scan\n\u001b[1m\u001b[94m169 |\u001b[0m         vulnerability_results = vulnerability_scanner.scan_all()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:170:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m168 |\u001b[0m         # Step 2: Run vulnerability scan\n\u001b[1m\u001b[94m169 |\u001b[0m         vulnerability_results = vulnerability_scanner.scan_all()\n\u001b[1m\u001b[94m170 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m171 |\u001b[0m         # Step 3: Run compliance check\n\u001b[1m\u001b[94m172 |\u001b[0m         compliance_results = compliance_checker.check_all_standards()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:173:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m171 |\u001b[0m         # Step 3: Run compliance check\n\u001b[1m\u001b[94m172 |\u001b[0m         compliance_results = compliance_checker.check_all_standards()\n\u001b[1m\u001b[94m173 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m174 |\u001b[0m         # Step 4: Start security monitoring\n\u001b[1m\u001b[94m175 |\u001b[0m         security_monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:176:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m174 |\u001b[0m         # Step 4: Start security monitoring\n\u001b[1m\u001b[94m175 |\u001b[0m         security_monitor.start_monitoring()\n\u001b[1m\u001b[94m176 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m         # Step 5: Generate security event\n\u001b[1m\u001b[94m178 |\u001b[0m         security_event = SecurityEvent(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:187:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m185 |\u001b[0m             details={\"user\": \"test_user\", \"attempts\": 3}\n\u001b[1m\u001b[94m186 |\u001b[0m         )\n\u001b[1m\u001b[94m187 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m188 |\u001b[0m         security_monitor._log_event(security_event)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:189:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m188 |\u001b[0m         security_monitor._log_event(security_event)\n\u001b[1m\u001b[94m189 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m         # Step 6: Verify workflow results\n\u001b[1m\u001b[94m191 |\u001b[0m         assert security_results[\"vulnerability_scan\"][\"success\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:195:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m         assert len(compliance_results) >= 4\n\u001b[1m\u001b[94m194 |\u001b[0m         assert len(security_monitor.events) > 0\n\u001b[1m\u001b[94m195 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m196 |\u001b[0m         # Step 7: Stop monitoring\n\u001b[1m\u001b[94m197 |\u001b[0m         security_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:198:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m196 |\u001b[0m         # Step 7: Stop monitoring\n\u001b[1m\u001b[94m197 |\u001b[0m         security_monitor.stop_monitoring()\n\u001b[1m\u001b[94m198 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m199 |\u001b[0m     def test_security_reporting_integration(self, vulnerability_scanner, compliance_checker, \n\u001b[1m\u001b[94m200 |\u001b[0m                                           security_monitor, temp_project_dir):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:199:93\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m         security_monitor.stop_monitoring()\n\u001b[1m\u001b[94m198 |\u001b[0m     \n\u001b[1m\u001b[94m199 |\u001b[0m     def test_security_reporting_integration(self, vulnerability_scanner, compliance_checker, \n    \u001b[1m\u001b[94m|\u001b[0m                                                                                             \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m                                           security_monitor, temp_project_dir):\n\u001b[1m\u001b[94m201 |\u001b[0m         \"\"\"Test security reporting integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:205:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m203 |\u001b[0m         vuln_report_file = vulnerability_scanner.generate_report(temp_project_dir / \"vulnerability_report.json\")\n\u001b[1m\u001b[94m204 |\u001b[0m         assert vuln_report_file.exists()\n\u001b[1m\u001b[94m205 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m206 |\u001b[0m         # Generate compliance report\n\u001b[1m\u001b[94m207 |\u001b[0m         compliance_report_file = compliance_checker.generate_compliance_report(temp_project_dir / \"compliance_report.json\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:209:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m         compliance_report_file = compliance_checker.generate_compliance_report(temp_project_dir / \"compliance_report.json\")\n\u001b[1m\u001b[94m208 |\u001b[0m         assert compliance_report_file.exists()\n\u001b[1m\u001b[94m209 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m         # Start monitoring and generate security report\n\u001b[1m\u001b[94m211 |\u001b[0m         security_monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:213:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m211 |\u001b[0m         security_monitor.start_monitoring()\n\u001b[1m\u001b[94m212 |\u001b[0m         time.sleep(1)\n\u001b[1m\u001b[94m213 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m         security_report_file = security_monitor.export_security_report(temp_project_dir / \"security_report.json\")\n\u001b[1m\u001b[94m215 |\u001b[0m         assert security_report_file.exists()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:216:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m         security_report_file = security_monitor.export_security_report(temp_project_dir / \"security_report.json\")\n\u001b[1m\u001b[94m215 |\u001b[0m         assert security_report_file.exists()\n\u001b[1m\u001b[94m216 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m217 |\u001b[0m         # Verify report contents\n\u001b[1m\u001b[94m218 |\u001b[0m         with open(vuln_report_file, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:218:37\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m217 |\u001b[0m         # Verify report contents\n\u001b[1m\u001b[94m218 |\u001b[0m         with open(vuln_report_file, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                     \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m219 |\u001b[0m             vuln_data = json.load(f)\n\u001b[1m\u001b[94m220 |\u001b[0m             assert \"scan_results\" in vuln_data\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:222:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m220 |\u001b[0m             assert \"scan_results\" in vuln_data\n\u001b[1m\u001b[94m221 |\u001b[0m             assert \"security_metrics\" in vuln_data\n\u001b[1m\u001b[94m222 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m         with open(compliance_report_file, 'r') as f:\n\u001b[1m\u001b[94m224 |\u001b[0m             compliance_data = json.load(f)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:223:43\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m221 |\u001b[0m             assert \"security_metrics\" in vuln_data\n\u001b[1m\u001b[94m222 |\u001b[0m         \n\u001b[1m\u001b[94m223 |\u001b[0m         with open(compliance_report_file, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                           \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m224 |\u001b[0m             compliance_data = json.load(f)\n\u001b[1m\u001b[94m225 |\u001b[0m             assert len(compliance_data) >= 4\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:226:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m224 |\u001b[0m             compliance_data = json.load(f)\n\u001b[1m\u001b[94m225 |\u001b[0m             assert len(compliance_data) >= 4\n\u001b[1m\u001b[94m226 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m227 |\u001b[0m         with open(security_report_file, 'r') as f:\n\u001b[1m\u001b[94m228 |\u001b[0m             security_data = json.load(f)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:227:41\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m225 |\u001b[0m             assert len(compliance_data) >= 4\n\u001b[1m\u001b[94m226 |\u001b[0m         \n\u001b[1m\u001b[94m227 |\u001b[0m         with open(security_report_file, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                         \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m228 |\u001b[0m             security_data = json.load(f)\n\u001b[1m\u001b[94m229 |\u001b[0m             assert \"report_metadata\" in security_data\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:232:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m230 |\u001b[0m             assert \"events\" in security_data\n\u001b[1m\u001b[94m231 |\u001b[0m             assert \"alerts\" in security_data\n\u001b[1m\u001b[94m232 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m         security_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:234:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m         security_monitor.stop_monitoring()\n\u001b[1m\u001b[94m234 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m     def test_security_alerting_integration(self, security_monitor):\n\u001b[1m\u001b[94m236 |\u001b[0m         \"\"\"Test security alerting integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:239:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m237 |\u001b[0m         alert_callback_called = False\n\u001b[1m\u001b[94m238 |\u001b[0m         alert_received = None\n\u001b[1m\u001b[94m239 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m240 |\u001b[0m         def alert_callback(alert):\n\u001b[1m\u001b[94m241 |\u001b[0m             nonlocal alert_callback_called, alert_received\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:244:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m242 |\u001b[0m             alert_callback_called = True\n\u001b[1m\u001b[94m243 |\u001b[0m             alert_received = alert\n\u001b[1m\u001b[94m244 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m245 |\u001b[0m         # Add alert callback\n\u001b[1m\u001b[94m246 |\u001b[0m         security_monitor.add_alert_callback(alert_callback)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:247:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m245 |\u001b[0m         # Add alert callback\n\u001b[1m\u001b[94m246 |\u001b[0m         security_monitor.add_alert_callback(alert_callback)\n\u001b[1m\u001b[94m247 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m248 |\u001b[0m         # Start monitoring\n\u001b[1m\u001b[94m249 |\u001b[0m         security_monitor.start_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:250:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m248 |\u001b[0m         # Start monitoring\n\u001b[1m\u001b[94m249 |\u001b[0m         security_monitor.start_monitoring()\n\u001b[1m\u001b[94m250 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m251 |\u001b[0m         # Create high severity event that should trigger alert\n\u001b[1m\u001b[94m252 |\u001b[0m         critical_event = SecurityEvent(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:261:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m259 |\u001b[0m             details={\"critical\": True}\n\u001b[1m\u001b[94m260 |\u001b[0m         )\n\u001b[1m\u001b[94m261 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m262 |\u001b[0m         security_monitor._log_event(critical_event)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:263:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m262 |\u001b[0m         security_monitor._log_event(critical_event)\n\u001b[1m\u001b[94m263 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m264 |\u001b[0m         # Wait for alert processing\n\u001b[1m\u001b[94m265 |\u001b[0m         time.sleep(2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:266:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m264 |\u001b[0m         # Wait for alert processing\n\u001b[1m\u001b[94m265 |\u001b[0m         time.sleep(2)\n\u001b[1m\u001b[94m266 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m267 |\u001b[0m         # Verify alert was triggered\n\u001b[1m\u001b[94m268 |\u001b[0m         assert alert_callback_called\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:271:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m         assert alert_received is not None\n\u001b[1m\u001b[94m270 |\u001b[0m         assert alert_received.severity == SecuritySeverity.CRITICAL\n\u001b[1m\u001b[94m271 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m272 |\u001b[0m         # Verify alert is in alerts list\n\u001b[1m\u001b[94m273 |\u001b[0m         assert len(security_monitor.alerts) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:274:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m272 |\u001b[0m         # Verify alert is in alerts list\n\u001b[1m\u001b[94m273 |\u001b[0m         assert len(security_monitor.alerts) > 0\n\u001b[1m\u001b[94m274 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m275 |\u001b[0m         # Test alert acknowledgment\n\u001b[1m\u001b[94m276 |\u001b[0m         alert_id = security_monitor.alerts[0].alert_id\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:279:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m277 |\u001b[0m         assert security_monitor.acknowledge_alert(alert_id)\n\u001b[1m\u001b[94m278 |\u001b[0m         assert security_monitor.alerts[0].acknowledged\n\u001b[1m\u001b[94m279 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m280 |\u001b[0m         # Test alert resolution\n\u001b[1m\u001b[94m281 |\u001b[0m         assert security_monitor.resolve_alert(alert_id)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:283:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m         assert security_monitor.resolve_alert(alert_id)\n\u001b[1m\u001b[94m282 |\u001b[0m         assert security_monitor.alerts[0].resolved\n\u001b[1m\u001b[94m283 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m284 |\u001b[0m         security_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:285:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m284 |\u001b[0m         security_monitor.stop_monitoring()\n\u001b[1m\u001b[94m285 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m286 |\u001b[0m     def test_security_metrics_integration(self, security_monitor):\n\u001b[1m\u001b[94m287 |\u001b[0m         \"\"\"Test security metrics integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:290:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m288 |\u001b[0m         # Start monitoring\n\u001b[1m\u001b[94m289 |\u001b[0m         security_monitor.start_monitoring()\n\u001b[1m\u001b[94m290 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m291 |\u001b[0m         # Create various security events\n\u001b[1m\u001b[94m292 |\u001b[0m         events = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:304:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m302 |\u001b[0m             for i in range(5)\n\u001b[1m\u001b[94m303 |\u001b[0m         ]\n\u001b[1m\u001b[94m304 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m305 |\u001b[0m         for event in events:\n\u001b[1m\u001b[94m306 |\u001b[0m             security_monitor._log_event(event)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:307:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m305 |\u001b[0m         for event in events:\n\u001b[1m\u001b[94m306 |\u001b[0m             security_monitor._log_event(event)\n\u001b[1m\u001b[94m307 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m308 |\u001b[0m         # Wait for metrics update\n\u001b[1m\u001b[94m309 |\u001b[0m         time.sleep(3)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:310:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m308 |\u001b[0m         # Wait for metrics update\n\u001b[1m\u001b[94m309 |\u001b[0m         time.sleep(3)\n\u001b[1m\u001b[94m310 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m311 |\u001b[0m         # Verify metrics were generated\n\u001b[1m\u001b[94m312 |\u001b[0m         assert len(security_monitor.metrics) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:313:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m311 |\u001b[0m         # Verify metrics were generated\n\u001b[1m\u001b[94m312 |\u001b[0m         assert len(security_monitor.metrics) > 0\n\u001b[1m\u001b[94m313 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m314 |\u001b[0m         latest_metrics = security_monitor.metrics[-1]\n\u001b[1m\u001b[94m315 |\u001b[0m         assert latest_metrics.total_events >= 5\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:319:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m317 |\u001b[0m         assert latest_metrics.events_by_severity is not None\n\u001b[1m\u001b[94m318 |\u001b[0m         assert latest_metrics.security_score >= 0\n\u001b[1m\u001b[94m319 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m320 |\u001b[0m         # Test dashboard data\n\u001b[1m\u001b[94m321 |\u001b[0m         dashboard_data = security_monitor.get_security_dashboard_data()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:326:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m324 |\u001b[0m         assert \"recent_events\" in dashboard_data\n\u001b[1m\u001b[94m325 |\u001b[0m         assert \"security_trends\" in dashboard_data\n\u001b[1m\u001b[94m326 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m327 |\u001b[0m         security_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:328:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m327 |\u001b[0m         security_monitor.stop_monitoring()\n\u001b[1m\u001b[94m328 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m329 |\u001b[0m     def test_security_thresholds_integration(self, security_monitor):\n\u001b[1m\u001b[94m330 |\u001b[0m         \"\"\"Test security thresholds integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:333:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m331 |\u001b[0m         # Start monitoring\n\u001b[1m\u001b[94m332 |\u001b[0m         security_monitor.start_monitoring()\n\u001b[1m\u001b[94m333 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m334 |\u001b[0m         # Create multiple authentication failure events to trigger threshold\n\u001b[1m\u001b[94m335 |\u001b[0m         for i in range(6):  # More than max_failed_auth_attempts (5)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:346:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m344 |\u001b[0m             )\n\u001b[1m\u001b[94m345 |\u001b[0m             security_monitor._log_event(auth_failure)\n\u001b[1m\u001b[94m346 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m347 |\u001b[0m         # Wait for anomaly detection\n\u001b[1m\u001b[94m348 |\u001b[0m         time.sleep(3)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:349:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m347 |\u001b[0m         # Wait for anomaly detection\n\u001b[1m\u001b[94m348 |\u001b[0m         time.sleep(3)\n\u001b[1m\u001b[94m349 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m350 |\u001b[0m         # Verify alert was created for threshold violation\n\u001b[1m\u001b[94m351 |\u001b[0m         auth_alerts = [alert for alert in security_monitor.alerts \n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:351:66\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m350 |\u001b[0m         # Verify alert was created for threshold violation\n\u001b[1m\u001b[94m351 |\u001b[0m         auth_alerts = [alert for alert in security_monitor.alerts \n    \u001b[1m\u001b[94m|\u001b[0m                                                                  \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m352 |\u001b[0m                       if \"Brute Force\" in alert.title]\n\u001b[1m\u001b[94m353 |\u001b[0m         assert len(auth_alerts) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:354:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m352 |\u001b[0m                       if \"Brute Force\" in alert.title]\n\u001b[1m\u001b[94m353 |\u001b[0m         assert len(auth_alerts) > 0\n\u001b[1m\u001b[94m354 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m355 |\u001b[0m         security_monitor.stop_monitoring()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:356:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m355 |\u001b[0m         security_monitor.stop_monitoring()\n\u001b[1m\u001b[94m356 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m357 |\u001b[0m     def test_security_configuration_integration(self, temp_project_dir):\n\u001b[1m\u001b[94m358 |\u001b[0m         \"\"\"Test security configuration integration.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m\u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:366:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m364 |\u001b[0m     enabled: true\n\u001b[1m\u001b[94m365 |\u001b[0m     severity_threshold: \"medium\"\n\u001b[1m\u001b[94m366 |\u001b[0m   \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^\u001b[0m\n\u001b[1m\u001b[94m367 |\u001b[0m security_monitoring:\n\u001b[1m\u001b[94m368 |\u001b[0m   enabled: true\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:372:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m370 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m371 |\u001b[0m         config_file.write_text(config_content)\n\u001b[1m\u001b[94m372 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m373 |\u001b[0m         # Test that configuration can be loaded\n\u001b[1m\u001b[94m374 |\u001b[0m         import yaml\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:375:32\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m373 |\u001b[0m         # Test that configuration can be loaded\n\u001b[1m\u001b[94m374 |\u001b[0m         import yaml\n\u001b[1m\u001b[94m375 |\u001b[0m         with open(config_file, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m376 |\u001b[0m             config = yaml.safe_load(f)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:377:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m375 |\u001b[0m         with open(config_file, 'r') as f:\n\u001b[1m\u001b[94m376 |\u001b[0m             config = yaml.safe_load(f)\n\u001b[1m\u001b[94m377 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m         assert config[\"security_scanning\"][\"bandit\"][\"enabled\"]\n\u001b[1m\u001b[94m379 |\u001b[0m         assert config[\"security_monitoring\"][\"enabled\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:380:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m         assert config[\"security_scanning\"][\"bandit\"][\"enabled\"]\n\u001b[1m\u001b[94m379 |\u001b[0m         assert config[\"security_monitoring\"][\"enabled\"]\n\u001b[1m\u001b[94m380 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m     @pytest.mark.performance\n\u001b[1m\u001b[94m382 |\u001b[0m     def test_security_performance_integration(self, security_test_suite, vulnerability_scanner):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:385:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m383 |\u001b[0m         \"\"\"Test security components performance.\"\"\"\n\u001b[1m\u001b[94m384 |\u001b[0m         import time\n\u001b[1m\u001b[94m385 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m386 |\u001b[0m         # Test security test suite performance\n\u001b[1m\u001b[94m387 |\u001b[0m         start_time = time.time()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:390:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m388 |\u001b[0m         security_results = security_test_suite.run_security_scan()\n\u001b[1m\u001b[94m389 |\u001b[0m         security_time = time.time() - start_time\n\u001b[1m\u001b[94m390 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m         # Test vulnerability scanner performance\n\u001b[1m\u001b[94m392 |\u001b[0m         start_time = time.time()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:395:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m393 |\u001b[0m         vuln_results = vulnerability_scanner.scan_all()\n\u001b[1m\u001b[94m394 |\u001b[0m         vuln_time = time.time() - start_time\n\u001b[1m\u001b[94m395 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m396 |\u001b[0m         # Verify performance is within acceptable limits\n\u001b[1m\u001b[94m397 |\u001b[0m         assert security_time < 60, f\"Security test suite took too long: {security_time}s\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:399:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m397 |\u001b[0m         assert security_time < 60, f\"Security test suite took too long: {security_time}s\"\n\u001b[1m\u001b[94m398 |\u001b[0m         assert vuln_time < 120, f\"Vulnerability scanner took too long: {vuln_time}s\"\n\u001b[1m\u001b[94m399 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m400 |\u001b[0m         # Verify results are still valid\n\u001b[1m\u001b[94m401 |\u001b[0m         assert security_results[\"vulnerability_scan\"][\"success\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:421:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m419 |\u001b[0m class TestSecurityMarkers:\n\u001b[1m\u001b[94m420 |\u001b[0m     \"\"\"Test security-specific pytest markers.\"\"\"\n\u001b[1m\u001b[94m421 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m422 |\u001b[0m     def test_security_marker_works(self):\n\u001b[1m\u001b[94m423 |\u001b[0m         \"\"\"Test that security marker works.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:425:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m423 |\u001b[0m         \"\"\"Test that security marker works.\"\"\"\n\u001b[1m\u001b[94m424 |\u001b[0m         assert True\n\u001b[1m\u001b[94m425 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m426 |\u001b[0m     @pytest.mark.slow\n\u001b[1m\u001b[94m427 |\u001b[0m     def test_slow_security_test(self):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:438:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m436 |\u001b[0m     # This test simulates what would happen in CI/CD\n\u001b[1m\u001b[94m437 |\u001b[0m     security_suite = SecurityTestSuite()\n\u001b[1m\u001b[94m438 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m439 |\u001b[0m     # Run security scan\n\u001b[1m\u001b[94m440 |\u001b[0m     results = security_suite.run_security_scan()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:441:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m439 |\u001b[0m     # Run security scan\n\u001b[1m\u001b[94m440 |\u001b[0m     results = security_suite.run_security_scan()\n\u001b[1m\u001b[94m441 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m442 |\u001b[0m     # Verify all security checks pass\n\u001b[1m\u001b[94m443 |\u001b[0m     assert results[\"vulnerability_scan\"][\"success\"], \"Vulnerability scan failed in CI/CD\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/test_security_integration.py:448:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m446 |\u001b[0m     assert results[\"configuration_security\"][\"success\"], \"Configuration security failed in CI/CD\"\n\u001b[1m\u001b[94m447 |\u001b[0m     assert results[\"data_security\"][\"success\"], \"Data security checks failed in CI/CD\"\n\u001b[1m\u001b[94m448 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m449 |\u001b[0m     # Verify compliance\n\u001b[1m\u001b[94m450 |\u001b[0m     compliance_results = results[\"compliance_check\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:11:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m10 |\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import os\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import subprocess\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import sys\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import time\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pathlib import Path\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Dict, List, Any, Optional\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from dataclasses import dataclass\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from datetime import datetime\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_____________________________^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`os` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:11:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m10 |\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m import os\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import json\n\u001b[1m\u001b[94m13 |\u001b[0m import subprocess\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `os`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:50:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m48 |\u001b[0m class VulnerabilityScanner:\n\u001b[1m\u001b[94m49 |\u001b[0m     \"\"\"Comprehensive vulnerability scanner for SparkForge.\"\"\"\n\u001b[1m\u001b[94m50 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m51 |\u001b[0m     def __init__(self, project_root: Optional[Path] = None):\n\u001b[1m\u001b[94m52 |\u001b[0m         self.project_root = project_root or Path.cwd()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:55:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m53 |\u001b[0m         self.scan_results = []\n\u001b[1m\u001b[94m54 |\u001b[0m         self.security_metrics = None\n\u001b[1m\u001b[94m55 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m56 |\u001b[0m     def scan_all(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m57 |\u001b[0m         \"\"\"Run comprehensive vulnerability scan.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:59:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m57 |\u001b[0m         \"\"\"Run comprehensive vulnerability scan.\"\"\"\n\u001b[1m\u001b[94m58 |\u001b[0m         start_time = time.time()\n\u001b[1m\u001b[94m59 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m60 |\u001b[0m         scan_results = {\n\u001b[1m\u001b[94m61 |\u001b[0m             \"static_analysis\": self.scan_static_code(),\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:68:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m             \"supply_chain_scan\": self.scan_supply_chain()\n\u001b[1m\u001b[94m67 |\u001b[0m         }\n\u001b[1m\u001b[94m68 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m69 |\u001b[0m         end_time = time.time()\n\u001b[1m\u001b[94m70 |\u001b[0m         self.security_metrics = SecurityMetrics(\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:79:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m             scan_duration=end_time - start_time\n\u001b[1m\u001b[94m78 |\u001b[0m         )\n\u001b[1m\u001b[94m79 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m80 |\u001b[0m         return {\n\u001b[1m\u001b[94m81 |\u001b[0m             \"scan_results\": scan_results,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:85:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m             \"summary\": self._generate_summary()\n\u001b[1m\u001b[94m84 |\u001b[0m         }\n\u001b[1m\u001b[94m85 |\u001b[0m     \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m     def scan_static_code(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m87 |\u001b[0m         \"\"\"Scan static code for vulnerabilities using bandit.\"\"\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:91:48\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m89 |\u001b[0m             # Run bandit security scan\n\u001b[1m\u001b[94m90 |\u001b[0m             result = subprocess.run([\n\u001b[1m\u001b[94m91 |\u001b[0m                 sys.executable, \"-m\", \"bandit\", \n   \u001b[1m\u001b[94m|\u001b[0m                                                \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m                 \"-r\", str(self.project_root / \"sparkforge\"),\n\u001b[1m\u001b[94m93 |\u001b[0m                 \"-f\", \"json\",\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:96:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m94 |\u001b[0m                 \"-ll\"  # Low confidence, low severity\n\u001b[1m\u001b[94m95 |\u001b[0m             ], capture_output=True, text=True, cwd=self.project_root)\n\u001b[1m\u001b[94m96 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m97 |\u001b[0m             if result.returncode != 0:\n\u001b[1m\u001b[94m98 |\u001b[0m                 return {\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:103:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m101 |\u001b[0m                     \"vulnerabilities\": []\n\u001b[1m\u001b[94m102 |\u001b[0m                 }\n\u001b[1m\u001b[94m103 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m104 |\u001b[0m             bandit_data = json.loads(result.stdout)\n\u001b[1m\u001b[94m105 |\u001b[0m             vulnerabilities = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:106:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m104 |\u001b[0m             bandit_data = json.loads(result.stdout)\n\u001b[1m\u001b[94m105 |\u001b[0m             vulnerabilities = []\n\u001b[1m\u001b[94m106 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m107 |\u001b[0m             for issue in bandit_data.get(\"results\", []):\n\u001b[1m\u001b[94m108 |\u001b[0m                 vulnerability = VulnerabilityReport(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:119:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m117 |\u001b[0m                 )\n\u001b[1m\u001b[94m118 |\u001b[0m                 vulnerabilities.append(vulnerability)\n\u001b[1m\u001b[94m119 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m             return {\n\u001b[1m\u001b[94m121 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:127:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m125 |\u001b[0m                 \"version\": self._get_bandit_version()\n\u001b[1m\u001b[94m126 |\u001b[0m             }\n\u001b[1m\u001b[94m127 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m129 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:134:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m132 |\u001b[0m                 \"vulnerabilities\": []\n\u001b[1m\u001b[94m133 |\u001b[0m             }\n\u001b[1m\u001b[94m134 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m135 |\u001b[0m     def scan_dependencies(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m136 |\u001b[0m         \"\"\"Scan dependencies for known vulnerabilities.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:142:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m140 |\u001b[0m                 sys.executable, \"-m\", \"safety\", \"check\", \"--json\"\n\u001b[1m\u001b[94m141 |\u001b[0m             ], capture_output=True, text=True, cwd=self.project_root)\n\u001b[1m\u001b[94m142 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m143 |\u001b[0m             vulnerabilities = []\n\u001b[1m\u001b[94m144 |\u001b[0m             if safety_result.stdout:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:160:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m158 |\u001b[0m                 except json.JSONDecodeError:\n\u001b[1m\u001b[94m159 |\u001b[0m                     pass\n\u001b[1m\u001b[94m160 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m161 |\u001b[0m             # Run pip-audit if available\n\u001b[1m\u001b[94m162 |\u001b[0m             audit_vulnerabilities = []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:167:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m165 |\u001b[0m                     sys.executable, \"-m\", \"pip\", \"audit\", \"--json\"\n\u001b[1m\u001b[94m166 |\u001b[0m                 ], capture_output=True, text=True, cwd=self.project_root)\n\u001b[1m\u001b[94m167 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m168 |\u001b[0m                 if audit_result.returncode == 0 and audit_result.stdout:\n\u001b[1m\u001b[94m169 |\u001b[0m                     audit_data = json.loads(audit_result.stdout)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:178:41\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m176 |\u001b[0m                             line_number=0,\n\u001b[1m\u001b[94m177 |\u001b[0m                             cve_id=vuln.get(\"id\"),\n\u001b[1m\u001b[94m178 |\u001b[0m                             remediation=f\"Update package to fix vulnerability\"\n    \u001b[1m\u001b[94m|\u001b[0m                                         \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m179 |\u001b[0m                         )\n\u001b[1m\u001b[94m180 |\u001b[0m                         audit_vulnerabilities.append(vulnerability)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:183:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m181 |\u001b[0m             except Exception:\n\u001b[1m\u001b[94m182 |\u001b[0m                 pass  # pip-audit not available\n\u001b[1m\u001b[94m183 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m             all_vulnerabilities = vulnerabilities + audit_vulnerabilities\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:185:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m             all_vulnerabilities = vulnerabilities + audit_vulnerabilities\n\u001b[1m\u001b[94m185 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m             return {\n\u001b[1m\u001b[94m187 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mC401 \u001b[0m\u001b[1mUnnecessary generator (rewrite as a set comprehension)\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:190:44\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m188 |\u001b[0m                 \"vulnerabilities\": [v.__dict__ for v in all_vulnerabilities],\n\u001b[1m\u001b[94m189 |\u001b[0m                 \"scanner\": \"safety + pip-audit\",\n\u001b[1m\u001b[94m190 |\u001b[0m                 \"vulnerable_packages\": len(set(v.package for v in all_vulnerabilities if hasattr(v, 'package')))\n    \u001b[1m\u001b[94m|\u001b[0m                                            \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m191 |\u001b[0m             }\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRewrite as a set comprehension\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:192:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m                 \"vulnerable_packages\": len(set(v.package for v in all_vulnerabilities if hasattr(v, 'package')))\n\u001b[1m\u001b[94m191 |\u001b[0m             }\n\u001b[1m\u001b[94m192 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m194 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:199:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m                 \"vulnerabilities\": []\n\u001b[1m\u001b[94m198 |\u001b[0m             }\n\u001b[1m\u001b[94m199 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m     def scan_configuration(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m201 |\u001b[0m         \"\"\"Scan configuration files for security issues.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:203:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m         \"\"\"Scan configuration files for security issues.\"\"\"\n\u001b[1m\u001b[94m202 |\u001b[0m         vulnerabilities = []\n\u001b[1m\u001b[94m203 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m         # Check for hardcoded secrets in configuration files\n\u001b[1m\u001b[94m205 |\u001b[0m         config_files = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:215:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m213 |\u001b[0m             \"*.yaml\"\n\u001b[1m\u001b[94m214 |\u001b[0m         ]\n\u001b[1m\u001b[94m215 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m216 |\u001b[0m         for pattern in config_files:\n\u001b[1m\u001b[94m217 |\u001b[0m             for config_file in self.project_root.glob(pattern):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:221:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m219 |\u001b[0m                     vulns = self._scan_config_file(config_file)\n\u001b[1m\u001b[94m220 |\u001b[0m                     vulnerabilities.extend(vulns)\n\u001b[1m\u001b[94m221 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m222 |\u001b[0m         # Check for insecure default configurations\n\u001b[1m\u001b[94m223 |\u001b[0m         vulns = self._check_default_configurations()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:225:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m         vulns = self._check_default_configurations()\n\u001b[1m\u001b[94m224 |\u001b[0m         vulnerabilities.extend(vulns)\n\u001b[1m\u001b[94m225 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m226 |\u001b[0m         return {\n\u001b[1m\u001b[94m227 |\u001b[0m             \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:231:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m229 |\u001b[0m             \"scanner\": \"custom_config_scanner\"\n\u001b[1m\u001b[94m230 |\u001b[0m         }\n\u001b[1m\u001b[94m231 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m232 |\u001b[0m     def _scan_config_file(self, config_file: Path) -> List[VulnerabilityReport]:\n\u001b[1m\u001b[94m233 |\u001b[0m         \"\"\"Scan individual configuration file for security issues.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:235:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m         \"\"\"Scan individual configuration file for security issues.\"\"\"\n\u001b[1m\u001b[94m234 |\u001b[0m         vulnerabilities = []\n\u001b[1m\u001b[94m235 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m236 |\u001b[0m         try:\n\u001b[1m\u001b[94m237 |\u001b[0m             with open(config_file, 'r', encoding='utf-8') as f:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:237:36\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m236 |\u001b[0m         try:\n\u001b[1m\u001b[94m237 |\u001b[0m             with open(config_file, 'r', encoding='utf-8') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                    \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m238 |\u001b[0m                 content = f.read()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:239:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m237 |\u001b[0m             with open(config_file, 'r', encoding='utf-8') as f:\n\u001b[1m\u001b[94m238 |\u001b[0m                 content = f.read()\n\u001b[1m\u001b[94m239 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m240 |\u001b[0m             # Check for hardcoded secrets\n\u001b[1m\u001b[94m241 |\u001b[0m             secret_patterns = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:249:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m247 |\u001b[0m                 (\"private_key\", \"Hardcoded private key detected\")\n\u001b[1m\u001b[94m248 |\u001b[0m             ]\n\u001b[1m\u001b[94m249 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m250 |\u001b[0m             for pattern, description in secret_patterns:\n\u001b[1m\u001b[94m251 |\u001b[0m                 if pattern in content.lower() and \"=\" in content:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:267:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m265 |\u001b[0m                             )\n\u001b[1m\u001b[94m266 |\u001b[0m                             vulnerabilities.append(vulnerability)\n\u001b[1m\u001b[94m267 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m268 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m269 |\u001b[0m             pass  # Skip files that can't be read\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:270:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m268 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m269 |\u001b[0m             pass  # Skip files that can't be read\n\u001b[1m\u001b[94m270 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m         return vulnerabilities\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:272:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m         return vulnerabilities\n\u001b[1m\u001b[94m272 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m273 |\u001b[0m     def _check_default_configurations(self) -> List[VulnerabilityReport]:\n\u001b[1m\u001b[94m274 |\u001b[0m         \"\"\"Check for insecure default configurations.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:276:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m274 |\u001b[0m         \"\"\"Check for insecure default configurations.\"\"\"\n\u001b[1m\u001b[94m275 |\u001b[0m         vulnerabilities = []\n\u001b[1m\u001b[94m276 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m277 |\u001b[0m         # Check pyproject.toml for insecure defaults\n\u001b[1m\u001b[94m278 |\u001b[0m         pyproject_file = self.project_root / \"pyproject.toml\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:281:43\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m279 |\u001b[0m         if pyproject_file.exists():\n\u001b[1m\u001b[94m280 |\u001b[0m             try:\n\u001b[1m\u001b[94m281 |\u001b[0m                 with open(pyproject_file, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                           \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m282 |\u001b[0m                     content = f.read()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:283:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m                 with open(pyproject_file, 'r') as f:\n\u001b[1m\u001b[94m282 |\u001b[0m                     content = f.read()\n\u001b[1m\u001b[94m283 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m284 |\u001b[0m                 # Check for debug mode enabled\n\u001b[1m\u001b[94m285 |\u001b[0m                 if \"debug = true\" in content.lower():\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:295:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m293 |\u001b[0m                     )\n\u001b[1m\u001b[94m294 |\u001b[0m                     vulnerabilities.append(vulnerability)\n\u001b[1m\u001b[94m295 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m296 |\u001b[0m                 # Check for insecure HTTP URLs\n\u001b[1m\u001b[94m297 |\u001b[0m                 if \"http://\" in content and \"https://\" not in content:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:307:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m305 |\u001b[0m                     )\n\u001b[1m\u001b[94m306 |\u001b[0m                     vulnerabilities.append(vulnerability)\n\u001b[1m\u001b[94m307 |\u001b[0m                     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m308 |\u001b[0m             except Exception:\n\u001b[1m\u001b[94m309 |\u001b[0m                 pass\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:310:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m308 |\u001b[0m             except Exception:\n\u001b[1m\u001b[94m309 |\u001b[0m                 pass\n\u001b[1m\u001b[94m310 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m311 |\u001b[0m         return vulnerabilities\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:312:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m311 |\u001b[0m         return vulnerabilities\n\u001b[1m\u001b[94m312 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m313 |\u001b[0m     def scan_secrets(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m314 |\u001b[0m         \"\"\"Scan for accidentally committed secrets.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:316:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m314 |\u001b[0m         \"\"\"Scan for accidentally committed secrets.\"\"\"\n\u001b[1m\u001b[94m315 |\u001b[0m         vulnerabilities = []\n\u001b[1m\u001b[94m316 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m317 |\u001b[0m         # Common secret patterns\n\u001b[1m\u001b[94m318 |\u001b[0m         secret_patterns = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:331:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m329 |\u001b[0m             (r'-----BEGIN RSA PRIVATE KEY-----', \"RSA private key in code\")\n\u001b[1m\u001b[94m330 |\u001b[0m         ]\n\u001b[1m\u001b[94m331 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m332 |\u001b[0m         # Scan Python files\n\u001b[1m\u001b[94m333 |\u001b[0m         for py_file in self.project_root.rglob(\"*.py\"):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:336:40\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m334 |\u001b[0m             if py_file.is_file() and not any(part.startswith('.') for part in py_file.parts):\n\u001b[1m\u001b[94m335 |\u001b[0m                 try:\n\u001b[1m\u001b[94m336 |\u001b[0m                     with open(py_file, 'r', encoding='utf-8') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                        \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m337 |\u001b[0m                         content = f.read()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:338:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m336 |\u001b[0m                     with open(py_file, 'r', encoding='utf-8') as f:\n\u001b[1m\u001b[94m337 |\u001b[0m                         content = f.read()\n\u001b[1m\u001b[94m338 |\u001b[0m                     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m339 |\u001b[0m                     for pattern, description in secret_patterns:\n\u001b[1m\u001b[94m340 |\u001b[0m                         import re\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:346:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m344 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m                     line_start = content.rfind('\\n', 0, match.start()) + 1\n\u001b[1m\u001b[94m345 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m                     line_content = content[line_start:match.end()]\n\u001b[1m\u001b[94m346 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m                     \n\u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m347 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m                     if not (line_content.strip().startswith('#') or 'test' in py_file.name.lower()):\n\u001b[1m\u001b[94m348 |\u001b[0m \u001b[1m\u001b[94m\u2026\u001b[0m                         vulnerability = VulnerabilityReport(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:357:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m355 |\u001b[0m                                 )\n\u001b[1m\u001b[94m356 |\u001b[0m                                 vulnerabilities.append(vulnerability)\n\u001b[1m\u001b[94m357 |\u001b[0m                                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m358 |\u001b[0m                 except Exception:\n\u001b[1m\u001b[94m359 |\u001b[0m                     pass  # Skip files that can't be read\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:360:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m358 |\u001b[0m                 except Exception:\n\u001b[1m\u001b[94m359 |\u001b[0m                     pass  # Skip files that can't be read\n\u001b[1m\u001b[94m360 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m361 |\u001b[0m         return {\n\u001b[1m\u001b[94m362 |\u001b[0m             \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:366:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m364 |\u001b[0m             \"scanner\": \"custom_secrets_scanner\"\n\u001b[1m\u001b[94m365 |\u001b[0m         }\n\u001b[1m\u001b[94m366 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m367 |\u001b[0m     def scan_licenses(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m368 |\u001b[0m         \"\"\"Scan for license compliance issues.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:374:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m372 |\u001b[0m                 sys.executable, \"-m\", \"pip-licenses\", \"--format=json\"\n\u001b[1m\u001b[94m373 |\u001b[0m             ], capture_output=True, text=True, cwd=self.project_root)\n\u001b[1m\u001b[94m374 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m375 |\u001b[0m             if result.returncode != 0:\n\u001b[1m\u001b[94m376 |\u001b[0m                 return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:381:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m379 |\u001b[0m                     \"vulnerabilities\": []\n\u001b[1m\u001b[94m380 |\u001b[0m                 }\n\u001b[1m\u001b[94m381 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m382 |\u001b[0m             licenses_data = json.loads(result.stdout)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:383:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m382 |\u001b[0m             licenses_data = json.loads(result.stdout)\n\u001b[1m\u001b[94m383 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m384 |\u001b[0m             # Check for problematic licenses\n\u001b[1m\u001b[94m385 |\u001b[0m             problematic_licenses = [\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:389:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m387 |\u001b[0m                 \"Commercial\", \"Proprietary\", \"Unknown\"\n\u001b[1m\u001b[94m388 |\u001b[0m             ]\n\u001b[1m\u001b[94m389 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m390 |\u001b[0m             vulnerabilities = []\n\u001b[1m\u001b[94m391 |\u001b[0m             for package in licenses_data:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:403:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m401 |\u001b[0m                     )\n\u001b[1m\u001b[94m402 |\u001b[0m                     vulnerabilities.append(vulnerability)\n\u001b[1m\u001b[94m403 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m404 |\u001b[0m             return {\n\u001b[1m\u001b[94m405 |\u001b[0m                 \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:410:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m408 |\u001b[0m                 \"total_packages\": len(licenses_data)\n\u001b[1m\u001b[94m409 |\u001b[0m             }\n\u001b[1m\u001b[94m410 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m411 |\u001b[0m         except Exception as e:\n\u001b[1m\u001b[94m412 |\u001b[0m             return {\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:417:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m415 |\u001b[0m                 \"vulnerabilities\": []\n\u001b[1m\u001b[94m416 |\u001b[0m             }\n\u001b[1m\u001b[94m417 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m418 |\u001b[0m     def scan_supply_chain(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m419 |\u001b[0m         \"\"\"Scan for supply chain security issues.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:421:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m419 |\u001b[0m         \"\"\"Scan for supply chain security issues.\"\"\"\n\u001b[1m\u001b[94m420 |\u001b[0m         vulnerabilities = []\n\u001b[1m\u001b[94m421 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m422 |\u001b[0m         # Check for pinned dependencies\n\u001b[1m\u001b[94m423 |\u001b[0m         requirements_file = self.project_root / \"requirements.txt\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mUP015 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mUnnecessary mode argument\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:426:46\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m424 |\u001b[0m         if requirements_file.exists():\n\u001b[1m\u001b[94m425 |\u001b[0m             try:\n\u001b[1m\u001b[94m426 |\u001b[0m                 with open(requirements_file, 'r') as f:\n    \u001b[1m\u001b[94m|\u001b[0m                                              \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m427 |\u001b[0m                     requirements = f.read()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove mode argument\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:428:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m426 |\u001b[0m                 with open(requirements_file, 'r') as f:\n\u001b[1m\u001b[94m427 |\u001b[0m                     requirements = f.read()\n\u001b[1m\u001b[94m428 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m429 |\u001b[0m                 unpinned_deps = []\n\u001b[1m\u001b[94m430 |\u001b[0m                 for line in requirements.split('\\n'):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:435:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m433 |\u001b[0m                         if '==' not in line and '>=' not in line and '<=' not in line and '~=' not in line:\n\u001b[1m\u001b[94m434 |\u001b[0m                             unpinned_deps.append(line.split('==')[0])\n\u001b[1m\u001b[94m435 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m436 |\u001b[0m                 if unpinned_deps:\n\u001b[1m\u001b[94m437 |\u001b[0m                     vulnerability = VulnerabilityReport(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:446:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m444 |\u001b[0m                     )\n\u001b[1m\u001b[94m445 |\u001b[0m                     vulnerabilities.append(vulnerability)\n\u001b[1m\u001b[94m446 |\u001b[0m                     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m447 |\u001b[0m             except Exception:\n\u001b[1m\u001b[94m448 |\u001b[0m                 pass\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:449:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m447 |\u001b[0m             except Exception:\n\u001b[1m\u001b[94m448 |\u001b[0m                 pass\n\u001b[1m\u001b[94m449 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m450 |\u001b[0m         return {\n\u001b[1m\u001b[94m451 |\u001b[0m             \"success\": True,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:455:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m453 |\u001b[0m             \"scanner\": \"supply_chain_scanner\"\n\u001b[1m\u001b[94m454 |\u001b[0m         }\n\u001b[1m\u001b[94m455 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m456 |\u001b[0m     def _map_safety_severity(self, severity: str) -> str:\n\u001b[1m\u001b[94m457 |\u001b[0m         \"\"\"Map safety severity to standard severity levels.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:460:32\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m458 |\u001b[0m         severity_map = {\n\u001b[1m\u001b[94m459 |\u001b[0m             \"high\": \"high\",\n\u001b[1m\u001b[94m460 |\u001b[0m             \"medium\": \"medium\", \n    \u001b[1m\u001b[94m|\u001b[0m                                \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m461 |\u001b[0m             \"low\": \"low\",\n\u001b[1m\u001b[94m462 |\u001b[0m             \"info\": \"info\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:465:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m463 |\u001b[0m         }\n\u001b[1m\u001b[94m464 |\u001b[0m         return severity_map.get(severity.lower(), \"medium\")\n\u001b[1m\u001b[94m465 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m466 |\u001b[0m     def _get_bandit_version(self) -> str:\n\u001b[1m\u001b[94m467 |\u001b[0m         \"\"\"Get bandit version.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:469:83\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m467 |\u001b[0m         \"\"\"Get bandit version.\"\"\"\n\u001b[1m\u001b[94m468 |\u001b[0m         try:\n\u001b[1m\u001b[94m469 |\u001b[0m             result = subprocess.run([sys.executable, \"-m\", \"bandit\", \"--version\"], \n    \u001b[1m\u001b[94m|\u001b[0m                                                                                   \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m470 |\u001b[0m                                   capture_output=True, text=True)\n\u001b[1m\u001b[94m471 |\u001b[0m             return result.stdout.strip()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:474:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m472 |\u001b[0m         except Exception:\n\u001b[1m\u001b[94m473 |\u001b[0m             return \"unknown\"\n\u001b[1m\u001b[94m474 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m475 |\u001b[0m     def _generate_summary(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m476 |\u001b[0m         \"\"\"Generate security scan summary.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:479:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m477 |\u001b[0m         if not self.security_metrics:\n\u001b[1m\u001b[94m478 |\u001b[0m             return {}\n\u001b[1m\u001b[94m479 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m480 |\u001b[0m         return {\n\u001b[1m\u001b[94m481 |\u001b[0m             \"total_vulnerabilities\": self.security_metrics.total_vulnerabilities,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:493:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m491 |\u001b[0m             \"recommendations\": self._generate_recommendations()\n\u001b[1m\u001b[94m492 |\u001b[0m         }\n\u001b[1m\u001b[94m493 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m494 |\u001b[0m     def _calculate_risk_score(self) -> float:\n\u001b[1m\u001b[94m495 |\u001b[0m         \"\"\"Calculate overall risk score (0-100, higher is worse).\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:498:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m496 |\u001b[0m         if not self.security_metrics:\n\u001b[1m\u001b[94m497 |\u001b[0m             return 0.0\n\u001b[1m\u001b[94m498 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m499 |\u001b[0m         # Weighted risk calculation\n\u001b[1m\u001b[94m500 |\u001b[0m         high_weight = 10\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:504:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m502 |\u001b[0m         low_weight = 2\n\u001b[1m\u001b[94m503 |\u001b[0m         info_weight = 1\n\u001b[1m\u001b[94m504 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m505 |\u001b[0m         total_weighted = (\n\u001b[1m\u001b[94m506 |\u001b[0m             self.security_metrics.high_severity * high_weight +\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:511:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m509 |\u001b[0m             self.security_metrics.info_severity * info_weight\n\u001b[1m\u001b[94m510 |\u001b[0m         )\n\u001b[1m\u001b[94m511 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m512 |\u001b[0m         # Normalize to 0-100 scale (max reasonable score is 50)\n\u001b[1m\u001b[94m513 |\u001b[0m         return min(100.0, (total_weighted / 50.0) * 100.0)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:514:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m512 |\u001b[0m         # Normalize to 0-100 scale (max reasonable score is 50)\n\u001b[1m\u001b[94m513 |\u001b[0m         return min(100.0, (total_weighted / 50.0) * 100.0)\n\u001b[1m\u001b[94m514 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m515 |\u001b[0m     def _generate_recommendations(self) -> List[str]:\n\u001b[1m\u001b[94m516 |\u001b[0m         \"\"\"Generate security recommendations based on scan results.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:518:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m516 |\u001b[0m         \"\"\"Generate security recommendations based on scan results.\"\"\"\n\u001b[1m\u001b[94m517 |\u001b[0m         recommendations = []\n\u001b[1m\u001b[94m518 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m519 |\u001b[0m         if not self.security_metrics:\n\u001b[1m\u001b[94m520 |\u001b[0m             return recommendations\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:521:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m519 |\u001b[0m         if not self.security_metrics:\n\u001b[1m\u001b[94m520 |\u001b[0m             return recommendations\n\u001b[1m\u001b[94m521 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m522 |\u001b[0m         if self.security_metrics.high_severity > 0:\n\u001b[1m\u001b[94m523 |\u001b[0m             recommendations.append(\"Address high severity vulnerabilities immediately\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:524:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m522 |\u001b[0m         if self.security_metrics.high_severity > 0:\n\u001b[1m\u001b[94m523 |\u001b[0m             recommendations.append(\"Address high severity vulnerabilities immediately\")\n\u001b[1m\u001b[94m524 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m525 |\u001b[0m         if self.security_metrics.medium_severity > 5:\n\u001b[1m\u001b[94m526 |\u001b[0m             recommendations.append(\"Review and fix medium severity vulnerabilities\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:527:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m525 |\u001b[0m         if self.security_metrics.medium_severity > 5:\n\u001b[1m\u001b[94m526 |\u001b[0m             recommendations.append(\"Review and fix medium severity vulnerabilities\")\n\u001b[1m\u001b[94m527 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m528 |\u001b[0m         if self.security_metrics.total_vulnerabilities > 20:\n\u001b[1m\u001b[94m529 |\u001b[0m             recommendations.append(\"Consider implementing automated security testing in CI/CD\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:530:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m528 |\u001b[0m         if self.security_metrics.total_vulnerabilities > 20:\n\u001b[1m\u001b[94m529 |\u001b[0m             recommendations.append(\"Consider implementing automated security testing in CI/CD\")\n\u001b[1m\u001b[94m530 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m531 |\u001b[0m         recommendations.extend([\n\u001b[1m\u001b[94m532 |\u001b[0m             \"Regularly update dependencies to latest secure versions\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:538:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m536 |\u001b[0m             \"Implement runtime security monitoring\"\n\u001b[1m\u001b[94m537 |\u001b[0m         ])\n\u001b[1m\u001b[94m538 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m539 |\u001b[0m         return recommendations\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:540:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m539 |\u001b[0m         return recommendations\n\u001b[1m\u001b[94m540 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m541 |\u001b[0m     def generate_report(self, output_file: Optional[Path] = None) -> Path:\n\u001b[1m\u001b[94m542 |\u001b[0m         \"\"\"Generate detailed security report.\"\"\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:544:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m542 |\u001b[0m         \"\"\"Generate detailed security report.\"\"\"\n\u001b[1m\u001b[94m543 |\u001b[0m         scan_results = self.scan_all()\n\u001b[1m\u001b[94m544 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m545 |\u001b[0m         if output_file is None:\n\u001b[1m\u001b[94m546 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:548:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m546 |\u001b[0m             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\u001b[1m\u001b[94m547 |\u001b[0m             output_file = self.project_root / f\"security_report_{timestamp}.json\"\n\u001b[1m\u001b[94m548 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m549 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m550 |\u001b[0m             json.dump(scan_results, f, indent=2, default=str)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:551:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m549 |\u001b[0m         with open(output_file, 'w') as f:\n\u001b[1m\u001b[94m550 |\u001b[0m             json.dump(scan_results, f, indent=2, default=str)\n\u001b[1m\u001b[94m551 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m552 |\u001b[0m         return output_file\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:558:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m556 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m557 |\u001b[0m     import argparse\n\u001b[1m\u001b[94m558 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m559 |\u001b[0m     parser = argparse.ArgumentParser(description=\"SparkForge Vulnerability Scanner\")\n\u001b[1m\u001b[94m560 |\u001b[0m     parser.add_argument(\"--project-root\", type=Path, help=\"Project root directory\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:563:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m561 |\u001b[0m     parser.add_argument(\"--output\", type=Path, help=\"Output file for report\")\n\u001b[1m\u001b[94m562 |\u001b[0m     parser.add_argument(\"--format\", choices=[\"json\", \"html\"], default=\"json\", help=\"Output format\")\n\u001b[1m\u001b[94m563 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m564 |\u001b[0m     args = parser.parse_args()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:565:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m564 |\u001b[0m     args = parser.parse_args()\n\u001b[1m\u001b[94m565 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m566 |\u001b[0m     scanner = VulnerabilityScanner(args.project_root)\n\u001b[1m\u001b[94m567 |\u001b[0m     report_file = scanner.generate_report(args.output)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:568:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m566 |\u001b[0m     scanner = VulnerabilityScanner(args.project_root)\n\u001b[1m\u001b[94m567 |\u001b[0m     report_file = scanner.generate_report(args.output)\n\u001b[1m\u001b[94m568 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m569 |\u001b[0m     print(f\"Security scan completed. Report saved to: {report_file}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:570:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m569 |\u001b[0m     print(f\"Security scan completed. Report saved to: {report_file}\")\n\u001b[1m\u001b[94m570 |\u001b[0m     \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m571 |\u001b[0m     # Print summary\n\u001b[1m\u001b[94m572 |\u001b[0m     if scanner.security_metrics:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF541 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mf-string without any placeholders\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/security/vulnerability_scanner.py:573:15\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m571 |\u001b[0m     # Print summary\n\u001b[1m\u001b[94m572 |\u001b[0m     if scanner.security_metrics:\n\u001b[1m\u001b[94m573 |\u001b[0m         print(f\"\\nSecurity Summary:\")\n    \u001b[1m\u001b[94m|\u001b[0m               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m574 |\u001b[0m         print(f\"Total vulnerabilities: {scanner.security_metrics.total_vulnerabilities}\")\n\u001b[1m\u001b[94m575 |\u001b[0m         print(f\"High severity: {scanner.security_metrics.high_severity}\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove extraneous `f` prefix\u001b[0m\n\n\u001b[1m\u001b[91mF821 \u001b[0m\u001b[1mUndefined name `unittest`\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/system/test_improved_user_experience.py:261:5\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m if __name__ == \"__main__\":\n\u001b[1m\u001b[94m261 |\u001b[0m     unittest.main()\n    \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:172:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m170 |\u001b[0m         \"\"\"Test analyze_dependencies warning scenarios.\"\"\"\n\u001b[1m\u001b[94m171 |\u001b[0m         analyzer = DependencyAnalyzer()\n\u001b[1m\u001b[94m172 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m173 |\u001b[0m         # Test with logger to capture warnings\n\u001b[1m\u001b[94m174 |\u001b[0m         with patch.object(analyzer.logger, 'warning') as mock_warning:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `result` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:185:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m                 )\n\u001b[1m\u001b[94m184 |\u001b[0m             }\n\u001b[1m\u001b[94m185 |\u001b[0m             result = analyzer.analyze_dependencies(silver_steps=silver_steps)\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m             \n\u001b[1m\u001b[94m187 |\u001b[0m             # Check that warning was logged for missing bronze\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `result`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:186:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m             }\n\u001b[1m\u001b[94m185 |\u001b[0m             result = analyzer.analyze_dependencies(silver_steps=silver_steps)\n\u001b[1m\u001b[94m186 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m187 |\u001b[0m             # Check that warning was logged for missing bronze\n\u001b[1m\u001b[94m188 |\u001b[0m             mock_warning.assert_any_call(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:195:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m193 |\u001b[0m         \"\"\"Test analyze_dependencies with silver step depends_on warning.\"\"\"\n\u001b[1m\u001b[94m194 |\u001b[0m         analyzer = DependencyAnalyzer()\n\u001b[1m\u001b[94m195 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m196 |\u001b[0m         # Create a silver step with depends_on that references non-existent step\n\u001b[1m\u001b[94m197 |\u001b[0m         silver_step = SilverStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:206:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m         # Manually add depends_on attribute\n\u001b[1m\u001b[94m205 |\u001b[0m         silver_step.depends_on = [\"missing_dep\"]\n\u001b[1m\u001b[94m206 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m         with patch.object(analyzer.logger, 'warning') as mock_warning:\n\u001b[1m\u001b[94m208 |\u001b[0m             result = analyzer.analyze_dependencies(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `result` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:208:13\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m         with patch.object(analyzer.logger, 'warning') as mock_warning:\n\u001b[1m\u001b[94m208 |\u001b[0m             result = analyzer.analyze_dependencies(\n    \u001b[1m\u001b[94m|\u001b[0m             \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m209 |\u001b[0m                 bronze_steps={\"bronze1\": Mock()},\n\u001b[1m\u001b[94m210 |\u001b[0m                 silver_steps={\"silver1\": silver_step}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `result`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:212:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m                 silver_steps={\"silver1\": silver_step}\n\u001b[1m\u001b[94m211 |\u001b[0m             )\n\u001b[1m\u001b[94m212 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m213 |\u001b[0m             # Check that warning was logged for missing dependency\n\u001b[1m\u001b[94m214 |\u001b[0m             mock_warning.assert_any_call(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:377:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m375 |\u001b[0m         \"\"\"Test analyze_dependencies with cycle detection warning.\"\"\"\n\u001b[1m\u001b[94m376 |\u001b[0m         analyzer = DependencyAnalyzer()\n\u001b[1m\u001b[94m377 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m         # Mock the entire analyze_dependencies method to test warning scenarios\n\u001b[1m\u001b[94m379 |\u001b[0m         with patch.object(analyzer, '_build_dependency_graph') as mock_build_graph:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:385:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m383 |\u001b[0m             mock_graph.get_stats.return_value = {\"total_steps\": 2, \"average_dependencies\": 1}\n\u001b[1m\u001b[94m384 |\u001b[0m             mock_build_graph.return_value = mock_graph\n\u001b[1m\u001b[94m385 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m386 |\u001b[0m             # Mock _resolve_cycles to return the same graph\n\u001b[1m\u001b[94m387 |\u001b[0m             with patch.object(analyzer, '_resolve_cycles', return_value=mock_graph):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `result` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:393:29\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m                     with patch.object(analyzer, '_generate_recommendations', return_value=[]):\n\u001b[1m\u001b[94m392 |\u001b[0m                         with patch.object(analyzer.logger, 'warning') as mock_warning:\n\u001b[1m\u001b[94m393 |\u001b[0m                             result = analyzer.analyze_dependencies()\n    \u001b[1m\u001b[94m|\u001b[0m                             \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m394 |\u001b[0m                             \n\u001b[1m\u001b[94m395 |\u001b[0m                             # Check that warning was logged for cycles\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `result`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:394:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m392 |\u001b[0m                         with patch.object(analyzer.logger, 'warning') as mock_warning:\n\u001b[1m\u001b[94m393 |\u001b[0m                             result = analyzer.analyze_dependencies()\n\u001b[1m\u001b[94m394 |\u001b[0m                             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m395 |\u001b[0m                             # Check that warning was logged for cycles\n\u001b[1m\u001b[94m396 |\u001b[0m                             mock_warning.assert_any_call(\"Detected 1 circular dependencies\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:401:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m399 |\u001b[0m         \"\"\"Test analyze_dependencies with conflict detection warning.\"\"\"\n\u001b[1m\u001b[94m400 |\u001b[0m         analyzer = DependencyAnalyzer()\n\u001b[1m\u001b[94m401 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m402 |\u001b[0m         # Mock the entire analyze_dependencies method to test warning scenarios\n\u001b[1m\u001b[94m403 |\u001b[0m         with patch.object(analyzer, '_build_dependency_graph') as mock_build_graph:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:409:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m407 |\u001b[0m             mock_graph.get_stats.return_value = {\"total_steps\": 2, \"average_dependencies\": 1}\n\u001b[1m\u001b[94m408 |\u001b[0m             mock_build_graph.return_value = mock_graph\n\u001b[1m\u001b[94m409 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m410 |\u001b[0m             # Mock _detect_conflicts to return conflicts\n\u001b[1m\u001b[94m411 |\u001b[0m             with patch.object(analyzer, '_detect_conflicts', return_value=[\"test conflict\"]):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `result` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:415:25\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m413 |\u001b[0m                 with patch.object(analyzer, '_generate_recommendations', return_value=[]):\n\u001b[1m\u001b[94m414 |\u001b[0m                     with patch.object(analyzer.logger, 'warning') as mock_warning:\n\u001b[1m\u001b[94m415 |\u001b[0m                         result = analyzer.analyze_dependencies()\n    \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m416 |\u001b[0m                         \n\u001b[1m\u001b[94m417 |\u001b[0m             # Check that warning was logged for conflicts\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `result`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:416:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m414 |\u001b[0m                     with patch.object(analyzer.logger, 'warning') as mock_warning:\n\u001b[1m\u001b[94m415 |\u001b[0m                         result = analyzer.analyze_dependencies()\n\u001b[1m\u001b[94m416 |\u001b[0m                         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m417 |\u001b[0m             # Check that warning was logged for conflicts\n\u001b[1m\u001b[94m418 |\u001b[0m             mock_warning.assert_any_call(\"Detected 1 dependency conflicts\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:423:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m421 |\u001b[0m         \"\"\"Test analyze_dependencies with silver step having valid depends_on.\"\"\"\n\u001b[1m\u001b[94m422 |\u001b[0m         analyzer = DependencyAnalyzer()\n\u001b[1m\u001b[94m423 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m424 |\u001b[0m         # Create a silver step with valid depends_on\n\u001b[1m\u001b[94m425 |\u001b[0m         silver_step = SilverStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:434:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m432 |\u001b[0m         # Manually add depends_on attribute\n\u001b[1m\u001b[94m433 |\u001b[0m         silver_step.depends_on = [\"bronze2\"]\n\u001b[1m\u001b[94m434 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m435 |\u001b[0m         bronze_steps = {\n\u001b[1m\u001b[94m436 |\u001b[0m             \"bronze1\": Mock(),\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:439:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m437 |\u001b[0m             \"bronze2\": Mock(),\n\u001b[1m\u001b[94m438 |\u001b[0m         }\n\u001b[1m\u001b[94m439 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m440 |\u001b[0m         result = analyzer.analyze_dependencies(\n\u001b[1m\u001b[94m441 |\u001b[0m             bronze_steps=bronze_steps,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:444:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m442 |\u001b[0m             silver_steps={\"silver1\": silver_step}\n\u001b[1m\u001b[94m443 |\u001b[0m         )\n\u001b[1m\u001b[94m444 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m445 |\u001b[0m         # Check that the dependency was added\n\u001b[1m\u001b[94m446 |\u001b[0m         assert \"silver1\" in result.graph.nodes\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:472:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m470 |\u001b[0m                     seen_names.add(node_name)\n\u001b[1m\u001b[94m471 |\u001b[0m                 return conflicts\n\u001b[1m\u001b[94m472 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m473 |\u001b[0m             mock_detect_conflicts.side_effect = mock_detect_conflicts_impl\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_analyzer.py:474:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m473 |\u001b[0m             mock_detect_conflicts.side_effect = mock_detect_conflicts_impl\n\u001b[1m\u001b[94m474 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m475 |\u001b[0m             conflicts = analyzer._detect_conflicts(graph)\n\u001b[1m\u001b[94m476 |\u001b[0m             assert len(conflicts) > 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:5:1\n  \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m3 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m4 |\u001b[0m\n\u001b[1m\u001b[94m5 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import pytest\n\u001b[1m\u001b[94m6 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import patch\n\u001b[1m\u001b[94m7 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m8 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.dependencies.graph import DependencyGraph, StepNode, StepType\n  \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_____________________________________________________________________________^\u001b[0m\n  \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:17:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m         \"\"\"Test add_dependency with missing nodes.\"\"\"\n\u001b[1m\u001b[94m16 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m17 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m         # Add one node\n\u001b[1m\u001b[94m19 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:20:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m         # Add one node\n\u001b[1m\u001b[94m19 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n\u001b[1m\u001b[94m20 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m         # Try to add dependency with missing node\n\u001b[1m\u001b[94m22 |\u001b[0m         with pytest.raises(ValueError, match=\"Steps step1 or missing_step not found in graph\"):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:24:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m         with pytest.raises(ValueError, match=\"Steps step1 or missing_step not found in graph\"):\n\u001b[1m\u001b[94m23 |\u001b[0m             graph.add_dependency(\"step1\", \"missing_step\")\n\u001b[1m\u001b[94m24 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m         with pytest.raises(ValueError, match=\"Steps missing_step or step1 not found in graph\"):\n\u001b[1m\u001b[94m26 |\u001b[0m             graph.add_dependency(\"missing_step\", \"step1\")\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:31:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m29 |\u001b[0m         \"\"\"Test get_dependencies with missing node.\"\"\"\n\u001b[1m\u001b[94m30 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m31 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m32 |\u001b[0m         # Get dependencies for non-existent node\n\u001b[1m\u001b[94m33 |\u001b[0m         deps = graph.get_dependencies(\"missing_step\")\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:39:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m37 |\u001b[0m         \"\"\"Test get_dependents with missing node.\"\"\"\n\u001b[1m\u001b[94m38 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m39 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m40 |\u001b[0m         # Get dependents for non-existent node\n\u001b[1m\u001b[94m41 |\u001b[0m         deps = graph.get_dependents(\"missing_step\")\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:47:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m45 |\u001b[0m         \"\"\"Test detect_cycles method.\"\"\"\n\u001b[1m\u001b[94m46 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m47 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m48 |\u001b[0m         # Create a cycle: step1 -> step2 -> step1\n\u001b[1m\u001b[94m49 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:53:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m51 |\u001b[0m         graph.add_dependency(\"step1\", \"step2\")\n\u001b[1m\u001b[94m52 |\u001b[0m         graph.add_dependency(\"step2\", \"step1\")\n\u001b[1m\u001b[94m53 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m54 |\u001b[0m         # Test cycle detection\n\u001b[1m\u001b[94m55 |\u001b[0m         cycles = graph.detect_cycles()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:56:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m54 |\u001b[0m         # Test cycle detection\n\u001b[1m\u001b[94m55 |\u001b[0m         cycles = graph.detect_cycles()\n\u001b[1m\u001b[94m56 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m57 |\u001b[0m         assert len(cycles) > 0\n\u001b[1m\u001b[94m58 |\u001b[0m         assert any(\"step1\" in cycle and \"step2\" in cycle for cycle in cycles)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:63:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         \"\"\"Test get_execution_groups with missing dependency.\"\"\"\n\u001b[1m\u001b[94m62 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m63 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m         # Add nodes\n\u001b[1m\u001b[94m65 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:67:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m65 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n\u001b[1m\u001b[94m66 |\u001b[0m         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n\u001b[1m\u001b[94m67 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m68 |\u001b[0m         # Manually add a dependency to a missing node\n\u001b[1m\u001b[94m69 |\u001b[0m         graph.nodes[\"step2\"].dependencies.add(\"missing_step\")\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:70:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m68 |\u001b[0m         # Manually add a dependency to a missing node\n\u001b[1m\u001b[94m69 |\u001b[0m         graph.nodes[\"step2\"].dependencies.add(\"missing_step\")\n\u001b[1m\u001b[94m70 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m         with patch('sparkforge.dependencies.graph.logger') as mock_logger:\n\u001b[1m\u001b[94m72 |\u001b[0m             groups = graph.get_execution_groups()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:73:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m         with patch('sparkforge.dependencies.graph.logger') as mock_logger:\n\u001b[1m\u001b[94m72 |\u001b[0m             groups = graph.get_execution_groups()\n\u001b[1m\u001b[94m73 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m             # Check that warning was logged\n\u001b[1m\u001b[94m75 |\u001b[0m             mock_logger.warning.assert_any_call(\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:78:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m76 |\u001b[0m                 \"Dependency missing_step not found in levels for node step2\"\n\u001b[1m\u001b[94m77 |\u001b[0m             )\n\u001b[1m\u001b[94m78 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m             # Check that groups were still calculated\n\u001b[1m\u001b[94m80 |\u001b[0m             assert len(groups) > 0\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:85:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m         \"\"\"Test validate method with cycles.\"\"\"\n\u001b[1m\u001b[94m84 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m85 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m         # Create a cycle\n\u001b[1m\u001b[94m87 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:91:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m89 |\u001b[0m         graph.add_dependency(\"step1\", \"step2\")\n\u001b[1m\u001b[94m90 |\u001b[0m         graph.add_dependency(\"step2\", \"step1\")\n\u001b[1m\u001b[94m91 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m         issues = graph.validate()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:93:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m         issues = graph.validate()\n\u001b[1m\u001b[94m93 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m94 |\u001b[0m         assert len(issues) > 0\n\u001b[1m\u001b[94m95 |\u001b[0m         assert any(\"Circular dependency detected\" in issue for issue in issues)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:100:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 98 |\u001b[0m         \"\"\"Test validate method with missing dependencies.\"\"\"\n\u001b[1m\u001b[94m 99 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m100 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m101 |\u001b[0m         # Add node with missing dependency\n\u001b[1m\u001b[94m102 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:104:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m102 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n\u001b[1m\u001b[94m103 |\u001b[0m         graph.nodes[\"step1\"].dependencies.add(\"missing_step\")\n\u001b[1m\u001b[94m104 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m105 |\u001b[0m         issues = graph.validate()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:106:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m105 |\u001b[0m         issues = graph.validate()\n\u001b[1m\u001b[94m106 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m107 |\u001b[0m         assert len(issues) > 0\n\u001b[1m\u001b[94m108 |\u001b[0m         assert any(\"depends on missing node missing_step\" in issue for issue in issues)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:113:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m         \"\"\"Test get_execution_groups method.\"\"\"\n\u001b[1m\u001b[94m112 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m113 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m114 |\u001b[0m         # Add nodes\n\u001b[1m\u001b[94m115 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:118:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m116 |\u001b[0m         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n\u001b[1m\u001b[94m117 |\u001b[0m         graph.add_dependency(\"step1\", \"step2\")\n\u001b[1m\u001b[94m118 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m         groups = graph.get_execution_groups()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:120:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m         groups = graph.get_execution_groups()\n\u001b[1m\u001b[94m120 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m121 |\u001b[0m         assert len(groups) == 2\n\u001b[1m\u001b[94m122 |\u001b[0m         assert [\"step1\"] in groups\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:128:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m         \"\"\"Test get_stats method.\"\"\"\n\u001b[1m\u001b[94m127 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m128 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m129 |\u001b[0m         # Add nodes\n\u001b[1m\u001b[94m130 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:133:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m131 |\u001b[0m         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n\u001b[1m\u001b[94m132 |\u001b[0m         graph.add_dependency(\"step1\", \"step2\")\n\u001b[1m\u001b[94m133 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m134 |\u001b[0m         stats = graph.get_stats()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:135:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m134 |\u001b[0m         stats = graph.get_stats()\n\u001b[1m\u001b[94m135 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m136 |\u001b[0m         assert stats[\"total_nodes\"] == 2\n\u001b[1m\u001b[94m137 |\u001b[0m         assert stats[\"total_edges\"] == 1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:143:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m141 |\u001b[0m         \"\"\"Test get_parallel_candidates method.\"\"\"\n\u001b[1m\u001b[94m142 |\u001b[0m         graph = DependencyGraph()\n\u001b[1m\u001b[94m143 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m144 |\u001b[0m         # Add nodes\n\u001b[1m\u001b[94m145 |\u001b[0m         graph.add_node(StepNode(\"step1\", StepType.BRONZE, set()))\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:148:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         graph.add_node(StepNode(\"step2\", StepType.SILVER, set()))\n\u001b[1m\u001b[94m147 |\u001b[0m         graph.add_dependency(\"step1\", \"step2\")\n\u001b[1m\u001b[94m148 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m         candidates = graph.get_parallel_candidates()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/dependencies/test_graph.py:150:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m         candidates = graph.get_parallel_candidates()\n\u001b[1m\u001b[94m150 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m151 |\u001b[0m         assert len(candidates) == 2\n\u001b[1m\u001b[94m152 |\u001b[0m         assert [\"step1\"] in candidates\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF821 \u001b[0m\u001b[1mUndefined name `ErrorContextValue`\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_errors.py:35:28\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m33 |\u001b[0m         \"\"\"Test that ErrorContextValue accepts only valid types.\"\"\"\n\u001b[1m\u001b[94m34 |\u001b[0m         # Valid types\n\u001b[1m\u001b[94m35 |\u001b[0m         valid_values: List[ErrorContextValue] = [\n   \u001b[1m\u001b[94m|\u001b[0m                            \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m36 |\u001b[0m             \"string\",\n\u001b[1m\u001b[94m37 |\u001b[0m             42,\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_errors.py:96:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m94 |\u001b[0m         assert error.severity == ErrorSeverity.HIGH\n\u001b[1m\u001b[94m95 |\u001b[0m         assert error.context == {\"key\": \"value\"}\n\u001b[1m\u001b[94m96 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m97 |\u001b[0m         # Test __str__ method includes error_code\n\u001b[1m\u001b[94m98 |\u001b[0m         error_str = str(error)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_errors.py:390:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m388 |\u001b[0m             suggestions=[\"Check connection\", \"Verify permissions\"]\n\u001b[1m\u001b[94m389 |\u001b[0m         )\n\u001b[1m\u001b[94m390 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m391 |\u001b[0m         assert error.message == \"Resource not found\"\n\u001b[1m\u001b[94m392 |\u001b[0m         assert error.error_code == \"RESOURCE_001\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from typing import Any, Dict, List, Optional\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BaseModel,\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelinePhase,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineValidationError,\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverDependencyInfo,\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationError,\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ParallelConfig,\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.List` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:9:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Optional` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:9:37\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                                     \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.Mock` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:10:27\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Optional\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `unittest.mock.Mock`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.PipelinePhase` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:19:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m     PipelinePhase,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m     PipelineValidationError,\n\u001b[1m\u001b[94m21 |\u001b[0m     SilverDependencyInfo,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.ValidationError` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:23:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m     SilverDependencyInfo,\n\u001b[1m\u001b[94m22 |\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m23 |\u001b[0m     ValidationError,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m24 |\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m25 |\u001b[0m     ParallelConfig,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:66:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m         \"\"\"Test PipelineConfig property access.\"\"\"\n\u001b[1m\u001b[94m65 |\u001b[0m         config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m66 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m67 |\u001b[0m         # Test property access\n\u001b[1m\u001b[94m68 |\u001b[0m         assert isinstance(config.min_gold_rate, float)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:81:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m         \"\"\"Test get_threshold with invalid phase type.\"\"\"\n\u001b[1m\u001b[94m80 |\u001b[0m         thresholds = ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n\u001b[1m\u001b[94m81 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m82 |\u001b[0m         # This should handle invalid types gracefully\n\u001b[1m\u001b[94m83 |\u001b[0m         with pytest.raises(KeyError):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:94:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m             gold=100.0\n\u001b[1m\u001b[94m93 |\u001b[0m         )\n\u001b[1m\u001b[94m94 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m95 |\u001b[0m         assert thresholds.bronze == 0.0\n\u001b[1m\u001b[94m96 |\u001b[0m         assert thresholds.silver == 50.0\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:111:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m109 |\u001b[0m             timeout_secs=1\n\u001b[1m\u001b[94m110 |\u001b[0m         )\n\u001b[1m\u001b[94m111 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m112 |\u001b[0m         assert config.enabled is True\n\u001b[1m\u001b[94m113 |\u001b[0m         assert config.max_workers == 1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:124:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m122 |\u001b[0m             timeout_secs=3600\n\u001b[1m\u001b[94m123 |\u001b[0m         )\n\u001b[1m\u001b[94m124 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m125 |\u001b[0m         assert config.max_workers == 32\n\u001b[1m\u001b[94m126 |\u001b[0m         assert config.timeout_secs == 3600\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:140:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m138 |\u001b[0m             incremental_col=\"\",  # Empty string should be valid\n\u001b[1m\u001b[94m139 |\u001b[0m         )\n\u001b[1m\u001b[94m140 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m141 |\u001b[0m         assert step.incremental_col == \"\"\n\u001b[1m\u001b[94m142 |\u001b[0m         assert step.has_incremental_capability is True\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:151:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m             rules={\"id\": [\"not_null\"]},\n\u001b[1m\u001b[94m150 |\u001b[0m         )\n\u001b[1m\u001b[94m151 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m         # This should not raise an error\n\u001b[1m\u001b[94m153 |\u001b[0m         step.validate()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:164:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m162 |\u001b[0m         def mock_transform(spark, df, bronze_dfs):\n\u001b[1m\u001b[94m163 |\u001b[0m             return df\n\u001b[1m\u001b[94m164 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m165 |\u001b[0m         step = SilverStep(\n\u001b[1m\u001b[94m166 |\u001b[0m             name=\"test_step\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:172:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m170 |\u001b[0m             table_name=\"test_table\"\n\u001b[1m\u001b[94m171 |\u001b[0m         )\n\u001b[1m\u001b[94m172 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m173 |\u001b[0m         # This should not raise an error\n\u001b[1m\u001b[94m174 |\u001b[0m         step.validate()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:185:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m         def mock_transform(spark, silver_dfs):\n\u001b[1m\u001b[94m184 |\u001b[0m             return list(silver_dfs.values())[0] if silver_dfs else None\n\u001b[1m\u001b[94m185 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         step = GoldStep(\n\u001b[1m\u001b[94m187 |\u001b[0m             name=\"test_step\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:193:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m191 |\u001b[0m             table_name=\"test_table\"\n\u001b[1m\u001b[94m192 |\u001b[0m         )\n\u001b[1m\u001b[94m193 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m194 |\u001b[0m         # This should not raise an error\n\u001b[1m\u001b[94m195 |\u001b[0m         step.validate()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:210:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m208 |\u001b[0m             execution_group=1\n\u001b[1m\u001b[94m209 |\u001b[0m         )\n\u001b[1m\u001b[94m210 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m211 |\u001b[0m         # This should not raise an error\n\u001b[1m\u001b[94m212 |\u001b[0m         info.validate()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:225:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m                 self.name = name\n\u001b[1m\u001b[94m224 |\u001b[0m                 self.value = value\n\u001b[1m\u001b[94m225 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m226 |\u001b[0m             def validate(self) -> None:\n\u001b[1m\u001b[94m227 |\u001b[0m                 pass\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:228:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m226 |\u001b[0m             def validate(self) -> None:\n\u001b[1m\u001b[94m227 |\u001b[0m                 pass\n\u001b[1m\u001b[94m228 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m229 |\u001b[0m         model = TestModel(\"test\", {\"nested\": {\"value\": 123}})\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:230:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m229 |\u001b[0m         model = TestModel(\"test\", {\"nested\": {\"value\": 123}})\n\u001b[1m\u001b[94m230 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m231 |\u001b[0m         # Test serialization\n\u001b[1m\u001b[94m232 |\u001b[0m         result_dict = model.to_dict()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:234:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m232 |\u001b[0m         result_dict = model.to_dict()\n\u001b[1m\u001b[94m233 |\u001b[0m         assert isinstance(result_dict, dict)\n\u001b[1m\u001b[94m234 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m         result_json = model.to_json()\n\u001b[1m\u001b[94m236 |\u001b[0m         assert isinstance(result_json, str)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_additional_coverage.py:237:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m         result_json = model.to_json()\n\u001b[1m\u001b[94m236 |\u001b[0m         assert isinstance(result_json, str)\n\u001b[1m\u001b[94m237 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m238 |\u001b[0m         # Test string representation\n\u001b[1m\u001b[94m239 |\u001b[0m         str_repr = str(model)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from typing import Any, Dict, List\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.errors import PipelineValidationError\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BaseModel,\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ModelValue,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ParallelConfig,\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverDependencyInfo,\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     Validatable,\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     Serializable,\n\u001b[1m\u001b[94m27 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Any` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:9:20\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                    \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.List` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:9:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.Mock` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:10:27\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `unittest.mock.Mock`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.BaseModel` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:16:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.errors import PipelineValidationError\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m     BaseModel,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m18 |\u001b[0m     GoldStep,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.GoldStep` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:18:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m     BaseModel,\n\u001b[1m\u001b[94m17 |\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m18 |\u001b[0m     GoldStep,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m     ModelValue,\n\u001b[1m\u001b[94m20 |\u001b[0m     ParallelConfig,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.SilverDependencyInfo` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:22:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m     ParallelConfig,\n\u001b[1m\u001b[94m21 |\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m22 |\u001b[0m     SilverDependencyInfo,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m23 |\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m24 |\u001b[0m     ValidationThresholds,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:37:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m35 |\u001b[0m         # Test that Validatable protocol is properly defined\n\u001b[1m\u001b[94m36 |\u001b[0m         assert hasattr(Validatable, 'validate')\n\u001b[1m\u001b[94m37 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m38 |\u001b[0m         # Test implementing the protocol\n\u001b[1m\u001b[94m39 |\u001b[0m         class MockValidatable:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:42:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m40 |\u001b[0m             def validate(self) -> None:\n\u001b[1m\u001b[94m41 |\u001b[0m                 pass\n\u001b[1m\u001b[94m42 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m43 |\u001b[0m         # Should be able to use as Validatable\n\u001b[1m\u001b[94m44 |\u001b[0m         validator: Validatable = MockValidatable()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:52:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m50 |\u001b[0m         assert hasattr(Serializable, 'to_dict')\n\u001b[1m\u001b[94m51 |\u001b[0m         assert hasattr(Serializable, 'to_json')\n\u001b[1m\u001b[94m52 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m53 |\u001b[0m         # Test implementing the protocol\n\u001b[1m\u001b[94m54 |\u001b[0m         class MockSerializable:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:57:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m55 |\u001b[0m             def to_dict(self) -> Dict[str, ModelValue]:\n\u001b[1m\u001b[94m56 |\u001b[0m                 return {\"test\": \"value\"}\n\u001b[1m\u001b[94m57 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m             def to_json(self) -> str:\n\u001b[1m\u001b[94m59 |\u001b[0m                 return '{\"test\": \"value\"}'\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:60:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m             def to_json(self) -> str:\n\u001b[1m\u001b[94m59 |\u001b[0m                 return '{\"test\": \"value\"}'\n\u001b[1m\u001b[94m60 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         # Should be able to use as Serializable\n\u001b[1m\u001b[94m62 |\u001b[0m         serializer: Serializable = MockSerializable()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:76:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m         with pytest.raises(PipelineValidationError, match=\"Schema name must be a non-empty string\"):\n\u001b[1m\u001b[94m75 |\u001b[0m             config.validate()\n\u001b[1m\u001b[94m76 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m         # Test with None schema\n\u001b[1m\u001b[94m78 |\u001b[0m         config = PipelineConfig(\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:85:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m         with pytest.raises(PipelineValidationError, match=\"Schema name must be a non-empty string\"):\n\u001b[1m\u001b[94m84 |\u001b[0m             config.validate()\n\u001b[1m\u001b[94m85 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m         # Test with non-string schema\n\u001b[1m\u001b[94m87 |\u001b[0m         config = PipelineConfig(\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:104:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m102 |\u001b[0m                 incremental_col=\"updated_at\"\n\u001b[1m\u001b[94m103 |\u001b[0m             )\n\u001b[1m\u001b[94m104 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m105 |\u001b[0m         # Test with None name\n\u001b[1m\u001b[94m106 |\u001b[0m         with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:112:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m110 |\u001b[0m                 incremental_col=\"updated_at\"\n\u001b[1m\u001b[94m111 |\u001b[0m             )\n\u001b[1m\u001b[94m112 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m113 |\u001b[0m         # Test with non-string name\n\u001b[1m\u001b[94m114 |\u001b[0m         with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:120:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m118 |\u001b[0m                 incremental_col=\"updated_at\"\n\u001b[1m\u001b[94m119 |\u001b[0m             )\n\u001b[1m\u001b[94m120 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m121 |\u001b[0m         # Test with non-dict rules\n\u001b[1m\u001b[94m122 |\u001b[0m         with pytest.raises(PipelineValidationError, match=\"Rules must be a non-empty dictionary\"):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:128:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m                 incremental_col=\"updated_at\"\n\u001b[1m\u001b[94m127 |\u001b[0m             )\n\u001b[1m\u001b[94m128 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m129 |\u001b[0m         # Test with non-string incremental_col\n\u001b[1m\u001b[94m130 |\u001b[0m         with pytest.raises(PipelineValidationError, match=\"Incremental column must be a string\"):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:146:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m144 |\u001b[0m         )\n\u001b[1m\u001b[94m145 |\u001b[0m         assert step.has_incremental_capability is True\n\u001b[1m\u001b[94m146 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m         # Test without incremental column\n\u001b[1m\u001b[94m148 |\u001b[0m         step = BronzeStep(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:159:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m         def mock_transform(spark, bronze_df, prior_silvers):\n\u001b[1m\u001b[94m158 |\u001b[0m             return bronze_df\n\u001b[1m\u001b[94m159 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         # Test with empty name\n\u001b[1m\u001b[94m161 |\u001b[0m         with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:169:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m167 |\u001b[0m                 table_name=\"silver_table\"\n\u001b[1m\u001b[94m168 |\u001b[0m             )\n\u001b[1m\u001b[94m169 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m170 |\u001b[0m         # Test with non-string name\n\u001b[1m\u001b[94m171 |\u001b[0m         with pytest.raises(PipelineValidationError, match=\"Step name must be a non-empty string\"):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:179:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m                 table_name=\"silver_table\"\n\u001b[1m\u001b[94m178 |\u001b[0m             )\n\u001b[1m\u001b[94m179 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m180 |\u001b[0m\n\u001b[1m\u001b[94m181 |\u001b[0m     def test_validation_thresholds_boundary_values(self) -> None:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:194:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m192 |\u001b[0m         parallel = ParallelConfig(enabled=True, max_workers=1)\n\u001b[1m\u001b[94m193 |\u001b[0m         assert parallel.max_workers == 1\n\u001b[1m\u001b[94m194 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m195 |\u001b[0m         # Test with large max_workers\n\u001b[1m\u001b[94m196 |\u001b[0m         parallel = ParallelConfig(enabled=True, max_workers=1000)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:206:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m         assert thresholds.silver == 99.5\n\u001b[1m\u001b[94m205 |\u001b[0m         assert thresholds.gold == 99.9\n\u001b[1m\u001b[94m206 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m         # Test ValidationThresholds.create_permissive with actual values (if it exists)\n\u001b[1m\u001b[94m208 |\u001b[0m         if hasattr(ValidationThresholds, 'create_permissive'):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:213:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m211 |\u001b[0m             assert thresholds.silver == 75.0\n\u001b[1m\u001b[94m212 |\u001b[0m             assert thresholds.gold == 80.0\n\u001b[1m\u001b[94m213 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m214 |\u001b[0m         # Test ParallelConfig factory methods if they exist\n\u001b[1m\u001b[94m215 |\u001b[0m         if hasattr(ParallelConfig, 'create_conservative'):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:219:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m217 |\u001b[0m             assert parallel.enabled is True\n\u001b[1m\u001b[94m218 |\u001b[0m             assert parallel.max_workers == 2\n\u001b[1m\u001b[94m219 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m220 |\u001b[0m         if hasattr(ParallelConfig, 'create_aggressive'):\n\u001b[1m\u001b[94m221 |\u001b[0m             parallel = ParallelConfig.create_aggressive()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:239:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m237 |\u001b[0m         thresholds2 = ValidationThresholds(bronze=80.0, silver=85.0, gold=90.0)\n\u001b[1m\u001b[94m238 |\u001b[0m         thresholds3 = ValidationThresholds(bronze=75.0, silver=85.0, gold=90.0)\n\u001b[1m\u001b[94m239 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m240 |\u001b[0m         assert thresholds1 == thresholds2\n\u001b[1m\u001b[94m241 |\u001b[0m         assert thresholds1 != thresholds3\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:242:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m240 |\u001b[0m         assert thresholds1 == thresholds2\n\u001b[1m\u001b[94m241 |\u001b[0m         assert thresholds1 != thresholds3\n\u001b[1m\u001b[94m242 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m243 |\u001b[0m         # Test ParallelConfig comparison\n\u001b[1m\u001b[94m244 |\u001b[0m         parallel1 = ParallelConfig(enabled=True, max_workers=4)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:247:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m245 |\u001b[0m         parallel2 = ParallelConfig(enabled=True, max_workers=4)\n\u001b[1m\u001b[94m246 |\u001b[0m         parallel3 = ParallelConfig(enabled=False, max_workers=4)\n\u001b[1m\u001b[94m247 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m248 |\u001b[0m         assert parallel1 == parallel2\n\u001b[1m\u001b[94m249 |\u001b[0m         assert parallel1 != parallel3\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:259:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         assert \"silver=85.0\" in str_repr\n\u001b[1m\u001b[94m258 |\u001b[0m         assert \"gold=90.0\" in str_repr\n\u001b[1m\u001b[94m259 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m         # Test ParallelConfig string representation\n\u001b[1m\u001b[94m261 |\u001b[0m         parallel = ParallelConfig(enabled=True, max_workers=4)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:278:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m276 |\u001b[0m             # Should catch the first validation error\n\u001b[1m\u001b[94m277 |\u001b[0m             assert \"Step name must be a non-empty string\" in str(e)\n\u001b[1m\u001b[94m278 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m279 |\u001b[0m         # Test ValidationThresholds with multiple invalid values\n\u001b[1m\u001b[94m280 |\u001b[0m         try:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:293:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m291 |\u001b[0m         assert thresholds.silver == 50.0\n\u001b[1m\u001b[94m292 |\u001b[0m         assert thresholds.gold == 100.0\n\u001b[1m\u001b[94m293 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m294 |\u001b[0m         # Test ParallelConfig boundary values\n\u001b[1m\u001b[94m295 |\u001b[0m         parallel = ParallelConfig(enabled=True, max_workers=1)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_final_coverage.py:297:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m295 |\u001b[0m         parallel = ParallelConfig(enabled=True, max_workers=1)\n\u001b[1m\u001b[94m296 |\u001b[0m         assert parallel.max_workers == 1\n\u001b[1m\u001b[94m297 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m298 |\u001b[0m         # Test with very large max_workers\n\u001b[1m\u001b[94m299 |\u001b[0m         parallel = ParallelConfig(enabled=True, max_workers=1000)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mE402 \u001b[0m\u001b[1mModule level import not at top of file\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_new.py:829:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m828 |\u001b[0m # Import dataclass for test models\n\u001b[1m\u001b[94m829 |\u001b[0m from dataclasses import dataclass\n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mF811 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mRedefinition of unused `dataclass` from line 9\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_new.py:829:25\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m828 |\u001b[0m # Import dataclass for test models\n\u001b[1m\u001b[94m829 |\u001b[0m from dataclasses import dataclass\n    \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m \u001b[1m\u001b[91m`dataclass` redefined here\u001b[0m\n    \u001b[1m\u001b[94m|\u001b[0m\n   \u001b[1m\u001b[94m:::\u001b[0m tests/unit/test_models_new.py:9:25\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m  8 |\u001b[0m import json\n\u001b[1m\u001b[94m  9 |\u001b[0m from dataclasses import dataclass\n    \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[33m---------\u001b[0m \u001b[1m\u001b[33mprevious definition of `dataclass` here\u001b[0m\n\u001b[1m\u001b[94m 10 |\u001b[0m\n\u001b[1m\u001b[94m 11 |\u001b[0m import pytest\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove definition: `dataclass`\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from typing import Any, Dict, List, Union, Optional\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from hypothesis import given, strategies as st, settings, example\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.errors import PipelineValidationError\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ParallelConfig,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Any` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:9:20\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Union, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                    \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Dict` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:9:25\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Union, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Union` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:9:37\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Union, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                                     \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.Mock` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:10:27\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Union, Optional\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m                           \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `unittest.mock.Mock`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pytest` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:12:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m from hypothesis import given, strategies as st, settings, example\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `pytest`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`hypothesis.example` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:13:59\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m from hypothesis import given, strategies as st, settings, example\n   \u001b[1m\u001b[94m|\u001b[0m                                                           \u001b[1m\u001b[91m^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.errors import PipelineValidationError\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `hypothesis.example`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.errors.PipelineValidationError` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:15:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m from hypothesis import given, strategies as st, settings, example\n\u001b[1m\u001b[94m14 |\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.errors import PipelineValidationError\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m17 |\u001b[0m     BronzeStep,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `sparkforge.errors.PipelineValidationError`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:38:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m36 |\u001b[0m         \"\"\"Test ValidationThresholds with generated float values.\"\"\"\n\u001b[1m\u001b[94m37 |\u001b[0m         thresholds = ValidationThresholds(bronze=bronze, silver=silver, gold=gold)\n\u001b[1m\u001b[94m38 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m39 |\u001b[0m         # Property: All threshold values should be preserved\n\u001b[1m\u001b[94m40 |\u001b[0m         assert thresholds.bronze == bronze\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:43:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m41 |\u001b[0m         assert thresholds.silver == silver\n\u001b[1m\u001b[94m42 |\u001b[0m         assert thresholds.gold == gold\n\u001b[1m\u001b[94m43 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m44 |\u001b[0m         # Property: Thresholds should be accessible via get_threshold\n\u001b[1m\u001b[94m45 |\u001b[0m         from sparkforge.models import PipelinePhase\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:58:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m56 |\u001b[0m         \"\"\"Test ParallelConfig with generated boolean and integer values.\"\"\"\n\u001b[1m\u001b[94m57 |\u001b[0m         parallel = ParallelConfig(enabled=enabled, max_workers=max_workers)\n\u001b[1m\u001b[94m58 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m59 |\u001b[0m         # Property: All config values should be preserved\n\u001b[1m\u001b[94m60 |\u001b[0m         assert parallel.enabled == enabled\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:73:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m         if all(c.isalnum() or c in '_-' for c in name) and all(c.isalnum() or c in '_-' for c in schema):\n\u001b[1m\u001b[94m72 |\u001b[0m             config = PipelineConfig.create_default(schema)\n\u001b[1m\u001b[94m73 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m             # Property: Schema should be preserved\n\u001b[1m\u001b[94m75 |\u001b[0m             assert config.schema == schema\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:76:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m             # Property: Schema should be preserved\n\u001b[1m\u001b[94m75 |\u001b[0m             assert config.schema == schema\n\u001b[1m\u001b[94m76 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m             # Property: Config should have default thresholds and parallel settings\n\u001b[1m\u001b[94m78 |\u001b[0m             assert isinstance(config.thresholds, ValidationThresholds)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:91:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m89 |\u001b[0m         if all(c.isalnum() or c in '_-' for c in name):\n\u001b[1m\u001b[94m90 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"], \"col2\": [\"col2 IS NOT NULL\"]}\n\u001b[1m\u001b[94m91 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m             step = BronzeStep(\n\u001b[1m\u001b[94m93 |\u001b[0m                 name=name,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:97:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m95 |\u001b[0m                 incremental_col=incremental_col\n\u001b[1m\u001b[94m96 |\u001b[0m             )\n\u001b[1m\u001b[94m97 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m98 |\u001b[0m             # Property: All values should be preserved\n\u001b[1m\u001b[94m99 |\u001b[0m             assert step.name == name\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:102:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m100 |\u001b[0m             assert step.rules == rules\n\u001b[1m\u001b[94m101 |\u001b[0m             assert step.incremental_col == incremental_col\n\u001b[1m\u001b[94m102 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m103 |\u001b[0m             # Property: Incremental capability should match incremental_col\n\u001b[1m\u001b[94m104 |\u001b[0m             assert step.has_incremental_capability == (incremental_col is not None)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:118:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m116 |\u001b[0m             all(c.isalnum() or c in '_-' for c in source_bronze) and\n\u001b[1m\u001b[94m117 |\u001b[0m             all(c.isalnum() or c in '_-' for c in table_name)):\n\u001b[1m\u001b[94m118 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m             def mock_transform(spark, bronze_df, prior_silvers):\n\u001b[1m\u001b[94m120 |\u001b[0m                 return bronze_df\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:121:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m119 |\u001b[0m             def mock_transform(spark, bronze_df, prior_silvers):\n\u001b[1m\u001b[94m120 |\u001b[0m                 return bronze_df\n\u001b[1m\u001b[94m121 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m122 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:123:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m122 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m123 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m             step = SilverStep(\n\u001b[1m\u001b[94m125 |\u001b[0m                 name=name,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:131:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m129 |\u001b[0m                 table_name=table_name\n\u001b[1m\u001b[94m130 |\u001b[0m             )\n\u001b[1m\u001b[94m131 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m132 |\u001b[0m             # Property: All values should be preserved\n\u001b[1m\u001b[94m133 |\u001b[0m             assert step.name == name\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:148:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         if (all(c.isalnum() or c in '_-' for c in name) and\n\u001b[1m\u001b[94m147 |\u001b[0m             all(c.isalnum() or c in '_-' for c in table_name)):\n\u001b[1m\u001b[94m148 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m             def mock_transform(spark, silver_dfs):\n\u001b[1m\u001b[94m150 |\u001b[0m                 return silver_dfs\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:151:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m149 |\u001b[0m             def mock_transform(spark, silver_dfs):\n\u001b[1m\u001b[94m150 |\u001b[0m                 return silver_dfs\n\u001b[1m\u001b[94m151 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m153 |\u001b[0m             source_silvers = [\"silver1\", \"silver2\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:154:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m153 |\u001b[0m             source_silvers = [\"silver1\", \"silver2\"]\n\u001b[1m\u001b[94m154 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m155 |\u001b[0m             step = GoldStep(\n\u001b[1m\u001b[94m156 |\u001b[0m                 name=name,\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:162:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m                 source_silvers=source_silvers\n\u001b[1m\u001b[94m161 |\u001b[0m             )\n\u001b[1m\u001b[94m162 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m163 |\u001b[0m             # Property: All values should be preserved\n\u001b[1m\u001b[94m164 |\u001b[0m             assert step.name == name\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:186:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m         thresholds1 = ValidationThresholds(bronze=bronze1, silver=silver1, gold=gold1)\n\u001b[1m\u001b[94m185 |\u001b[0m         thresholds2 = ValidationThresholds(bronze=bronze2, silver=silver2, gold=gold2)\n\u001b[1m\u001b[94m186 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m187 |\u001b[0m         # Property: Equality should be based on all threshold values\n\u001b[1m\u001b[94m188 |\u001b[0m         expected_equal = (bronze1 == bronze2 and silver1 == silver2 and gold1 == gold2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:190:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m188 |\u001b[0m         expected_equal = (bronze1 == bronze2 and silver1 == silver2 and gold1 == gold2)\n\u001b[1m\u001b[94m189 |\u001b[0m         assert (thresholds1 == thresholds2) == expected_equal\n\u001b[1m\u001b[94m190 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m191 |\u001b[0m         # Property: Self-equality should always be true\n\u001b[1m\u001b[94m192 |\u001b[0m         assert thresholds1 == thresholds1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:208:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m206 |\u001b[0m         parallel1 = ParallelConfig(enabled=enabled1, max_workers=max_workers1)\n\u001b[1m\u001b[94m207 |\u001b[0m         parallel2 = ParallelConfig(enabled=enabled2, max_workers=max_workers2)\n\u001b[1m\u001b[94m208 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m209 |\u001b[0m         # Property: Equality should be based on both enabled and max_workers\n\u001b[1m\u001b[94m210 |\u001b[0m         expected_equal = (enabled1 == enabled2 and max_workers1 == max_workers2)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:212:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m         expected_equal = (enabled1 == enabled2 and max_workers1 == max_workers2)\n\u001b[1m\u001b[94m211 |\u001b[0m         assert (parallel1 == parallel2) == expected_equal\n\u001b[1m\u001b[94m212 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m213 |\u001b[0m         # Property: Self-equality should always be true\n\u001b[1m\u001b[94m214 |\u001b[0m         assert parallel1 == parallel1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:231:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m229 |\u001b[0m                 col_name = f\"col{i}\"\n\u001b[1m\u001b[94m230 |\u001b[0m                 rules[col_name] = [f\"{col_name} > 0\", f\"{col_name} IS NOT NULL\"]\n\u001b[1m\u001b[94m231 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m232 |\u001b[0m             step = BronzeStep(name=name, rules=rules, incremental_col=None)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:233:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m232 |\u001b[0m             step = BronzeStep(name=name, rules=rules, incremental_col=None)\n\u001b[1m\u001b[94m233 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m234 |\u001b[0m             # Property: Rules should be preserved exactly\n\u001b[1m\u001b[94m235 |\u001b[0m             assert step.rules == rules\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:237:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m             assert step.rules == rules\n\u001b[1m\u001b[94m236 |\u001b[0m             assert len(step.rules) == rules_size\n\u001b[1m\u001b[94m237 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m238 |\u001b[0m             # Property: All rule keys should be accessible\n\u001b[1m\u001b[94m239 |\u001b[0m             for col_name in rules:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:254:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m252 |\u001b[0m         \"\"\"Test ValidationThresholds factory methods with generated values.\"\"\"\n\u001b[1m\u001b[94m253 |\u001b[0m         bronze, silver, gold = threshold_values\n\u001b[1m\u001b[94m254 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m255 |\u001b[0m         # Test create_strict method\n\u001b[1m\u001b[94m256 |\u001b[0m         strict_thresholds = ValidationThresholds.create_strict()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:257:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m255 |\u001b[0m         # Test create_strict method\n\u001b[1m\u001b[94m256 |\u001b[0m         strict_thresholds = ValidationThresholds.create_strict()\n\u001b[1m\u001b[94m257 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m258 |\u001b[0m         # Property: Strict thresholds should all be high values\n\u001b[1m\u001b[94m259 |\u001b[0m         assert strict_thresholds.bronze >= 95.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_models_property_based.py:262:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m         assert strict_thresholds.silver >= 95.0\n\u001b[1m\u001b[94m261 |\u001b[0m         assert strict_thresholds.gold >= 95.0\n\u001b[1m\u001b[94m262 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m         # Property: All strict thresholds should be <= 100.0\n\u001b[1m\u001b[94m264 |\u001b[0m         assert strict_thresholds.bronze <= 100.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mB017 \u001b[0m\u001b[1mDo not assert blind exception: `Exception`\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_performance.py:225:14\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m         mock_df = MagicMock()\n\u001b[1m\u001b[94m224 |\u001b[0m\n\u001b[1m\u001b[94m225 |\u001b[0m         with pytest.raises(Exception):\n    \u001b[1m\u001b[94m|\u001b[0m              \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m226 |\u001b[0m             time_write_operation(\"overwrite\", mock_df, \"test.table\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_table_operations.py:8:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 6 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 7 |\u001b[0m\n\u001b[1m\u001b[94m 8 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import pytest\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import patch\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.errors import TableOperationError\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.table_operations import (\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     drop_table,\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     fqn,\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     read_table,\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     table_exists,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     write_append_table,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     write_overwrite_table,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m   # Using shared spark_session fixture from conftest.py\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_table_operations.py:196:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m194 |\u001b[0m         with patch.object(spark_session, 'table') as mock_table:\n\u001b[1m\u001b[94m195 |\u001b[0m             mock_table.side_effect = Exception(\"General error\")\n\u001b[1m\u001b[94m196 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m             with pytest.raises(TableOperationError, match=\"Failed to read table\"):\n\u001b[1m\u001b[94m198 |\u001b[0m                 read_table(spark_session, \"test_schema.test_table\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_table_operations.py:258:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m256 |\u001b[0m         with patch.object(spark_session, 'table') as mock_table:\n\u001b[1m\u001b[94m257 |\u001b[0m             mock_table.side_effect = Exception(\"General error\")\n\u001b[1m\u001b[94m258 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m259 |\u001b[0m             with patch('sparkforge.table_operations.logger') as mock_logger:\n\u001b[1m\u001b[94m260 |\u001b[0m                 result = table_exists(spark_session, \"test_schema.test_table\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_table_operations.py:262:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m260 |\u001b[0m                 result = table_exists(spark_session, \"test_schema.test_table\")\n\u001b[1m\u001b[94m261 |\u001b[0m                 assert result is False\n\u001b[1m\u001b[94m262 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m                 # Verify that warning was logged\n\u001b[1m\u001b[94m264 |\u001b[0m                 mock_logger.warning.assert_called_once()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_table_operations.py:309:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m307 |\u001b[0m             with patch.object(spark_session, 'sql') as mock_sql:\n\u001b[1m\u001b[94m308 |\u001b[0m                 mock_sql.side_effect = Exception(\"General error\")\n\u001b[1m\u001b[94m309 |\u001b[0m                 \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m310 |\u001b[0m                 result = drop_table(spark_session, \"test_schema.test_table\")\n\u001b[1m\u001b[94m311 |\u001b[0m                 assert result is False\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from typing import Any, Dict, List, Union\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.types import (\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BronzeTransformFunction,\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ColumnRules,\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     Duration,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ErrorCode,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ErrorContext,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ErrorSuggestions,\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ExecutionConfig,\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ExecutionContext,\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ExecutionId,\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ExecutionResult,\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     FilterFunction,\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     GenericDict,\n\u001b[1m\u001b[94m27 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     GoldTransformFunction,\n\u001b[1m\u001b[94m28 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     MonitoringConfig,\n\u001b[1m\u001b[94m29 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     NumericDict,\n\u001b[1m\u001b[94m30 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     OptionalDict,\n\u001b[1m\u001b[94m31 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     OptionalList,\n\u001b[1m\u001b[94m32 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m33 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineId,\n\u001b[1m\u001b[94m34 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineMode,\n\u001b[1m\u001b[94m35 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineResult,\n\u001b[1m\u001b[94m36 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     QualityRate,\n\u001b[1m\u001b[94m37 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     QualityThresholds,\n\u001b[1m\u001b[94m38 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     RowCount,\n\u001b[1m\u001b[94m39 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SchemaName,\n\u001b[1m\u001b[94m40 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     Serializable,\n\u001b[1m\u001b[94m41 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverTransformFunction,\n\u001b[1m\u001b[94m42 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     StepContext,\n\u001b[1m\u001b[94m43 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     StepName,\n\u001b[1m\u001b[94m44 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     StepResult,\n\u001b[1m\u001b[94m45 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     StepStatus,\n\u001b[1m\u001b[94m46 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     StepType,\n\u001b[1m\u001b[94m47 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     StringDict,\n\u001b[1m\u001b[94m48 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     TableName,\n\u001b[1m\u001b[94m49 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     TransformFunction,\n\u001b[1m\u001b[94m50 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationConfig,\n\u001b[1m\u001b[94m51 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationResult,\n\u001b[1m\u001b[94m52 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     Validatable,\n\u001b[1m\u001b[94m53 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pytest` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:12:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.types import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `pytest`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.BronzeTransformFunction` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:15:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.types import (\n\u001b[1m\u001b[94m15 |\u001b[0m     BronzeTransformFunction,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m     ColumnRules,\n\u001b[1m\u001b[94m17 |\u001b[0m     Duration,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.ColumnRules` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:16:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.types import (\n\u001b[1m\u001b[94m15 |\u001b[0m     BronzeTransformFunction,\n\u001b[1m\u001b[94m16 |\u001b[0m     ColumnRules,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m     Duration,\n\u001b[1m\u001b[94m18 |\u001b[0m     ErrorCode,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.ExecutionConfig` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:21:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m     ErrorContext,\n\u001b[1m\u001b[94m20 |\u001b[0m     ErrorSuggestions,\n\u001b[1m\u001b[94m21 |\u001b[0m     ExecutionConfig,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m     ExecutionContext,\n\u001b[1m\u001b[94m23 |\u001b[0m     ExecutionId,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.ExecutionContext` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:22:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m     ErrorSuggestions,\n\u001b[1m\u001b[94m21 |\u001b[0m     ExecutionConfig,\n\u001b[1m\u001b[94m22 |\u001b[0m     ExecutionContext,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m23 |\u001b[0m     ExecutionId,\n\u001b[1m\u001b[94m24 |\u001b[0m     ExecutionResult,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.ExecutionResult` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:24:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m     ExecutionContext,\n\u001b[1m\u001b[94m23 |\u001b[0m     ExecutionId,\n\u001b[1m\u001b[94m24 |\u001b[0m     ExecutionResult,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m     FilterFunction,\n\u001b[1m\u001b[94m26 |\u001b[0m     GenericDict,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.FilterFunction` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:25:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m23 |\u001b[0m     ExecutionId,\n\u001b[1m\u001b[94m24 |\u001b[0m     ExecutionResult,\n\u001b[1m\u001b[94m25 |\u001b[0m     FilterFunction,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m26 |\u001b[0m     GenericDict,\n\u001b[1m\u001b[94m27 |\u001b[0m     GoldTransformFunction,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.GoldTransformFunction` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:27:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m     FilterFunction,\n\u001b[1m\u001b[94m26 |\u001b[0m     GenericDict,\n\u001b[1m\u001b[94m27 |\u001b[0m     GoldTransformFunction,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m28 |\u001b[0m     MonitoringConfig,\n\u001b[1m\u001b[94m29 |\u001b[0m     NumericDict,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.MonitoringConfig` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:28:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m26 |\u001b[0m     GenericDict,\n\u001b[1m\u001b[94m27 |\u001b[0m     GoldTransformFunction,\n\u001b[1m\u001b[94m28 |\u001b[0m     MonitoringConfig,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m29 |\u001b[0m     NumericDict,\n\u001b[1m\u001b[94m30 |\u001b[0m     OptionalDict,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.PipelineResult` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:35:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m33 |\u001b[0m     PipelineId,\n\u001b[1m\u001b[94m34 |\u001b[0m     PipelineMode,\n\u001b[1m\u001b[94m35 |\u001b[0m     PipelineResult,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m36 |\u001b[0m     QualityRate,\n\u001b[1m\u001b[94m37 |\u001b[0m     QualityThresholds,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.QualityThresholds` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:37:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m35 |\u001b[0m     PipelineResult,\n\u001b[1m\u001b[94m36 |\u001b[0m     QualityRate,\n\u001b[1m\u001b[94m37 |\u001b[0m     QualityThresholds,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m38 |\u001b[0m     RowCount,\n\u001b[1m\u001b[94m39 |\u001b[0m     SchemaName,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.Serializable` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:40:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m38 |\u001b[0m     RowCount,\n\u001b[1m\u001b[94m39 |\u001b[0m     SchemaName,\n\u001b[1m\u001b[94m40 |\u001b[0m     Serializable,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m41 |\u001b[0m     SilverTransformFunction,\n\u001b[1m\u001b[94m42 |\u001b[0m     StepContext,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.SilverTransformFunction` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:41:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m39 |\u001b[0m     SchemaName,\n\u001b[1m\u001b[94m40 |\u001b[0m     Serializable,\n\u001b[1m\u001b[94m41 |\u001b[0m     SilverTransformFunction,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m42 |\u001b[0m     StepContext,\n\u001b[1m\u001b[94m43 |\u001b[0m     StepName,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.TransformFunction` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:49:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m47 |\u001b[0m     StringDict,\n\u001b[1m\u001b[94m48 |\u001b[0m     TableName,\n\u001b[1m\u001b[94m49 |\u001b[0m     TransformFunction,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m50 |\u001b[0m     ValidationConfig,\n\u001b[1m\u001b[94m51 |\u001b[0m     ValidationResult,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.ValidationConfig` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:50:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m48 |\u001b[0m     TableName,\n\u001b[1m\u001b[94m49 |\u001b[0m     TransformFunction,\n\u001b[1m\u001b[94m50 |\u001b[0m     ValidationConfig,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m51 |\u001b[0m     ValidationResult,\n\u001b[1m\u001b[94m52 |\u001b[0m     Validatable,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.ValidationResult` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:51:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m49 |\u001b[0m     TransformFunction,\n\u001b[1m\u001b[94m50 |\u001b[0m     ValidationConfig,\n\u001b[1m\u001b[94m51 |\u001b[0m     ValidationResult,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m52 |\u001b[0m     Validatable,\n\u001b[1m\u001b[94m53 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.types.Validatable` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:52:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m50 |\u001b[0m     ValidationConfig,\n\u001b[1m\u001b[94m51 |\u001b[0m     ValidationResult,\n\u001b[1m\u001b[94m52 |\u001b[0m     Validatable,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m53 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:62:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m60 |\u001b[0m         \"\"\"Test string type aliases.\"\"\"\n\u001b[1m\u001b[94m61 |\u001b[0m         # Test that aliases are properly defined\n\u001b[1m\u001b[94m62 |\u001b[0m         assert StepName == str\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         assert PipelineId == str\n\u001b[1m\u001b[94m64 |\u001b[0m         assert ExecutionId == str\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:63:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         # Test that aliases are properly defined\n\u001b[1m\u001b[94m62 |\u001b[0m         assert StepName == str\n\u001b[1m\u001b[94m63 |\u001b[0m         assert PipelineId == str\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m         assert ExecutionId == str\n\u001b[1m\u001b[94m65 |\u001b[0m         assert TableName == str\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:64:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m62 |\u001b[0m         assert StepName == str\n\u001b[1m\u001b[94m63 |\u001b[0m         assert PipelineId == str\n\u001b[1m\u001b[94m64 |\u001b[0m         assert ExecutionId == str\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m65 |\u001b[0m         assert TableName == str\n\u001b[1m\u001b[94m66 |\u001b[0m         assert SchemaName == str\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:65:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         assert PipelineId == str\n\u001b[1m\u001b[94m64 |\u001b[0m         assert ExecutionId == str\n\u001b[1m\u001b[94m65 |\u001b[0m         assert TableName == str\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m         assert SchemaName == str\n\u001b[1m\u001b[94m67 |\u001b[0m         assert ErrorCode == str\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:66:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m         assert ExecutionId == str\n\u001b[1m\u001b[94m65 |\u001b[0m         assert TableName == str\n\u001b[1m\u001b[94m66 |\u001b[0m         assert SchemaName == str\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m67 |\u001b[0m         assert ErrorCode == str\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:67:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m65 |\u001b[0m         assert TableName == str\n\u001b[1m\u001b[94m66 |\u001b[0m         assert SchemaName == str\n\u001b[1m\u001b[94m67 |\u001b[0m         assert ErrorCode == str\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m68 |\u001b[0m\n\u001b[1m\u001b[94m69 |\u001b[0m     def test_numeric_type_aliases(self) -> None:\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:71:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m69 |\u001b[0m     def test_numeric_type_aliases(self) -> None:\n\u001b[1m\u001b[94m70 |\u001b[0m         \"\"\"Test numeric type aliases.\"\"\"\n\u001b[1m\u001b[94m71 |\u001b[0m         assert QualityRate == float\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m72 |\u001b[0m         assert Duration == float\n\u001b[1m\u001b[94m73 |\u001b[0m         assert RowCount == int\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:72:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m70 |\u001b[0m         \"\"\"Test numeric type aliases.\"\"\"\n\u001b[1m\u001b[94m71 |\u001b[0m         assert QualityRate == float\n\u001b[1m\u001b[94m72 |\u001b[0m         assert Duration == float\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m73 |\u001b[0m         assert RowCount == int\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mE721 \u001b[0m\u001b[1mUse `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:73:16\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m71 |\u001b[0m         assert QualityRate == float\n\u001b[1m\u001b[94m72 |\u001b[0m         assert Duration == float\n\u001b[1m\u001b[94m73 |\u001b[0m         assert RowCount == int\n   \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m\n\u001b[1m\u001b[94m75 |\u001b[0m     def test_dictionary_type_aliases(self) -> None:\n   \u001b[1m\u001b[94m|\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:95:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m93 |\u001b[0m         assert StepType.SILVER.value == \"silver\"\n\u001b[1m\u001b[94m94 |\u001b[0m         assert StepType.GOLD.value == \"gold\"\n\u001b[1m\u001b[94m95 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m96 |\u001b[0m         # Test enum iteration\n\u001b[1m\u001b[94m97 |\u001b[0m         step_types = list(StepType)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:110:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m108 |\u001b[0m         assert StepStatus.FAILED.value == \"failed\"\n\u001b[1m\u001b[94m109 |\u001b[0m         assert StepStatus.SKIPPED.value == \"skipped\"\n\u001b[1m\u001b[94m110 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m         # Test enum iteration\n\u001b[1m\u001b[94m112 |\u001b[0m         statuses = list(StepStatus)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:125:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         assert PipelineMode.INCREMENTAL.value == \"incremental\"\n\u001b[1m\u001b[94m124 |\u001b[0m         assert PipelineMode.FULL_REFRESH.value == \"full_refresh\"\n\u001b[1m\u001b[94m125 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m126 |\u001b[0m         # Test enum iteration\n\u001b[1m\u001b[94m127 |\u001b[0m         modes = list(PipelineMode)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:142:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m140 |\u001b[0m         def test_transform(spark, df):\n\u001b[1m\u001b[94m141 |\u001b[0m             return df\n\u001b[1m\u001b[94m142 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m143 |\u001b[0m         # This should not raise an error\n\u001b[1m\u001b[94m144 |\u001b[0m         assert callable(test_transform)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:145:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m143 |\u001b[0m         # This should not raise an error\n\u001b[1m\u001b[94m144 |\u001b[0m         assert callable(test_transform)\n\u001b[1m\u001b[94m145 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m146 |\u001b[0m         # Test BronzeTransformFunction\n\u001b[1m\u001b[94m147 |\u001b[0m         def test_bronze_transform(spark, df):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:149:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m         def test_bronze_transform(spark, df):\n\u001b[1m\u001b[94m148 |\u001b[0m             return df\n\u001b[1m\u001b[94m149 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m         assert callable(test_bronze_transform)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:151:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m         assert callable(test_bronze_transform)\n\u001b[1m\u001b[94m151 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m         # Test SilverTransformFunction\n\u001b[1m\u001b[94m153 |\u001b[0m         def test_silver_transform(spark, df, bronze_dfs):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:155:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m153 |\u001b[0m         def test_silver_transform(spark, df, bronze_dfs):\n\u001b[1m\u001b[94m154 |\u001b[0m             return df\n\u001b[1m\u001b[94m155 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m156 |\u001b[0m         assert callable(test_silver_transform)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:157:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m156 |\u001b[0m         assert callable(test_silver_transform)\n\u001b[1m\u001b[94m157 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m158 |\u001b[0m         # Test GoldTransformFunction\n\u001b[1m\u001b[94m159 |\u001b[0m         def test_gold_transform(spark, silver_dfs):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:161:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m159 |\u001b[0m         def test_gold_transform(spark, silver_dfs):\n\u001b[1m\u001b[94m160 |\u001b[0m             return list(silver_dfs.values())[0] if silver_dfs else None\n\u001b[1m\u001b[94m161 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m162 |\u001b[0m         assert callable(test_gold_transform)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:168:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m166 |\u001b[0m         def test_filter(df):\n\u001b[1m\u001b[94m167 |\u001b[0m             return df\n\u001b[1m\u001b[94m168 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m169 |\u001b[0m         assert callable(test_filter)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:179:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m         # Create mock column\n\u001b[1m\u001b[94m178 |\u001b[0m         mock_column = Mock()\n\u001b[1m\u001b[94m179 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m180 |\u001b[0m         # Test with string rules\n\u001b[1m\u001b[94m181 |\u001b[0m         rules_str = {\"col1\": [\"not_null\", \"positive\"]}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:182:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m180 |\u001b[0m         # Test with string rules\n\u001b[1m\u001b[94m181 |\u001b[0m         rules_str = {\"col1\": [\"not_null\", \"positive\"]}\n\u001b[1m\u001b[94m182 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m         # Test with column rules\n\u001b[1m\u001b[94m184 |\u001b[0m         rules_col = {\"col1\": [mock_column]}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:185:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m183 |\u001b[0m         # Test with column rules\n\u001b[1m\u001b[94m184 |\u001b[0m         rules_col = {\"col1\": [mock_column]}\n\u001b[1m\u001b[94m185 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         # Test with mixed rules\n\u001b[1m\u001b[94m187 |\u001b[0m         rules_mixed = {\"col1\": [\"not_null\", mock_column]}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:188:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         # Test with mixed rules\n\u001b[1m\u001b[94m187 |\u001b[0m         rules_mixed = {\"col1\": [\"not_null\", mock_column]}\n\u001b[1m\u001b[94m188 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m189 |\u001b[0m         # These should not raise errors\n\u001b[1m\u001b[94m190 |\u001b[0m         assert isinstance(rules_str, dict)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:199:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m         step_result = {\"status\": \"completed\", \"rows\": 100}\n\u001b[1m\u001b[94m198 |\u001b[0m         assert isinstance(step_result, dict)\n\u001b[1m\u001b[94m199 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m         # Test PipelineResult\n\u001b[1m\u001b[94m201 |\u001b[0m         pipeline_result = {\"steps\": [\"bronze\", \"silver\"], \"total_rows\": 1000}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:203:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m201 |\u001b[0m         pipeline_result = {\"steps\": [\"bronze\", \"silver\"], \"total_rows\": 1000}\n\u001b[1m\u001b[94m202 |\u001b[0m         assert isinstance(pipeline_result, dict)\n\u001b[1m\u001b[94m203 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m204 |\u001b[0m         # Test ExecutionResult\n\u001b[1m\u001b[94m205 |\u001b[0m         execution_result = {\"execution_id\": \"exec_123\", \"duration\": 30.5}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:207:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m205 |\u001b[0m         execution_result = {\"execution_id\": \"exec_123\", \"duration\": 30.5}\n\u001b[1m\u001b[94m206 |\u001b[0m         assert isinstance(execution_result, dict)\n\u001b[1m\u001b[94m207 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m208 |\u001b[0m         # Test ValidationResult\n\u001b[1m\u001b[94m209 |\u001b[0m         validation_result = {\"valid_rows\": 950, \"invalid_rows\": 50}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:217:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m215 |\u001b[0m         step_context = {\"step_name\": \"bronze_step\", \"config\": {}}\n\u001b[1m\u001b[94m216 |\u001b[0m         assert isinstance(step_context, dict)\n\u001b[1m\u001b[94m217 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m218 |\u001b[0m         # Test ExecutionContext\n\u001b[1m\u001b[94m219 |\u001b[0m         execution_context = {\"pipeline_id\": \"pipeline_123\", \"start_time\": \"2023-01-01\"}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:227:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m225 |\u001b[0m         pipeline_config = {\"name\": \"test_pipeline\", \"steps\": []}\n\u001b[1m\u001b[94m226 |\u001b[0m         assert isinstance(pipeline_config, dict)\n\u001b[1m\u001b[94m227 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m228 |\u001b[0m         # Test ExecutionConfig\n\u001b[1m\u001b[94m229 |\u001b[0m         execution_config = {\"parallel\": True, \"timeout\": 300}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:231:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m229 |\u001b[0m         execution_config = {\"parallel\": True, \"timeout\": 300}\n\u001b[1m\u001b[94m230 |\u001b[0m         assert isinstance(execution_config, dict)\n\u001b[1m\u001b[94m231 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m232 |\u001b[0m         # Test ValidationConfig\n\u001b[1m\u001b[94m233 |\u001b[0m         validation_config = {\"thresholds\": {\"bronze\": 80.0}}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:235:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m         validation_config = {\"thresholds\": {\"bronze\": 80.0}}\n\u001b[1m\u001b[94m234 |\u001b[0m         assert isinstance(validation_config, dict)\n\u001b[1m\u001b[94m235 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m236 |\u001b[0m         # Test MonitoringConfig\n\u001b[1m\u001b[94m237 |\u001b[0m         monitoring_config = {\"metrics\": True, \"logging\": \"debug\"}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:251:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m249 |\u001b[0m         error_context = {\"error_code\": \"VALIDATION_FAILED\", \"details\": {}}\n\u001b[1m\u001b[94m250 |\u001b[0m         assert isinstance(error_context, dict)\n\u001b[1m\u001b[94m251 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m252 |\u001b[0m         # Test ErrorSuggestions\n\u001b[1m\u001b[94m253 |\u001b[0m         error_suggestions = [\"Check data quality\", \"Verify schema\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:266:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m264 |\u001b[0m             def validate(self) -> None:\n\u001b[1m\u001b[94m265 |\u001b[0m                 pass\n\u001b[1m\u001b[94m266 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m267 |\u001b[0m         obj = MockValidatable()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:268:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m267 |\u001b[0m         obj = MockValidatable()\n\u001b[1m\u001b[94m268 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m         # Test that the object implements the protocol\n\u001b[1m\u001b[94m270 |\u001b[0m         assert hasattr(obj, \"validate\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:272:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m270 |\u001b[0m         assert hasattr(obj, \"validate\")\n\u001b[1m\u001b[94m271 |\u001b[0m         assert callable(obj.validate)\n\u001b[1m\u001b[94m272 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m273 |\u001b[0m         # Test protocol validation\n\u001b[1m\u001b[94m274 |\u001b[0m         obj.validate()  # Should not raise an error\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:281:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m279 |\u001b[0m             def to_dict(self) -> Dict[str, Any]:\n\u001b[1m\u001b[94m280 |\u001b[0m                 return {\"test\": \"value\"}\n\u001b[1m\u001b[94m281 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m282 |\u001b[0m         obj = MockSerializable()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:283:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m282 |\u001b[0m         obj = MockSerializable()\n\u001b[1m\u001b[94m283 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m284 |\u001b[0m         # Test that the object implements the protocol\n\u001b[1m\u001b[94m285 |\u001b[0m         assert hasattr(obj, \"to_dict\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:287:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m285 |\u001b[0m         assert hasattr(obj, \"to_dict\")\n\u001b[1m\u001b[94m286 |\u001b[0m         assert callable(obj.to_dict)\n\u001b[1m\u001b[94m287 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m288 |\u001b[0m         # Test protocol usage\n\u001b[1m\u001b[94m289 |\u001b[0m         result = obj.to_dict()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:302:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m300 |\u001b[0m         from sparkforge.types import PipelinePhase\n\u001b[1m\u001b[94m301 |\u001b[0m         assert PipelinePhase == StepType\n\u001b[1m\u001b[94m302 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m303 |\u001b[0m         # Test WriteMode alias\n\u001b[1m\u001b[94m304 |\u001b[0m         from sparkforge.types import WriteMode\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:328:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m326 |\u001b[0m             }\n\u001b[1m\u001b[94m327 |\u001b[0m         }\n\u001b[1m\u001b[94m328 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m329 |\u001b[0m         assert isinstance(config, dict)\n\u001b[1m\u001b[94m330 |\u001b[0m         assert config[\"name\"] == \"test_pipeline\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:343:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m341 |\u001b[0m             \"errors\": []\n\u001b[1m\u001b[94m342 |\u001b[0m         }\n\u001b[1m\u001b[94m343 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m344 |\u001b[0m         assert isinstance(result, dict)\n\u001b[1m\u001b[94m345 |\u001b[0m         assert result[\"status\"] == StepStatus.COMPLETED.value\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:361:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m359 |\u001b[0m             }\n\u001b[1m\u001b[94m360 |\u001b[0m         }\n\u001b[1m\u001b[94m361 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m362 |\u001b[0m         assert isinstance(context, dict)\n\u001b[1m\u001b[94m363 |\u001b[0m         assert isinstance(context[\"config\"], dict)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:377:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m375 |\u001b[0m             }\n\u001b[1m\u001b[94m376 |\u001b[0m         }\n\u001b[1m\u001b[94m377 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m378 |\u001b[0m         suggestions: ErrorSuggestions = [\n\u001b[1m\u001b[94m379 |\u001b[0m             \"Check data quality for user_id column\",\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_types.py:383:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m381 |\u001b[0m             \"Consider adjusting validation thresholds\"\n\u001b[1m\u001b[94m382 |\u001b[0m         ]\n\u001b[1m\u001b[94m383 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m384 |\u001b[0m         assert isinstance(error_context, dict)\n\u001b[1m\u001b[94m385 |\u001b[0m         assert isinstance(suggestions, list)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from typing import Any, Dict, List, Optional\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock, MagicMock\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pyspark.sql import DataFrame, SparkSession\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pyspark.sql.functions import Column, col, expr\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     _convert_rule_to_expression,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     _convert_rules_to_expressions,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     and_all_rules,\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     apply_column_rules,\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     apply_validation_rules,\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     assess_data_quality,\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     get_dataframe_info,\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     safe_divide,\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     validate_dataframe_schema,\n\u001b[1m\u001b[94m27 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Any` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:9:20\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                    \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock, MagicMock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Dict` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:9:25\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock, MagicMock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.List` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:9:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock, MagicMock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Optional` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:9:37\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Optional\n   \u001b[1m\u001b[94m|\u001b[0m                                     \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock, MagicMock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`unittest.mock.MagicMock` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:10:33\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List, Optional\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock, MagicMock\n   \u001b[1m\u001b[94m|\u001b[0m                                 \u001b[1m\u001b[91m^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `unittest.mock.MagicMock`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pyspark.sql.SparkSession` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:13:36\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m from pyspark.sql import DataFrame, SparkSession\n   \u001b[1m\u001b[94m|\u001b[0m                                    \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from pyspark.sql.functions import Column, col, expr\n\u001b[1m\u001b[94m15 |\u001b[0m from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `pyspark.sql.SparkSession`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pyspark.sql.functions.expr` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:14:48\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m from pyspark.sql import DataFrame, SparkSession\n\u001b[1m\u001b[94m14 |\u001b[0m from pyspark.sql.functions import Column, col, expr\n   \u001b[1m\u001b[94m|\u001b[0m                                                \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `pyspark.sql.functions.expr`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pyspark.sql.types.StructField` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:15:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m from pyspark.sql import DataFrame, SparkSession\n\u001b[1m\u001b[94m14 |\u001b[0m from pyspark.sql.functions import Column, col, expr\n\u001b[1m\u001b[94m15 |\u001b[0m from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from sparkforge.validation import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pyspark.sql.types.StructType` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:15:44\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m from pyspark.sql import DataFrame, SparkSession\n\u001b[1m\u001b[94m14 |\u001b[0m from pyspark.sql.functions import Column, col, expr\n\u001b[1m\u001b[94m15 |\u001b[0m from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n   \u001b[1m\u001b[94m|\u001b[0m                                            \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from sparkforge.validation import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pyspark.sql.types.StringType` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:15:56\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m from pyspark.sql import DataFrame, SparkSession\n\u001b[1m\u001b[94m14 |\u001b[0m from pyspark.sql.functions import Column, col, expr\n\u001b[1m\u001b[94m15 |\u001b[0m from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n   \u001b[1m\u001b[94m|\u001b[0m                                                        \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from sparkforge.validation import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pyspark.sql.types.IntegerType` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:15:68\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m from pyspark.sql import DataFrame, SparkSession\n\u001b[1m\u001b[94m14 |\u001b[0m from pyspark.sql.functions import Column, col, expr\n\u001b[1m\u001b[94m15 |\u001b[0m from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n   \u001b[1m\u001b[94m|\u001b[0m                                                                    \u001b[1m\u001b[91m^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m from sparkforge.validation import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:60:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m         mock_df.count.return_value = 100\n\u001b[1m\u001b[94m59 |\u001b[0m         mock_df.limit.return_value = mock_df\n\u001b[1m\u001b[94m60 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         rules = {}  # Empty rules should return all rows as valid\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:62:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         rules = {}  # Empty rules should return all rows as valid\n\u001b[1m\u001b[94m62 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         result = apply_column_rules(\n\u001b[1m\u001b[94m64 |\u001b[0m             mock_df, \n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:64:21\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         result = apply_column_rules(\n\u001b[1m\u001b[94m64 |\u001b[0m             mock_df, \n   \u001b[1m\u001b[94m|\u001b[0m                     \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m65 |\u001b[0m             rules, \n\u001b[1m\u001b[94m66 |\u001b[0m             stage=\"test_stage\",\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:65:19\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         result = apply_column_rules(\n\u001b[1m\u001b[94m64 |\u001b[0m             mock_df, \n\u001b[1m\u001b[94m65 |\u001b[0m             rules, \n   \u001b[1m\u001b[94m|\u001b[0m                   \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m             stage=\"test_stage\",\n\u001b[1m\u001b[94m67 |\u001b[0m             step=\"test_step\"\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:69:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m67 |\u001b[0m             step=\"test_step\"\n\u001b[1m\u001b[94m68 |\u001b[0m         )\n\u001b[1m\u001b[94m69 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m70 |\u001b[0m         assert result is not None\n\u001b[1m\u001b[94m71 |\u001b[0m         mock_df.limit.assert_called_once_with(0)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:80:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m78 |\u001b[0m         mock_df.filter.return_value = mock_df\n\u001b[1m\u001b[94m79 |\u001b[0m         mock_df.limit.return_value = mock_df\n\u001b[1m\u001b[94m80 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m81 |\u001b[0m         rules = {\"col1\": [\"not_null\"]}\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:82:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m81 |\u001b[0m         rules = {\"col1\": [\"not_null\"]}\n\u001b[1m\u001b[94m82 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m         result = apply_column_rules(\n\u001b[1m\u001b[94m84 |\u001b[0m             mock_df, \n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:84:21\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m         result = apply_column_rules(\n\u001b[1m\u001b[94m84 |\u001b[0m             mock_df, \n   \u001b[1m\u001b[94m|\u001b[0m                     \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m85 |\u001b[0m             rules, \n\u001b[1m\u001b[94m86 |\u001b[0m             stage=\"test_stage\",\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:85:19\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m         result = apply_column_rules(\n\u001b[1m\u001b[94m84 |\u001b[0m             mock_df, \n\u001b[1m\u001b[94m85 |\u001b[0m             rules, \n   \u001b[1m\u001b[94m|\u001b[0m                   \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m             stage=\"test_stage\",\n\u001b[1m\u001b[94m87 |\u001b[0m             step=\"test_step\",\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:90:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m88 |\u001b[0m             filter_columns_by_rules=False\n\u001b[1m\u001b[94m89 |\u001b[0m         )\n\u001b[1m\u001b[94m90 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m91 |\u001b[0m         assert result is not None\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:98:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 96 |\u001b[0m         result = safe_divide(10, 0)\n\u001b[1m\u001b[94m 97 |\u001b[0m         assert result == 0.0\n\u001b[1m\u001b[94m 98 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m 99 |\u001b[0m         # Test with zero denominator and custom default\n\u001b[1m\u001b[94m100 |\u001b[0m         result = safe_divide(10, 0, default=1.0)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:108:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m106 |\u001b[0m         mock_df = Mock(spec=DataFrame)\n\u001b[1m\u001b[94m107 |\u001b[0m         mock_df.columns = [\"col1\", \"col2\"]\n\u001b[1m\u001b[94m108 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m109 |\u001b[0m         # Test with empty expected columns\n\u001b[1m\u001b[94m110 |\u001b[0m         result = validate_dataframe_schema(mock_df, [])\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:119:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m117 |\u001b[0m         mock_df.count.return_value = 0\n\u001b[1m\u001b[94m118 |\u001b[0m         mock_df.columns = []\n\u001b[1m\u001b[94m119 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m         # Test with empty DataFrame\n\u001b[1m\u001b[94m121 |\u001b[0m         result = get_dataframe_info(mock_df)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:130:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m         mock_df = Mock(spec=DataFrame)\n\u001b[1m\u001b[94m129 |\u001b[0m         mock_df.count.side_effect = Exception(\"Test error\")\n\u001b[1m\u001b[94m130 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m131 |\u001b[0m         # This should handle the exception gracefully\n\u001b[1m\u001b[94m132 |\u001b[0m         result = get_dataframe_info(mock_df)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:140:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m138 |\u001b[0m         mock_df = Mock(spec=DataFrame)\n\u001b[1m\u001b[94m139 |\u001b[0m         mock_df.count.return_value = 100\n\u001b[1m\u001b[94m140 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m141 |\u001b[0m         # Test with empty rules\n\u001b[1m\u001b[94m142 |\u001b[0m         result = assess_data_quality(mock_df, {})\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:150:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m148 |\u001b[0m         mock_df = Mock(spec=DataFrame)\n\u001b[1m\u001b[94m149 |\u001b[0m         mock_df.count.return_value = 100\n\u001b[1m\u001b[94m150 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m151 |\u001b[0m         # Test with empty rules\n\u001b[1m\u001b[94m152 |\u001b[0m         result = apply_validation_rules(mock_df, {}, \"test_stage\", \"test_step\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:163:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m161 |\u001b[0m             \"col3\": [None, \"invalid\"]\n\u001b[1m\u001b[94m162 |\u001b[0m         }\n\u001b[1m\u001b[94m163 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m164 |\u001b[0m         result = _convert_rules_to_expressions(rules)\n\u001b[1m\u001b[94m165 |\u001b[0m         assert isinstance(result, dict)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:199:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m197 |\u001b[0m             \"LENGTH(col1) > 5\"\n\u001b[1m\u001b[94m198 |\u001b[0m         ]\n\u001b[1m\u001b[94m199 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m         for rule in test_cases:\n\u001b[1m\u001b[94m201 |\u001b[0m             result = _convert_rule_to_expression(rule, \"col1\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_additional_coverage.py:209:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m207 |\u001b[0m         with pytest.raises((AttributeError, TypeError)):\n\u001b[1m\u001b[94m208 |\u001b[0m             apply_column_rules(None, {\"col1\": [\"not_null\"]})  # type: ignore\n\u001b[1m\u001b[94m209 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m210 |\u001b[0m         with pytest.raises((AttributeError, TypeError)):\n\u001b[1m\u001b[94m211 |\u001b[0m             apply_column_rules(\"invalid\", {\"col1\": [\"not_null\"]})  # type: ignore\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Any, Dict, List\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.errors import ValidationError\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     UnifiedValidator,\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     apply_column_rules,\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     apply_validation_rules,\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     assess_data_quality,\n\u001b[1m\u001b[94m27 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     get_dataframe_info,\n\u001b[1m\u001b[94m28 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     safe_divide,\n\u001b[1m\u001b[94m29 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     validate_dataframe_schema,\n\u001b[1m\u001b[94m30 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Any` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:10:20\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m10 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                    \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Dict` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:10:25\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m10 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.List` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:10:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m10 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pytest` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:12:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from typing import Any, Dict, List\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.errors import ValidationError\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `pytest`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.errors.ValidationError` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:14:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.errors import ValidationError\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m     BronzeStep,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `sparkforge.errors.ValidationError`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.BronzeStep` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:16:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.errors import ValidationError\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m     BronzeStep,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.GoldStep` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:17:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m17 |\u001b[0m     GoldStep,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m     SilverStep,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.SilverStep` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:19:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m     SilverStep,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m21 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.ValidationThresholds` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:20:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m20 |\u001b[0m     ValidationThresholds,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m )\n\u001b[1m\u001b[94m22 |\u001b[0m from sparkforge.validation import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.validation.apply_column_rules` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:24:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m23 |\u001b[0m     UnifiedValidator,\n\u001b[1m\u001b[94m24 |\u001b[0m     apply_column_rules,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m     apply_validation_rules,\n\u001b[1m\u001b[94m26 |\u001b[0m     assess_data_quality,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `sparkforge.validation.apply_column_rules`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:39:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m37 |\u001b[0m         \"\"\"Test and_all_rules with string expressions that need conversion.\"\"\"\n\u001b[1m\u001b[94m38 |\u001b[0m         from sparkforge.validation import and_all_rules\n\u001b[1m\u001b[94m39 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m40 |\u001b[0m         # Mock F.expr to avoid Spark context issues\n\u001b[1m\u001b[94m41 |\u001b[0m         with patch('sparkforge.validation.F') as mock_f:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:44:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m42 |\u001b[0m             mock_expr = Mock()\n\u001b[1m\u001b[94m43 |\u001b[0m             mock_f.expr.return_value = mock_expr\n\u001b[1m\u001b[94m44 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m45 |\u001b[0m             # Test with string expressions\n\u001b[1m\u001b[94m46 |\u001b[0m             rules = {\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:50:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m48 |\u001b[0m                 \"col2\": [\"col2 != ''\", \"LENGTH(col2) > 5\"]\n\u001b[1m\u001b[94m49 |\u001b[0m             }\n\u001b[1m\u001b[94m50 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m51 |\u001b[0m             result = and_all_rules(rules)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:52:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m51 |\u001b[0m             result = and_all_rules(rules)\n\u001b[1m\u001b[94m52 |\u001b[0m             \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m53 |\u001b[0m             # Should call F.expr for each string rule\n\u001b[1m\u001b[94m54 |\u001b[0m             assert mock_f.expr.call_count == 4\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF811 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mRedefinition of unused `apply_column_rules` from line 24\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:59:43\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m57 |\u001b[0m     def test_apply_column_rules_no_validation_predicate(self) -> None:\n\u001b[1m\u001b[94m58 |\u001b[0m         \"\"\"Test apply_column_rules when validation_predicate is True.\"\"\"\n\u001b[1m\u001b[94m59 |\u001b[0m         from sparkforge.validation import apply_column_rules\n   \u001b[1m\u001b[94m|\u001b[0m                                           \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m \u001b[1m\u001b[91m`apply_column_rules` redefined here\u001b[0m\n\u001b[1m\u001b[94m60 |\u001b[0m         \n\u001b[1m\u001b[94m61 |\u001b[0m         # Create mock DataFrame\n   \u001b[1m\u001b[94m|\u001b[0m\n  \u001b[1m\u001b[94m:::\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:24:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m23 |\u001b[0m     UnifiedValidator,\n\u001b[1m\u001b[94m24 |\u001b[0m     apply_column_rules,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[33m------------------\u001b[0m \u001b[1m\u001b[33mprevious definition of `apply_column_rules` here\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m     apply_validation_rules,\n\u001b[1m\u001b[94m26 |\u001b[0m     assess_data_quality,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove definition: `apply_column_rules`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:60:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m58 |\u001b[0m         \"\"\"Test apply_column_rules when validation_predicate is True.\"\"\"\n\u001b[1m\u001b[94m59 |\u001b[0m         from sparkforge.validation import apply_column_rules\n\u001b[1m\u001b[94m60 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         # Create mock DataFrame\n\u001b[1m\u001b[94m62 |\u001b[0m         mock_df = Mock()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:65:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m         mock_df.count.return_value = 100\n\u001b[1m\u001b[94m64 |\u001b[0m         mock_df.limit.return_value = mock_df\n\u001b[1m\u001b[94m65 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m         # Mock and_all_rules to return True (no rules)\n\u001b[1m\u001b[94m67 |\u001b[0m         with patch('sparkforge.validation.and_all_rules', return_value=True):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:72:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m70 |\u001b[0m                     mock_df, {}, \"bronze\", \"test_step\"\n\u001b[1m\u001b[94m71 |\u001b[0m                 )\n\u001b[1m\u001b[94m72 |\u001b[0m                 \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m73 |\u001b[0m                 # Should return all data as valid\n\u001b[1m\u001b[94m74 |\u001b[0m                 assert valid_df is mock_df\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:85:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m83 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m84 |\u001b[0m         mock_df.count.return_value = 0\n\u001b[1m\u001b[94m85 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m         result = assess_data_quality(mock_df, {})\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:87:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m         result = assess_data_quality(mock_df, {})\n\u001b[1m\u001b[94m87 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m88 |\u001b[0m         assert result[\"total_rows\"] == 0\n\u001b[1m\u001b[94m89 |\u001b[0m         assert result[\"valid_rows\"] == 0\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:98:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 96 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m 97 |\u001b[0m         mock_df.count.return_value = 100\n\u001b[1m\u001b[94m 98 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m 99 |\u001b[0m         mock_valid_df = Mock()\n\u001b[1m\u001b[94m100 |\u001b[0m         mock_invalid_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:106:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m104 |\u001b[0m         mock_stats.invalid_rows = 5\n\u001b[1m\u001b[94m105 |\u001b[0m         mock_stats.validation_rate = 95.0\n\u001b[1m\u001b[94m106 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m107 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules', \n\u001b[1m\u001b[94m108 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:107:63\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m105 |\u001b[0m         mock_stats.validation_rate = 95.0\n\u001b[1m\u001b[94m106 |\u001b[0m         \n\u001b[1m\u001b[94m107 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules', \n    \u001b[1m\u001b[94m|\u001b[0m                                                               \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m108 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n\u001b[1m\u001b[94m109 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:111:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m109 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m110 |\u001b[0m             result = assess_data_quality(mock_df, rules)\n\u001b[1m\u001b[94m111 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m112 |\u001b[0m             assert result[\"total_rows\"] == 100\n\u001b[1m\u001b[94m113 |\u001b[0m             assert result[\"valid_rows\"] == 95\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:122:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m120 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m121 |\u001b[0m         mock_df.count.return_value = 50\n\u001b[1m\u001b[94m122 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         result = assess_data_quality(mock_df, None)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:124:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m123 |\u001b[0m         result = assess_data_quality(mock_df, None)\n\u001b[1m\u001b[94m124 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m125 |\u001b[0m         assert result[\"total_rows\"] == 50\n\u001b[1m\u001b[94m126 |\u001b[0m         assert result[\"valid_rows\"] == 50\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:137:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m135 |\u001b[0m         assert validator.logger is not None\n\u001b[1m\u001b[94m136 |\u001b[0m         assert validator.custom_validators == []\n\u001b[1m\u001b[94m137 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m138 |\u001b[0m         # Test with custom logger\n\u001b[1m\u001b[94m139 |\u001b[0m         mock_logger = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:149:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m         validator = UnifiedValidator(mock_logger)\n\u001b[1m\u001b[94m148 |\u001b[0m         mock_validator = Mock()\n\u001b[1m\u001b[94m149 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m         validator.add_validator(mock_validator)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:151:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m         validator.add_validator(mock_validator)\n\u001b[1m\u001b[94m151 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m152 |\u001b[0m         assert len(validator.custom_validators) == 1\n\u001b[1m\u001b[94m153 |\u001b[0m         assert validator.custom_validators[0] is mock_validator\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:160:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m158 |\u001b[0m         \"\"\"Test UnifiedValidator validate_pipeline method.\"\"\"\n\u001b[1m\u001b[94m159 |\u001b[0m         validator = UnifiedValidator()\n\u001b[1m\u001b[94m160 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m161 |\u001b[0m         # Mock all validation methods\n\u001b[1m\u001b[94m162 |\u001b[0m         with patch.object(validator, '_validate_config', return_value=[]), \\\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:167:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m165 |\u001b[0m              patch.object(validator, '_validate_gold_steps', return_value=([], [])), \\\n\u001b[1m\u001b[94m166 |\u001b[0m              patch.object(validator, '_validate_dependencies', return_value=([], [])):\n\u001b[1m\u001b[94m167 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m168 |\u001b[0m             config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m169 |\u001b[0m             result = validator.validate_pipeline(config, {}, {}, {})\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:170:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m168 |\u001b[0m             config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m169 |\u001b[0m             result = validator.validate_pipeline(config, {}, {}, {})\n\u001b[1m\u001b[94m170 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m171 |\u001b[0m             assert result.is_valid is True\n\u001b[1m\u001b[94m172 |\u001b[0m             assert result.errors == []\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:179:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m         \"\"\"Test UnifiedValidator validate_pipeline with validation errors.\"\"\"\n\u001b[1m\u001b[94m178 |\u001b[0m         validator = UnifiedValidator()\n\u001b[1m\u001b[94m179 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m180 |\u001b[0m         # Mock validation methods to return errors\n\u001b[1m\u001b[94m181 |\u001b[0m         with patch.object(validator, '_validate_config', return_value=[\"Config error\"]), \\\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:186:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m184 |\u001b[0m              patch.object(validator, '_validate_gold_steps', return_value=([], [])), \\\n\u001b[1m\u001b[94m185 |\u001b[0m              patch.object(validator, '_validate_dependencies', return_value=([], [])):\n\u001b[1m\u001b[94m186 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m187 |\u001b[0m             config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m188 |\u001b[0m             result = validator.validate_pipeline(config, {}, {}, {})\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:189:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m187 |\u001b[0m             config = PipelineConfig.create_default(\"test_schema\")\n\u001b[1m\u001b[94m188 |\u001b[0m             result = validator.validate_pipeline(config, {}, {}, {})\n\u001b[1m\u001b[94m189 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m190 |\u001b[0m             assert result.is_valid is False\n\u001b[1m\u001b[94m191 |\u001b[0m             assert len(result.errors) == 2\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:201:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m199 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m200 |\u001b[0m         mock_df.count.return_value = 200\n\u001b[1m\u001b[94m201 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m202 |\u001b[0m         # Mock apply_column_rules\n\u001b[1m\u001b[94m203 |\u001b[0m         mock_valid_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:211:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m209 |\u001b[0m         mock_stats.validation_rate = 90.0\n\u001b[1m\u001b[94m210 |\u001b[0m         mock_stats.duration = 1.5\n\u001b[1m\u001b[94m211 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m212 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m213 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:214:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m212 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m213 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n\u001b[1m\u001b[94m214 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m215 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m216 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"silver\", \"test_step\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:217:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m215 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m216 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"silver\", \"test_step\")\n\u001b[1m\u001b[94m217 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m218 |\u001b[0m             assert stats.total_rows == 200\n\u001b[1m\u001b[94m219 |\u001b[0m             assert stats.valid_rows == 180\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:232:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m230 |\u001b[0m         mock_schema.__str__ = Mock(return_value=\"struct<col1:string,col2:int,col3:double>\")\n\u001b[1m\u001b[94m231 |\u001b[0m         mock_df.schema = mock_schema\n\u001b[1m\u001b[94m232 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m         result = get_dataframe_info(mock_df)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:234:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m233 |\u001b[0m         result = get_dataframe_info(mock_df)\n\u001b[1m\u001b[94m234 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m235 |\u001b[0m         assert result[\"row_count\"] == 150\n\u001b[1m\u001b[94m236 |\u001b[0m         assert result[\"column_count\"] == 3\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:245:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m243 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m244 |\u001b[0m         mock_df.columns = [\"col1\", \"col2\", \"col3\"]\n\u001b[1m\u001b[94m245 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m246 |\u001b[0m         # Test valid schema\n\u001b[1m\u001b[94m247 |\u001b[0m         expected_columns = [\"col1\", \"col2\", \"col3\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:250:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m248 |\u001b[0m         result = validate_dataframe_schema(mock_df, expected_columns)\n\u001b[1m\u001b[94m249 |\u001b[0m         assert result is True\n\u001b[1m\u001b[94m250 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m251 |\u001b[0m         # Test missing columns\n\u001b[1m\u001b[94m252 |\u001b[0m         expected_columns = [\"col1\", \"col2\", \"col3\", \"col4\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:255:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m253 |\u001b[0m         result = validate_dataframe_schema(mock_df, expected_columns)\n\u001b[1m\u001b[94m254 |\u001b[0m         assert result is False\n\u001b[1m\u001b[94m255 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m256 |\u001b[0m         # Test extra columns (should still be valid since all expected columns are present)\n\u001b[1m\u001b[94m257 |\u001b[0m         expected_columns = [\"col1\", \"col2\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:265:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m         # Test normal division\n\u001b[1m\u001b[94m264 |\u001b[0m         assert safe_divide(10.0, 2.0) == 5.0\n\u001b[1m\u001b[94m265 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m         # Test division by zero\n\u001b[1m\u001b[94m267 |\u001b[0m         assert safe_divide(10.0, 0.0) == 0.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:268:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m         # Test division by zero\n\u001b[1m\u001b[94m267 |\u001b[0m         assert safe_divide(10.0, 0.0) == 0.0\n\u001b[1m\u001b[94m268 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m         # Test zero numerator\n\u001b[1m\u001b[94m270 |\u001b[0m         assert safe_divide(0.0, 5.0) == 0.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:271:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m         # Test zero numerator\n\u001b[1m\u001b[94m270 |\u001b[0m         assert safe_divide(0.0, 5.0) == 0.0\n\u001b[1m\u001b[94m271 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m272 |\u001b[0m         # Test both zero\n\u001b[1m\u001b[94m273 |\u001b[0m         assert safe_divide(0.0, 0.0) == 0.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:280:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m278 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m279 |\u001b[0m         mock_df.count.return_value = 1000000  # 1M rows\n\u001b[1m\u001b[94m280 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m281 |\u001b[0m         mock_valid_df = Mock()\n\u001b[1m\u001b[94m282 |\u001b[0m         mock_invalid_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:289:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m287 |\u001b[0m         mock_stats.validation_rate = 95.0\n\u001b[1m\u001b[94m288 |\u001b[0m         mock_stats.duration = 10.5\n\u001b[1m\u001b[94m289 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m290 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m291 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:292:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m290 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m291 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n\u001b[1m\u001b[94m292 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m293 |\u001b[0m             rules = {\n\u001b[1m\u001b[94m294 |\u001b[0m                 \"user_id\": [\"user_id IS NOT NULL\", \"user_id > 0\"],\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:298:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m296 |\u001b[0m                 \"created_at\": [\"created_at IS NOT NULL\"]\n\u001b[1m\u001b[94m297 |\u001b[0m             }\n\u001b[1m\u001b[94m298 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m299 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"bronze\", \"users\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:300:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m299 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"bronze\", \"users\")\n\u001b[1m\u001b[94m300 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m301 |\u001b[0m             assert stats.total_rows == 1000000\n\u001b[1m\u001b[94m302 |\u001b[0m             assert stats.valid_rows == 950000\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:312:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m310 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m311 |\u001b[0m         mock_df.count.side_effect = Exception(\"Database connection failed\")\n\u001b[1m\u001b[94m312 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m313 |\u001b[0m         # Test error handling in assess_data_quality\n\u001b[1m\u001b[94m314 |\u001b[0m         result = assess_data_quality(mock_df, {})\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:317:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m315 |\u001b[0m         assert \"error\" in result\n\u001b[1m\u001b[94m316 |\u001b[0m         assert \"Database connection failed\" in result[\"error\"]\n\u001b[1m\u001b[94m317 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m318 |\u001b[0m         # Test error handling in get_dataframe_info\n\u001b[1m\u001b[94m319 |\u001b[0m         result = get_dataframe_info(mock_df)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`time` imported but unused\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:325:16\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m323 |\u001b[0m     def test_validation_performance_scenarios(self) -> None:\n\u001b[1m\u001b[94m324 |\u001b[0m         \"\"\"Test validation performance-related scenarios.\"\"\"\n\u001b[1m\u001b[94m325 |\u001b[0m         import time\n    \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m326 |\u001b[0m         \n\u001b[1m\u001b[94m327 |\u001b[0m         mock_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `time`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:326:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m324 |\u001b[0m         \"\"\"Test validation performance-related scenarios.\"\"\"\n\u001b[1m\u001b[94m325 |\u001b[0m         import time\n\u001b[1m\u001b[94m326 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m327 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m328 |\u001b[0m         mock_df.count.return_value = 50000\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:329:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m327 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m328 |\u001b[0m         mock_df.count.return_value = 50000\n\u001b[1m\u001b[94m329 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m330 |\u001b[0m         mock_valid_df = Mock()\n\u001b[1m\u001b[94m331 |\u001b[0m         mock_invalid_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:337:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m335 |\u001b[0m         mock_stats.invalid_rows = 2000\n\u001b[1m\u001b[94m336 |\u001b[0m         mock_stats.validation_rate = 96.0\n\u001b[1m\u001b[94m337 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m338 |\u001b[0m         # Mock time to simulate performance timing\n\u001b[1m\u001b[94m339 |\u001b[0m         with patch('sparkforge.validation.time.time', side_effect=[0.0, 2.5]), \\\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:342:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m340 |\u001b[0m              patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m341 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n\u001b[1m\u001b[94m342 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m343 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m344 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"silver\", \"performance_test\")\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_comprehensive_coverage.py:345:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m343 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m344 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(mock_df, rules, \"silver\", \"performance_test\")\n\u001b[1m\u001b[94m345 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m346 |\u001b[0m             assert stats.validation_rate == 96.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from typing import Any, Dict, List\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.errors import ValidationError\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m21 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n\u001b[1m\u001b[94m22 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m23 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     UnifiedValidator,\n\u001b[1m\u001b[94m24 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     apply_column_rules,\n\u001b[1m\u001b[94m25 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     apply_validation_rules,\n\u001b[1m\u001b[94m26 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     assess_data_quality,\n\u001b[1m\u001b[94m27 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     get_dataframe_info,\n\u001b[1m\u001b[94m28 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     safe_divide,\n\u001b[1m\u001b[94m29 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     validate_dataframe_schema,\n\u001b[1m\u001b[94m30 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Any` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:10:20\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m10 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                    \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Dict` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:10:25\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m10 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.List` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:10:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from unittest.mock import Mock, patch\n\u001b[1m\u001b[94m10 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pytest` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:12:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from typing import Any, Dict, List\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.errors import ValidationError\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `pytest`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.errors.ValidationError` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:14:31\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.errors import ValidationError\n   \u001b[1m\u001b[94m|\u001b[0m                               \u001b[1m\u001b[91m^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m     BronzeStep,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `sparkforge.errors.ValidationError`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.BronzeStep` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:16:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m from sparkforge.errors import ValidationError\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m     BronzeStep,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.GoldStep` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:17:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.models import (\n\u001b[1m\u001b[94m16 |\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m17 |\u001b[0m     GoldStep,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m     SilverStep,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.PipelineConfig` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:18:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m     BronzeStep,\n\u001b[1m\u001b[94m17 |\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m20 |\u001b[0m     ValidationThresholds,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.SilverStep` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:19:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m     GoldStep,\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m     SilverStep,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m     ValidationThresholds,\n\u001b[1m\u001b[94m21 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.models.ValidationThresholds` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:20:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m18 |\u001b[0m     PipelineConfig,\n\u001b[1m\u001b[94m19 |\u001b[0m     SilverStep,\n\u001b[1m\u001b[94m20 |\u001b[0m     ValidationThresholds,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m )\n\u001b[1m\u001b[94m22 |\u001b[0m from sparkforge.validation import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.validation.UnifiedValidator` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:23:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m21 |\u001b[0m )\n\u001b[1m\u001b[94m22 |\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m23 |\u001b[0m     UnifiedValidator,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m24 |\u001b[0m     apply_column_rules,\n\u001b[1m\u001b[94m25 |\u001b[0m     apply_validation_rules,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.validation.apply_column_rules` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:24:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m23 |\u001b[0m     UnifiedValidator,\n\u001b[1m\u001b[94m24 |\u001b[0m     apply_column_rules,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m     apply_validation_rules,\n\u001b[1m\u001b[94m26 |\u001b[0m     assess_data_quality,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF811 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mRedefinition of unused `apply_column_rules` from line 24\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:38:43\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m36 |\u001b[0m     def test_apply_column_rules_column_filtering_edge_cases(self) -> None:\n\u001b[1m\u001b[94m37 |\u001b[0m         \"\"\"Test apply_column_rules column filtering edge cases.\"\"\"\n\u001b[1m\u001b[94m38 |\u001b[0m         from sparkforge.validation import apply_column_rules\n   \u001b[1m\u001b[94m|\u001b[0m                                           \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m \u001b[1m\u001b[91m`apply_column_rules` redefined here\u001b[0m\n\u001b[1m\u001b[94m39 |\u001b[0m         \n\u001b[1m\u001b[94m40 |\u001b[0m         # Create mock DataFrame\n   \u001b[1m\u001b[94m|\u001b[0m\n  \u001b[1m\u001b[94m:::\u001b[0m tests/unit/test_validation_final_push.py:24:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m23 |\u001b[0m     UnifiedValidator,\n\u001b[1m\u001b[94m24 |\u001b[0m     apply_column_rules,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[33m------------------\u001b[0m \u001b[1m\u001b[33mprevious definition of `apply_column_rules` here\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m     apply_validation_rules,\n\u001b[1m\u001b[94m26 |\u001b[0m     assess_data_quality,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove definition: `apply_column_rules`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:39:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m37 |\u001b[0m         \"\"\"Test apply_column_rules column filtering edge cases.\"\"\"\n\u001b[1m\u001b[94m38 |\u001b[0m         from sparkforge.validation import apply_column_rules\n\u001b[1m\u001b[94m39 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m40 |\u001b[0m         # Create mock DataFrame\n\u001b[1m\u001b[94m41 |\u001b[0m         mock_df = Mock()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:45:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m43 |\u001b[0m         mock_df.columns = [\"col1\", \"col2\", \"col3\"]\n\u001b[1m\u001b[94m44 |\u001b[0m         mock_df.select.return_value = mock_df\n\u001b[1m\u001b[94m45 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m46 |\u001b[0m         # Mock and_all_rules to return a mock Column\n\u001b[1m\u001b[94m47 |\u001b[0m         mock_column = Mock()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:50:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m48 |\u001b[0m         mock_df.filter.return_value = mock_df\n\u001b[1m\u001b[94m49 |\u001b[0m         mock_df.__invert__ = Mock(return_value=mock_column)\n\u001b[1m\u001b[94m50 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m51 |\u001b[0m         with patch('sparkforge.validation.and_all_rules', return_value=mock_column):\n\u001b[1m\u001b[94m52 |\u001b[0m             with patch('sparkforge.validation.time.time', return_value=0.0):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:55:76\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m53 |\u001b[0m                 # Test with column filtering enabled\n\u001b[1m\u001b[94m54 |\u001b[0m                 valid_df, invalid_df, stats = apply_column_rules(\n\u001b[1m\u001b[94m55 |\u001b[0m                     mock_df, {\"col1\": [\"col1 > 0\"]}, \"bronze\", \"test_step\", \n   \u001b[1m\u001b[94m|\u001b[0m                                                                            \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m56 |\u001b[0m                     filter_columns_by_rules=True\n\u001b[1m\u001b[94m57 |\u001b[0m                 )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:58:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m56 |\u001b[0m                     filter_columns_by_rules=True\n\u001b[1m\u001b[94m57 |\u001b[0m                 )\n\u001b[1m\u001b[94m58 |\u001b[0m                 \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m59 |\u001b[0m                 assert stats.total_rows == 100\n\u001b[1m\u001b[94m60 |\u001b[0m                 # Should call select to filter columns\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF811 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mRedefinition of unused `apply_column_rules` from line 24\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:65:43\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m63 |\u001b[0m     def test_apply_column_rules_no_column_filtering(self) -> None:\n\u001b[1m\u001b[94m64 |\u001b[0m         \"\"\"Test apply_column_rules without column filtering.\"\"\"\n\u001b[1m\u001b[94m65 |\u001b[0m         from sparkforge.validation import apply_column_rules\n   \u001b[1m\u001b[94m|\u001b[0m                                           \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m \u001b[1m\u001b[91m`apply_column_rules` redefined here\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m         \n\u001b[1m\u001b[94m67 |\u001b[0m         # Create mock DataFrame\n   \u001b[1m\u001b[94m|\u001b[0m\n  \u001b[1m\u001b[94m:::\u001b[0m tests/unit/test_validation_final_push.py:24:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m22 |\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m23 |\u001b[0m     UnifiedValidator,\n\u001b[1m\u001b[94m24 |\u001b[0m     apply_column_rules,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[33m------------------\u001b[0m \u001b[1m\u001b[33mprevious definition of `apply_column_rules` here\u001b[0m\n\u001b[1m\u001b[94m25 |\u001b[0m     apply_validation_rules,\n\u001b[1m\u001b[94m26 |\u001b[0m     assess_data_quality,\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove definition: `apply_column_rules`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:66:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m         \"\"\"Test apply_column_rules without column filtering.\"\"\"\n\u001b[1m\u001b[94m65 |\u001b[0m         from sparkforge.validation import apply_column_rules\n\u001b[1m\u001b[94m66 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m67 |\u001b[0m         # Create mock DataFrame\n\u001b[1m\u001b[94m68 |\u001b[0m         mock_df = Mock()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:71:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m69 |\u001b[0m         mock_df.count.return_value = 50\n\u001b[1m\u001b[94m70 |\u001b[0m         mock_df.columns = [\"col1\", \"col2\", \"col3\"]\n\u001b[1m\u001b[94m71 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m72 |\u001b[0m         # Mock and_all_rules to return a mock Column\n\u001b[1m\u001b[94m73 |\u001b[0m         mock_column = Mock()\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:76:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m74 |\u001b[0m         mock_df.filter.return_value = mock_df\n\u001b[1m\u001b[94m75 |\u001b[0m         mock_df.__invert__ = Mock(return_value=mock_column)\n\u001b[1m\u001b[94m76 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m77 |\u001b[0m         with patch('sparkforge.validation.and_all_rules', return_value=mock_column):\n\u001b[1m\u001b[94m78 |\u001b[0m             with patch('sparkforge.validation.time.time', return_value=0.0):\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW291 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mTrailing whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:81:76\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m79 |\u001b[0m                 # Test with column filtering disabled\n\u001b[1m\u001b[94m80 |\u001b[0m                 valid_df, invalid_df, stats = apply_column_rules(\n\u001b[1m\u001b[94m81 |\u001b[0m                     mock_df, {\"col1\": [\"col1 > 0\"]}, \"bronze\", \"test_step\", \n   \u001b[1m\u001b[94m|\u001b[0m                                                                            \u001b[1m\u001b[91m^\u001b[0m\n\u001b[1m\u001b[94m82 |\u001b[0m                     filter_columns_by_rules=False\n\u001b[1m\u001b[94m83 |\u001b[0m                 )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove trailing whitespace\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:84:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m82 |\u001b[0m                     filter_columns_by_rules=False\n\u001b[1m\u001b[94m83 |\u001b[0m                 )\n\u001b[1m\u001b[94m84 |\u001b[0m                 \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m85 |\u001b[0m                 assert stats.total_rows == 50\n\u001b[1m\u001b[94m86 |\u001b[0m                 # Should not call select\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:94:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m92 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m93 |\u001b[0m         mock_df.count.return_value = 75\n\u001b[1m\u001b[94m94 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m95 |\u001b[0m         # Test with empty rules - should return all as valid\n\u001b[1m\u001b[94m96 |\u001b[0m         result = assess_data_quality(mock_df, {})\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:97:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m95 |\u001b[0m         # Test with empty rules - should return all as valid\n\u001b[1m\u001b[94m96 |\u001b[0m         result = assess_data_quality(mock_df, {})\n\u001b[1m\u001b[94m97 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m98 |\u001b[0m         assert result[\"total_rows\"] == 75\n\u001b[1m\u001b[94m99 |\u001b[0m         assert result[\"valid_rows\"] == 75\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:108:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m106 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m107 |\u001b[0m         mock_df.count.side_effect = Exception(\"DataFrame access failed\")\n\u001b[1m\u001b[94m108 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m109 |\u001b[0m         result = get_dataframe_info(mock_df)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:110:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m109 |\u001b[0m         result = get_dataframe_info(mock_df)\n\u001b[1m\u001b[94m110 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m111 |\u001b[0m         assert \"error\" in result\n\u001b[1m\u001b[94m112 |\u001b[0m         assert \"DataFrame access failed\" in result[\"error\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:123:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m121 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m122 |\u001b[0m         mock_df.columns = []\n\u001b[1m\u001b[94m123 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m124 |\u001b[0m         # Test with empty DataFrame\n\u001b[1m\u001b[94m125 |\u001b[0m         result = validate_dataframe_schema(mock_df, [])\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:127:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m125 |\u001b[0m         result = validate_dataframe_schema(mock_df, [])\n\u001b[1m\u001b[94m126 |\u001b[0m         assert result is True\n\u001b[1m\u001b[94m127 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m128 |\u001b[0m         # Test with empty DataFrame and non-empty expected columns\n\u001b[1m\u001b[94m129 |\u001b[0m         result = validate_dataframe_schema(mock_df, [\"col1\"])\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:136:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m134 |\u001b[0m         # Test with custom default value\n\u001b[1m\u001b[94m135 |\u001b[0m         assert safe_divide(10.0, 0.0, 999.0) == 999.0\n\u001b[1m\u001b[94m136 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m137 |\u001b[0m         # Test with negative numbers\n\u001b[1m\u001b[94m138 |\u001b[0m         assert safe_divide(-10.0, 2.0) == -5.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:141:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m139 |\u001b[0m         assert safe_divide(10.0, -2.0) == -5.0\n\u001b[1m\u001b[94m140 |\u001b[0m         assert safe_divide(-10.0, -2.0) == 5.0\n\u001b[1m\u001b[94m141 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m142 |\u001b[0m         # Test with very small numbers\n\u001b[1m\u001b[94m143 |\u001b[0m         assert safe_divide(0.0001, 0.0002) == 0.5\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:149:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m147 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m148 |\u001b[0m         mock_df.count.return_value = 25\n\u001b[1m\u001b[94m149 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m150 |\u001b[0m         # Mock apply_column_rules\n\u001b[1m\u001b[94m151 |\u001b[0m         mock_valid_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:159:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m157 |\u001b[0m         mock_stats.validation_rate = 100.0\n\u001b[1m\u001b[94m158 |\u001b[0m         mock_stats.duration = 0.1\n\u001b[1m\u001b[94m159 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m161 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:162:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m160 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m161 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n\u001b[1m\u001b[94m162 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m163 |\u001b[0m             # Test with empty rules\n\u001b[1m\u001b[94m164 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:167:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m165 |\u001b[0m                 mock_df, {}, \"gold\", \"test_step\"\n\u001b[1m\u001b[94m166 |\u001b[0m             )\n\u001b[1m\u001b[94m167 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m168 |\u001b[0m             assert stats.total_rows == 25\n\u001b[1m\u001b[94m169 |\u001b[0m             assert stats.valid_rows == 25\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`time` imported but unused\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:176:16\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m174 |\u001b[0m     def test_validation_performance_edge_cases(self) -> None:\n\u001b[1m\u001b[94m175 |\u001b[0m         \"\"\"Test validation performance edge cases.\"\"\"\n\u001b[1m\u001b[94m176 |\u001b[0m         import time\n    \u001b[1m\u001b[94m|\u001b[0m                \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m177 |\u001b[0m         \n\u001b[1m\u001b[94m178 |\u001b[0m         mock_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `time`\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:177:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m175 |\u001b[0m         \"\"\"Test validation performance edge cases.\"\"\"\n\u001b[1m\u001b[94m176 |\u001b[0m         import time\n\u001b[1m\u001b[94m177 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m178 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m179 |\u001b[0m         mock_df.count.return_value = 10\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:180:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m178 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m179 |\u001b[0m         mock_df.count.return_value = 10\n\u001b[1m\u001b[94m180 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m181 |\u001b[0m         mock_valid_df = Mock()\n\u001b[1m\u001b[94m182 |\u001b[0m         mock_invalid_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:188:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m186 |\u001b[0m         mock_stats.invalid_rows = 0\n\u001b[1m\u001b[94m187 |\u001b[0m         mock_stats.validation_rate = 100.0\n\u001b[1m\u001b[94m188 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m189 |\u001b[0m         # Test with very fast execution\n\u001b[1m\u001b[94m190 |\u001b[0m         with patch('sparkforge.validation.time.time', side_effect=[0.0, 0.001]), \\\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:193:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m191 |\u001b[0m              patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m192 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n\u001b[1m\u001b[94m193 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m194 |\u001b[0m             rules = {\"col1\": [\"col1 > 0\"]}\n\u001b[1m\u001b[94m195 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:198:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m196 |\u001b[0m                 mock_df, rules, \"bronze\", \"fast_test\"\n\u001b[1m\u001b[94m197 |\u001b[0m             )\n\u001b[1m\u001b[94m198 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m199 |\u001b[0m             assert stats.validation_rate == 100.0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:205:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m203 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m204 |\u001b[0m         mock_df.count.return_value = 5000000  # 5M rows\n\u001b[1m\u001b[94m205 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m206 |\u001b[0m         mock_valid_df = Mock()\n\u001b[1m\u001b[94m207 |\u001b[0m         mock_invalid_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:214:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m212 |\u001b[0m         mock_stats.validation_rate = 99.0\n\u001b[1m\u001b[94m213 |\u001b[0m         mock_stats.duration = 25.0\n\u001b[1m\u001b[94m214 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m215 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m216 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:217:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m215 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m216 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n\u001b[1m\u001b[94m217 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m218 |\u001b[0m             rules = {\n\u001b[1m\u001b[94m219 |\u001b[0m                 \"id\": [\"id IS NOT NULL\", \"id > 0\"],\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:225:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m223 |\u001b[0m                 \"updated_at\": [\"updated_at IS NOT NULL\"]\n\u001b[1m\u001b[94m224 |\u001b[0m             }\n\u001b[1m\u001b[94m225 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m226 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(\n\u001b[1m\u001b[94m227 |\u001b[0m                 mock_df, rules, \"bronze\", \"large_dataset\"\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:229:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m227 |\u001b[0m                 mock_df, rules, \"bronze\", \"large_dataset\"\n\u001b[1m\u001b[94m228 |\u001b[0m             )\n\u001b[1m\u001b[94m229 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m230 |\u001b[0m             assert stats.total_rows == 5000000\n\u001b[1m\u001b[94m231 |\u001b[0m             assert stats.valid_rows == 4950000\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:240:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m238 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m239 |\u001b[0m         mock_df.count.side_effect = Exception(\"Connection timeout\")\n\u001b[1m\u001b[94m240 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m241 |\u001b[0m         # Test error handling in assess_data_quality\n\u001b[1m\u001b[94m242 |\u001b[0m         result = assess_data_quality(mock_df, {})\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:245:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m243 |\u001b[0m         assert \"error\" in result\n\u001b[1m\u001b[94m244 |\u001b[0m         assert \"Connection timeout\" in result[\"error\"]\n\u001b[1m\u001b[94m245 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m246 |\u001b[0m         # Test error handling in get_dataframe_info\n\u001b[1m\u001b[94m247 |\u001b[0m         result = get_dataframe_info(mock_df)\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:256:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m254 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m255 |\u001b[0m         mock_df.count.return_value = 1  # Single row\n\u001b[1m\u001b[94m256 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m257 |\u001b[0m         mock_valid_df = Mock()\n\u001b[1m\u001b[94m258 |\u001b[0m         mock_invalid_df = Mock()\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:265:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m263 |\u001b[0m         mock_stats.validation_rate = 0.0\n\u001b[1m\u001b[94m264 |\u001b[0m         mock_stats.duration = 0.01\n\u001b[1m\u001b[94m265 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m267 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:268:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m266 |\u001b[0m         with patch('sparkforge.validation.apply_column_rules',\n\u001b[1m\u001b[94m267 |\u001b[0m                    return_value=(mock_valid_df, mock_invalid_df, mock_stats)):\n\u001b[1m\u001b[94m268 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m269 |\u001b[0m             rules = {\"col1\": [\"col1 > 1000\"]}  # Very strict rule\n\u001b[1m\u001b[94m270 |\u001b[0m             valid_df, invalid_df, stats = apply_validation_rules(\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_final_push.py:273:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m271 |\u001b[0m                 mock_df, rules, \"bronze\", \"boundary_test\"\n\u001b[1m\u001b[94m272 |\u001b[0m             )\n\u001b[1m\u001b[94m273 |\u001b[0m             \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m274 |\u001b[0m             assert stats.total_rows == 1\n\u001b[1m\u001b[94m275 |\u001b[0m             assert stats.valid_rows == 0\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:9:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m   \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m from typing import Any, Dict, List\n\u001b[1m\u001b[94m10 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from hypothesis import given, strategies as st, settings, example\n\u001b[1m\u001b[94m14 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m from sparkforge.validation import (\n\u001b[1m\u001b[94m16 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     safe_divide,\n\u001b[1m\u001b[94m17 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     validate_dataframe_schema,\n\u001b[1m\u001b[94m18 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     get_dataframe_info,\n\u001b[1m\u001b[94m19 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m     assess_data_quality,\n\u001b[1m\u001b[94m20 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|_^\u001b[0m\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Any` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:9:20\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                    \u001b[1m\u001b[91m^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`typing.Dict` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:9:25\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 7 |\u001b[0m \"\"\"\n\u001b[1m\u001b[94m 8 |\u001b[0m\n\u001b[1m\u001b[94m 9 |\u001b[0m from typing import Any, Dict, List\n   \u001b[1m\u001b[94m|\u001b[0m                         \u001b[1m\u001b[91m^^^^\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`pytest` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:12:8\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m10 |\u001b[0m from unittest.mock import Mock\n\u001b[1m\u001b[94m11 |\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^^^\u001b[0m\n\u001b[1m\u001b[94m13 |\u001b[0m from hypothesis import given, strategies as st, settings, example\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `pytest`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`hypothesis.example` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:13:59\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m12 |\u001b[0m import pytest\n\u001b[1m\u001b[94m13 |\u001b[0m from hypothesis import given, strategies as st, settings, example\n   \u001b[1m\u001b[94m|\u001b[0m                                                           \u001b[1m\u001b[91m^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m14 |\u001b[0m\n\u001b[1m\u001b[94m15 |\u001b[0m from sparkforge.validation import (\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `hypothesis.example`\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.validation.get_dataframe_info` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:18:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m16 |\u001b[0m     safe_divide,\n\u001b[1m\u001b[94m17 |\u001b[0m     validate_dataframe_schema,\n\u001b[1m\u001b[94m18 |\u001b[0m     get_dataframe_info,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m19 |\u001b[0m     assess_data_quality,\n\u001b[1m\u001b[94m20 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`sparkforge.validation.assess_data_quality` imported but unused\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:19:5\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m17 |\u001b[0m     validate_dataframe_schema,\n\u001b[1m\u001b[94m18 |\u001b[0m     get_dataframe_info,\n\u001b[1m\u001b[94m19 |\u001b[0m     assess_data_quality,\n   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m20 |\u001b[0m )\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:35:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m33 |\u001b[0m         \"\"\"Test safe_divide with generated float values.\"\"\"\n\u001b[1m\u001b[94m34 |\u001b[0m         result = safe_divide(numerator, denominator, default_value)\n\u001b[1m\u001b[94m35 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m36 |\u001b[0m         # Property: When denominator is non-zero, should return actual division\n\u001b[1m\u001b[94m37 |\u001b[0m         expected = numerator / denominator\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:48:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m46 |\u001b[0m         \"\"\"Test safe_divide with zero denominator.\"\"\"\n\u001b[1m\u001b[94m47 |\u001b[0m         result = safe_divide(numerator, 0.0, default_value)\n\u001b[1m\u001b[94m48 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m49 |\u001b[0m         # Property: When denominator is zero, should return default value\n\u001b[1m\u001b[94m50 |\u001b[0m         assert result == default_value\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:63:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m61 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m62 |\u001b[0m         mock_df.columns = actual_columns\n\u001b[1m\u001b[94m63 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m         result = validate_dataframe_schema(mock_df, expected_columns)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:65:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m64 |\u001b[0m         result = validate_dataframe_schema(mock_df, expected_columns)\n\u001b[1m\u001b[94m65 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m         # Property: Result should be boolean\n\u001b[1m\u001b[94m67 |\u001b[0m         assert isinstance(result, bool)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:68:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m66 |\u001b[0m         # Property: Result should be boolean\n\u001b[1m\u001b[94m67 |\u001b[0m         assert isinstance(result, bool)\n\u001b[1m\u001b[94m68 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m69 |\u001b[0m         # Property: Should be True if and only if all expected columns are in actual columns\n\u001b[1m\u001b[94m70 |\u001b[0m         expected_result = all(col in actual_columns for col in expected_columns)\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:72:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m70 |\u001b[0m         expected_result = all(col in actual_columns for col in expected_columns)\n\u001b[1m\u001b[94m71 |\u001b[0m         assert result == expected_result\n\u001b[1m\u001b[94m72 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m73 |\u001b[0m         # Property: If actual columns is empty, result should be True only if expected is also empty\n\u001b[1m\u001b[94m74 |\u001b[0m         if not actual_columns:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:88:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m86 |\u001b[0m         mock_df = Mock()\n\u001b[1m\u001b[94m87 |\u001b[0m         mock_df.columns = columns\n\u001b[1m\u001b[94m88 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m89 |\u001b[0m         # Test with empty expected columns\n\u001b[1m\u001b[94m90 |\u001b[0m         result = validate_dataframe_schema(mock_df, [])\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n  \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:92:1\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m90 |\u001b[0m         result = validate_dataframe_schema(mock_df, [])\n\u001b[1m\u001b[94m91 |\u001b[0m         assert result is True  # Empty expected columns should always pass\n\u001b[1m\u001b[94m92 |\u001b[0m         \n   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m93 |\u001b[0m         # Test with subset of actual columns\n\u001b[1m\u001b[94m94 |\u001b[0m         if columns:\n   \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\n\u001b[1m\u001b[91mW293 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mBlank line contains whitespace\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m tests/unit/test_validation_property_based.py:98:1\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m 96 |\u001b[0m             result = validate_dataframe_schema(mock_df, subset)\n\u001b[1m\u001b[94m 97 |\u001b[0m             assert result is True  # Subset should always pass\n\u001b[1m\u001b[94m 98 |\u001b[0m         \n    \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^\u001b[0m\n\u001b[1m\u001b[94m 99 |\u001b[0m         # Test with non-existent columns\n\u001b[1m\u001b[94m100 |\u001b[0m         non_existent = [\"non_existent_col\"]\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove whitespace from blank line\u001b[0m\n\nFound 1757 errors.\n[\u001b[36m*\u001b[0m] 1703 fixable with the `--fix` option (25 hidden fixes can be enabled with the `--unsafe-fixes` option).\n\u001b[1;33mwarning\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe top-level linter settings are deprecated in favour of their counterparts in the `lint` section. Please update the following options in `pyproject.toml`:\n  - 'ignore' -> 'lint.ignore'\n  - 'select' -> 'lint.select'\n  - 'per-file-ignores' -> 'lint.per-file-ignores'\u001b[0m\n"
  },
  "mypy": {
    "success": true,
    "output": "Success: no issues found in 20 source files\n"
  },
  "pylint": {
    "success": false,
    "output": "************* Module /Users/odosmatthews/Documents/coding/pipe/sparkforge/.pylintrc\n.pylintrc:1:0: E0015: Unrecognized option found: known-first-party (unrecognized-option)\n.pylintrc:1:0: R0022: Useless option value for '--disable', 'C0330' was removed from pylint, see https://github.com/pylint-dev/pylint/pull/3571. (useless-option-value)\n.pylintrc:1:0: R0022: Useless option value for '--disable', 'C0326' was removed from pylint, see https://github.com/pylint-dev/pylint/pull/3577. (useless-option-value)\n************* Module sparkforge\nsparkforge/__init__.py:4:0: C0301: Line too long (90/88) (line-too-long)\nsparkforge/__init__.py:49:0: C0301: Line too long (116/88) (line-too-long)\nsparkforge/__init__.py:148:0: C0301: Line too long (104/88) (line-too-long)\nsparkforge/__init__.py:157:0: W0622: Redefining built-in 'TimeoutError' (redefined-builtin)\nsparkforge/__init__.py:76:0: W0622: Redefining built-in 'SystemError' (redefined-builtin)\nsparkforge/__init__.py:223:4: E0603: Undefined variable name 'PipelineId' in __all__ (undefined-all-variable)\nsparkforge/__init__.py:224:4: E0603: Undefined variable name 'ExecutionId' in __all__ (undefined-all-variable)\nsparkforge/__init__.py:242:4: E0603: Undefined variable name 'ErrorCode' in __all__ (undefined-all-variable)\n************* Module sparkforge.logging\nsparkforge/logging.py:218:4: W0603: Using the global statement (global-statement)\nsparkforge/logging.py:226:4: W0603: Using the global statement (global-statement)\nsparkforge/logging.py:252:4: W0603: Using the global statement (global-statement)\n************* Module sparkforge.execution\nsparkforge/execution.py:10:0: C0301: Line too long (92/88) (line-too-long)\nsparkforge/execution.py:207:0: C0301: Line too long (94/88) (line-too-long)\nsparkforge/execution.py:88:0: R0902: Too many instance attributes (9/7) (too-many-instance-attributes)\nsparkforge/execution.py:107:0: R0902: Too many instance attributes (8/7) (too-many-instance-attributes)\nsparkforge/execution.py:202:24: E0601: Using variable 'output_df' before assignment (used-before-assignment)\nsparkforge/execution.py:328:19: W0718: Catching too general exception Exception (broad-exception-caught)\n************* Module sparkforge.models\nsparkforge/models.py:190:0: C0301: Line too long (103/88) (line-too-long)\nsparkforge/models.py:425:0: C0301: Line too long (92/88) (line-too-long)\nsparkforge/models.py:426:0: C0301: Line too long (100/88) (line-too-long)\nsparkforge/models.py:496:0: C0301: Line too long (110/88) (line-too-long)\nsparkforge/models.py:533:0: C0301: Line too long (106/88) (line-too-long)\nsparkforge/models.py:620:0: C0301: Line too long (107/88) (line-too-long)\nsparkforge/models.py:639:0: C0301: Line too long (93/88) (line-too-long)\nsparkforge/models.py:762:0: C0301: Line too long (118/88) (line-too-long)\nsparkforge/models.py:1:0: C0302: Too many lines in module (1315/1000) (too-many-lines)\nsparkforge/models.py:41:4: W0107: Unnecessary pass statement (unnecessary-pass)\nsparkforge/models.py:47:4: W0107: Unnecessary pass statement (unnecessary-pass)\nsparkforge/models.py:115:8: W2301: Unnecessary ellipsis constant (unnecessary-ellipsis)\nsparkforge/models.py:123:8: W2301: Unnecessary ellipsis constant (unnecessary-ellipsis)\nsparkforge/models.py:127:8: W2301: Unnecessary ellipsis constant (unnecessary-ellipsis)\nsparkforge/models.py:171:8: W0107: Unnecessary pass statement (unnecessary-pass)\nsparkforge/models.py:176:12: W0621: Redefining name 'field' from outer scope (line 24) (redefined-outer-name)\nsparkforge/models.py:176:21: E1101: Instance of 'BaseModel' has no '__dataclass_fields__' member (no-member)\nsparkforge/models.py:477:0: R0902: Too many instance attributes (8/7) (too-many-instance-attributes)\nsparkforge/models.py:732:0: R0902: Too many instance attributes (9/7) (too-many-instance-attributes)\nsparkforge/models.py:860:0: R0902: Too many instance attributes (10/7) (too-many-instance-attributes)\nsparkforge/models.py:966:0: R0902: Too many instance attributes (15/7) (too-many-instance-attributes)\nsparkforge/models.py:1278:8: W0707: Consider explicitly re-raising using 'raise PipelineConfigurationError(f'Invalid pipeline configuration: {e}') from e' (raise-missing-from)\nsparkforge/models.py:1286:8: W0707: Consider explicitly re-raising using 'raise PipelineConfigurationError(f'Invalid step configuration: {e}') from e' (raise-missing-from)\n************* Module sparkforge.types\nsparkforge/types.py:129:0: C0413: Import \"from typing import Protocol\" should be placed at the top of the module (wrong-import-position)\nsparkforge/types.py:137:8: W2301: Unnecessary ellipsis constant (unnecessary-ellipsis)\nsparkforge/types.py:145:8: W2301: Unnecessary ellipsis constant (unnecessary-ellipsis)\nsparkforge/types.py:16:4: W0611: Unused StructType imported from pyspark.sql.types (unused-import)\nsparkforge/types.py:16:4: W0611: Unused DataType imported from pyspark.sql.types (unused-import)\n************* Module sparkforge.errors\nsparkforge/errors.py:290:0: W0622: Redefining built-in 'SystemError' (redefined-builtin)\nsparkforge/errors.py:428:0: W0622: Redefining built-in 'TimeoutError' (redefined-builtin)\nsparkforge/errors.py:42:0: R0902: Too many instance attributes (8/7) (too-many-instance-attributes)\n************* Module sparkforge.performance\nsparkforge/performance.py:90:0: C0301: Line too long (96/88) (line-too-long)\nsparkforge/performance.py:41:4: R1705: Unnecessary \"elif\" after \"return\", remove the leading \"el\" from \"elif\" (no-else-return)\nsparkforge/performance.py:58:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/performance.py:63:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/performance.py:67:16: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/performance.py:81:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/performance.py:86:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/performance.py:89:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/performance.py:95:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/performance.py:119:4: C0415: Import outside toplevel (table_operations.write_append_table, table_operations.write_overwrite_table) (import-outside-toplevel)\nsparkforge/performance.py:138:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/performance.py:145:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\n************* Module sparkforge.table_operations\nsparkforge/table_operations.py:46:41: W0621: Redefining name 'fqn' from outer scope (line 26) (redefined-outer-name)\nsparkforge/table_operations.py:76:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/table_operations.py:80:8: W0707: Consider explicitly re-raising using 'raise TableOperationError(f'Failed to write table {fqn}: {e}') from e' (raise-missing-from)\nsparkforge/table_operations.py:84:38: W0621: Redefining name 'fqn' from outer scope (line 26) (redefined-outer-name)\nsparkforge/table_operations.py:110:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/table_operations.py:114:8: W0707: Consider explicitly re-raising using 'raise TableOperationError(f'Failed to write table {fqn}: {e}') from e' (raise-missing-from)\nsparkforge/table_operations.py:117:36: W0621: Redefining name 'fqn' from outer scope (line 26) (redefined-outer-name)\nsparkforge/table_operations.py:133:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/table_operations.py:136:8: W0707: Consider explicitly re-raising using 'raise TableOperationError(f'Table {fqn} does not exist: {e}') from e' (raise-missing-from)\nsparkforge/table_operations.py:138:8: W0707: Consider explicitly re-raising using 'raise TableOperationError(f'Failed to read table {fqn}: {e}') from e' (raise-missing-from)\nsparkforge/table_operations.py:141:38: W0621: Redefining name 'fqn' from outer scope (line 26) (redefined-outer-name)\nsparkforge/table_operations.py:158:11: W0718: Catching too general exception Exception (broad-exception-caught)\nsparkforge/table_operations.py:156:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/table_operations.py:159:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/table_operations.py:163:36: W0621: Redefining name 'fqn' from outer scope (line 26) (redefined-outer-name)\nsparkforge/table_operations.py:180:11: W0718: Catching too general exception Exception (broad-exception-caught)\nsparkforge/table_operations.py:177:12: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/table_operations.py:181:8: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\n************* Module sparkforge.validation\nsparkforge/validation.py:28:0: C0301: Line too long (101/88) (line-too-long)\nsparkforge/validation.py:30:0: C0301: Line too long (93/88) (line-too-long)\nsparkforge/validation.py:136:0: C0301: Line too long (94/88) (line-too-long)\nsparkforge/validation.py:143:0: C0301: Line too long (91/88) (line-too-long)\nsparkforge/validation.py:509:0: C0301: Line too long (103/88) (line-too-long)\nsparkforge/validation.py:529:0: C0301: Line too long (102/88) (line-too-long)\nsparkforge/validation.py:66:4: R1705: Unnecessary \"elif\" after \"return\", remove the leading \"el\" from \"elif\" (no-else-return)\nsparkforge/validation.py:224:4: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\nsparkforge/validation.py:278:11: W0718: Catching too general exception Exception (broad-exception-caught)\nsparkforge/validation.py:331:11: W0718: Catching too general exception Exception (broad-exception-caught)\nsparkforge/validation.py:314:8: R1705: Unnecessary \"else\" after \"return\", remove the \"else\" and de-indent the code inside it (no-else-return)\nsparkforge/validation.py:315:12: W0612: Unused variable 'valid_df' (unused-variable)\nsparkforge/validation.py:315:22: W0612: Unused variable 'invalid_df' (unused-variable)\nsparkforge/validation.py:352:8: W2301: Unnecessary ellipsis constant (unnecessary-ellipsis)\nsparkforge/validation.py:377:23: W0621: Redefining name 'logger' from outer scope (line 56) (redefined-outer-name)\nsparkforge/validation.py:451:19: W0718: Catching too general exception Exception (broad-exception-caught)\n************* Module sparkforge.pipeline.runner\nsparkforge/pipeline/runner.py:106:15: W0718: Catching too general exception Exception (broad-exception-caught)\nsparkforge/pipeline/runner.py:177:8: C0415: Import outside toplevel (models.PipelineMetrics) (import-outside-toplevel)\nsparkforge/pipeline/runner.py:208:8: C0415: Import outside toplevel (models.PipelineMetrics) (import-outside-toplevel)\n************* Module sparkforge.pipeline.models\nsparkforge/pipeline/models.py:46:0: R0902: Too many instance attributes (14/7) (too-many-instance-attributes)\n************* Module sparkforge.pipeline.builder\nsparkforge/pipeline/builder.py:39:0: C0301: Line too long (98/88) (line-too-long)\nsparkforge/pipeline/builder.py:41:0: C0301: Line too long (89/88) (line-too-long)\nsparkforge/pipeline/builder.py:42:0: C0301: Line too long (93/88) (line-too-long)\nsparkforge/pipeline/builder.py:53:0: C0301: Line too long (90/88) (line-too-long)\nsparkforge/pipeline/builder.py:82:0: C0301: Line too long (118/88) (line-too-long)\nsparkforge/pipeline/builder.py:93:0: C0301: Line too long (104/88) (line-too-long)\nsparkforge/pipeline/builder.py:105:0: C0301: Line too long (108/88) (line-too-long)\nsparkforge/pipeline/builder.py:132:0: C0301: Line too long (92/88) (line-too-long)\nsparkforge/pipeline/builder.py:141:0: C0301: Line too long (100/88) (line-too-long)\nsparkforge/pipeline/builder.py:179:0: C0301: Line too long (100/88) (line-too-long)\nsparkforge/pipeline/builder.py:264:0: C0301: Line too long (102/88) (line-too-long)\nsparkforge/pipeline/builder.py:265:0: C0301: Line too long (89/88) (line-too-long)\nsparkforge/pipeline/builder.py:267:0: C0301: Line too long (117/88) (line-too-long)\nsparkforge/pipeline/builder.py:287:0: C0301: Line too long (112/88) (line-too-long)\nsparkforge/pipeline/builder.py:367:0: C0301: Line too long (117/88) (line-too-long)\nsparkforge/pipeline/builder.py:478:0: C0301: Line too long (90/88) (line-too-long)\nsparkforge/pipeline/builder.py:485:0: C0301: Line too long (98/88) (line-too-long)\nsparkforge/pipeline/builder.py:487:0: C0301: Line too long (114/88) (line-too-long)\nsparkforge/pipeline/builder.py:494:0: C0301: Line too long (90/88) (line-too-long)\nsparkforge/pipeline/builder.py:497:0: C0301: Line too long (92/88) (line-too-long)\nsparkforge/pipeline/builder.py:498:0: C0301: Line too long (117/88) (line-too-long)\nsparkforge/pipeline/builder.py:504:0: C0301: Line too long (95/88) (line-too-long)\nsparkforge/pipeline/builder.py:505:0: C0301: Line too long (89/88) (line-too-long)\nsparkforge/pipeline/builder.py:527:0: C0301: Line too long (110/88) (line-too-long)\nsparkforge/pipeline/builder.py:656:0: C0301: Line too long (94/88) (line-too-long)\nsparkforge/pipeline/builder.py:659:0: C0301: Line too long (115/88) (line-too-long)\nsparkforge/pipeline/builder.py:665:0: C0301: Line too long (95/88) (line-too-long)\nsparkforge/pipeline/builder.py:666:0: C0301: Line too long (89/88) (line-too-long)\nsparkforge/pipeline/builder.py:688:0: C0301: Line too long (126/88) (line-too-long)\nsparkforge/pipeline/builder.py:796:0: C0301: Line too long (89/88) (line-too-long)\nsparkforge/pipeline/builder.py:924:0: C0301: Line too long (89/88) (line-too-long)\nsparkforge/pipeline/builder.py:975:0: C0301: Line too long (98/88) (line-too-long)\nsparkforge/pipeline/builder.py:999:0: C0301: Line too long (103/88) (line-too-long)\nsparkforge/pipeline/builder.py:1000:0: C0301: Line too long (102/88) (line-too-long)\nsparkforge/pipeline/builder.py:1100:0: C0301: Line too long (90/88) (line-too-long)\nsparkforge/pipeline/builder.py:1124:0: C0301: Line too long (114/88) (line-too-long)\nsparkforge/pipeline/builder.py:1149:0: C0301: Line too long (149/88) (line-too-long)\nsparkforge/pipeline/builder.py:1:0: C0302: Too many lines in module (1152/1000) (too-many-lines)\nsparkforge/pipeline/builder.py:37:0: R0902: Too many instance attributes (10/7) (too-many-instance-attributes)\nsparkforge/pipeline/builder.py:206:8: C0415: Import outside toplevel (models.ValidationThresholds) (import-outside-toplevel)\nsparkforge/pipeline/builder.py:932:8: C0415: Import outside toplevel (pyspark.sql.functions) (import-outside-toplevel)\nsparkforge/pipeline/builder.py:955:8: C0415: Import outside toplevel (pyspark.sql.functions) (import-outside-toplevel)\nsparkforge/pipeline/builder.py:978:8: C0415: Import outside toplevel (pyspark.sql.functions) (import-outside-toplevel)\nsparkforge/pipeline/builder.py:1003:8: C0415: Import outside toplevel (pyspark.sql.functions) (import-outside-toplevel)\nsparkforge/pipeline/builder.py:1067:12: W0707: Consider explicitly re-raising using 'raise StepError(f\"Schema '{schema}' does not exist or is not accessible: {str(e)}\", context={'step_name': 'schema_validation', 'step_type': 'validation'}, suggestions=[f'Create the schema first: CREATE SCHEMA IF NOT EXISTS {schema}', 'Check schema permissions', 'Verify schema name spelling']) from e' (raise-missing-from)\nsparkforge/pipeline/builder.py:1088:12: W0707: Consider explicitly re-raising using 'raise StepError(f\"Failed to create schema '{schema}': {str(e)}\", context={'step_name': 'schema_creation', 'step_type': 'validation'}, suggestions=['Check schema permissions', 'Verify schema name is valid', 'Check for naming conflicts']) from e' (raise-missing-from)\n************* Module sparkforge.dependencies.graph\nsparkforge/dependencies/graph.py:52:0: R0902: Too many instance attributes (8/7) (too-many-instance-attributes)\nsparkforge/dependencies/graph.py:185:24: W1203: Use lazy % formatting in logging functions (logging-fstring-interpolation)\n************* Module sparkforge.dependencies.analyzer\nsparkforge/dependencies/analyzer.py:182:0: C0301: Line too long (113/88) (line-too-long)\nsparkforge/dependencies/analyzer.py:191:0: C0301: Line too long (94/88) (line-too-long)\nsparkforge/dependencies/analyzer.py:209:0: C0301: Line too long (93/88) (line-too-long)\nsparkforge/dependencies/analyzer.py:87:8: C0415: Import outside toplevel (time) (import-outside-toplevel)\nsparkforge/dependencies/analyzer.py:147:12: W0707: Consider explicitly re-raising using 'raise DependencyError(f'Dependency analysis failed: {str(e)}') from e' (raise-missing-from)\nsparkforge/dependencies/analyzer.py:149:4: R0912: Too many branches (17/15) (too-many-branches)\nsparkforge/dependencies/analyzer.py:230:30: W0212: Access to a protected member _adjacency_list of a client class (protected-access)\nsparkforge/dependencies/analyzer.py:231:20: W0212: Access to a protected member _adjacency_list of a client class (protected-access)\nsparkforge/dependencies/analyzer.py:232:32: W0212: Access to a protected member _reverse_adjacency_list of a client class (protected-access)\nsparkforge/dependencies/analyzer.py:233:20: W0212: Access to a protected member _reverse_adjacency_list of a client class (protected-access)\nsparkforge/dependencies/analyzer.py:300:8: C0415: Import outside toplevel (hashlib) (import-outside-toplevel)\n************* Module sparkforge.dependencies.exceptions\nsparkforge/dependencies/exceptions.py:1:0: R0801: Similar lines in 2 files\n==sparkforge.execution:[68:78]\n==sparkforge.types:[61:71]\nclass StepStatus(Enum):\n    \"\"\"Step execution status.\"\"\"\n\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n\n (duplicate-code)\nsparkforge/dependencies/exceptions.py:1:0: R0801: Similar lines in 2 files\n==sparkforge.models:[797:802]\n==sparkforge.validation:[215:220]\n        step=step,\n        total_rows=total_rows,\n        valid_rows=valid_rows,\n        invalid_rows=invalid_rows,\n        validation_rate=validation_rate, (duplicate-code)\nsparkforge/dependencies/exceptions.py:1:0: R0801: Similar lines in 2 files\n==sparkforge.dependencies.graph:[42:51]\n==sparkforge.execution:[78:87]\nclass StepType(Enum):\n    \"\"\"Types of pipeline steps.\"\"\"\n\n    BRONZE = \"bronze\"\n    SILVER = \"silver\"\n    GOLD = \"gold\"\n\n\n@dataclass (duplicate-code)\nsparkforge/dependencies/exceptions.py:1:0: R0401: Cyclic import (sparkforge.performance -> sparkforge.table_operations) (cyclic-import)\n\n-----------------------------------\nYour code has been rated at 9.03/10\n\n"
  },
  "bandit": {
    "success": true,
    "output": "{\n  \"errors\": [],\n  \"generated_at\": \"2025-09-20T16:53:52Z\",\n  \"metrics\": {\n    \"_totals\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 4881,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/__init__.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 204,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/constants.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 27,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/dependencies/__init__.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 36,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/dependencies/analyzer.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 229,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/dependencies/exceptions.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 30,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/dependencies/graph.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 154,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/errors.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 387,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/execution.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 287,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/logging.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 194,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/models.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 1065,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/performance.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 128,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/pipeline/__init__.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 25,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/pipeline/builder.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 939,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/pipeline/models.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 92,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/pipeline/monitor.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 88,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/pipeline/runner.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 188,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/reporting.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 156,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/table_operations.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 134,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/types.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 82,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"sparkforge/validation.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 436,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    }\n  },\n  \"results\": []\n}[main]\tINFO\tprofile include tests: None\n[main]\tINFO\tprofile exclude tests: None\n[main]\tINFO\tcli include tests: None\n[main]\tINFO\tcli exclude tests: None\n"
  },
  "tests": {
    "success": false,
    "output": "============================= test session starts ==============================\nplatform darwin -- Python 3.8.18, pytest-8.3.5, pluggy-1.5.0 -- /Users/odosmatthews/Documents/coding/pipe/sparkforge/venv38/bin/python\ncachedir: .pytest_cache\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/Users/odosmatthews/Documents/coding/pipe/sparkforge/.hypothesis/examples'))\nrootdir: /Users/odosmatthews/Documents/coding/pipe/sparkforge\nconfigfile: pytest.ini\nplugins: cov-5.0.0, hypothesis-6.113.0, xdist-3.6.1\ncollecting ... collected 880 items\n\ntests/integration/test_execution_engine.py::TestExecutionMode::test_execution_mode_values PASSED [  0%]\ntests/integration/test_execution_engine.py::TestExecutionMode::test_execution_mode_enumeration PASSED [  0%]\ntests/integration/test_execution_engine.py::TestStepStatus::test_step_status_values PASSED [  0%]\ntests/integration/test_execution_engine.py::TestStepStatus::test_step_status_enumeration PASSED [  0%]\ntests/integration/test_execution_engine.py::TestStepType::test_step_type_values PASSED [  0%]\ntests/integration/test_execution_engine.py::TestStepType::test_step_type_enumeration PASSED [  0%]\ntests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_creation PASSED [  0%]\ntests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_with_all_fields PASSED [  0%]\ntests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_duration_calculation PASSED [  1%]\ntests/integration/test_execution_engine.py::TestStepExecutionResult::test_step_execution_result_no_duration_without_end_time PASSED [  1%]\ntests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_creation PASSED [  1%]\ntests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_with_all_fields PASSED [  1%]\ntests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_duration_calculation PASSED [  1%]\ntests/integration/test_execution_engine.py::TestExecutionResult::test_execution_result_steps_initialization PASSED [  1%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execution_engine_initialization_with_logger PASSED [  1%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execution_engine_initialization_without_logger PASSED [  1%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_bronze_success PASSED [  1%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_success PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_success PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_unknown_type PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_bronze_without_source_path PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_bronze_step_not_in_context PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_bronze_step_fallback_logging PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_without_dependencies PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_missing_dependency PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_silver_without_transform PASSED [  2%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_without_dependencies PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_missing_dependency PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_gold_without_transform PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_validation_only_mode PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_step_exception_handling PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_success PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_failure PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_with_different_step_types PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_with_max_workers PASSED [  3%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_empty_steps PASSED [  4%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_execute_pipeline_step_failure_continues PASSED [  4%]\ntests/integration/test_execution_engine.py::TestExecutionEngine::test_backward_compatibility_aliases PASSED [  4%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_engine_initialization PASSED [  4%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_engine_without_logger PASSED [  4%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_type_detection PASSED [  4%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_context_creation PASSED [  4%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_execution_result_creation PASSED [  4%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngine::test_execution_mode_enum PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_status_enum PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngine::test_step_type_enum PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_bronze_step_validation PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_silver_step_validation PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_gold_step_validation PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_pipeline_config_validation PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_execution_engine_with_mock_steps PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineIntegration::test_execution_engine_error_handling PASSED [  5%]\ntests/integration/test_execution_engine_new.py::TestExecutionEnginePerformance::test_execution_engine_memory_usage PASSED [  6%]\ntests/integration/test_execution_engine_new.py::TestExecutionEnginePerformance::test_execution_engine_concurrent_creation PASSED [  6%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_logging_initialization PASSED [  6%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_default_logging PASSED [  6%]\ntests/integration/test_execution_engine_new.py::TestExecutionEngineLogging::test_execution_engine_logging_methods PASSED [  6%]\ntests/integration/test_pipeline_builder.py::TestPipelineMode::test_pipeline_mode_values PASSED [  6%]\ntests/integration/test_pipeline_builder.py::TestPipelineStatus::test_pipeline_status_values PASSED [  6%]\ntests/integration/test_pipeline_builder.py::TestPipelineMetrics::test_pipeline_metrics_creation PASSED [  6%]\ntests/integration/test_pipeline_builder.py::TestPipelineReport::test_pipeline_report_creation PASSED [  6%]\ntests/integration/test_pipeline_builder.py::TestPipelineReport::test_pipeline_report_to_dict PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_add_gold_transform PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_add_silver_transform PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_builder_creation PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_builder_creation_with_custom_params PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_to_pipeline_success PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_to_pipeline_validation_error PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_errors PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_validate_pipeline_success PASSED [  7%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_with_bronze_rules PASSED [  8%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilder::test_with_silver_rules PASSED [  8%]\ntests/integration/test_pipeline_builder.py::TestPipelineBuilderIntegration::test_complex_pipeline_construction PASSED [  8%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_creation PASSED [  8%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_bronze_step_creation PASSED [  8%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_silver_step_creation PASSED [  8%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_gold_step_creation PASSED [  8%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_validation PASSED [  8%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_builder_to_pipeline PASSED [  8%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_execution_with_mock_data PASSED [  9%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_configuration_creation PASSED [  9%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_execution_engine_with_pipeline_config PASSED [  9%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_step_execution_with_real_data PASSED [  9%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_step_validation_with_real_data PASSED [  9%]\ntests/integration/test_pipeline_execution.py::TestPipelineExecutionFlow::test_pipeline_execution_flow_integration PASSED [  9%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_initialization_with_logger PASSED [  9%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_initialization_without_logger PASSED [  9%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution_with_empty_steps PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_start_execution_with_mocked_time PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_success PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_failure PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_without_active_report PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_update_step_execution_failure_without_error_message PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_success PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_failure PASSED [ 10%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_without_active_report PASSED [ 11%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_with_zero_steps PASSED [ 11%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_with_mocked_time PASSED [ 11%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_finish_execution_mixed_results PASSED [ 11%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_pipeline_monitor_alias PASSED [ 11%]\ntests/integration/test_pipeline_monitor.py::TestSimplePipelineMonitor::test_monitor_logging_calls PASSED [ 11%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_all_parameters PASSED [ 11%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_minimal_parameters PASSED [ 11%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_runner_initialization_with_none_steps PASSED [ 11%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_initial PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_incremental PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_full_refresh PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_validation_only PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_convert_mode_unknown PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_success PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_with_bronze_sources PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_without_bronze_sources PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_pipeline_execution_failure PASSED [ 12%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_initial_load_with_steps PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_initial_load_without_steps PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_incremental PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_full_refresh PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_run_validation_only PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_success PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_failure PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_without_end_time PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_error_report PASSED [ 13%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_pipeline_runner_alias PASSED [ 14%]\ntests/integration/test_pipeline_runner.py::TestSimplePipelineRunner::test_create_pipeline_report_with_empty_steps PASSED [ 14%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_bronze_step_execution_flow PASSED [ 14%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_silver_step_execution_flow PASSED [ 14%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_gold_step_execution_flow PASSED [ 14%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_validation_flow PASSED [ 14%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_type_detection_flow PASSED [ 14%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_context_flow PASSED [ 14%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_result_flow PASSED [ 15%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_mode_flow PASSED [ 15%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_status_flow PASSED [ 15%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_pipeline_configuration_flow PASSED [ 15%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_execution_engine_initialization_flow PASSED [ 15%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_error_handling_flow PASSED [ 15%]\ntests/integration/test_step_execution.py::TestStepExecutionFlow::test_step_execution_with_mock_data PASSED [ 15%]\ntests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_not_null_rule PASSED [ 15%]\ntests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_positive_rule PASSED [ 15%]\ntests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_non_negative_rule PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_non_zero_rule PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestConvertRuleToExpression::test_unknown_rule PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_string_rules_conversion PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_mixed_rules_conversion PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestConvertRulesToExpressions::test_empty_rules PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestAndAllRules::test_empty_rules PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestAndAllRules::test_single_column_single_rule PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestAndAllRules::test_single_column_multiple_rules PASSED [ 16%]\ntests/integration/test_validation_integration.py::TestAndAllRules::test_multiple_columns PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestAndAllRules::test_complex_rules PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestAndAllRules::test_empty_rules_returns_true PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestAndAllRules::test_no_valid_expressions_returns_true PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_valid_schema PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_missing_columns PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_extra_columns PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_empty_expected_columns PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestValidateDataframeSchema::test_empty_dataframe PASSED [ 17%]\ntests/integration/test_validation_integration.py::TestSafeDivide::test_normal_division PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestSafeDivide::test_division_by_zero PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestSafeDivide::test_division_by_zero_custom_default PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestSafeDivide::test_float_division PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestSafeDivide::test_negative_numbers PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestSafeDivide::test_zero_numerator PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestGetDataframeInfo::test_basic_info PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestGetDataframeInfo::test_empty_dataframe PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestGetDataframeInfo::test_error_handling PASSED [ 18%]\ntests/integration/test_validation_integration.py::TestAssessDataQuality::test_empty_dataframe PASSED [ 19%]\ntests/integration/test_validation_integration.py::TestAssessDataQuality::test_dataframe_without_rules PASSED [ 19%]\ntests/integration/test_validation_integration.py::TestAssessDataQuality::test_dataframe_with_rules PASSED [ 19%]\ntests/integration/test_validation_integration.py::TestAssessDataQuality::test_error_handling PASSED [ 19%]\ntests/integration/test_validation_integration.py::TestValidationResult::test_validation_result_creation PASSED [ 19%]\ntests/integration/test_validation_integration.py::TestValidationResult::test_validation_result_defaults PASSED [ 19%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_unified_validator_initialization PASSED [ 19%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_unified_validator_with_custom_logger PASSED [ 19%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_add_validator PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_step_with_custom_validators PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_step_validator_exception PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_pipeline_config_validation PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_pipeline_success PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_bronze_steps PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_silver_steps PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_gold_steps PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestUnifiedValidator::test_validate_dependencies PASSED [ 20%]\ntests/integration/test_validation_integration.py::TestApplyValidationRules::test_apply_validation_rules_deprecated PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_performance_profiler_integration PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_caching_strategies_integration PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_performance_monitoring_integration PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_memory_optimization_integration PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_performance_benchmarking_integration PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_load_testing_integration PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_caching_decorators_integration PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_memory_monitoring_decorator_integration PASSED [ 21%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_performance_profiler_decorator_integration PASSED [ 22%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_comprehensive_performance_workflow PASSED [ 22%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_performance_optimization_workflow PASSED [ 22%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_performance_reporting_integration PASSED [ 22%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_error_handling_integration FAILED [ 22%]\ntests/performance/test_performance_integration.py::TestPerformanceIntegration::test_concurrent_performance_operations PASSED [ 22%]\ntests/performance/test_performance_integration.py::TestPerformanceMarkers::test_performance_marker_works PASSED [ 22%]\ntests/performance/test_performance_integration.py::TestPerformanceMarkers::test_slow_performance_test PASSED [ 22%]\ntests/performance/test_performance_integration.py::test_performance_cicd_integration PASSED [ 22%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_test_suite_integration FAILED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_vulnerability_scanner_integration PASSED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_compliance_checker_integration PASSED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_monitor_integration PASSED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_components_workflow PASSED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_reporting_integration PASSED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_alerting_integration PASSED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_metrics_integration FAILED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_thresholds_integration PASSED [ 23%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_configuration_integration PASSED [ 24%]\ntests/security/test_security_integration.py::TestSecurityIntegration::test_security_performance_integration PASSED [ 24%]\ntests/security/test_security_integration.py::TestSecurityMarkers::test_security_marker_works PASSED [ 24%]\ntests/security/test_security_integration.py::TestSecurityMarkers::test_slow_security_test PASSED [ 24%]\ntests/security/test_security_integration.py::test_security_cicd_integration FAILED [ 24%]\ntests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_auto_infer_single_bronze_step FAILED [ 24%]\ntests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_auto_infer_multiple_bronze_steps FAILED [ 24%]\ntests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_explicit_source_bronze_still_works FAILED [ 24%]\ntests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_no_bronze_steps_raises_error FAILED [ 25%]\ntests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_invalid_source_bronze_raises_error FAILED [ 25%]\ntests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_logging_auto_inference FAILED [ 25%]\ntests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_chaining_works_with_auto_inference FAILED [ 25%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_bronze_step_without_incremental_col ERROR [ 25%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_bronze_step_with_incremental_col FAILED [ 25%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_silver_step_creation ERROR [ 25%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_gold_step_creation ERROR [ 25%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_validation ERROR [ 25%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_creation ERROR [ 26%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_dataframe_operations ERROR [ 26%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_execution_engine_initialization PASSED [ 26%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_type_detection FAILED [ 26%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_configuration PASSED [ 26%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_execution_mode_enum PASSED [ 26%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_status_enum PASSED [ 26%]\ntests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_type_enum PASSED [ 26%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_bronze_step_creation ERROR [ 26%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_silver_step_creation ERROR [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_gold_step_creation ERROR [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_builder_validation ERROR [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_creation ERROR [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_dataframe_operations ERROR [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_execution_engine_initialization PASSED [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_type_detection ERROR [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_configuration PASSED [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_execution_mode_enum PASSED [ 27%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_status_enum PASSED [ 28%]\ntests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_type_enum PASSED [ 28%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_acid_transactions FAILED [ 28%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_schema_evolution FAILED [ 28%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_time_travel FAILED [ 28%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_merge_operations FAILED [ 28%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_optimization FAILED [ 28%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_history_and_metadata FAILED [ 28%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_concurrent_writes FAILED [ 28%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_performance_characteristics FAILED [ 29%]\ntests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_data_quality_constraints FAILED [ 29%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_source_silvers ERROR [ 29%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_source_silvers_explicit ERROR [ 29%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_no_silver_steps_error ERROR [ 29%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_development ERROR [ 29%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_production ERROR [ 29%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_testing ERROR [ 29%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_not_null_rules ERROR [ 30%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_positive_number_rules ERROR [ 30%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_string_not_empty_rules ERROR [ 30%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_timestamp_rules ERROR [ 30%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_detect_timestamp_columns ERROR [ 30%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_detect_timestamp_columns_list ERROR [ 30%]\ntests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_chaining_with_auto_inference ERROR [ 30%]\ntests/system/test_logger.py::TestPipelineLogger::test_basic_logging_methods PASSED [ 30%]\ntests/system/test_logger.py::TestPipelineLogger::test_log_level_management PASSED [ 30%]\ntests/system/test_logger.py::TestPipelineLogger::test_logger_creation PASSED [ 31%]\ntests/system/test_logger.py::TestPipelineLogger::test_logger_with_file PASSED [ 31%]\ntests/system/test_logger.py::TestGlobalLogger::test_get_global_logger PASSED [ 31%]\ntests/system/test_logger.py::TestGlobalLogger::test_reset_global_logger PASSED [ 31%]\ntests/system/test_logger.py::TestGlobalLogger::test_set_global_logger PASSED [ 31%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_bronze_rules_with_schema ERROR [ 31%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_bronze_rules_without_schema ERROR [ 31%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_silver_rules_with_schema ERROR [ 31%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_silver_transform_with_schema ERROR [ 31%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_gold_transform_with_schema ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_success ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_failure ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_get_effective_schema ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_creation ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_creation_failure ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_cross_schema_pipeline ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_mixed_schema_usage ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_integration ERROR [ 32%]\ntests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_backward_compatibility ERROR [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_dataframe_operations ERROR [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_transformations ERROR [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_validation_rules ERROR [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_data_quality ERROR [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_metadata_operations ERROR [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_performance FAILED [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_error_handling ERROR [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_schema_operations ERROR [ 33%]\ntests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_joins FAILED [ 34%]\ntests/system/test_system_exceptions.py::TestValidationError::test_validation_error_creation PASSED [ 34%]\ntests/system/test_system_exceptions.py::TestValidationError::test_validation_error_inheritance PASSED [ 34%]\ntests/system/test_system_exceptions.py::TestTableOperationError::test_table_operation_error_creation PASSED [ 34%]\ntests/system/test_system_exceptions.py::TestTableOperationError::test_table_operation_error_inheritance PASSED [ 34%]\ntests/system/test_system_exceptions.py::TestPerformanceError::test_performance_error_creation PASSED [ 34%]\ntests/system/test_system_exceptions.py::TestPerformanceError::test_performance_error_inheritance PASSED [ 34%]\ntests/system/test_system_exceptions.py::TestPipelineValidationError::test_pipeline_validation_error_creation PASSED [ 34%]\ntests/system/test_system_exceptions.py::TestPipelineValidationError::test_pipeline_validation_error_inheritance PASSED [ 35%]\ntests/system/test_system_exceptions.py::TestExecutionError::test_execution_error_creation PASSED [ 35%]\ntests/system/test_system_exceptions.py::TestExecutionError::test_execution_error_inheritance PASSED [ 35%]\ntests/system/test_system_exceptions.py::TestConfigurationError::test_configuration_error_creation PASSED [ 35%]\ntests/system/test_system_exceptions.py::TestConfigurationError::test_configuration_error_inheritance PASSED [ 35%]\ntests/system/test_system_exceptions.py::TestExceptionChaining::test_exception_with_cause PASSED [ 35%]\ntests/system/test_system_exceptions.py::TestExceptionChaining::test_exception_context PASSED [ 35%]\ntests/system/test_utils.py::TestDataValidation::test_and_all_rules ERROR [ 35%]\ntests/system/test_utils.py::TestDataValidation::test_and_all_rules_empty ERROR [ 35%]\ntests/system/test_utils.py::TestDataValidation::test_apply_column_rules ERROR [ 36%]\ntests/system/test_utils.py::TestDataValidation::test_apply_column_rules_none_rules ERROR [ 36%]\ntests/system/test_utils.py::TestDataValidation::test_assess_data_quality ERROR [ 36%]\ntests/system/test_utils.py::TestDataValidation::test_get_dataframe_info ERROR [ 36%]\ntests/system/test_utils.py::TestDataValidation::test_validate_dataframe_schema ERROR [ 36%]\ntests/system/test_utils.py::TestDataTransformationUtilities::test_basic_dataframe_operations ERROR [ 36%]\ntests/system/test_utils.py::TestDataTransformationUtilities::test_dataframe_filtering ERROR [ 36%]\ntests/system/test_utils.py::TestFactoryFunctions::test_create_validation_dict PASSED [ 36%]\ntests/system/test_utils.py::TestFactoryFunctions::test_create_write_dict PASSED [ 36%]\ntests/system/test_utils.py::TestPerformanceWithRealData::test_large_dataset_validation FAILED [ 37%]\ntests/system/test_utils.py::TestPerformanceWithRealData::test_complex_transformations FAILED [ 37%]\ntests/unit/dependencies/test_analyzer.py::TestAnalysisStrategy::test_analysis_strategy_values PASSED [ 37%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalysisResult::test_dependency_analysis_result_creation PASSED [ 37%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_dependency_analyzer_creation_default PASSED [ 37%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_dependency_analyzer_creation_custom PASSED [ 37%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_empty PASSED [ 37%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_bronze_only PASSED [ 37%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_with_bronze PASSED [ 37%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_gold_with_silver PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_missing_bronze_dependency PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_warning_scenarios PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_depends_on_warning PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_missing_silver_dependency PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_force_refresh PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_cached PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_exception PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_empty PASSED [ 38%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_bronze_steps PASSED [ 39%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_silver_steps PASSED [ 39%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_build_dependency_graph_gold_steps PASSED [ 39%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_resolve_cycles PASSED [ 39%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_detect_conflicts PASSED [ 39%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_cycle_warning PASSED [ 39%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_conflict_warning PASSED [ 39%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_analyze_dependencies_silver_valid_dependency PASSED [ 39%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_detect_conflicts_duplicate_names PASSED [ 40%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_no_issues PASSED [ 40%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_with_cycles PASSED [ 40%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_with_conflicts PASSED [ 40%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_high_dependencies PASSED [ 40%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_generate_recommendations_large_pipeline PASSED [ 40%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_create_cache_key PASSED [ 40%]\ntests/unit/dependencies/test_analyzer.py::TestDependencyAnalyzer::test_clear_cache PASSED [ 40%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestAnalysisStrategy::test_strategy_values PASSED [ 40%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestExecutionMode::test_execution_mode_values PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_analysis_result_creation PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_get_parallelization_ratio PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalysisResult::test_get_total_execution_time PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_basic PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_caching PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_force_refresh PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_with_bronze_steps PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyze_dependencies_with_gold_steps PASSED [ 41%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyzer_creation PASSED [ 42%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzer::test_analyzer_creation_with_custom_params PASSED [ 42%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_complex_pipeline_analysis PASSED [ 42%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_different_strategies_comparison PASSED [ 42%]\ntests/unit/dependencies/test_analyzer_comprehensive.py::TestDependencyAnalyzerIntegration::test_error_handling PASSED [ 42%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_basic PASSED [ 42%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_step_name PASSED [ 42%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_inheritance PASSED [ 42%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_empty_message PASSED [ 42%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyError::test_dependency_error_with_none_step_name PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_basic PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_step_name PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_analysis_step PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_with_both_steps PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_inheritance PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyAnalysisError::test_dependency_analysis_error_constructor_parameters PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_basic PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_with_step_name PASSED [ 43%]\ntests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_empty_cycle PASSED [ 44%]\ntests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_single_step_cycle PASSED [ 44%]\ntests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_inheritance PASSED [ 44%]\ntests/unit/dependencies/test_exceptions.py::TestCircularDependencyError::test_circular_dependency_error_cycle_immutability PASSED [ 44%]\ntests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_basic PASSED [ 44%]\ntests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_with_step_name PASSED [ 44%]\ntests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_empty_list PASSED [ 44%]\ntests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_single_dependency PASSED [ 44%]\ntests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_inheritance PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestInvalidDependencyError::test_invalid_dependency_error_list_immutability PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_basic PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_with_step_name PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_empty_list PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_single_step PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_inheritance PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestDependencyConflictError::test_dependency_conflict_error_list_immutability PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_dependency_error_chaining PASSED [ 45%]\ntests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_circular_dependency_error_chaining PASSED [ 46%]\ntests/unit/dependencies/test_exceptions.py::TestExceptionChaining::test_exception_attributes_preserved PASSED [ 46%]\ntests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_error_str PASSED [ 46%]\ntests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_analysis_error_str PASSED [ 46%]\ntests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_circular_dependency_error_str PASSED [ 46%]\ntests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_invalid_dependency_error_str PASSED [ 46%]\ntests/unit/dependencies/test_exceptions.py::TestExceptionStringRepresentation::test_dependency_conflict_error_str PASSED [ 46%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_add_dependency_missing_nodes PASSED [ 46%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_dependencies_missing_node PASSED [ 46%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_dependents_missing_node PASSED [ 47%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_detect_cycles PASSED [ 47%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_execution_groups_missing_dependency PASSED [ 47%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_validate_cycles PASSED [ 47%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_validate_missing_dependencies PASSED [ 47%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_execution_groups PASSED [ 47%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_stats PASSED [ 47%]\ntests/unit/dependencies/test_graph.py::TestDependencyGraph::test_get_parallel_candidates PASSED [ 47%]\ntests/unit/test_constants.py::TestConstantsValues::test_default_memory_limits PASSED [ 47%]\ntests/unit/test_constants.py::TestConstantsValues::test_error_constants PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsValues::test_file_size_constants PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsValues::test_logging_constants PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsValues::test_memory_constants PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsValues::test_performance_constants PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsValues::test_performance_monitoring_constants PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsValues::test_schema_constants PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsValues::test_timeout_constants PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsValues::test_validation_thresholds PASSED [ 48%]\ntests/unit/test_constants.py::TestConstantsUsage::test_constants_are_immutable PASSED [ 49%]\ntests/unit/test_constants.py::TestConstantsUsage::test_parallel_execution_uses_constants PASSED [ 49%]\ntests/unit/test_constants.py::TestConstantsUsage::test_performance_cache_uses_constants PASSED [ 49%]\ntests/unit/test_constants.py::TestConstantsCompleteness::test_all_required_constants_exist PASSED [ 49%]\ntests/unit/test_constants.py::TestConstantsCompleteness::test_constants_have_appropriate_types PASSED [ 49%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_value_type_validation PASSED [ 49%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_type_validation PASSED [ 49%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_error_suggestions_type_validation PASSED [ 49%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_base_error_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_configuration_error_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_data_error_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_pipeline_error_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_step_error_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_execution_error_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_system_error_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_performance_error_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_error_serialization_explicit_types PASSED [ 50%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_error_context_manipulation_explicit_types PASSED [ 51%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_error_suggestion_manipulation_explicit_types PASSED [ 51%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_no_any_types_in_error_classes PASSED [ 51%]\ntests/unit/test_errors.py::TestErrorTypeSafety::test_no_args_kwargs_in_error_constructors PASSED [ 51%]\ntests/unit/test_errors.py::TestErrorBackwardCompatibility::test_existing_error_usage_still_works PASSED [ 51%]\ntests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_inheritance_still_works PASSED [ 51%]\ntests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_context_manipulation_still_works PASSED [ 51%]\ntests/unit/test_errors.py::TestErrorBackwardCompatibility::test_error_serialization_still_works PASSED [ 51%]\ntests/unit/test_errors.py::TestErrorBackwardCompatibility::test_resource_error_creation PASSED [ 51%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_start PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_start_custom_mode PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_end_success PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_pipeline_end_failure PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_performance_metric PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_performance_metric_custom_unit PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_format_message_with_kwargs PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_format_message_without_kwargs PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_context_manager PASSED [ 52%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_context_manager_with_existing_extra PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_start PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_start_different_stage PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_complete PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_complete_no_rows PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_failed PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_step_failed_no_duration PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_validation_passed PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_validation_failed PASSED [ 53%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_start PASSED [ 54%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_end PASSED [ 54%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_end_nonexistent PASSED [ 54%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_context_manager PASSED [ 54%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_timer_context_manager_exception PASSED [ 54%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_console_only PASSED [ 54%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_with_file PASSED [ 54%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_setup_handlers_verbose_false PASSED [ 54%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_custom_name PASSED [ 55%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_custom_level PASSED [ 55%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_logger_creation_with_file PASSED [ 55%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_basic_logging_methods PASSED [ 55%]\ntests/unit/test_logging.py::TestPipelineLoggerComprehensive::test_set_level PASSED [ 55%]\ntests/unit/test_logging.py::TestGlobalLoggerFunctions::test_get_logger_default PASSED [ 55%]\ntests/unit/test_logging.py::TestGlobalLoggerFunctions::test_set_logger PASSED [ 55%]\ntests/unit/test_logging.py::TestGlobalLoggerFunctions::test_create_logger_default PASSED [ 55%]\ntests/unit/test_logging.py::TestGlobalLoggerFunctions::test_create_logger_custom PASSED [ 55%]\ntests/unit/test_logging.py::TestGlobalLoggerFunctions::test_create_logger_with_file PASSED [ 56%]\ntests/unit/test_logging.py::TestGlobalLoggerFunctions::test_get_global_logger PASSED [ 56%]\ntests/unit/test_logging.py::TestGlobalLoggerFunctions::test_set_global_logger PASSED [ 56%]\ntests/unit/test_logging.py::TestGlobalLoggerFunctions::test_reset_global_logger PASSED [ 56%]\ntests/unit/test_logging.py::TestTimerContextManager::test_timer_context_manager_success PASSED [ 56%]\ntests/unit/test_logging.py::TestTimerContextManager::test_timer_context_manager_exception PASSED [ 56%]\ntests/unit/test_models.py::TestExceptions::test_pipeline_configuration_error PASSED [ 56%]\ntests/unit/test_models.py::TestExceptions::test_pipeline_execution_error PASSED [ 56%]\ntests/unit/test_models.py::TestEnums::test_pipeline_phase_enum PASSED    [ 56%]\ntests/unit/test_models.py::TestEnums::test_execution_mode_enum PASSED    [ 57%]\ntests/unit/test_models.py::TestEnums::test_write_mode_enum PASSED        [ 57%]\ntests/unit/test_models.py::TestEnums::test_validation_result_enum PASSED [ 57%]\ntests/unit/test_models.py::TestTypeDefinitions::test_model_value_types PASSED [ 57%]\ntests/unit/test_models.py::TestTypeDefinitions::test_column_rule_types PASSED [ 57%]\ntests/unit/test_models.py::TestTypeDefinitions::test_resource_value_types PASSED [ 57%]\ntests/unit/test_models.py::TestBaseModel::test_base_model_abstract PASSED [ 57%]\ntests/unit/test_models.py::TestBaseModel::test_base_model_validation_abstract PASSED [ 57%]\ntests/unit/test_models.py::TestBronzeStep::test_bronze_step_creation FAILED [ 57%]\ntests/unit/test_models.py::TestBronzeStep::test_bronze_step_creation_minimal FAILED [ 58%]\ntests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_success FAILED [ 58%]\ntests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_empty_name FAILED [ 58%]\ntests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_none_name FAILED [ 58%]\ntests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_invalid_rules_type PASSED [ 58%]\ntests/unit/test_models.py::TestBronzeStep::test_bronze_step_has_incremental_capability FAILED [ 58%]\ntests/unit/test_models.py::TestSilverStep::test_silver_step_creation FAILED [ 58%]\ntests/unit/test_models.py::TestSilverStep::test_silver_step_creation_minimal PASSED [ 58%]\ntests/unit/test_models.py::TestSilverStep::test_silver_step_validation_success PASSED [ 58%]\ntests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_name FAILED [ 59%]\ntests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_source_bronze FAILED [ 59%]\ntests/unit/test_models.py::TestSilverStep::test_silver_step_validation_none_transform FAILED [ 59%]\ntests/unit/test_models.py::TestSilverStep::test_silver_step_validation_invalid_rules_type PASSED [ 59%]\ntests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_table_name FAILED [ 59%]\ntests/unit/test_models.py::TestGoldStep::test_gold_step_creation FAILED  [ 59%]\ntests/unit/test_models.py::TestGoldStep::test_gold_step_creation_minimal FAILED [ 59%]\ntests/unit/test_models.py::TestGoldStep::test_gold_step_validation_success FAILED [ 59%]\ntests/unit/test_models.py::TestGoldStep::test_gold_step_validation_empty_name FAILED [ 60%]\ntests/unit/test_models.py::TestGoldStep::test_gold_step_validation_none_transform FAILED [ 60%]\ntests/unit/test_models.py::TestGoldStep::test_gold_step_validation_invalid_rules_type PASSED [ 60%]\ntests/unit/test_models.py::TestGoldStep::test_gold_step_validation_empty_table_name FAILED [ 60%]\ntests/unit/test_models.py::TestStepResult::test_step_result_creation PASSED [ 60%]\ntests/unit/test_models.py::TestStepResult::test_step_result_is_high_quality PASSED [ 60%]\ntests/unit/test_models.py::TestStepResult::test_step_result_create_success PASSED [ 60%]\ntests/unit/test_models.py::TestStepResult::test_step_result_create_failure PASSED [ 60%]\ntests/unit/test_models.py::TestPipelineMetrics::test_pipeline_metrics_creation_default PASSED [ 60%]\ntests/unit/test_models.py::TestPipelineMetrics::test_pipeline_metrics_creation_custom PASSED [ 61%]\ntests/unit/test_models.py::TestPipelineConfig::test_pipeline_config_creation_default PASSED [ 61%]\ntests/unit/test_models.py::TestPipelineConfig::test_pipeline_config_creation_custom PASSED [ 61%]\ntests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_creation PASSED [ 61%]\ntests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_success PASSED [ 61%]\ntests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_empty_step_name PASSED [ 61%]\ntests/unit/test_models.py::TestSilverDependencyInfo::test_silver_dependency_info_validation_empty_source_bronze PASSED [ 61%]\ntests/unit/test_models.py::TestParallelConfig::test_parallel_config_creation PASSED [ 61%]\ntests/unit/test_models.py::TestParallelConfig::test_parallel_config_creation_minimal PASSED [ 61%]\ntests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_success PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_negative_max_workers PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_negative_worker_timeout PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_base_model_to_dict_with_nested_objects PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_base_model_to_dict_without_nested_objects PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_validation_edge_cases PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_get_threshold PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_pipeline_config_validation_invalid_schema PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_default PASSED [ 62%]\ntests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_high_performance PASSED [ 63%]\ntests/unit/test_models.py::TestParallelConfig::test_base_model_to_json PASSED [ 63%]\ntests/unit/test_models.py::TestParallelConfig::test_base_model_str_representation PASSED [ 63%]\ntests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_create_loose PASSED [ 63%]\ntests/unit/test_models.py::TestParallelConfig::test_validation_thresholds_create_strict PASSED [ 63%]\ntests/unit/test_models.py::TestParallelConfig::test_parallel_config_validation_max_workers_exceeded PASSED [ 63%]\ntests/unit/test_models.py::TestParallelConfig::test_pipeline_config_properties PASSED [ 63%]\ntests/unit/test_models.py::TestParallelConfig::test_pipeline_config_create_conservative PASSED [ 63%]\ntests/unit/test_models_additional_coverage.py::TestProtocolImplementations::test_validatable_protocol_implementation PASSED [ 63%]\ntests/unit/test_models_additional_coverage.py::TestProtocolImplementations::test_serializable_protocol_implementation PASSED [ 64%]\ntests/unit/test_models_additional_coverage.py::TestPipelineConfigProperties::test_pipeline_config_properties PASSED [ 64%]\ntests/unit/test_models_additional_coverage.py::TestValidationThresholdsEdgeCases::test_validation_thresholds_get_threshold_invalid_type PASSED [ 64%]\ntests/unit/test_models_additional_coverage.py::TestValidationThresholdsEdgeCases::test_validation_thresholds_edge_values PASSED [ 64%]\ntests/unit/test_models_additional_coverage.py::TestParallelConfigEdgeCases::test_parallel_config_edge_values PASSED [ 64%]\ntests/unit/test_models_additional_coverage.py::TestParallelConfigEdgeCases::test_parallel_config_max_values PASSED [ 64%]\ntests/unit/test_models_additional_coverage.py::TestBronzeStepEdgeCases::test_bronze_step_incremental_col_edge_cases PASSED [ 64%]\ntests/unit/test_models_additional_coverage.py::TestBronzeStepEdgeCases::test_bronze_step_validation_edge_cases PASSED [ 64%]\ntests/unit/test_models_additional_coverage.py::TestSilverStepEdgeCases::test_silver_step_validation_edge_cases PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestGoldStepEdgeCases::test_gold_step_validation_edge_cases PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestSilverDependencyInfoEdgeCases::test_silver_dependency_info_edge_cases PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestBaseModelEdgeCases::test_base_model_serialization_edge_cases PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestModelValidationErrorPaths::test_bronze_step_validation_error_paths PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestModelValidationErrorPaths::test_silver_step_validation_error_paths PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestModelValidationErrorPaths::test_gold_step_validation_error_paths PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestModelValidationErrorPaths::test_silver_dependency_info_validation_error_paths PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestPipelineConfigValidation::test_pipeline_config_validation_error_paths PASSED [ 65%]\ntests/unit/test_models_additional_coverage.py::TestModelFactoryMethods::test_pipeline_config_factory_methods PASSED [ 66%]\ntests/unit/test_models_additional_coverage.py::TestModelFactoryMethods::test_validation_thresholds_factory_methods PASSED [ 66%]\ntests/unit/test_models_additional_coverage.py::TestModelFactoryMethods::test_parallel_config_factory_methods PASSED [ 66%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_validatable_protocol_definition PASSED [ 66%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_serializable_protocol_definition PASSED [ 66%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_pipeline_config_validation_edge_cases PASSED [ 66%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_bronze_step_validation_edge_cases PASSED [ 66%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_bronze_step_incremental_capability PASSED [ 66%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_silver_step_validation_edge_cases PASSED [ 66%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_validation_thresholds_boundary_values PASSED [ 67%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_parallel_config_boundary_values PASSED [ 67%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_model_factory_methods_values PASSED [ 67%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_model_properties_basic PASSED [ 67%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_model_comparison_edge_cases PASSED [ 67%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_model_string_representations PASSED [ 67%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_model_error_handling_comprehensive PASSED [ 67%]\ntests/unit/test_models_final_coverage.py::TestModelsFinalCoverage::test_model_boundary_conditions PASSED [ 67%]\ntests/unit/test_models_new.py::TestBaseModel::test_validate_default PASSED [ 67%]\ntests/unit/test_models_new.py::TestBaseModel::test_to_dict_simple PASSED [ 68%]\ntests/unit/test_models_new.py::TestBaseModel::test_to_dict_nested_objects PASSED [ 68%]\ntests/unit/test_models_new.py::TestBaseModel::test_to_json PASSED        [ 68%]\ntests/unit/test_models_new.py::TestBaseModel::test_str_representation PASSED [ 68%]\ntests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_creation PASSED [ 68%]\ntests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_defaults PASSED [ 68%]\ntests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_validation PASSED [ 68%]\ntests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_bronze PASSED [ 68%]\ntests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_silver PASSED [ 68%]\ntests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_invalid_gold PASSED [ 69%]\ntests/unit/test_models_new.py::TestValidationThresholds::test_validation_thresholds_hierarchy PASSED [ 69%]\ntests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_creation PASSED [ 69%]\ntests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_defaults PASSED [ 69%]\ntests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_validation PASSED [ 69%]\ntests/unit/test_models_new.py::TestParallelConfig::test_parallel_config_invalid_max_workers PASSED [ 69%]\ntests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_creation PASSED [ 69%]\ntests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_validation PASSED [ 69%]\ntests/unit/test_models_new.py::TestPipelineConfig::test_pipeline_config_invalid_schema PASSED [ 70%]\ntests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_creation FAILED [ 70%]\ntests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_validation FAILED [ 70%]\ntests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_invalid_name FAILED [ 70%]\ntests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_invalid_rules PASSED [ 70%]\ntests/unit/test_models_new.py::TestSilverStep::test_silver_step_creation FAILED [ 70%]\ntests/unit/test_models_new.py::TestSilverStep::test_silver_step_validation FAILED [ 70%]\ntests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_source_bronze FAILED [ 70%]\ntests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_transform FAILED [ 70%]\ntests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_table_name FAILED [ 71%]\ntests/unit/test_models_new.py::TestGoldStep::test_gold_step_creation FAILED [ 71%]\ntests/unit/test_models_new.py::TestGoldStep::test_gold_step_validation FAILED [ 71%]\ntests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_transform FAILED [ 71%]\ntests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_table_name FAILED [ 71%]\ntests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_source_silvers FAILED [ 71%]\ntests/unit/test_models_new.py::TestStageStats::test_stage_stats_creation PASSED [ 71%]\ntests/unit/test_models_new.py::TestStageStats::test_stage_stats_validation PASSED [ 71%]\ntests/unit/test_models_new.py::TestStageStats::test_stage_stats_invalid_validation_rate PASSED [ 71%]\ntests/unit/test_models_new.py::TestStageStats::test_stage_stats_negative_values PASSED [ 72%]\ntests/unit/test_models_new.py::TestExecutionContext::test_execution_context_creation PASSED [ 72%]\ntests/unit/test_models_new.py::TestExecutionContext::test_execution_context_validation PASSED [ 72%]\ntests/unit/test_models_new.py::TestExecutionContext::test_execution_context_invalid_pipeline_id PASSED [ 72%]\ntests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_creation PASSED [ 72%]\ntests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_validation PASSED [ 72%]\ntests/unit/test_models_new.py::TestPipelineMetrics::test_pipeline_metrics_invalid_values PASSED [ 72%]\ntests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_creation PASSED [ 72%]\ntests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_validation PASSED [ 72%]\ntests/unit/test_models_new.py::TestPipelineReport::test_pipeline_report_invalid_pipeline_id PASSED [ 73%]\ntests/unit/test_models_new.py::TestEnums::test_pipeline_status_enum PASSED [ 73%]\ntests/unit/test_models_new.py::TestEnums::test_pipeline_mode_enum PASSED [ 73%]\ntests/unit/test_models_new.py::TestEnums::test_step_status_enum PASSED   [ 73%]\ntests/unit/test_models_new.py::TestEnums::test_step_type_enum PASSED     [ 73%]\ntests/unit/test_models_new.py::TestStepResult::test_step_result_creation PASSED [ 73%]\ntests/unit/test_models_new.py::TestStepResult::test_step_result_validation PASSED [ 73%]\ntests/unit/test_models_new.py::TestStepResult::test_step_result_invalid_step_name PASSED [ 73%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_validation_thresholds_properties PASSED [ 73%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_parallel_config_properties PASSED [ 74%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_pipeline_config_properties PASSED [ 74%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_bronze_step_properties PASSED [ 74%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_silver_step_properties PASSED [ 74%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_gold_step_properties PASSED [ 74%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_validation_thresholds_equality_properties PASSED [ 74%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_parallel_config_equality_properties PASSED [ 74%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_bronze_step_rules_properties PASSED [ 74%]\ntests/unit/test_models_property_based.py::TestModelsPropertyBased::test_validation_thresholds_factory_methods_properties PASSED [ 75%]\ntests/unit/test_models_simple.py::TestBaseModel::test_validate_default PASSED [ 75%]\ntests/unit/test_models_simple.py::TestBaseModel::test_to_dict_simple PASSED [ 75%]\ntests/unit/test_models_simple.py::TestBaseModel::test_to_dict_nested_objects PASSED [ 75%]\ntests/unit/test_models_simple.py::TestBaseModel::test_to_json PASSED     [ 75%]\ntests/unit/test_models_simple.py::TestBaseModel::test_str_representation PASSED [ 75%]\ntests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_creation PASSED [ 75%]\ntests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_validation PASSED [ 75%]\ntests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_bronze PASSED [ 75%]\ntests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_silver PASSED [ 76%]\ntests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_invalid_gold PASSED [ 76%]\ntests/unit/test_models_simple.py::TestValidationThresholds::test_validation_thresholds_hierarchy PASSED [ 76%]\ntests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_creation PASSED [ 76%]\ntests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_validation PASSED [ 76%]\ntests/unit/test_models_simple.py::TestParallelConfig::test_parallel_config_invalid_max_workers PASSED [ 76%]\ntests/unit/test_models_simple.py::TestPipelineConfig::test_pipeline_config_creation PASSED [ 76%]\ntests/unit/test_models_simple.py::TestPipelineConfig::test_pipeline_config_validation PASSED [ 76%]\ntests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_creation FAILED [ 76%]\ntests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_validation FAILED [ 77%]\ntests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_invalid_name FAILED [ 77%]\ntests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_invalid_rules PASSED [ 77%]\ntests/unit/test_models_simple.py::TestSilverStep::test_silver_step_creation FAILED [ 77%]\ntests/unit/test_models_simple.py::TestSilverStep::test_silver_step_validation FAILED [ 77%]\ntests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_source_bronze FAILED [ 77%]\ntests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_transform FAILED [ 77%]\ntests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_table_name FAILED [ 77%]\ntests/unit/test_models_simple.py::TestGoldStep::test_gold_step_creation FAILED [ 77%]\ntests/unit/test_models_simple.py::TestGoldStep::test_gold_step_validation FAILED [ 78%]\ntests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_transform FAILED [ 78%]\ntests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_table_name FAILED [ 78%]\ntests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_source_silvers FAILED [ 78%]\ntests/unit/test_models_simple.py::TestStageStats::test_stage_stats_creation PASSED [ 78%]\ntests/unit/test_models_simple.py::TestStageStats::test_stage_stats_validation PASSED [ 78%]\ntests/unit/test_models_simple.py::TestStageStats::test_stage_stats_invalid_validation_rate PASSED [ 78%]\ntests/unit/test_models_simple.py::TestStageStats::test_stage_stats_negative_values PASSED [ 78%]\ntests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_creation PASSED [ 78%]\ntests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_validation PASSED [ 79%]\ntests/unit/test_models_simple.py::TestExecutionContext::test_execution_context_invalid_execution_id PASSED [ 79%]\ntests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_creation PASSED [ 79%]\ntests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_validation PASSED [ 79%]\ntests/unit/test_models_simple.py::TestPipelineMetrics::test_pipeline_metrics_invalid_values PASSED [ 79%]\ntests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_creation PASSED [ 79%]\ntests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_validation PASSED [ 79%]\ntests/unit/test_models_simple.py::TestPipelineReport::test_pipeline_report_invalid_pipeline_id PASSED [ 79%]\ntests/unit/test_models_simple.py::TestStepResult::test_step_result_creation PASSED [ 80%]\ntests/unit/test_models_simple.py::TestStepResult::test_step_result_validation PASSED [ 80%]\ntests/unit/test_models_simple.py::TestStepResult::test_step_result_invalid_step_name PASSED [ 80%]\ntests/unit/test_models_simple.py::TestEnums::test_pipeline_phase_enum PASSED [ 80%]\ntests/unit/test_models_simple.py::TestEnums::test_execution_mode_enum PASSED [ 80%]\ntests/unit/test_models_simple.py::TestEnums::test_write_mode_enum PASSED [ 80%]\ntests/unit/test_models_simple.py::TestEnums::test_validation_result_enum PASSED [ 80%]\ntests/unit/test_models_simple.py::TestEnums::test_step_status_enum PASSED [ 80%]\ntests/unit/test_models_simple.py::TestEnums::test_step_type_enum PASSED  [ 80%]\ntests/unit/test_models_simple.py::TestEnums::test_pipeline_status_enum PASSED [ 81%]\ntests/unit/test_models_simple.py::TestEnums::test_pipeline_mode_enum PASSED [ 81%]\ntests/unit/test_performance.py::TestNowDt::test_now_dt_returns_datetime PASSED [ 81%]\ntests/unit/test_performance.py::TestNowDt::test_now_dt_returns_utc PASSED [ 81%]\ntests/unit/test_performance.py::TestNowDt::test_now_dt_returns_recent_time PASSED [ 81%]\ntests/unit/test_performance.py::TestFormatDuration::test_format_seconds PASSED [ 81%]\ntests/unit/test_performance.py::TestFormatDuration::test_format_minutes PASSED [ 81%]\ntests/unit/test_performance.py::TestFormatDuration::test_format_hours PASSED [ 81%]\ntests/unit/test_performance.py::TestFormatDuration::test_format_zero PASSED [ 81%]\ntests/unit/test_performance.py::TestFormatDuration::test_format_very_small PASSED [ 82%]\ntests/unit/test_performance.py::TestFormatDuration::test_format_large_hours PASSED [ 82%]\ntests/unit/test_performance.py::TestTimeOperation::test_time_operation_success PASSED [ 82%]\ntests/unit/test_performance.py::TestTimeOperation::test_time_operation_exception PASSED [ 82%]\ntests/unit/test_performance.py::TestTimeOperation::test_time_operation_logging PASSED [ 82%]\ntests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_success PASSED [ 82%]\ntests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_exception PASSED [ 82%]\ntests/unit/test_performance.py::TestPerformanceMonitor::test_performance_monitor_max_duration_warning PASSED [ 82%]\ntests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_overwrite PASSED [ 82%]\ntests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_append PASSED [ 83%]\ntests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_invalid_mode PASSED [ 83%]\ntests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_with_options PASSED [ 83%]\ntests/unit/test_performance.py::TestTimeWriteOperation::test_time_write_operation_exception PASSED [ 83%]\ntests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_success PASSED [ 83%]\ntests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_with_max_duration PASSED [ 83%]\ntests/unit/test_performance.py::TestMonitorPerformance::test_monitor_performance_exception PASSED [ 83%]\ntests/unit/test_performance.py::TestPerformanceIntegration::test_multiple_operations_timing PASSED [ 83%]\ntests/unit/test_performance.py::TestPerformanceIntegration::test_nested_performance_monitors PASSED [ 83%]\ntests/unit/test_reporting.py::TestCreateValidationDict::test_with_valid_stats PASSED [ 84%]\ntests/unit/test_reporting.py::TestCreateValidationDict::test_with_none_stats PASSED [ 84%]\ntests/unit/test_reporting.py::TestCreateValidationDict::test_rounding_precision PASSED [ 84%]\ntests/unit/test_reporting.py::TestCreateTransformDict::test_basic_transform_dict PASSED [ 84%]\ntests/unit/test_reporting.py::TestCreateTransformDict::test_skipped_transform PASSED [ 84%]\ntests/unit/test_reporting.py::TestCreateTransformDict::test_type_conversion PASSED [ 84%]\ntests/unit/test_reporting.py::TestCreateWriteDict::test_basic_write_dict PASSED [ 84%]\ntests/unit/test_reporting.py::TestCreateWriteDict::test_append_mode PASSED [ 84%]\ntests/unit/test_reporting.py::TestCreateWriteDict::test_skipped_write PASSED [ 85%]\ntests/unit/test_reporting.py::TestCreateWriteDict::test_type_conversion PASSED [ 85%]\ntests/unit/test_reporting.py::TestCreateSummaryReport::test_basic_summary_report PASSED [ 85%]\ntests/unit/test_reporting.py::TestCreateSummaryReport::test_perfect_success_rate PASSED [ 85%]\ntests/unit/test_reporting.py::TestCreateSummaryReport::test_complete_failure PASSED [ 85%]\ntests/unit/test_reporting.py::TestCreateSummaryReport::test_rounding_precision PASSED [ 85%]\ntests/unit/test_reporting.py::TestCreateSummaryReport::test_zero_division_handling PASSED [ 85%]\ntests/unit/test_reporting.py::TestCreateSummaryReport::test_duration_formatting PASSED [ 85%]\ntests/unit/test_table_operations.py::TestFqn::test_basic_fqn PASSED      [ 85%]\ntests/unit/test_table_operations.py::TestFqn::test_empty_schema_raises_error PASSED [ 86%]\ntests/unit/test_table_operations.py::TestFqn::test_empty_table_raises_error PASSED [ 86%]\ntests/unit/test_table_operations.py::TestFqn::test_none_schema_raises_error PASSED [ 86%]\ntests/unit/test_table_operations.py::TestFqn::test_none_table_raises_error PASSED [ 86%]\ntests/unit/test_table_operations.py::TestFqn::test_special_characters PASSED [ 86%]\ntests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_success ERROR [ 86%]\ntests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_with_options ERROR [ 86%]\ntests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_nonexistent_database ERROR [ 86%]\ntests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_success ERROR [ 86%]\ntests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_with_options ERROR [ 87%]\ntests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_nonexistent_database ERROR [ 87%]\ntests/unit/test_table_operations.py::TestReadTable::test_read_existing_table ERROR [ 87%]\ntests/unit/test_table_operations.py::TestReadTable::test_read_nonexistent_table PASSED [ 87%]\ntests/unit/test_table_operations.py::TestReadTable::test_read_table_general_exception PASSED [ 87%]\ntests/unit/test_table_operations.py::TestReadTable::test_read_table_data_integrity ERROR [ 87%]\ntests/unit/test_table_operations.py::TestTableExists::test_existing_table ERROR [ 87%]\ntests/unit/test_table_operations.py::TestTableExists::test_nonexistent_table PASSED [ 87%]\ntests/unit/test_table_operations.py::TestTableExists::test_nonexistent_database PASSED [ 87%]\ntests/unit/test_table_operations.py::TestTableExists::test_table_exists_general_exception PASSED [ 88%]\ntests/unit/test_table_operations.py::TestDropTable::test_drop_existing_table ERROR [ 88%]\ntests/unit/test_table_operations.py::TestDropTable::test_drop_nonexistent_table PASSED [ 88%]\ntests/unit/test_table_operations.py::TestDropTable::test_drop_table_error_handling PASSED [ 88%]\ntests/unit/test_table_operations.py::TestDropTable::test_drop_table_general_exception PASSED [ 88%]\ntests/unit/test_types.py::TestTypeAliases::test_string_type_aliases PASSED [ 88%]\ntests/unit/test_types.py::TestTypeAliases::test_numeric_type_aliases PASSED [ 88%]\ntests/unit/test_types.py::TestTypeAliases::test_dictionary_type_aliases PASSED [ 88%]\ntests/unit/test_types.py::TestTypeAliases::test_optional_type_aliases PASSED [ 88%]\ntests/unit/test_types.py::TestEnums::test_step_type_enum PASSED          [ 89%]\ntests/unit/test_types.py::TestEnums::test_step_status_enum PASSED        [ 89%]\ntests/unit/test_types.py::TestEnums::test_pipeline_mode_enum PASSED      [ 89%]\ntests/unit/test_types.py::TestFunctionTypes::test_transform_function_types PASSED [ 89%]\ntests/unit/test_types.py::TestFunctionTypes::test_filter_function_type PASSED [ 89%]\ntests/unit/test_types.py::TestDataTypeAliases::test_column_rules_type PASSED [ 89%]\ntests/unit/test_types.py::TestDataTypeAliases::test_result_type_aliases PASSED [ 89%]\ntests/unit/test_types.py::TestDataTypeAliases::test_context_type_aliases PASSED [ 89%]\ntests/unit/test_types.py::TestDataTypeAliases::test_config_type_aliases PASSED [ 90%]\ntests/unit/test_types.py::TestDataTypeAliases::test_quality_thresholds_type PASSED [ 90%]\ntests/unit/test_types.py::TestDataTypeAliases::test_error_type_aliases PASSED [ 90%]\ntests/unit/test_types.py::TestProtocols::test_validatable_protocol PASSED [ 90%]\ntests/unit/test_types.py::TestProtocols::test_serializable_protocol PASSED [ 90%]\ntests/unit/test_types.py::TestBackwardCompatibility::test_backward_compatibility_aliases PASSED [ 90%]\ntests/unit/test_types.py::TestTypeUsage::test_pipeline_configuration_usage PASSED [ 90%]\ntests/unit/test_types.py::TestTypeUsage::test_step_result_usage PASSED   [ 90%]\ntests/unit/test_types.py::TestTypeUsage::test_validation_context_usage PASSED [ 90%]\ntests/unit/test_types.py::TestTypeUsage::test_error_handling_usage PASSED [ 91%]\ntests/unit/test_validation.py::TestAndAllRules::test_empty_rules PASSED  [ 91%]\ntests/unit/test_validation.py::TestAndAllRules::test_single_rule FAILED  [ 91%]\ntests/unit/test_validation.py::TestAndAllRules::test_multiple_rules FAILED [ 91%]\ntests/unit/test_validation.py::TestValidateDataframeSchema::test_valid_schema ERROR [ 91%]\ntests/unit/test_validation.py::TestValidateDataframeSchema::test_missing_columns ERROR [ 91%]\ntests/unit/test_validation.py::TestValidateDataframeSchema::test_extra_columns ERROR [ 91%]\ntests/unit/test_validation.py::TestValidateDataframeSchema::test_empty_expected_columns ERROR [ 91%]\ntests/unit/test_validation.py::TestGetDataframeInfo::test_basic_info ERROR [ 91%]\ntests/unit/test_validation.py::TestGetDataframeInfo::test_empty_dataframe FAILED [ 92%]\ntests/unit/test_validation.py::TestGetDataframeInfo::test_error_handling PASSED [ 92%]\ntests/unit/test_validation.py::TestApplyColumnRules::test_basic_validation ERROR [ 92%]\ntests/unit/test_validation.py::TestApplyColumnRules::test_none_rules_raises_error ERROR [ 92%]\ntests/unit/test_validation.py::TestApplyColumnRules::test_empty_rules ERROR [ 92%]\ntests/unit/test_validation.py::TestApplyColumnRules::test_complex_rules ERROR [ 92%]\ntests/unit/test_validation.py::TestSafeDivide::test_normal_division PASSED [ 92%]\ntests/unit/test_validation.py::TestSafeDivide::test_division_by_zero PASSED [ 92%]\ntests/unit/test_validation.py::TestSafeDivide::test_division_by_zero_custom_default PASSED [ 92%]\ntests/unit/test_validation.py::TestSafeDivide::test_float_division PASSED [ 93%]\ntests/unit/test_validation.py::TestSafeDivide::test_negative_numbers PASSED [ 93%]\ntests/unit/test_validation.py::TestSafeDivide::test_zero_numerator PASSED [ 93%]\ntests/unit/test_validation.py::TestConvertRuleToExpression::test_not_null_rule FAILED [ 93%]\ntests/unit/test_validation.py::TestConvertRuleToExpression::test_positive_rule FAILED [ 93%]\ntests/unit/test_validation.py::TestConvertRuleToExpression::test_non_negative_rule FAILED [ 93%]\ntests/unit/test_validation.py::TestConvertRuleToExpression::test_non_zero_rule FAILED [ 93%]\ntests/unit/test_validation.py::TestConvertRuleToExpression::test_custom_expression_rule FAILED [ 93%]\ntests/unit/test_validation.py::TestConvertRulesToExpressions::test_string_rules_conversion FAILED [ 93%]\ntests/unit/test_validation.py::TestConvertRulesToExpressions::test_mixed_rules_conversion FAILED [ 94%]\ntests/unit/test_validation.py::TestAssessDataQuality::test_basic_data_quality_assessment ERROR [ 94%]\ntests/unit/test_validation.py::TestAssessDataQuality::test_data_quality_with_rules ERROR [ 94%]\ntests/unit/test_validation.py::TestApplyValidationRules::test_apply_validation_rules_basic ERROR [ 94%]\ntests/unit/test_validation.py::TestApplyValidationRules::test_apply_validation_rules_empty ERROR [ 94%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rule_to_expression_string_handling FAILED [ 94%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_empty_expressions PASSED [ 94%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_no_column_expressions PASSED [ 94%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_validation_predicate_true PASSED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_with_rules FAILED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_safe_divide_edge_cases PASSED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_validate_dataframe_schema_edge_cases PASSED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_get_dataframe_info_edge_cases PASSED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_get_dataframe_info_error_handling PASSED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_assess_data_quality_edge_cases PASSED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_validation_rules_edge_cases PASSED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rules_to_expressions_complex_cases FAILED [ 95%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rule_to_expression_edge_cases FAILED [ 96%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_single_expression FAILED [ 96%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_multiple_expressions FAILED [ 96%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_string_rule_conversion_edge_cases FAILED [ 96%]\ntests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_validation_error_handling PASSED [ 96%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_and_all_rules_with_string_expressions PASSED [ 96%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_apply_column_rules_no_validation_predicate PASSED [ 96%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_assess_data_quality_empty_dataframe PASSED [ 96%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_assess_data_quality_with_rules PASSED [ 96%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_assess_data_quality_no_rules PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_unified_validator_initialization PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_unified_validator_add_validator PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_unified_validator_validate_pipeline PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_unified_validator_validate_pipeline_with_errors PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_apply_validation_rules_comprehensive PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_get_dataframe_info_comprehensive PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_validate_dataframe_schema_comprehensive PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_safe_divide_comprehensive PASSED [ 97%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_validation_with_large_dataset_simulation PASSED [ 98%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_validation_error_handling_comprehensive PASSED [ 98%]\ntests/unit/test_validation_comprehensive_coverage.py::TestValidationComprehensiveCoverage::test_validation_performance_scenarios PASSED [ 98%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_apply_column_rules_column_filtering_edge_cases PASSED [ 98%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_apply_column_rules_no_column_filtering PASSED [ 98%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_assess_data_quality_with_empty_rules PASSED [ 98%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_get_dataframe_info_with_exception PASSED [ 98%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_validate_dataframe_schema_edge_cases PASSED [ 98%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_safe_divide_edge_cases PASSED [ 98%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_apply_validation_rules_edge_cases PASSED [ 99%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_validation_performance_edge_cases PASSED [ 99%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_validation_large_dataset_edge_cases PASSED [ 99%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_validation_error_handling_edge_cases PASSED [ 99%]\ntests/unit/test_validation_final_push.py::TestValidationFinalPush::test_validation_boundary_conditions PASSED [ 99%]\ntests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_safe_divide_properties PASSED [ 99%]\ntests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_safe_divide_zero_denominator_properties PASSED [ 99%]\ntests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_validate_dataframe_schema_properties PASSED [ 99%]\ntests/unit/test_validation_property_based.py::TestValidationPropertyBased::test_dataframe_schema_edge_cases_properties PASSED [100%]\n\n==================================== ERRORS ====================================\n_ ERROR at setup of TestBronzeNoDatetime.test_bronze_step_without_incremental_col _\ntests/system/test_bronze_no_datetime.py:44: in sample_data_no_datetime\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_______ ERROR at setup of TestBronzeNoDatetime.test_silver_step_creation _______\ntests/system/test_bronze_no_datetime.py:44: in sample_data_no_datetime\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n________ ERROR at setup of TestBronzeNoDatetime.test_gold_step_creation ________\ntests/system/test_bronze_no_datetime.py:44: in sample_data_no_datetime\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_______ ERROR at setup of TestBronzeNoDatetime.test_pipeline_validation ________\ntests/system/test_bronze_no_datetime.py:44: in sample_data_no_datetime\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n________ ERROR at setup of TestBronzeNoDatetime.test_pipeline_creation _________\ntests/system/test_bronze_no_datetime.py:44: in sample_data_no_datetime\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_______ ERROR at setup of TestBronzeNoDatetime.test_dataframe_operations _______\ntests/system/test_bronze_no_datetime.py:44: in sample_data_no_datetime\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_______ ERROR at setup of TestDataFrameAccess.test_bronze_step_creation ________\ntests/system/test_dataframe_access.py:44: in sample_bronze_rules\n    \"user_id\": [F.col(\"user_id\").isNotNull()],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_______ ERROR at setup of TestDataFrameAccess.test_silver_step_creation ________\ntests/system/test_dataframe_access.py:44: in sample_bronze_rules\n    \"user_id\": [F.col(\"user_id\").isNotNull()],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________ ERROR at setup of TestDataFrameAccess.test_gold_step_creation _________\ntests/system/test_dataframe_access.py:44: in sample_bronze_rules\n    \"user_id\": [F.col(\"user_id\").isNotNull()],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n____ ERROR at setup of TestDataFrameAccess.test_pipeline_builder_validation ____\ntests/system/test_dataframe_access.py:44: in sample_bronze_rules\n    \"user_id\": [F.col(\"user_id\").isNotNull()],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_________ ERROR at setup of TestDataFrameAccess.test_pipeline_creation _________\ntests/system/test_dataframe_access.py:44: in sample_bronze_rules\n    \"user_id\": [F.col(\"user_id\").isNotNull()],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_______ ERROR at setup of TestDataFrameAccess.test_dataframe_operations ________\ntests/system/test_dataframe_access.py:38: in sample_bronze_data\n    return spark_session.createDataFrame(data, [\"user_id\", \"action\", \"timestamp\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n________ ERROR at setup of TestDataFrameAccess.test_step_type_detection ________\ntests/system/test_dataframe_access.py:44: in sample_bronze_rules\n    \"user_id\": [F.col(\"user_id\").isNotNull()],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_ ERROR at setup of TestImprovedUserExperience.test_auto_infer_gold_source_silvers _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:24 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_auto_infer_gold_source_silvers_explicit _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:24 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_auto_infer_gold_no_silver_steps_error _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:24 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_preset_configurations_development _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:24 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_preset_configurations_production _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:24 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_preset_configurations_testing _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:24 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_validation_helper_not_null_rules _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:24 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_validation_helper_positive_number_rules _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_validation_helper_string_not_empty_rules _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_validation_helper_timestamp_rules _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n__ ERROR at setup of TestImprovedUserExperience.test_detect_timestamp_columns __\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_detect_timestamp_columns_list _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_ ERROR at setup of TestImprovedUserExperience.test_chaining_with_auto_inference _\ntests/system/test_improved_user_experience.py:43: in setup_test\n    self.rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n____ ERROR at setup of TestMultiSchemaSupport.test_bronze_rules_with_schema ____\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n__ ERROR at setup of TestMultiSchemaSupport.test_bronze_rules_without_schema ___\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n____ ERROR at setup of TestMultiSchemaSupport.test_silver_rules_with_schema ____\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n__ ERROR at setup of TestMultiSchemaSupport.test_silver_transform_with_schema __\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n___ ERROR at setup of TestMultiSchemaSupport.test_gold_transform_with_schema ___\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n___ ERROR at setup of TestMultiSchemaSupport.test_schema_validation_success ____\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n___ ERROR at setup of TestMultiSchemaSupport.test_schema_validation_failure ____\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n______ ERROR at setup of TestMultiSchemaSupport.test_get_effective_schema ______\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n________ ERROR at setup of TestMultiSchemaSupport.test_schema_creation _________\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n____ ERROR at setup of TestMultiSchemaSupport.test_schema_creation_failure _____\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n_____ ERROR at setup of TestMultiSchemaSupport.test_cross_schema_pipeline ______\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n_______ ERROR at setup of TestMultiSchemaSupport.test_mixed_schema_usage _______\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n_ ERROR at setup of TestMultiSchemaSupport.test_schema_validation_integration __\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n_____ ERROR at setup of TestMultiSchemaSupport.test_backward_compatibility _____\ntests/system/test_multi_schema_support.py:42: in setup_test\n    self.test_df = self.spark.createDataFrame(self.test_data, self.test_schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n---------------------------- Captured stdout setup -----------------------------\n12:55:25 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: default_schema)\n_ ERROR at setup of TestRealSparkOperations.test_real_spark_dataframe_operations _\ntests/system/test_simple_real_spark.py:45: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n__ ERROR at setup of TestRealSparkOperations.test_real_spark_transformations ___\ntests/system/test_simple_real_spark.py:45: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n__ ERROR at setup of TestRealSparkOperations.test_real_spark_validation_rules __\ntests/system/test_simple_real_spark.py:45: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n____ ERROR at setup of TestRealSparkOperations.test_real_spark_data_quality ____\ntests/system/test_simple_real_spark.py:45: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_ ERROR at setup of TestRealSparkOperations.test_real_spark_metadata_operations _\ntests/system/test_simple_real_spark.py:45: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___ ERROR at setup of TestRealSparkOperations.test_real_spark_error_handling ___\ntests/system/test_simple_real_spark.py:45: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_ ERROR at setup of TestRealSparkOperations.test_real_spark_schema_operations __\ntests/system/test_simple_real_spark.py:45: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___________ ERROR at setup of TestDataValidation.test_and_all_rules ____________\ntests/system/test_utils.py:49: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n________ ERROR at setup of TestDataValidation.test_and_all_rules_empty _________\ntests/system/test_utils.py:49: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_________ ERROR at setup of TestDataValidation.test_apply_column_rules _________\ntests/system/test_utils.py:49: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___ ERROR at setup of TestDataValidation.test_apply_column_rules_none_rules ____\ntests/system/test_utils.py:49: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n________ ERROR at setup of TestDataValidation.test_assess_data_quality _________\ntests/system/test_utils.py:49: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_________ ERROR at setup of TestDataValidation.test_get_dataframe_info _________\ntests/system/test_utils.py:49: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_____ ERROR at setup of TestDataValidation.test_validate_dataframe_schema ______\ntests/system/test_utils.py:49: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_ ERROR at setup of TestDataTransformationUtilities.test_basic_dataframe_operations _\ntests/system/test_utils.py:168: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n__ ERROR at setup of TestDataTransformationUtilities.test_dataframe_filtering __\ntests/system/test_utils.py:168: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n____ ERROR at setup of TestWriteOverwriteTable.test_write_overwrite_success ____\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_ ERROR at setup of TestWriteOverwriteTable.test_write_overwrite_with_options __\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_ ERROR at setup of TestWriteOverwriteTable.test_write_overwrite_nonexistent_database _\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_______ ERROR at setup of TestWriteAppendTable.test_write_append_success _______\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n____ ERROR at setup of TestWriteAppendTable.test_write_append_with_options _____\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_ ERROR at setup of TestWriteAppendTable.test_write_append_nonexistent_database _\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___________ ERROR at setup of TestReadTable.test_read_existing_table ___________\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n________ ERROR at setup of TestReadTable.test_read_table_data_integrity ________\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n____________ ERROR at setup of TestTableExists.test_existing_table _____________\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___________ ERROR at setup of TestDropTable.test_drop_existing_table ___________\ntests/unit/test_table_operations.py:36: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_______ ERROR at setup of TestValidateDataframeSchema.test_valid_schema ________\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n______ ERROR at setup of TestValidateDataframeSchema.test_missing_columns ______\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_______ ERROR at setup of TestValidateDataframeSchema.test_extra_columns _______\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n__ ERROR at setup of TestValidateDataframeSchema.test_empty_expected_columns ___\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n____________ ERROR at setup of TestGetDataframeInfo.test_basic_info ____________\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_________ ERROR at setup of TestApplyColumnRules.test_basic_validation _________\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_____ ERROR at setup of TestApplyColumnRules.test_none_rules_raises_error ______\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___________ ERROR at setup of TestApplyColumnRules.test_empty_rules ____________\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n__________ ERROR at setup of TestApplyColumnRules.test_complex_rules ___________\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n__ ERROR at setup of TestAssessDataQuality.test_basic_data_quality_assessment __\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_____ ERROR at setup of TestAssessDataQuality.test_data_quality_with_rules _____\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_ ERROR at setup of TestApplyValidationRules.test_apply_validation_rules_basic _\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_ ERROR at setup of TestApplyValidationRules.test_apply_validation_rules_empty _\ntests/unit/test_validation.py:50: in sample_dataframe\n    return spark_session.createDataFrame(data, schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n=================================== FAILURES ===================================\n__________ TestPerformanceIntegration.test_error_handling_integration __________\ntests/performance/test_performance_integration.py:362: in test_error_handling_integration\n    error_results = [r for r in performance_profiler.metrics if not r.success]\ntests/performance/test_performance_integration.py:362: in <listcomp>\n    error_results = [r for r in performance_profiler.metrics if not r.success]\nE   AttributeError: 'PerformanceMetrics' object has no attribute 'success'\n_________ TestSecurityIntegration.test_security_test_suite_integration _________\ntests/security/test_security_integration.py:77: in test_security_test_suite_integration\n    assert \"success\" in result or \"compliant\" in result\nE   AssertionError: assert ('success' in {'command_injection_test': {'message': 'Command injection prevention tests passed', 'success': True}, 'information_disclosure_test': {'message': 'Information disclosure prevention tests passed', 'success': True}, 'overall_success': False, 'path_traversal_test': {'message': 'Path traversal prevention tests passed', 'success': True}, ...} or 'compliant' in {'command_injection_test': {'message': 'Command injection prevention tests passed', 'success': True}, 'information_disclosure_test': {'message': 'Information disclosure prevention tests passed', 'success': True}, 'overall_success': False, 'path_traversal_test': {'message': 'Path traversal prevention tests passed', 'success': True}, ...})\n__________ TestSecurityIntegration.test_security_metrics_integration ___________\ntests/security/test_security_integration.py:312: in test_security_metrics_integration\n    assert len(security_monitor.metrics) > 0\nE   assert 0 > 0\nE    +  where 0 = len([])\nE    +    where [] = <tests.security.security_monitoring.SecurityMonitor object at 0x137ff9eb0>.metrics\n------------------------------ Captured log call -------------------------------\nINFO     security_monitor:security_monitoring.py:160 Security monitoring started\nINFO     security_monitor:security_monitoring.py:454 Security Event: configuration_change - Security monitoring started\nINFO     security_monitor:security_monitoring.py:454 Security Event: suspicious_activity - Test event 0 for metrics\nINFO     security_monitor:security_monitoring.py:454 Security Event: suspicious_activity - Test event 1 for metrics\nINFO     security_monitor:security_monitoring.py:454 Security Event: suspicious_activity - Test event 2 for metrics\nINFO     security_monitor:security_monitoring.py:454 Security Event: suspicious_activity - Test event 3 for metrics\nINFO     security_monitor:security_monitoring.py:454 Security Event: suspicious_activity - Test event 4 for metrics\nERROR    security_monitor:security_monitoring.py:302 Error monitoring network activity: (pid=14410)\nERROR    security_monitor:security_monitoring.py:372 Error monitoring process activity: '>' not supported between instances of 'NoneType' and 'int'\n________________________ test_security_cicd_integration ________________________\ntests/security/test_security_integration.py:444: in test_security_cicd_integration\n    assert results[\"dependency_check\"][\"success\"], \"Dependency check failed in CI/CD\"\nE   AssertionError: Dependency check failed in CI/CD\nE   assert False\n------------------------------ Captured log call -------------------------------\nERROR    security_monitor:security_monitoring.py:302 Error monitoring network activity: (pid=14410)\nERROR    security_monitor:security_monitoring.py:372 Error monitoring process activity: '>' not supported between instances of 'NoneType' and 'int'\n_________ TestAutoInferSourceBronze.test_auto_infer_single_bronze_step _________\ntests/system/test_auto_infer_source_bronze.py:30: in test_auto_infer_single_bronze_step\n    name=\"events\", rules={\"user_id\": [F.col(\"user_id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:22 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_______ TestAutoInferSourceBronze.test_auto_infer_multiple_bronze_steps ________\ntests/system/test_auto_infer_source_bronze.py:56: in test_auto_infer_multiple_bronze_steps\n    name=\"events\", rules={\"user_id\": [F.col(\"user_id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:22 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n______ TestAutoInferSourceBronze.test_explicit_source_bronze_still_works _______\ntests/system/test_auto_infer_source_bronze.py:82: in test_explicit_source_bronze_still_works\n    name=\"events\", rules={\"user_id\": [F.col(\"user_id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:22 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n_________ TestAutoInferSourceBronze.test_no_bronze_steps_raises_error __________\ntests/system/test_auto_infer_source_bronze.py:115: in test_no_bronze_steps_raises_error\n    rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:22 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n______ TestAutoInferSourceBronze.test_invalid_source_bronze_raises_error _______\ntests/system/test_auto_infer_source_bronze.py:128: in test_invalid_source_bronze_raises_error\n    name=\"events\", rules={\"user_id\": [F.col(\"user_id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:22 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n____________ TestAutoInferSourceBronze.test_logging_auto_inference _____________\ntests/system/test_auto_infer_source_bronze.py:152: in test_logging_auto_inference\n    name=\"events\", rules={\"user_id\": [F.col(\"user_id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:23 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n______ TestAutoInferSourceBronze.test_chaining_works_with_auto_inference _______\ntests/system/test_auto_infer_source_bronze.py:174: in test_chaining_works_with_auto_inference\n    name=\"events\", rules={\"user_id\": [F.col(\"user_id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n---------------------------- Captured stdout setup -----------------------------\n12:55:23 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log setup ------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n__________ TestBronzeNoDatetime.test_bronze_step_with_incremental_col __________\ntests/system/test_bronze_no_datetime.py:79: in test_bronze_step_with_incremental_col\n    \"user_id\": [F.col(\"user_id\").isNotNull()],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n----------------------------- Captured stdout call -----------------------------\n12:55:23 - SparkForge - INFO - \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n------------------------------ Captured log call -------------------------------\nINFO     SparkForge:logging.py:82 \ud83d\udd27 PipelineBuilder initialized (schema: test_schema)\n________________ TestBronzeNoDatetime.test_step_type_detection _________________\ntests/system/test_bronze_no_datetime.py:341: in test_step_type_detection\n    rules={\"user_id\": [F.col(\"user_id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_________ TestDeltaLakeComprehensive.test_delta_lake_acid_transactions _________\ntests/system/test_delta_lake.py:20: in test_delta_lake_acid_transactions\n    df = spark_session.createDataFrame(data, [\"id\", \"name\", \"date\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_________ TestDeltaLakeComprehensive.test_delta_lake_schema_evolution __________\ntests/system/test_delta_lake.py:47: in test_delta_lake_schema_evolution\n    initial_df = spark_session.createDataFrame(initial_data, [\"id\", \"name\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n____________ TestDeltaLakeComprehensive.test_delta_lake_time_travel ____________\ntests/system/test_delta_lake.py:73: in test_delta_lake_time_travel\n    df = spark_session.createDataFrame(data, [\"id\", \"name\", \"date\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_________ TestDeltaLakeComprehensive.test_delta_lake_merge_operations __________\ntests/system/test_delta_lake.py:110: in test_delta_lake_merge_operations\n    target_df = spark_session.createDataFrame(target_data, [\"id\", \"name\", \"score\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___________ TestDeltaLakeComprehensive.test_delta_lake_optimization ____________\ntests/system/test_delta_lake.py:153: in test_delta_lake_optimization\n    df = spark_session.createDataFrame(data, [\"id\", \"name\", \"date\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_______ TestDeltaLakeComprehensive.test_delta_lake_history_and_metadata ________\ntests/system/test_delta_lake.py:173: in test_delta_lake_history_and_metadata\n    df = spark_session.createDataFrame(data, [\"id\", \"name\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_________ TestDeltaLakeComprehensive.test_delta_lake_concurrent_writes _________\ntests/system/test_delta_lake.py:214: in test_delta_lake_concurrent_writes\n    initial_df = spark_session.createDataFrame(initial_data, [\"id\", \"name\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n____ TestDeltaLakeComprehensive.test_delta_lake_performance_characteristics ____\ntests/system/test_delta_lake.py:242: in test_delta_lake_performance_characteristics\n    df = spark_session.createDataFrame(data, [\"id\", \"name\", \"date\", \"score\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_____ TestDeltaLakeComprehensive.test_delta_lake_data_quality_constraints ______\ntests/system/test_delta_lake.py:284: in test_delta_lake_data_quality_constraints\n    df = spark_session.createDataFrame(data, [\"id\", \"name\", \"age\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n_____________ TestRealSparkOperations.test_real_spark_performance ______________\ntests/system/test_simple_real_spark.py:147: in test_real_spark_performance\n    df = spark_session.createDataFrame(data, [\"user_id\", \"action\", \"timestamp\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n________________ TestRealSparkOperations.test_real_spark_joins _________________\ntests/system/test_simple_real_spark.py:210: in test_real_spark_joins\n    users_df = spark_session.createDataFrame(users_data, [\"user_id\", \"name\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n__________ TestPerformanceWithRealData.test_large_dataset_validation ___________\ntests/system/test_utils.py:267: in test_large_dataset_validation\n    df = spark_session.createDataFrame(data, [\"user_id\", \"action\", \"timestamp\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___________ TestPerformanceWithRealData.test_complex_transformations ___________\ntests/system/test_utils.py:294: in test_complex_transformations\n    df = spark_session.createDataFrame(data, [\"user_id\", \"action\", \"timestamp\"])\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n___________________ TestBronzeStep.test_bronze_step_creation ___________________\ntests/unit/test_models.py:175: in test_bronze_step_creation\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_______________ TestBronzeStep.test_bronze_step_creation_minimal _______________\ntests/unit/test_models.py:191: in test_bronze_step_creation_minimal\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestBronzeStep.test_bronze_step_validation_success ______________\ntests/unit/test_models.py:203: in test_bronze_step_validation_success\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n____________ TestBronzeStep.test_bronze_step_validation_empty_name _____________\ntests/unit/test_models.py:213: in test_bronze_step_validation_empty_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_____________ TestBronzeStep.test_bronze_step_validation_none_name _____________\ntests/unit/test_models.py:222: in test_bronze_step_validation_none_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________ TestBronzeStep.test_bronze_step_has_incremental_capability __________\ntests/unit/test_models.py:238: in test_bronze_step_has_incremental_capability\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___________________ TestSilverStep.test_silver_step_creation ___________________\ntests/unit/test_models.py:260: in test_silver_step_creation\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n____________ TestSilverStep.test_silver_step_validation_empty_name _____________\ntests/unit/test_models.py:303: in test_silver_step_validation_empty_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________ TestSilverStep.test_silver_step_validation_empty_source_bronze ________\ntests/unit/test_models.py:318: in test_silver_step_validation_empty_source_bronze\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________ TestSilverStep.test_silver_step_validation_none_transform ___________\ntests/unit/test_models.py:333: in test_silver_step_validation_none_transform\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_________ TestSilverStep.test_silver_step_validation_empty_table_name __________\ntests/unit/test_models.py:361: in test_silver_step_validation_empty_table_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_____________________ TestGoldStep.test_gold_step_creation _____________________\ntests/unit/test_models.py:383: in test_gold_step_creation\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_________________ TestGoldStep.test_gold_step_creation_minimal _________________\ntests/unit/test_models.py:397: in test_gold_step_creation_minimal\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________________ TestGoldStep.test_gold_step_validation_success ________________\ntests/unit/test_models.py:412: in test_gold_step_validation_success\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestGoldStep.test_gold_step_validation_empty_name _______________\ntests/unit/test_models.py:425: in test_gold_step_validation_empty_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n____________ TestGoldStep.test_gold_step_validation_none_transform _____________\ntests/unit/test_models.py:439: in test_gold_step_validation_none_transform\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___________ TestGoldStep.test_gold_step_validation_empty_table_name ____________\ntests/unit/test_models.py:460: in test_gold_step_validation_empty_table_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___________________ TestBronzeStep.test_bronze_step_creation ___________________\ntests/unit/test_models_new.py:280: in test_bronze_step_creation\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________________ TestBronzeStep.test_bronze_step_validation __________________\ntests/unit/test_models_new.py:288: in test_bronze_step_validation\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_________________ TestBronzeStep.test_bronze_step_invalid_name _________________\ntests/unit/test_models_new.py:295: in test_bronze_step_invalid_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___________________ TestSilverStep.test_silver_step_creation ___________________\ntests/unit/test_models_new.py:321: in test_silver_step_creation\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________________ TestSilverStep.test_silver_step_validation __________________\ntests/unit/test_models_new.py:342: in test_silver_step_validation\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n____________ TestSilverStep.test_silver_step_invalid_source_bronze _____________\ntests/unit/test_models_new.py:359: in test_silver_step_invalid_source_bronze\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestSilverStep.test_silver_step_invalid_transform _______________\ntests/unit/test_models_new.py:374: in test_silver_step_invalid_transform\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestSilverStep.test_silver_step_invalid_table_name ______________\ntests/unit/test_models_new.py:393: in test_silver_step_invalid_table_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_____________________ TestGoldStep.test_gold_step_creation _____________________\ntests/unit/test_models_new.py:416: in test_gold_step_creation\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n____________________ TestGoldStep.test_gold_step_validation ____________________\ntests/unit/test_models_new.py:437: in test_gold_step_validation\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________________ TestGoldStep.test_gold_step_invalid_transform _________________\ntests/unit/test_models_new.py:450: in test_gold_step_invalid_transform\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________________ TestGoldStep.test_gold_step_invalid_table_name ________________\ntests/unit/test_models_new.py:469: in test_gold_step_invalid_table_name\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestGoldStep.test_gold_step_invalid_source_silvers ______________\ntests/unit/test_models_new.py:488: in test_gold_step_invalid_source_silvers\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___________________ TestBronzeStep.test_bronze_step_creation ___________________\ntests/unit/test_models_simple.py:245: in test_bronze_step_creation\n    rules = {\"id\": [F.col(\"id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________________ TestBronzeStep.test_bronze_step_validation __________________\ntests/unit/test_models_simple.py:254: in test_bronze_step_validation\n    step = BronzeStep(name=\"test_bronze\", rules={\"id\": [F.col(\"id\").isNotNull()]})\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_________________ TestBronzeStep.test_bronze_step_invalid_name _________________\ntests/unit/test_models_simple.py:262: in test_bronze_step_invalid_name\n    BronzeStep(name=\"\", rules={\"id\": [F.col(\"id\").isNotNull()]})\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___________________ TestSilverStep.test_silver_step_creation ___________________\ntests/unit/test_models_simple.py:285: in test_silver_step_creation\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________________ TestSilverStep.test_silver_step_validation __________________\ntests/unit/test_models_simple.py:303: in test_silver_step_validation\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n____________ TestSilverStep.test_silver_step_invalid_source_bronze _____________\ntests/unit/test_models_simple.py:321: in test_silver_step_invalid_source_bronze\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestSilverStep.test_silver_step_invalid_transform _______________\ntests/unit/test_models_simple.py:334: in test_silver_step_invalid_transform\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestSilverStep.test_silver_step_invalid_table_name ______________\ntests/unit/test_models_simple.py:351: in test_silver_step_invalid_table_name\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_____________________ TestGoldStep.test_gold_step_creation _____________________\ntests/unit/test_models_simple.py:368: in test_gold_step_creation\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n____________________ TestGoldStep.test_gold_step_validation ____________________\ntests/unit/test_models_simple.py:386: in test_gold_step_validation\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________________ TestGoldStep.test_gold_step_invalid_transform _________________\ntests/unit/test_models_simple.py:400: in test_gold_step_invalid_transform\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________________ TestGoldStep.test_gold_step_invalid_table_name ________________\ntests/unit/test_models_simple.py:417: in test_gold_step_invalid_table_name\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestGoldStep.test_gold_step_invalid_source_silvers ______________\ntests/unit/test_models_simple.py:434: in test_gold_step_invalid_source_silvers\n    rules={\"id\": [F.col(\"id\").isNotNull()]},\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_______________________ TestAndAllRules.test_single_rule _______________________\ntests/unit/test_validation.py:64: in test_single_rule\n    rules = {\"user_id\": [F.col(\"user_id\").isNotNull()]}\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_____________________ TestAndAllRules.test_multiple_rules ______________________\ntests/unit/test_validation.py:72: in test_multiple_rules\n    \"user_id\": [F.col(\"user_id\").isNotNull()],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________________ TestGetDataframeInfo.test_empty_dataframe ___________________\ntests/unit/test_validation.py:122: in test_empty_dataframe\n    empty_df = spark_session.createDataFrame([], schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:675: in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:700: in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\nvenv38/lib/python3.8/site-packages/pyspark/sql/session.py:526: in _createFromLocal\n    return self._sc.parallelize(data), schema\nvenv38/lib/python3.8/site-packages/pyspark/context.py:538: in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\nvenv38/lib/python3.8/site-packages/pyspark/context.py:450: in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nE   AttributeError: 'NoneType' object has no attribute 'sc'\n________________ TestConvertRuleToExpression.test_not_null_rule ________________\ntests/unit/test_validation.py:249: in test_not_null_rule\n    result = _convert_rule_to_expression(\"not_null\", \"test_column\")\nsparkforge/validation.py:67: in _convert_rule_to_expression\n    return F.col(column_name).isNotNull()\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________________ TestConvertRuleToExpression.test_positive_rule ________________\ntests/unit/test_validation.py:255: in test_positive_rule\n    result = _convert_rule_to_expression(\"positive\", \"test_column\")\nsparkforge/validation.py:69: in _convert_rule_to_expression\n    return F.col(column_name) > 0\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______________ TestConvertRuleToExpression.test_non_negative_rule ______________\ntests/unit/test_validation.py:261: in test_non_negative_rule\n    result = _convert_rule_to_expression(\"non_negative\", \"test_column\")\nsparkforge/validation.py:71: in _convert_rule_to_expression\n    return F.col(column_name) >= 0\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________________ TestConvertRuleToExpression.test_non_zero_rule ________________\ntests/unit/test_validation.py:267: in test_non_zero_rule\n    result = _convert_rule_to_expression(\"non_zero\", \"test_column\")\nsparkforge/validation.py:73: in _convert_rule_to_expression\n    return F.col(column_name) != 0\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___________ TestConvertRuleToExpression.test_custom_expression_rule ____________\ntests/unit/test_validation.py:273: in test_custom_expression_rule\n    result = _convert_rule_to_expression(\"col('test_column') > 10\", \"test_column\")\nsparkforge/validation.py:76: in _convert_rule_to_expression\n    return F.expr(rule)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:1430: in expr\n    return Column(sc._jvm.functions.expr(str))\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________ TestConvertRulesToExpressions.test_string_rules_conversion __________\ntests/unit/test_validation.py:284: in test_string_rules_conversion\n    result = _convert_rules_to_expressions(rules)\nsparkforge/validation.py:89: in _convert_rules_to_expressions\n    _convert_rule_to_expression(rule, column_name)\nsparkforge/validation.py:67: in _convert_rule_to_expression\n    return F.col(column_name).isNotNull()\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________ TestConvertRulesToExpressions.test_mixed_rules_conversion ___________\ntests/unit/test_validation.py:294: in test_mixed_rules_conversion\n    \"col1\": [\"not_null\", F.col(\"col1\") > 0],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___ TestValidationEdgeCases.test_convert_rule_to_expression_string_handling ____\ntests/unit/test_validation_additional_coverage.py:36: in test_convert_rule_to_expression_string_handling\n    result = _convert_rule_to_expression(\"col1 > 0\", \"col1\")\nsparkforge/validation.py:76: in _convert_rule_to_expression\n    return F.expr(rule)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:1430: in expr\n    return Column(sc._jvm.functions.expr(str))\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n__________ TestValidationEdgeCases.test_apply_column_rules_with_rules __________\ntests/unit/test_validation_additional_coverage.py:83: in test_apply_column_rules_with_rules\n    result = apply_column_rules(\nsparkforge/validation.py:173: in apply_column_rules\n    validation_predicate = and_all_rules(rules)\nsparkforge/validation.py:102: in and_all_rules\n    converted_rules = _convert_rules_to_expressions(rules)\nsparkforge/validation.py:89: in _convert_rules_to_expressions\n    _convert_rule_to_expression(rule, column_name)\nsparkforge/validation.py:67: in _convert_rule_to_expression\n    return F.col(column_name).isNotNull()\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n___ TestValidationEdgeCases.test_convert_rules_to_expressions_complex_cases ____\ntests/unit/test_validation_additional_coverage.py:160: in test_convert_rules_to_expressions_complex_cases\n    \"col2\": [col(\"col2\") > 0],\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:115: in col\n    return _invoke_function(\"col\", col)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:57: in _invoke_function\n    jf = _get_get_jvm_function(name, SparkContext._active_spark_context)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:49: in _get_get_jvm_function\n    return getattr(sc._jvm.functions, name)\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n______ TestValidationEdgeCases.test_convert_rule_to_expression_edge_cases ______\ntests/unit/test_validation_additional_coverage.py:170: in test_convert_rule_to_expression_edge_cases\n    result = _convert_rule_to_expression(\"col1 > 0\", \"col1\")\nsparkforge/validation.py:76: in _convert_rule_to_expression\n    return F.expr(rule)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:1430: in expr\n    return Column(sc._jvm.functions.expr(str))\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_________ TestValidationEdgeCases.test_and_all_rules_single_expression _________\ntests/unit/test_validation_additional_coverage.py:177: in test_and_all_rules_single_expression\n    result = and_all_rules(rules)\nsparkforge/validation.py:102: in and_all_rules\n    converted_rules = _convert_rules_to_expressions(rules)\nsparkforge/validation.py:89: in _convert_rules_to_expressions\n    _convert_rule_to_expression(rule, column_name)\nsparkforge/validation.py:76: in _convert_rule_to_expression\n    return F.expr(rule)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:1430: in expr\n    return Column(sc._jvm.functions.expr(str))\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n_______ TestValidationEdgeCases.test_and_all_rules_multiple_expressions ________\ntests/unit/test_validation_additional_coverage.py:187: in test_and_all_rules_multiple_expressions\n    result = and_all_rules(rules)\nsparkforge/validation.py:102: in and_all_rules\n    converted_rules = _convert_rules_to_expressions(rules)\nsparkforge/validation.py:89: in _convert_rules_to_expressions\n    _convert_rule_to_expression(rule, column_name)\nsparkforge/validation.py:76: in _convert_rule_to_expression\n    return F.expr(rule)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:1430: in expr\n    return Column(sc._jvm.functions.expr(str))\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n________ TestValidationEdgeCases.test_string_rule_conversion_edge_cases ________\ntests/unit/test_validation_additional_coverage.py:201: in test_string_rule_conversion_edge_cases\n    result = _convert_rule_to_expression(rule, \"col1\")\nsparkforge/validation.py:76: in _convert_rule_to_expression\n    return F.expr(rule)\nvenv38/lib/python3.8/site-packages/pyspark/sql/functions.py:1430: in expr\n    return Column(sc._jvm.functions.expr(str))\nE   AttributeError: 'NoneType' object has no attribute '_jvm'\n=============================== warnings summary ===============================\ntests/performance/performance_tests.py:33\ntests/performance/performance_tests.py:33\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:33: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.performance\n\ntests/performance/performance_tests.py:34\ntests/performance/performance_tests.py:34\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:34: PytestUnknownMarkWarning: Unknown pytest.mark.benchmark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.benchmark\n\ntests/performance/performance_tests.py:58\ntests/performance/performance_tests.py:58\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:58: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.performance\n\ntests/performance/performance_tests.py:158\ntests/performance/performance_tests.py:158\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:158: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.performance\n\ntests/performance/performance_tests.py:159\ntests/performance/performance_tests.py:159\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:159: PytestUnknownMarkWarning: Unknown pytest.mark.benchmark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.benchmark\n\ntests/performance/performance_tests.py:375\ntests/performance/performance_tests.py:375\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:375: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.performance\n\ntests/performance/performance_tests.py:376\ntests/performance/performance_tests.py:376\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/performance/performance_tests.py:376: PytestUnknownMarkWarning: Unknown pytest.mark.memory - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.memory\n\ntests/security/test_security_integration.py:418\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/security/test_security_integration.py:418: PytestUnknownMarkWarning: Unknown pytest.mark.security - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.security\n\ntests/system/test_delta_lake.py:12\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_delta_lake.py:12: PytestUnknownMarkWarning: Unknown pytest.mark.delta - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.delta\n\ntests/system/test_simple_real_spark.py:47\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:47: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_simple_real_spark.py:66\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:66: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_simple_real_spark.py:92\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:92: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_simple_real_spark.py:109\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:109: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_simple_real_spark.py:123\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:123: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_simple_real_spark.py:139\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:139: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_simple_real_spark.py:164\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:164: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_simple_real_spark.py:183\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:183: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_simple_real_spark.py:201\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_simple_real_spark.py:201: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:51\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:51: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:66\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:66: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:75\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:75: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:96\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:96: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:110\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:110: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:123\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:123: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:133\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:133: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:170\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:170: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:186\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:186: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:205\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:205: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:231\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:231: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:259\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:259: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\ntests/system/test_utils.py:284\n  /Users/odosmatthews/Documents/coding/pipe/sparkforge/tests/system/test_utils.py:284: PytestUnknownMarkWarning: Unknown pytest.mark.spark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.spark\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED tests/performance/test_performance_integration.py::TestPerformanceIntegration::test_error_handling_integration\nFAILED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_test_suite_integration\nFAILED tests/security/test_security_integration.py::TestSecurityIntegration::test_security_metrics_integration\nFAILED tests/security/test_security_integration.py::test_security_cicd_integration\nFAILED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_auto_infer_single_bronze_step\nFAILED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_auto_infer_multiple_bronze_steps\nFAILED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_explicit_source_bronze_still_works\nFAILED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_no_bronze_steps_raises_error\nFAILED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_invalid_source_bronze_raises_error\nFAILED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_logging_auto_inference\nFAILED tests/system/test_auto_infer_source_bronze.py::TestAutoInferSourceBronze::test_chaining_works_with_auto_inference\nFAILED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_bronze_step_with_incremental_col\nFAILED tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_step_type_detection\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_acid_transactions\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_schema_evolution\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_time_travel\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_merge_operations\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_optimization\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_history_and_metadata\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_concurrent_writes\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_performance_characteristics\nFAILED tests/system/test_delta_lake.py::TestDeltaLakeComprehensive::test_delta_lake_data_quality_constraints\nFAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_performance\nFAILED tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_joins\nFAILED tests/system/test_utils.py::TestPerformanceWithRealData::test_large_dataset_validation\nFAILED tests/system/test_utils.py::TestPerformanceWithRealData::test_complex_transformations\nFAILED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_creation\nFAILED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_creation_minimal\nFAILED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_success\nFAILED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_empty_name\nFAILED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_validation_none_name\nFAILED tests/unit/test_models.py::TestBronzeStep::test_bronze_step_has_incremental_capability\nFAILED tests/unit/test_models.py::TestSilverStep::test_silver_step_creation\nFAILED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_name\nFAILED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_source_bronze\nFAILED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_none_transform\nFAILED tests/unit/test_models.py::TestSilverStep::test_silver_step_validation_empty_table_name\nFAILED tests/unit/test_models.py::TestGoldStep::test_gold_step_creation - Att...\nFAILED tests/unit/test_models.py::TestGoldStep::test_gold_step_creation_minimal\nFAILED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_success\nFAILED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_empty_name\nFAILED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_none_transform\nFAILED tests/unit/test_models.py::TestGoldStep::test_gold_step_validation_empty_table_name\nFAILED tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_creation\nFAILED tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_validation\nFAILED tests/unit/test_models_new.py::TestBronzeStep::test_bronze_step_invalid_name\nFAILED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_creation\nFAILED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_validation\nFAILED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_source_bronze\nFAILED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_transform\nFAILED tests/unit/test_models_new.py::TestSilverStep::test_silver_step_invalid_table_name\nFAILED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_creation\nFAILED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_validation\nFAILED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_transform\nFAILED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_table_name\nFAILED tests/unit/test_models_new.py::TestGoldStep::test_gold_step_invalid_source_silvers\nFAILED tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_creation\nFAILED tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_validation\nFAILED tests/unit/test_models_simple.py::TestBronzeStep::test_bronze_step_invalid_name\nFAILED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_creation\nFAILED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_validation\nFAILED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_source_bronze\nFAILED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_transform\nFAILED tests/unit/test_models_simple.py::TestSilverStep::test_silver_step_invalid_table_name\nFAILED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_creation\nFAILED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_validation\nFAILED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_transform\nFAILED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_table_name\nFAILED tests/unit/test_models_simple.py::TestGoldStep::test_gold_step_invalid_source_silvers\nFAILED tests/unit/test_validation.py::TestAndAllRules::test_single_rule - Att...\nFAILED tests/unit/test_validation.py::TestAndAllRules::test_multiple_rules - ...\nFAILED tests/unit/test_validation.py::TestGetDataframeInfo::test_empty_dataframe\nFAILED tests/unit/test_validation.py::TestConvertRuleToExpression::test_not_null_rule\nFAILED tests/unit/test_validation.py::TestConvertRuleToExpression::test_positive_rule\nFAILED tests/unit/test_validation.py::TestConvertRuleToExpression::test_non_negative_rule\nFAILED tests/unit/test_validation.py::TestConvertRuleToExpression::test_non_zero_rule\nFAILED tests/unit/test_validation.py::TestConvertRuleToExpression::test_custom_expression_rule\nFAILED tests/unit/test_validation.py::TestConvertRulesToExpressions::test_string_rules_conversion\nFAILED tests/unit/test_validation.py::TestConvertRulesToExpressions::test_mixed_rules_conversion\nFAILED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rule_to_expression_string_handling\nFAILED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_apply_column_rules_with_rules\nFAILED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rules_to_expressions_complex_cases\nFAILED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_convert_rule_to_expression_edge_cases\nFAILED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_single_expression\nFAILED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_and_all_rules_multiple_expressions\nFAILED tests/unit/test_validation_additional_coverage.py::TestValidationEdgeCases::test_string_rule_conversion_edge_cases\nERROR tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_bronze_step_without_incremental_col\nERROR tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_silver_step_creation\nERROR tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_gold_step_creation\nERROR tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_validation\nERROR tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_pipeline_creation\nERROR tests/system/test_bronze_no_datetime.py::TestBronzeNoDatetime::test_dataframe_operations\nERROR tests/system/test_dataframe_access.py::TestDataFrameAccess::test_bronze_step_creation\nERROR tests/system/test_dataframe_access.py::TestDataFrameAccess::test_silver_step_creation\nERROR tests/system/test_dataframe_access.py::TestDataFrameAccess::test_gold_step_creation\nERROR tests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_builder_validation\nERROR tests/system/test_dataframe_access.py::TestDataFrameAccess::test_pipeline_creation\nERROR tests/system/test_dataframe_access.py::TestDataFrameAccess::test_dataframe_operations\nERROR tests/system/test_dataframe_access.py::TestDataFrameAccess::test_step_type_detection\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_source_silvers\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_source_silvers_explicit\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_auto_infer_gold_no_silver_steps_error\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_development\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_production\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_preset_configurations_testing\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_not_null_rules\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_positive_number_rules\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_string_not_empty_rules\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_validation_helper_timestamp_rules\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_detect_timestamp_columns\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_detect_timestamp_columns_list\nERROR tests/system/test_improved_user_experience.py::TestImprovedUserExperience::test_chaining_with_auto_inference\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_bronze_rules_with_schema\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_bronze_rules_without_schema\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_silver_rules_with_schema\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_silver_transform_with_schema\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_gold_transform_with_schema\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_success\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_failure\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_get_effective_schema\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_creation\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_creation_failure\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_cross_schema_pipeline\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_mixed_schema_usage\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_schema_validation_integration\nERROR tests/system/test_multi_schema_support.py::TestMultiSchemaSupport::test_backward_compatibility\nERROR tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_dataframe_operations\nERROR tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_transformations\nERROR tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_validation_rules\nERROR tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_data_quality\nERROR tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_metadata_operations\nERROR tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_error_handling\nERROR tests/system/test_simple_real_spark.py::TestRealSparkOperations::test_real_spark_schema_operations\nERROR tests/system/test_utils.py::TestDataValidation::test_and_all_rules - At...\nERROR tests/system/test_utils.py::TestDataValidation::test_and_all_rules_empty\nERROR tests/system/test_utils.py::TestDataValidation::test_apply_column_rules\nERROR tests/system/test_utils.py::TestDataValidation::test_apply_column_rules_none_rules\nERROR tests/system/test_utils.py::TestDataValidation::test_assess_data_quality\nERROR tests/system/test_utils.py::TestDataValidation::test_get_dataframe_info\nERROR tests/system/test_utils.py::TestDataValidation::test_validate_dataframe_schema\nERROR tests/system/test_utils.py::TestDataTransformationUtilities::test_basic_dataframe_operations\nERROR tests/system/test_utils.py::TestDataTransformationUtilities::test_dataframe_filtering\nERROR tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_success\nERROR tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_with_options\nERROR tests/unit/test_table_operations.py::TestWriteOverwriteTable::test_write_overwrite_nonexistent_database\nERROR tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_success\nERROR tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_with_options\nERROR tests/unit/test_table_operations.py::TestWriteAppendTable::test_write_append_nonexistent_database\nERROR tests/unit/test_table_operations.py::TestReadTable::test_read_existing_table\nERROR tests/unit/test_table_operations.py::TestReadTable::test_read_table_data_integrity\nERROR tests/unit/test_table_operations.py::TestTableExists::test_existing_table\nERROR tests/unit/test_table_operations.py::TestDropTable::test_drop_existing_table\nERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_valid_schema\nERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_missing_columns\nERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_extra_columns\nERROR tests/unit/test_validation.py::TestValidateDataframeSchema::test_empty_expected_columns\nERROR tests/unit/test_validation.py::TestGetDataframeInfo::test_basic_info - ...\nERROR tests/unit/test_validation.py::TestApplyColumnRules::test_basic_validation\nERROR tests/unit/test_validation.py::TestApplyColumnRules::test_none_rules_raises_error\nERROR tests/unit/test_validation.py::TestApplyColumnRules::test_empty_rules\nERROR tests/unit/test_validation.py::TestApplyColumnRules::test_complex_rules\nERROR tests/unit/test_validation.py::TestAssessDataQuality::test_basic_data_quality_assessment\nERROR tests/unit/test_validation.py::TestAssessDataQuality::test_data_quality_with_rules\nERROR tests/unit/test_validation.py::TestApplyValidationRules::test_apply_validation_rules_basic\nERROR tests/unit/test_validation.py::TestApplyValidationRules::test_apply_validation_rules_empty\n====== 86 failed, 715 passed, 38 warnings, 79 errors in 101.16s (0:01:41) ======\n"
  }
}